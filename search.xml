<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Butterfly主题美化之路</title>
    <url>/posts/23d5be7c/</url>
    <content><![CDATA[
Butterfly主题美化之路
以下内容摘自Butterfly - A Simple and Card UI Design theme for Hexo
安装主题
在hexo根目录下执行
gitee（适合中国大陆）
git clone -b master https://gitee.com/immyw/hexo-theme-butterfly.git themes/butterfly
github（建议先用这个）
git clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly

升级方法：在主题目录下，运行git pull

也可以用npm方法安装
npm install hexo-theme-butterfly

升级方法：在 Hexo 根目录下，运行 npm update hexo-theme-butterfly

应用主题
在hexo根目录写下的_config.yml有个theme：修改为以下
theme: butterfly
如果你没有 pug 以及 stylus 的渲染器，请下载安装：
npm install hexo-renderer-pug hexo-renderer-stylus hexo-butterfly-extjs --save 
Front-matter
Front-matter 是 markdown 文件最上方以 — 分隔的区域，用于指定个别档案的变数。
Page Front-matter 用于 配置页面
Post Front-matter 用于 配置文章页

如果标注的参数，可根据自己需要添加，不用全部都写在 markdown 里 可选


Page Front-matter

---title:date:updated:type:comments:description:keywords:top_img:mathjax:katex:aside:aplayer:highlight_shrink:random:limit:  type:  value:---



参数
解释




title
【必需】页面标题


date
【必需】页面创建日期


type
【必需】标签、分类和友情链接三个页面需要配置


updated
【可选】页面更新日期


description
【可选】页面描述


keywords
【可选】页面关键词


comments
【可选】显示页面评论模块 （默认 true）


top_img
【可选】页面顶部图片


mathjax
【可选】显示 mathjax （当设置 mathjax 的 per_page： false 时，才需要配置，默认 false）


katex
【可选】显示 katex （当设置 katex 的 per_page： false 时，才需要配置，默认 false）


aside
【可选】显示侧边栏 （默认 true）


aplayer
【可选】在需要的页面加载 aplayer 的 js 和 css，请参考文章下面的 配置音樂


highlight_shrink
【可选】配置代码框是否展开 （true/false） （默认为设置中 highlight_shrink 的配置）


random
【可选】配置友情链接是否随机排序（默认为 false）


limit
【可选】配置说显示数量


limit.type
【可选】配置说显示数量的类型 （date 或者 num）


limit.value
【可选】配置说显示数量的值




Post Front-matter

---title:date:updated:tags:categories:keywords:description:top_img:comments:cover:toc:toc_number:toc_style_simple:copyright:copyright_author:copyright_author_href:copyright_url:copyright_info:mathjax:katex:aplayer:highlight_shrink:aside:abcjs:---



写法
解释




title
【必需】文章标题


date
【必需】页面创建日期


updated
【可选】页面更新日期


description
【可选】页面描述


tags
【可选】文章标签


keywords
【可选】页面关键词


top_img
【可选】页面顶部图片


categories
【可选】文章分类


cover
【可选】文章缩略图（如果没有设置 top_img，文章页顶部将显示缩略图，可设为 false/图片地址/留空）


comments
【可选】显示文章评论模块（默认 true）


toc
【可选】显示文章 TOC（默认为设置中 toc 的 enable 配置）


toc_number
【可选】显示 toc_number（默认为设置中 toc 的 number 配置）


toc_style_simple
【可选】显示 toc 简洁模式


copyright
【可选】显示文章版权模块（默认为设置中 post_copyright 的 enable 配置）


copyright_author
【可选】文章版权模块的文章作者


copyright_author_href
【可选】文章版权模块的链接文章作者


copyright_url
【可选】文章版权模块的链接文章链接


copyright_info
【可选】文章版权模块的文字版权声明


mathjax
【可选】显示 mathjax（当设置 mathjax 的 per_page： false 时，才需要配置，默认 false ）


katex
【可选】显示 katex （当设置 katex 的 per_page： false 时，才需要配置，默认 false ）


aplayer
【可选】在需要的页面加载 aplayer 的 js 和 css，请参考文章下面的 配置音乐


highlight_shrink
【可选】配置代码框是否展开（true/false）（默认为设置中 highlight_shrink 的配置）


aside
【可选】显示侧边栏 （默认 true）


abcjs
【可选】加载 abcjs （当设置 abcjs 的 per_page： false 时，才需要配置，默认 false ）



标签页

标签页文件名不一定是 tags， 例子中的 tags 只是一个示例
记得添加 type: &quot;tags&quot;



前往你的 Hexo 的根目录


输入 hexo new page tags


你会找到 这个文件source/tags/index.md


修改此文档 ：


---title: 标签date: 2018-01-05 00:00:00type: &quot;tags&quot;orderby: randomorder: 1---




参数
解释




type
【必须】页面类型，必须为 tags


orderby
【可选】排序方式 ：random - 随机排序 / name - 标签名字排序 / length - 标签数量排序


order
【可选】排序次序： 1（升序），-1（降序）



分类页

分类页文件名不一定是 categories， 例子中的 categories 只是一个示例
记得添加 type: &quot;categories&quot;


前往你的 Hexo 的根目录
输入 hexo new page categories
你会找到 这个文件source/categories/index.md
修改此文档 ：

---title: 分类date: 2024-11-05 12:44:54type: &quot;categories&quot;---

友情链接

友情链接页文件名不一定是 link， 例子中的 link 只是一个示例
记得添加 type: &quot;link&quot;


前往你的 Hexo 的根目录
输入 hexo new page link
你会找到 这个文件source/link/index.md
修改此文档 ：

---title: 友情链接date: 2024-11-05 12:47:52type: &quot;link&quot;---
在 Hexo 根目录中的 （如果没有 _data 文件夹，请自行创建），创建一个文件 source/_data/link.yml
- class_name: 友情链接  class_desc: 那些人，那些事  link_list:    - name: Hexo      link: https://hexo.io/zh-tw/      avatar: https://d33wubrfki0l68.cloudfront.net/6657ba50e702d84afb32fe846bed54fba1a77add/827ae/logo.svg      descr: 快速、簡單且強大的網誌框架- class_name: 网站  class_desc: 值得推荐的网站  link_list:    - name: 严千屹      link: https://blog.qianyios.top/      avatar: https://blog.qianyios.top/img/fluid.png      descr: 博客
404页面
# A simple 404 pageerror_404:  enable: false  subtitle: &#x27;页面不存在&#x27;  background: /img/error-page.png
配置文件速读
# --------------------------------------# Hexo Butterfly Theme Configuration# If you have any questions, please refer to the documentation# Chinese: https://butterfly.js.org/# English: https://butterfly.js.org/en/# --------------------------------------# --------------------------------------# Navigation Settings# --------------------------------------nav:  # 导航栏 Logo 图片  logo:  # 是否显示标题  display_title: true  # 是否固定导航栏  fixed: falsemenu:  # 首页: / || fas fa-home  # 列表||fas fa-list:  #   音乐: /music/ || fas fa-music  #   电影: /movies/ || fas fa-video  首页: / || fas fa-home  标签: /tags/ || fas fa-tags  分类: /categories/ || fas fa-folder-open  友链: /link/ || fas fa-link  列表 || fas fa-list || hide:    音乐: /music/ || fas fa-music    电影: /movies/ || fas fa-video# --------------------------------------# Code Blocks Settings# --------------------------------------code_blocks:  # 代码块主题: darker / pale night / light / ocean / false  theme: light  # 是否使用 Mac 风格  macStyle: true  # 代码块高度限制（单位: px）  height_limit: 200  # 是否自动换行  word_wrap: false  # 工具栏  # 是否显示复制按钮  copy: true  # 是否显示语言标籤  language: true  # true: 收缩代码块 | false: 展开代码块 | none: 展开代码块并隐藏按钮  shrink: false  # 是否显示全屏显示代码块按钮  fullpage: false# 社交媒体链接# 格式:#   icon: 链接 || 描述 || 颜色social:  # fab fa-github: https://github.com/xxxxx || Github || &#x27;#24292e&#x27;  # fas fa-envelope: mailto:xxxxxx@gmail.com || Email || &#x27;#4a7dbe&#x27;# --------------------------------------# 图片设置# --------------------------------------# 网站的 favicon 图标favicon: https://blog.qianyios.top/img/fluid.png# 头像设置avatar:# 头像图片链接  img: https://blog.qianyios.top/img/fluid.png  # 是否启用头像效果  effect: true# 禁用所有横幅图片disable_top_img: true# 如果页面未设置横幅，则显示默认的横幅图片default_top_img:# 主页的横幅图片index_img:# 归档页的横幅图片archive_img:# 注意: 是标籤页（单个标籤），不是标籤页面（所有标籤）tag_img:# 标籤页的横幅图片，可以为每个标籤设置横幅图片# 格式:#  - 标籤名: 图片链接tag_per_img:# 注意: 是分类页（单个分类），不是分类页面（所有分类）category_img:# 分类页的横幅图片，可以为每个分类设置横幅图片# 格式:#  - 分类名: 图片链接# Format:#  - category name: xxxxxcategory_per_img:# 页脚的背景图片footer_img: false# 网站背景# 可以设置为颜色或图片# 图片格式: url(http://xxxxxx.com/xxx.jpg)background:# 封面设置cover:  # 是否禁用封面  index_enable: true  aside_enable: true  archives_enable: true  # 当未设置封面时，显示默认封面  default_cover:    # - xxx.jpg# 替换损坏的图片error_img:# 友链页面的错误图片  flink: /img/friend_404.gif# 文章页面的错误图片  post_page: /img/404.jpg# 简单的 404 页面error_404:  # 是否启用 404 页面  enable: false  # 404 页面的副标题  subtitle: &#x27;页面不存在！&#x27;  # 404 页面的卡片背景图片  background: /img/error-page.png# 文章元数据设置post_meta:  # 主页页面  page:    # 日期类型: created / updated / both    date_type: created    # 日期格式: date / relative    date_format: date    # 是否显示分类    categories: true    # 是否显示标籤    tags: false    # 是否显示文字标籤    label: true  # 文章页面  post:    # 元数据位置: left / center    position: left    # 日期类型: created / updated / both    date_type: both    # 日期格式: date / relative    date_format: date    categories: true    tags: true    label: true# --------------------------------------# 首页设置# --------------------------------------# 首页头图的设置# 默认: 头图全屏，站点信息在中间# 站点信息的位置，例如: 300px/300em/300rem/10%index_site_info_top:# 头图的高度，例如: 300px/300em/300remindex_top_img_height:# 首页的副标题设置subtitle:  # 是否启用副标题  enable: false  # 是否启用打字机效果  effect: true  # 自定义 typed.js  # https://github.com/mattboldt/typed.js/#customization  typed_option:  # 来源 - 调用第三方服务 API（仅限中文）  # 它将首先显示来源，然后显示副标题内容  # 选择: false/1/2/3  # false - 禁用此功能  # 1 - hitokoto.cn  # 2 - yijuzhan.com  # 3 - jinrishici.com  source: false  # 如果关闭打字机效果，副标题将仅显示 sub 的第一行内容  sub:#首页上的文章布局# 1：封面在左边，信息在右边# 2：封面在右边，信息在左边# 3：封面和信息在左右交替# 4：封面在上面，信息在下面# 5：信息显示在封面上# 6：砌体布局——覆盖在顶部，信息在底部# 7：砌体布局-信息显示在封面上index_layout: 3# 在首页显示文章简介# 1: 描述# 2: 两者（如果存在描述，将显示描述，否则显示自动摘要）# 3: 自动摘要（默认）# false: 不显示文章简介index_post_content:  method: 1  # If you set method to 2 or 3, the length need to config  length: 500# --------------------------------------# 文章设置# --------------------------------------toc:# 是否在文章中显示目录  post: true# 是否在文章中显示目录  page: false# 是否显示目录编号  number: true# 是否默认展开目录  expand: false  # 是否使用简洁风格（仅适用于文章）  style_simple: false    # 是否显示滚动百分比  scroll_percent: truepost_copyright:# 是否启用版权声明  enable: true  # 是否进行文章 URL 解码  decode: false  # 作者链接  author_href: https://blog.qianyios.top  license: CC BY-NC-SA 4.0  license_url: https://creativecommons.org/licenses/by-nc-sa/4.0/# 赞助/打赏reward:# 是否启用打赏  enable: false  # 打赏案例文本  text:  QR_code:    # - img: /img/wechat.jpg    #   link:    #   text: wechat    # - img: /img/alipay.jpg    #   link:    #   text: alipay# 文章编辑# 轻鬆在线浏览和编辑博客源代码post_edit:# 是否启用在线编辑  enable: false  # url: https://github.com/user-name/repo-name/edit/branch-name/subdirectory-name/  # For example: https://github.com/jerryc127/butterfly.js.org/edit/main/source/  url:# 相关文章related_post:# 是否显示相关文章  enable: false  # 显示的文章数量  limit: 6  # Choose: created / updated  date_type: created# 选择: 1 / 2 / false# 1: “下一篇文章”将链接到旧文章# 2: “下一篇文章”将链接到新文章# false: 禁用分页post_pagination: 1# 显示文章过期通知noticeOutdate:  # 是否启用过期通知  enable: true  # Style: simple / flat  style: flat  # 多少天后显示通知  limit_day: 365  # Position: top / bottom  position: top  message_prev: 已经过了  message_next: 天自上次更新，文章内容可能已过时。# --------------------------------------# 页脚设置# --------------------------------------footer:  owner:    enable: true    since: 2024  # 自定义文本  custom_text: 严千屹博客  # Copyright of theme and framework  copyright: true# --------------------------------------# 侧边栏设置# --------------------------------------aside:# 是否启用侧边栏  enable: true  # 是否默认隐藏侧边栏  hide: false  # 是否在右下角显示隐藏侧边栏的按钮  button: true  # 移动设备上是否启用侧边栏  mobile: true   # 侧边栏位置：left / right  position: right  display:    # 归档页面是否显示侧边栏    archive: true    # 标籤页面是否显示侧边栏    tag: true    # 分类页面是否显示侧边栏    category: true  card_author:  # 是否显示作者信息卡片    enable: true    # 作者描述    description:    button:    # 是否显示按钮      enable: false      icon: fab fa-github      text: Follow Me      link: https://github.com/xxxxxx  card_announcement:  # 是否显示公告卡片    enable: true    # 公告内容    content: 鄙人初耕运维门道，华为云计算hcip证书获得者，往后深耕Linux，docker，k8s运维。  card_recent_post:  # 是否显示最近文章卡片    enable: true    # 显示文章数量，0 表示显示所有    limit: 5      # 排序方式：date / updated    sort: date    sort_order:  card_newest_comments:  # 是否显示最新评论卡片    enable: false    sort_order:    # 显示评论数量    limit: 6     # 单位：分钟，保存数据到 localStorage    storage: 10     # 是否显示头像    avatar: true  card_categories:   # 是否显示分类卡片    enable: true    # 显示分类数量，0 表示显示所有    limit: 8    # Choose: none / true / false    expand: none    sort_order:  card_tags:  # 是否显示标籤卡片    enable: true    # 显示标籤数量，0 表示显示所有    limit: 40    # 是否启用颜色    color: false    # 标籤排序方式：random/name/length    orderby: random    # 排序顺序：1 表示升序，-1 表示降序    order: 1    sort_order:  card_archives:      # 是否显示归档卡片    enable: true    # 归档类型：monthly / yearly    type: monthly    # 日期格式，例如：YYYY年MM月    format: MMMM YYYY    # 排序顺序：1 表示升序，-1 表示降序    order: -1    # 显示归档数量，0 表示显示所有    limit: 8    sort_order:  card_post_series:  # 是否显示系列文章卡片    enable: true    # 标题显示系列名称    series_title: false    # 排序方式：title 或 date    orderBy: &#x27;date&#x27;    # 排序顺序：1 表示升序，-1 表示降序    order: -1  card_webinfo:  # 是否显示网站信息卡片    enable: true    # 是否显示文章数量    post_count: true    # 是否显示最后推送日期    last_push_date: true    sort_order:    # 发佈日期与当前日期的时间差    # 格式：Month/Day/Year Time 或 Year/Month/Day Time    # 如果不启用此功能，请留空    runtime_date:# --------------------------------------# 右下角按钮设置# --------------------------------------# 右下角按钮与底部的距离（默认单位：px）rightside_bottom:# 简繁转换设置translate:# 是否启用简繁转换  enable: false  # 按钮文本  default: 繁  # 网站语言（1 - 繁体中文 / 2 - 简体中文）  defaultEncoding: 2  # 转换延迟  translateDelay: 0  # 按钮在简体中文时的文本  msgToTraditionalChinese: &#x27;繁&#x27;  # 按钮在繁体中文时的文本  msgToSimplifiedChinese: &#x27;簡&#x27;#閲读模式readmode: true# 暗黑模式设置darkmode:# 是否启用暗黑模式  enable: true  # 切换暗黑/明亮模式的按钮  button: true  # 是否自动切换暗黑/明亮模式  # autoChangeMode: 1  跟随系统设置，如果系统不支持暗黑模式，则在晚上 6 点到早上 6 点之间切换暗黑模式  # autoChangeMode: 2  在晚上 6 点到早上 6 点之间切换暗黑模式  # autoChangeMode: false  不自动切换  autoChangeMode: false  # 设置明亮模式时间，值在 0 到 24 之间。如果未设置，默认值为 6 和 18   start:  end:# 在返回顶部按钮中显示滚动百分比rightside_scroll_percent: false# 不要修改以下设置，除非你知道它们的工作原理# 选择：readmode,translate,darkmode,hideAside,toc,chat,comment# 不要重複相同的值rightside_item_order:# 是否启用右侧项目顺序  enable: false  # 隐藏的默认项目：readmode,translate,darkmode,hideAside  hide:  # 显示的默认项目：toc,chat,comment  show:# --------------------------------------# 全局设置# --------------------------------------# 锚点设置anchor:  # 滚动时，URL 将根据标题 ID 更新  auto_update: false # 点击标题滚动并更新锚点  click_to_scroll: false# 图片标题photofigcaption: false# 複制设置copy:# 是否启用複制功能  enable: true  # 在複制的内容后添加版权信息  copyright:    enable: true    # 当複制字符数超过 limit_count 时添加版权信息    limit_count: 150# 需要安装 hexo-wordcount 插件wordcount:# 是否启用字数统计  enable: true  # 在文章元信息中显示字数统计  post_wordcount: true  # 在文章元信息中显示閲读时间  min2read: true  # 在侧边栏网站信息中显示总字数  total_wordcount: true# 不蒜子 PV / UV 统计busuanzi:  site_uv: true  site_pv: true  page_pv: true# --------------------------------------# 数学公式设置# --------------------------------------# 关于 per_page# 如果设置为 true，将在每个页面加载 mathjax/katex 脚本# 如果设置为 false，将根据你的设置加载 mathjax/katex 脚本（在页面的 front-matter 中添加 &#x27;mathjax: true&#x27; 或者 &#x27;katex: true&#x27;）math:  # 选择：mathjax, katex  # 如果不需要数学公式，保持为空  use:  per_page: true  hide_scrollbar: false  mathjax:    # 启用上下文菜单    enableMenu: true    # 选择：all / ams / none，这控制是否对公式编号以及如何编号    tags: none  katex:    # 启用複制 KaTeX 公式    copy_tex: false# --------------------------------------# 搜索设置# --------------------------------------search:  # 选择：algolia_search / local_search / docsearch  # 如果不需要搜索功能，保持为空  use:  placeholder:  # Algolia 搜索  algolia_search:    # 每页搜索结果数量    hitsPerPage: 6  # 本地搜索  local_search:    # 页面加载时预加载搜索数据    preload: false    # 每篇文章显示的顶部 n 个搜索结果，设置为 -1 显示所有结果    top_n_per_article: 1    # 将 HTML 字符串反转义为可读内容    unescape: false    CDN:  # Docsearch  # https://docsearch.algolia.com/  docsearch:    appId:    apiKey:    indexName:    option:# --------------------------------------# 分享系统# --------------------------------------share:  # 选择：sharejs / addtoany  # 如果不需要分享功能，保持为空  use: sharejs  # Share.js  # https://github.com/overtrue/share.js  sharejs:    sites: facebook,twitter,wechat,weibo,qq  # AddToAny  # https://www.addtoany.com/  addtoany:    item: facebook,twitter,wechat,sina_weibo,facebook_messenger,email,copy_link# --------------------------------------# 评论系统# --------------------------------------comments:  # 最多两个评论系统，第一个将作为默认显示  # 如果不需要评论功能，保持为空  # 选择：Disqus/Disqusjs/Livere/Gitalk/Valine/Waline/Utterances/Facebook Comments/Twikoo/Giscus/Remark42/Artalk  # 两个评论系统的格式：Disqus,Waline  use:  # 按钮旁边显示评论系统名称  text: true  # 懒加载：评论系统将在评论元素进入浏览器视口时加载  # 如果设置为 true，评论计数将无效  lazyload: false  # 在文章顶部图片中显示评论计数  count: false  # 在主页显示评论计数  card_post_count: false# Disqus# https://disqus.com/disqus:  shortname:  # For newest comments widget  apikey:# Alternative Disqus - Render comments with Disqus API# https://github.com/SukkaW/DisqusJSdisqusjs:  shortname:  apikey:  option:# Livere# https://www.livere.com/livere:  uid:# Gitalk# https://github.com/gitalk/gitalkgitalk:  client_id:  client_secret:  repo:  owner:  admin:  option:# Valine# https://valine.js.orgvaline:  appId:  appKey:  avatar: monsterid  # This configuration is suitable for domestic custom domain name users, overseas version will be automatically detected (no need to manually fill in)  serverURLs:  bg:  # Use Valine visitor count as the page view count  visitor: false  option:# Waline - A simple comment system with backend support fork from Valine# https://waline.js.org/waline:  serverURL:  bg:  # Use Waline pageview count as the page view count  pageview: false  option:# Utterances# https://utteranc.es/utterances:  repo:  # Issue Mapping: pathname/url/title/og:title  issue_term: pathname  # Theme: github-light/github-dark/github-dark-orange/icy-dark/dark-blue/photon-dark  light_theme: github-light  dark_theme: photon-dark  js:  option:# Facebook Comments Plugin# https://developers.facebook.com/docs/plugins/comments/facebook_comments:  app_id:  # optional  user_id:  pageSize: 10  # Choose: social / time / reverse_time  order_by: social  lang: en_US# Twikoo# https://github.com/imaegoo/twikootwikoo:  envId:  region:  # Use Twikoo visitor count as the page view count  visitor: false  option:# Giscus# https://giscus.app/giscus:  repo:  repo_id:  category_id:  light_theme: light  dark_theme: dark  js:  option:# Remark42# https://remark42.com/docs/configuration/frontend/remark42:  host:  siteId:  option:# Artalk# https://artalk.js.org/guide/frontend/config.htmlartalk:  server:  site:  # Use Artalk visitor count as the page view count  visitor: false  option:# --------------------------------------# 聊天服务配置# --------------------------------------chat:  # 聊天服务类型，可选值：chatra/tidio/crisp，如果不需要聊天功能则留空  use:   # 推荐使用聊天按钮，会在网站右下角创建一个按钮，并隐藏原始按钮  rightside_button: false  # 原始聊天按钮在向上滚动时显示，向下滚动时隐藏  button_hide_show: false# https://chatra.io/chatra:  id:# https://www.tidio.com/tidio:  public_key:# https://crisp.chat/en/crisp:  website_id:# --------------------------------------# 分析服务配置# --------------------------------------# https://tongji.baidu.com/web/welcome/loginbaidu_analytics:# https://analytics.google.com/analytics/web/google_analytics:# https://www.cloudflare.com/zh-tw/web-analytics/cloudflare_analytics:# https://clarity.microsoft.com/microsoft_clarity:# https://umami.is/umami_analytics:  enable: false  # For self-hosted setups, configure the hostname of the Umami instance  serverURL:  website_id:  option:  UV_PV:    site_uv: false    site_pv: false    page_pv: false    # Umami Cloud (API key) / self-hosted Umami (token)    token:# --------------------------------------# 广告配置# --------------------------------------# Google Adsensegoogle_adsense:  enable: false  auto_ads: true  js: https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js  client:  enable_page_level_ads: true# Insert ads manually# Leave it empty if you don&#x27;t need adsad:  # Insert ads in the index (every three posts)  index:  # Insert ads in aside  aside:  # Insert ads in the post (before pagination)  post:# --------------------------------------# 站点验证配置# --------------------------------------site_verification:  # - name: google-site-verification  #   content: xxxxxx  # - name: baidu-site-verification  #   content: xxxxxxx# --------------------------------------# 美化 / 效果# --------------------------------------# 主题颜色自定义# 注意：颜色值必须用双引号，如 &quot;#000&quot;，否则可能会导致错误！# 主题颜色配置# theme_color:#   是否启用主题颜色#   enable: true#   主颜色#   main: &quot;#49B1F5&quot;#   分页器颜色#   paginator: &quot;#00c4b6&quot;#   按钮悬停颜色#   button_hover: &quot;#FF7242&quot;#   文本选择颜色#   text_selection: &quot;#00c4b6&quot;#   链接颜色#   link_color: &quot;#99a9bf&quot;#   元数据颜色#   meta_color: &quot;#858585&quot;#   水平线颜色#   hr_color: &quot;#A4D8FA&quot;#   代码前景色#   code_foreground: &quot;#F47466&quot;#   代码背景色#   code_background: &quot;rgba(27, 31, 35, .05)&quot;#   目录颜色#   toc_color: &quot;#00c4b6&quot;#   引用块填充颜色#   blockquote_padding_color: &quot;#49b1f5&quot;#   引用块背景颜色#   blockquote_background_color: &quot;#49b1f5&quot;#   滚动条颜色#   scrollbar_color: &quot;#49b1f5&quot;#   浅色模式下的主题颜色#   meta_theme_color_light: &quot;ffffff&quot;#   深色模式下的主题颜色#   meta_theme_color_dark: &quot;#0d0d0d&quot;# 分类和标籤页面的用户界面设置# 选择：index - 与主页 UI 相同 / default - 与归档 UI 相同# 留空或设置为 indexcategory_ui:tag_ui:# Rounded corners for UI elementsrounded_corners_ui: true# 拉伸行使每行宽度相等text_align_justify: false# 为页眉和页脚添加遮罩mask:  header: true  footer: true# 加载动画preloader:  enable: false  # 资源  # 1. 全屏加载  # 2. 进度条  source: 1  # pace theme (see https://codebyzach.github.io/pace/)  pace_css_url:# 页面过渡效果enter_transitions: true# 默认显示模式 - light (默认) / darkdisplay_mode: light# 美化文章内容的配置beautify:  # 是否启用美化  enable: false  # 指定美化的范围 (site 或 post)  field: post  # 指定标题前缀图标，如 &#x27;\f0c1&#x27;  title-prefix-icon:  # 指定标题前缀图标的颜色，如 &#x27;#F47466&#x27;  title-prefix-icon-color:# 全局字体设置# 除非您知道它们的工作原理，否则不要修改以下设置font:  global_font_size:  code_font_size:  font_family:  code_font_family:# 网站标题和副标题的字体设置blog_title_font:  font_link:  font_family:# 分隔符图标的设置hr_icon:# 是否启用分隔符图标  enable: true  # Font Awesome 图标的 unicode 值，如 &#x27;\3423&#x27;  icon:  icon_top:# 打字机效果# https://github.com/disjukr/activate-power-modeactivate_power_mode:  # 是否启用打字机效果  enable: false  # 是否启用彩色效果  colorful: true  # 是否启用震动效果  shake: true  # 是否在移动设备上启用  mobile: false# 背景效果# --------------------------------------# canvas_ribbon# 参见: https://github.com/hustcc/ribbon.jscanvas_ribbon:  # 是否启用 canvas_ribbon  enable: false  # ribbon 的大小  size: 150  # ribbon 的不透明度 (0 ~ 1)  alpha: 0.6  zIndex: -1  # 是否点击更改颜色  click_to_change: false  # 是否在移动设备上启用  mobile: false# Fluttering Ribboncanvas_fluttering_ribbon:  enable: false  mobile: false# canvas_nest# https://github.com/hustcc/canvas-nest.jscanvas_nest:  # 是否启用 canvas_nest  enable: true  # 线条颜色，默认: &#x27;0,0,0&#x27;; RGB 值: (R,G,B).(注意: 使用 &#x27;,&#x27; 分隔.)  color: &#x27;0,0,255&#x27;  # 线条的不透明度 (0~1)  opacity: 0.7  # 背景的 z-index 属性  zIndex: -1  # 线条数量  count: 99  # 是否在移动设备上启用  mobile: false# 鼠标点击效果: 烟花fireworks:  # 是否启用烟花效果  enable: true  zIndex: 9999  # 是否在移动设备上启用  mobile: false# 鼠标点击效果: 心形符号click_heart:  # 是否启用心形符号效果  enable: false  # 是否在移动设备上启用  mobile: false# 鼠标点击效果: 文字clickShowText:  # 是否启用文字效果  enable: false  text:    # - I    # - LOVE    # - YOU  fontSize: 15px  # 是否随机显示文字  random: false  # 是否在移动设备上启用  mobile: false# --------------------------------------# Lightbox Settings# --------------------------------------# Choose: fancybox / medium_zoom# https://github.com/francoischalifour/medium-zoom# https://fancyapps.com/fancybox/# Leave it empty if you don&#x27;t need lightboxlightbox:# --------------------------------------# 标籤外挂设置# --------------------------------------# 系列series:# 是否启用系列  enable: false  # 按标题或日期排序  orderBy: &#x27;title&#x27;  # 排序顺序：1 表示升序，-1 表示降序  order: 1  number: true# ABCJS - ABC 音乐符号插件# https://github.com/paulrosen/abcjsabcjs:  enable: false  per_page: true# Mermaid# https://github.com/mermaid-js/mermaidmermaid:  enable: false  # Write Mermaid diagrams using code blocks  code_write: false  # built-in themes: default / forest / dark / neutral  theme:    light: default    dark: dark# chartjs# see https://www.chartjs.org/docs/latest/chartjs:  enable: false  # Do not modify unless you understand how they work.  # The default settings are only used when the MD syntax is not specified.  # General font color for the chart  fontColor:    light: &#x27;rgba(0, 0, 0, 0.8)&#x27;    dark: &#x27;rgba(255, 255, 255, 0.8)&#x27;  # General border color for the chart  borderColor:    light: &#x27;rgba(0, 0, 0, 0.1)&#x27;    dark: &#x27;rgba(255, 255, 255, 0.2)&#x27;  # Background color for scale labels on radar and polar area charts  scale_ticks_backdropColor:    light: &#x27;transparent&#x27;    dark: &#x27;transparent&#x27;# Note - Bootstrap Calloutnote:  # Note tag style values:  #  - simple    bs-callout old alert style. Default.  #  - modern    bs-callout new (v2-v3) alert style.  #  - flat      flat callout style with background, like on Mozilla or StackOverflow.  #  - disabled  disable all CSS styles import of note tag.  style: flat  icons: true  border_radius: 3  # Offset lighter of background in % for modern and flat styles (modern: -12 | 12; flat: -18 | 6).  # Offset also applied to label tag variables. This option can work with disabled note tag.  light_bg_offset: 0# --------------------------------------# 其他设置# --------------------------------------# https://github.com/MoOx/pjaxpjax:# 是否启用 pjax  enable: false # 排除指定页面不使用 pjax，如 &#x27;/music/&#x27;  exclude:    # - /xxxxxx/# 注入 CSS 和脚本 (aplayer/meting)aplayerInject:# 是否启用注入  enable: false  # 是否每页启用  per_page: true# Snackbar - Toast Notification# https://github.com/polonel/SnackBar# position: top-left / top-center / top-right / bottom-left / bottom-center / bottom-rightsnackbar:  enable: false  position: bottom-left  # The background color of Toast Notification in light mode and dark mode  bg_light: &#x27;#49b1f5&#x27;  bg_dark: &#x27;#1f1f1f&#x27;# Instant.page# https://instant.page/instantpage: false# Pangu - 在中文字符和英文字符之间插入空格# https://github.com/vinta/pangu.jspangu:  enable: false  # Specify the field to use pangu (site or post)  field: site# Lazyload# https://github.com/verlok/vanilla-lazyloadlazyload:  enable: false  # Specify the field to use lazyload (site or post)  field: site  placeholder:  blur: false# PWA# See https://github.com/JLHwung/hexo-offline# ---------------pwa:  enable: false  manifest:  apple_touch_icon:  favicon_32_32:  favicon_16_16:  mask_icon:# Open graph meta tags# https://hexo.io/docs/helpers#open-graphOpen_Graph_meta:  enable: true  option:    # twitter_card:    # twitter_image:    # twitter_id:    # twitter_site:    # google_plus:    # fb_admins:    # fb_app_id:# Add the vendor prefixes to ensure compatibilitycss_prefix: true# Inject# 插入代码到 head（在 &#x27;&lt;/head&gt;&#x27; 标籤之前）和底部（在 &#x27;&lt;/body&gt;&#x27; 标籤之前）inject:  head:    # - &lt;link rel=&quot;stylesheet&quot; href=&quot;/xxx.css&quot;&gt;    - &lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;/&gt;  bottom:    # - &lt;script src=&quot;xxxx&quot;&gt;&lt;/script&gt;# CDN 设置# 除非你知道它们的工作原理，否则不要修改以下设置CDN:  # 内部和第三方脚本的 CDN 提供商  # 两者的选项：local/jsdelivr/unpkg/cdnjs/custom  # 注意： Dev 版本只能使用 &#x27;local&#x27; 作为内部脚本  # 注意：将第三方脚本设置为 &#x27;local&#x27; 时，需要安装 hexo-butterfly-extjs  internal_provider: local  third_party_provider: local  # 是否在 URL 中添加版本号，true 或 false  version: false  # 自定义格式  # 例如：https://cdn.staticfile.org/$&#123;cdnjs_name&#125;/$&#123;version&#125;/$&#123;min_cdnjs_file&#125;  custom_format:  option:    # abcjs_basic_js:    # activate_power_mode:    # algolia_js:    # algolia_search:    # aplayer_css:    # aplayer_js:    # artalk_css:    # artalk_js:    # blueimp_md5:    # busuanzi:    # canvas_fluttering_ribbon:    # canvas_nest:    # canvas_ribbon:    # chartjs:    # click_heart:    # clickShowText:    # disqusjs:    # disqusjs_css:    # docsearch_css:    # docsearch_js:    # egjs_infinitegrid:    # fancybox:    # fancybox_css:    # fireworks:    # fontawesome:    # gitalk:    # gitalk_css:    # giscus:    # instantpage:    # instantsearch:    # katex:    # katex_copytex:    # lazyload:    # local_search:    # main:    # main_css:    # mathjax:    # medium_zoom:    # mermaid:    # meting_js:    # pangu:    # prismjs_autoloader:    # prismjs_js:    # prismjs_lineNumber_js:    # pjax:    # sharejs:    # sharejs_css:    # snackbar:    # snackbar_css:    # translate:    # twikoo:    # typed:    # utils:    # valine:    # waline_css:    # waline_js:
语言
修改 Hexo 根目录下的配置文件 _config.yml
默认语言是 en
主题支持

default(en)
zh-CN （简体中文）
zh-TW （台湾繁体中文）
zh-HK （香港繁体中文）
ja （日语）
ko（韩语）


导航
参数设置
nav:  # 导航栏 Logo 图片  logo:  # 是否显示标题  display_title: true  # 是否固定导航栏  fixed: false
目录
导航的文字可自行更改：
格式：名字: 路径 || 图标名
例如：
menu:首页: / || fas fa-home时间轴: /archives/ || fas fa-archive标签: /tags/ || fas fa-tags分类: /categories/ || fas fa-folder-open清单||fa fa-heartbeat:照片: /Gallery/ || fas fa-images友链: /link/ || fas fa-link关于: /about/ || fas fa-heart列表||fas fa-list:  音乐: /music/ || fas fa-music  电影: /movies/ || fas fa-video
默认子目录是展开的，如果你想要隐藏，在子目录里添加hide:
列表 || fas fa-list || hide:  音乐: /music/ || fas fa-music  电影: /movies/ || fas fa-video
我的目录：
menu:  首页: / || fas fa-home  标签: /tags/ || fas fa-tags  分类: /categories/ || fas fa-folder-open  友链: /link/ || fas fa-link  列表 || fas fa-list || hide:    音乐: /music/ || fas fa-music    电影: /movies/ || fas fa-video
代码高亮
Highlight.js 是用 JavaScript 编写的语法高亮工具。它适用于 浏览器以及服务器上。它几乎可以与任何 标记，不依赖于任何其他框架，并且具有自动语言 检波。
#卸载默认的高亮插件npm uninstall  hexo-prism-plugin通过 NPM 包安装Highlight.js npm install highlight.jshexo clean &amp;&amp; hexo g &amp;&amp; hexo s
评论系统
我用的是twikoo（vercel部署）免费
具体配置可参考 云函数部署 | Twikoo 文档
twikoo:  envId: https://填写vercel部署好的域名，建议自定义域名  region:  # Use Twikoo visitor count as the page view count  visitor: false  option:
文章加密
npm install --save hexo-blog-encrypt
在文章开头添加passwd
---title: Hello Worldpassword: hello#密码就是hellowrong_pass_message: &#x27;抱歉, 这个密码看着不太对, 请再试试.&#x27;wrong_hash_message: &#x27;抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.&#x27;message: 前往关于页联系本人---
能否成功加密有个前提条件，访问网站的协议要是https，所以你要给你的网站添加ssl证书即可，没有添加的即使输入密码也没有效果，解锁不了的

文章自动编号
我是自己加了一个hexo-theme-butterfly\source\css\my.css添加了以下内容。
/*文章编号*//* 让 h1 触发重置 h2-h6 的计数 */.post-content h1 &#123;    counter-reset: main-section !important;  /* 关键：确保每个 h1 重新开始 h2 计数 */&#125;/* h2 作为主要编号（1. 2. 3.）*/.post-content h2 &#123;    counter-increment: main-section !important;    counter-reset: sub-section !important;&#125;.post-content h2::before &#123;    content: counter(main-section) &quot;. &quot; !important;&#125;/* h3 作为次级编号（1.1 1.2 2.1 2.2） */.post-content h3 &#123;    counter-increment: sub-section !important;    counter-reset: sub-sub-section !important;&#125;.post-content h3::before &#123;    content: counter(main-section) &quot;.&quot; counter(sub-section) &quot;. &quot; !important;&#125;/* h4 三级编号（1.1.1 2.1.1） */.post-content h4 &#123;    counter-increment: sub-sub-section !important;    counter-reset: sub-sub-sub-section !important;&#125;.post-content h4::before &#123;    content: counter(main-section) &quot;.&quot; counter(sub-section) &quot;.&quot; counter(sub-sub-section) &quot;. &quot; !important;&#125;/* h5 四级编号（1.1.1.1 2.1.1.1） */.post-content h5 &#123;    counter-increment: sub-sub-sub-section !important;    counter-reset: sub-sub-sub-sub-section !important;&#125;.post-content h5::before &#123;    content: counter(main-section) &quot;.&quot; counter(sub-section) &quot;.&quot; counter(sub-sub-section) &quot;.&quot; counter(sub-sub-sub-section) &quot;. &quot; !important;&#125;/* h6 五级编号（1.1.1.1.1 2.1.1.1.1） */.post-content h6 &#123;    counter-increment: sub-sub-sub-sub-section !important;&#125;.post-content h6::before &#123;    content: counter(main-section) &quot;.&quot; counter(sub-section) &quot;.&quot; counter(sub-sub-section) &quot;.&quot; counter(sub-sub-sub-section) &quot;.&quot; counter(sub-sub-sub-sub-section) &quot;. &quot; !important;&#125;/* 设置标题之间的间距 */.post-content h2 &#123; margin: 40px 0; &#125;.post-content h3 &#123; margin: 30px 0; &#125;.post-content h4 &#123; margin: 20px 0; &#125;.post-content h5 &#123; margin: 15px 0; &#125;.post-content h6 &#123; margin: 10px 0; &#125;/*侧边栏开始编号*//* 初始化 toc-level-2 的计数器 */.toc &#123;    counter-reset: toc-section;&#125;/* 让 toc-level-2 作为编号的起点 */.toc .toc-level-2 &#123;    counter-reset: toc-subsection; /* 每个 toc-level-2 开始新的子计数 */    counter-increment: toc-section; /* 增加一级计数 */&#125;/* 在链接前面添加编号 */.toc .toc-level-2 &gt; a::before &#123;    content: counter(toc-section) &quot;.&quot;;    font-weight: bold;    margin-right: 5px;&#125;/* toc-level-3 开始二级编号 */.toc .toc-level-3 &#123;    counter-reset: toc-subsubsection;    counter-increment: toc-subsection;&#125;.toc .toc-level-3 &gt; a::before &#123;    content: counter(toc-section) &quot;.&quot; counter(toc-subsection) &quot;.&quot;;    font-weight: bold;    margin-right: 5px;&#125;/* toc-level-4 开始三级编号 */.toc .toc-level-4 &#123;    counter-increment: toc-subsubsection;&#125;.toc .toc-level-4 &gt; a::before &#123;    content: counter(toc-section) &quot;.&quot; counter(toc-subsection) &quot;.&quot; counter(toc-subsubsection) &quot;.&quot;;    font-weight: bold;    margin-right: 5px;&#125;/* 隐藏主题自带的编号 */.toc .toc-number &#123;    display: none;&#125;span.toc-text &#123;    margin-left: -10px;&#125;
效果图

支持KaTeX数学公式和下标
上下标支持：
Hexo 需要使用 Markdown 渲染引擎将 md 文件渲染成 html 文件，Hexo 默认使用 hexo-renderer-marked，可以换成 hexo-renderer-markdown-it。
hexo-renderer-markdown-it 拥有更好的性能，而且可以通过插件扩展功能，如：上标、下标、引用注脚、emoji、KaTex 公式、多维表格等等。
先卸载 Hexo 默认引擎
npm un hexo-renderer-marked --save
安装 markdown-it 引擎
npm i hexo-renderer-markdown-it --save
在站点配置文件 _config.yml 中增加以下配置
markdown:  markdown:  preset: &#x27;default&#x27;  render:    html: true    xhtmlOut: false    langPrefix: &#x27;language-&#x27;    breaks: true    linkify: true    typographer: true    quotes: &#x27;“”‘’&#x27;  enable_rules:  disable_rules:  plugins:    - &#x27;markdown-it-footnote&#x27;    - &#x27;markdown-it-ins&#x27;    - &#x27;markdown-it-mark&#x27;    - &#x27;markdown-it-sub&#x27;    - &#x27;markdown-it-sup&#x27;  images:    lazyload: false    prepend_root: false    post_asset: true
配置中的 typographer: true 的作用是显示特殊格式字符写法：(c) (C) (r) (R) (tm) (TM) (p) (P) +-效果：© © ® ® ™ ™ (p) (P) ±
其他用法，可以参考 markdown-it 的官方效果演示 Demo
数学公式
npm i @traptitech/markdown-it-katex --save
在站点配置文件 _config.yml 中增加以下配置
markdown_it_katex:  plugins:  - plugin:    name: &#x27;@traptitech/markdown-it-katex&#x27;    options: # see https://katex.org/docs/options.html      blockClass: &quot;math-block&quot;      strict: false      throwOnError: false      errorColor: &quot;#cc0000&quot;


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Butterfly</tag>
      </tags>
  </entry>
  <entry>
    <title>Centos 8部署Docker</title>
    <url>/posts/51253/</url>
    <content><![CDATA[Centos 8部署Docker
基础信息



系统
ip  (NAT)
内存
硬盘




Centos 8.5
192.168.48.10
2G
40G



基本配置好可以访问互联网即可
切换阿里yum
mkdir repo.bakcp /etc/yum.repos.d/* repo.bak/rename &#x27;.repo&#x27; &#x27;.repo.bak&#x27; /etc/yum.repos.d/*.repowget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repoyum clean all &amp;&amp; yum makecachesystemctl disable --now firewalldsed -i &#x27;s/^SELINUX=.*/SELINUX=disabled/&#x27; /etc/selinux/config
安装Docker
删除旧版docker
sudo yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine -y
安装依赖
yum install -y yum-utils device-mapper-persistent-data lvm2
添加阿里docker存储库
dnf config-manager --add-repo=https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum clean all &amp;&amp; yum makecache 
安装docker软件
yum install -y --allowerasing docker-ce docker-ce-cli
启动docker服务
systemctl enable docker --now
验证是否安装成功
[root@Docker ~]# docker run hello-world#未检测到hello-world镜像Unable to find image &#x27;hello-world:latest&#x27; locally#从远程的DockerHub仓库拉取镜像latest: Pulling from library/hello-world2db29710123e: Pull completeDigest: sha256:aa0cc8055b82dc2509bed2e19b275c8f463506616377219d9642221ab53cf9feStatus: Downloaded newer image for hello-world:latest#出现下面说明docker运行成功Hello from Docker!  -------This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub.    (amd64) 3. The Docker daemon created a new container from that image which runs the    executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it    to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/
安装docker compose
# 设置 Docker 镜像加速器sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123;  &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker# 安装 Docker Composesudo curl -L &quot;https://get.daocloud.io/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-composesudo systemctl enable docker --nowsudo systemctl restart dockersudo usermod -aG docker $USER
使用Docker创建新的镜像
检索Docker仓库中的Ubuntu镜像
[root@Docker ~]# docker search ubuntuNAME                             DESCRIPTION                                     STARS     OFFICIAL   AUTOMATEDubuntu                           Ubuntu is a Debian-based Linux operating sys…   15464     [OK]     ......
获取Ubuntu镜像
[root@Docker ~]# docker pull ubuntuUsing default tag: latestlatest: Pulling from library/ubuntu6e3729cf69e0: Pull completeDigest: sha256:27cb6e6ccef575a4698b66f5de06c7ecd61589132d5a91d098f7f3f9285415a9Status: Downloaded newer image for ubuntu:latestdocker.io/library/ubuntu:latest
查看本地Docker镜像
[root@Docker ~]# docker imagesREPOSITORY    TAG       IMAGE ID       CREATED         SIZEubuntu        latest    6b7dfa7e8fdb   5 weeks ago     77.8MBhello-world   latest    feb5d9fea6a5   16 months ago   13.3kB

Docker镜像列表选项字段说明如下。REPOSITORY  镜像的名称TAG   		    镜像的标签IMAGE ID      镜像的IDCREATED       镜像创建时间SIZE             镜像大小

启动docker容器

使用docker run 命令可启动容器，Docker 启动时其执行过程如下。· 检查本地是否存在指定的Docker 镜像，如果不存在将从Docker仓库下载。· 使用Docker 镜像创建并启动Docker 容器。· 为Docker容器分配一个文件系统，并在只读的镜像层外面挂载一层可读写层。· 从Docker宿主机中配置的网桥接口中桥接一个虚拟接口到容器。· 从Docker 网络的地址池中分配一个IP地址给当前容器。· 执行用户指定的程序。` 执行完毕后终止容器。

创建基于本地ubuntu镜像的docker容器[root@Docker ~]# docker run -it ubuntu /bin/bashroot@136156e4c448:/#root@136156e4c448:/# exitexit[root@Docker ~]#
更新并创建Docker镜像
更新ubuntu镜像添加ping
当本地的镜像不能满足日常需求，可从已创建的容器中更新并提交镜像。最新版的ubuntu没有安装ping，本步骤将在ubuntu安装ping，并建立新的镜像#启动 ubuntu容器，每次创建容器会产生新的id（9eac21a09449）要记录好[root@Docker ~]# docker run -t -i  ubuntu:latest /bin/bashroot@9eac21a09449:/# apt updateGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]......root@9eac21a09449:/# ping jd.combash: ping: command not found#安装pingroot@9eac21a09449:/# apt-get install -y iputils-pingReading package lists... Doneroot@9eac21a09449:/# ping -c 2 jd.comPING jd.com (111.13.149.108) 56(84) bytes of data.64 bytes from 111.13.149.108 (111.13.149.108): icmp_seq=1 ttl=127 time=43.7 ms64 bytes from 111.13.149.108: icmp_seq=2 ttl=127 time=44.6 ms#退出root@9eac21a09449:/# exitexit#操作完成后可基于该docker容器创建新的docker镜像#创建新的docker镜像[root@Docker ~]# docker commit -m &#x27;ubuntu增加ping命令&#x27; -a &#x27;docker&#x27; 9eac21a09449 ping/ubuntu:v2sha256:7655d7758bcfa635dde3cf8064602a32ac625100a1b7763e7856ff6052c8abee[root@Docker ~]# docker imagesREPOSITORY    TAG       IMAGE ID       CREATED          SIZEping/ubuntu   v2        7655d7758bcf   10 seconds ago   120MBubuntu        latest    6b7dfa7e8fdb   5 weeks ago      77.8MBhello-world   latest    feb5d9fea6a5   16 months ago    13.3kB#运行新创建的容器，测试ping[root@Docker ~]# docker run -it ping/ubuntu:v2 /bin/bashroot@7015378380d7:/# ping jd.com -c 3PING jd.com (111.13.149.108) 56(84) bytes of data.64 bytes from 111.13.149.108 (111.13.149.108): icmp_seq=1 ttl=127 time=44.1 ms64 bytes from 111.13.149.108 (111.13.149.108): icmp_seq=2 ttl=127 time=44.9 ms64 bytes from 111.13.149.108: icmp_seq=3 ttl=127 time=45.1 ms
使用dockerfile创建镜像
Dockerfile是由一系列命令和参数脚本构成的脚本，这些命令可在基础镜像上一次执行，最终创建一个新的Docker镜像
编辑dockerfile
[root@Docker ~]# vi /root/Dockerfile#基础镜像信息FROM ubuntu#维护者信息MAINTAINER book_user book@123.com#镜像操作指令RUN apt updateRUN apt-get install -y iputils-ping#容器启动时执行的命令CMD cd /optCMD echo &#x27;new docker&#x27; &gt; readme.txt

Dockerfile中每一个指令都会在镜像上创建一个新的镜像层，每一个指令的前缀都必须是大写的，其创建过程如下

使用docker build 命令从dockerfile文件创建镜像
[root@Docker ~]# docker build -t &quot;book/ubuntu:v3&quot; .#末尾小数点别忘了，接下来，他就会按照dockerfile文件中的命令一一执行[root@Docker ~]# docker imagesREPOSITORY    TAG       IMAGE ID       CREATED          SIZEbook/ubuntu   v3        90bbd384b2a3   59 seconds ago   120MBping/ubuntu   v2        7655d7758bcf   17 minutes ago   120MBubuntu        latest    6b7dfa7e8fdb   5 weeks ago      77.8MBhello-world   latest    feb5d9fea6a5   16 months ago    13.3kB#删除本地的docker镜像[root@Docker ~]# docker rmi hello-worldError response from daemon: conflict: unable to remove repository reference &quot;hello-world&quot; (must force) - container 5542ae3caafa is using its referenced image feb5d9fea6a5出现报错说明container使用到了这个images，删除镜像前需要删除基于此镜像创建的容器#查询容器列表[root@Docker ~]# docker ps -aCONTAINER ID   IMAGE            COMMAND       CREATED          STATUS                        PORTS     NAMES7015378380d7   ping/ubuntu:v2   &quot;/bin/bash&quot;   19 minutes ago   Exited (127) 15 minutes ago             nervous_lamarr37ac7bf75dbc   ping/ubuntu:v2   &quot;/bin/bash&quot;   19 minutes ago   Exited (0) 19 minutes ago               peaceful_galois9eac21a09449   ubuntu:latest    &quot;/bin/bash&quot;   32 minutes ago   Exited (130) 26 minutes ago             angry_golick136156e4c448   ubuntu           &quot;/bin/bash&quot;   34 minutes ago   Exited (130) 33 minutes ago             intelligent_proskuriakova5542ae3caafa   hello-world      &quot;/hello&quot;      46 minutes ago   Exited (0) 46 minutes ago               pensive_newton#删除容器[root@Docker ~]# docker rm 5542ae3caafa5542ae3caafa删除镜像[root@Docker ~]# docker rmi hello-worldUntagged: hello-world:latestUntagged: hello-world@sha256:aa0cc8055b82dc2509bed2e19b275c8f463506616377219d9642221ab53cf9feDeleted: sha256:feb5d9fea6a5e9606aa995e879d862b825965ba48de054caab5ef356dc6b3412Deleted: sha256:e07ee1baac5fae6a26f30cabfe54a36d3402f96afda318fe0a96cec4ca393359

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Centos 8</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Ansible-总手册</title>
    <url>/posts/27532/</url>
    <content><![CDATA[
Ansible-总手册

Ansible三机部署
Ansible配置及相关指令
用户级ansible环境构建（小练习）
Ansible-常用模块
Ansible-playblock
Ansible-templates
Ansible-Roles
实验任务：安装httpd服务

Ansible三机部署
关于
ansible是新出现的自动化运维工具，基于Python开发，集合了众多运维工具（puppet、chef、func、fabric）的优点，实现了批量系统配置、批量程序部署、批量运行命令等功能。
　　ansible是基于 paramiko 开发的,并且基于模块化工作，本身没有批量部署的能力。真正具有批量部署的是ansible所运行的模块，ansible只是提供一种框架。ansible不需要在远程主机上安装client/agents，因为它们是基于ssh来和远
程主机通讯的。ansible目前已经已经被红帽官方收购，是自动化运维工具中大家认可度最高的，并且上手容易，学习简单。是每位运维工程师必须掌握的技能之一。
主机分布



主机名
ip
系统
内存
硬盘




controller
192.168.48.100
Centos7.9
4G
100G


node01
192.168.48.101
Centos7.9
4G
100G


node02
192.168.48.102
Centos7.9
4G
100G



修改主机名
controlle
hostnamectl set-hostname controller &amp;&amp; bash
node01
hostnamectl set-hostname node01 &amp;&amp; bash
node02
hostnamectl set-hostname node02 &amp;&amp; bash
三台机加入hosts
cat &gt;&gt; /etc/hosts &lt;&lt; EOF192.168.48.100 controller192.168.48.101 node01192.168.48.102 node02EOF
设置阿里yum
mkdir repo.bakmv /etc/yum.repos.d/* repo.bak/wget -O /etc/yum.repos.d/CentOSBase.repo https://mirrors.aliyun.com/repo/Centos-7.repoyum clean all &amp;&amp; yum makecachesystemctl disable firewalld --nowsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/sysconfig/selinuxsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/configyum update -y
安装python
已安装可以忽略。
确保python版本&gt;=2.6
sudo yum install epel-releasesudo yum install https://centos7.iuscommunity.org/ius-release.rpmsudo yum install python27
[root@controller ~]# python --versionPython 2.7.5#有显示说明python安装成功
安装Ansible
sudo yum install epel-release ansible openssh[root@controller ~]# ansible --versionansible 2.9.27  config file = /etc/ansible/ansible.cfg  configured module search path = [u&#x27;/root/.ansible/plugins/modules&#x27;, u&#x27;/usr/share/ansible/plugins/modules&#x27;]  ansible python module location = /usr/lib/python2.7/site-packages/ansible  executable location = /usr/bin/ansible  python version = 2.7.5 (default, Oct 14 2020, 14:45:30) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]
测试
测试主机是否存活
vim /etc/ansible/hosts#在末尾添加ip192.168.48.101192.168.48.102-----------------------
ansible 192.168.48.101 -m ping -kansible 192.168.48.102 -m ping -k
如果controller没有首次进行ssh至node01-02节点，则ansible会出错，如下图：


所以我们必须先ssh至各节点，让其生成缓存信息
[root@controller ~]# ssh 192.168.48.101##输入密码[root@node01 ~]# exit[root@controller ~]# ssh 192.168.48.102##输入密码[root@node02 ~]# exit
这时我们在进行测试

SSH免密配置
ssh-keygen   （一路回车，三次）[root@controller ~]# ls -al ~/.sshtotal 12drwx------  2 root root   57 Sep  6 00:43 .dr-xr-x---. 8 root root  236 Sep  9 23:05 ..-rw-------  1 root root 1675 Sep  6 00:42 id_rsa-rw-r--r--  1 root root  397 Sep  6 00:42 id_rsa.pub-rw-r--r--  1 root root  352 Sep  6 00:35 known_hosts#有密钥文件了#将密钥文件复制到node01-02节点，实现ssh免密登入(先yes 然后输入密码即可)ssh-copy-id root@192.168.48.101ssh-copy-id root@192.168.48.102
最后我们在进行测试


Ansible配置及相关指令
ansible 程序结构
安装目录如下(yum安装)：
　　配置文件目录：/etc/ansible/
　　执行文件目录：/usr/bin/
　　Lib库依赖目录：/usr/lib/pythonX.X/site-packages/ansible/
　　Help文档目录：/usr/share/doc/ansible-X.X.X/
　　Man文档目录：/usr/share/man/man1/
ansible配置文件查找顺序
ansible与我们其他的服务在这一点上有很大不同，这里的配置文件查找是从多个地方找的，顺序如下：


检查环境变量ANSIBLE_CONFIG指向的路径文件
(export ANSIBLE_CONFIG=/etc/ansible.cfg)；


~/.ansible.cfg，检查当前目录下的ansible.cfg配置文件；


/etc/ansible.cfg检查etc目录的配置文件。


ansible配置文件
ansible 的配置文件为/etc/ansible/ansible.cfg，ansible 有许多参数，下面我们列出一些常见的参数：
[defaults] #inventory      = /etc/ansible/hosts   //定义Inventory#library        = /usr/share/my_modules/  //自定义lib库存放目录 #remote_tmp     = ~/.ansible/tmp       //零时文件远程主机存放目录#local_tmp      = ~/.ansible/tmp       //零时文件本地存放目录#forks          = 5                    //默认开启的并发数#poll_interval  = 15                   //默认轮询时间间隔#sudo_user      = root                 //默认sudo用户#ask_sudo_pass = True                  //是否需要sudo密码#ask_pass      = True                  //是否需要密码#host_key_checking = False             //首次连接是否检查key认证#roles_path    = /etc/ansible/roles    //默认下载的Roles存放的目录#log_path = /var/log/ansible.log       //执行日志存放目录#module_name = command                 //默认执行的模块#action_plugins     = /usr/share/ansible/plugins/action //action插件存放目录#callback_plugins   = /usr/share/ansible/plugins/callback //callback插件存放目录#connection_plugins = /usr/share/ansible/plugins/connection  //connection插件存放目录#lookup_plugins     = /usr/share/ansible/plugins/lookup //lookup插件存放目录#vars_plugins       = /usr/share/ansible/plugins/vars //vars插件存放目录#filter_plugins     = /usr/share/ansible/plugins/filter //filter插件存放目录#test_plugins       = /usr/share/ansible/plugins/test //test插件存放目录#strategy_plugins   = /usr/share/ansible/plugins/strategy //strategy插件存放目录#fact_caching = memory                 //getfact缓存的主机信息存放方式#retry_files_enabled = False              #retry_files_save_path = ~/.ansible-retry  //错误重启文件存放目录
配置文件的分类与优先级
Ansible只有一个配置文件ansible.cfg，但配置文件可以存在不同的位置，并且只有一个可用 (数字代表优先级，数字越小代表优先级越高) :

配置文件选项



官网配置参考网址
Ansible Configuration Settings — Ansible Documentation

ansuble主机清单
1、 定义单独主机：	## green.example.com#	# blue.example.com#	# 192.168.100.1	# 192.168.100.102、 定义一个主机组[组名]把地址或主机名加进去	[mysql_test]	192.168.253.159	192.168.253.160	192.168.253.153
需要注意的是，这里的组成员可以使用通配符来匹配，这样对于一些标准化的管理来说就很轻松方便了。
　　我们可以根据实际情况来配置我们的主机列表，具体操作如下：
[root@server ~]# vim /etc/ansible/hosts	[web]	192.168.37.122	192.168.37.133
3、 定义嵌套组    [web-mysql]       mysql_test       web4、 定义范围化ip    172.16.[0:4].[2:254]  
ansible 常用命令

/usr/bin/ansible　　Ansibe AD-Hoc 临时命令执行工具，常用于临时命令的执行
/usr/bin/ansible-doc 　Ansible 模块功能查看工具
/usr/bin/ansible-galaxy　　下载/上传优秀代码或Roles模块 的官网平台，基于网络的
/usr/bin/ansible-playbook　　Ansible 定制自动化的任务集编排工具
/usr/bin/ansible-pull　　Ansible远程执行命令的工具，拉取配置而非推送配置（使用较少，海量机器时使用，对运维的架构能力要求较高）
/usr/bin/ansible-vault　　Ansible 文件加密工具
/usr/bin/ansible-console　　Ansible基于Linux Consoble界面可与用户交互的命令执行工具

其中，我们比较常用的是/usr/bin/ansible和/usr/bin/ansible-playbook。
ansible 命令详解
命令的具体格式如下：
ansible &lt;host-pattern&gt; [-f forks] [-m module_name] [-a args]
也可以通过ansible -h来查看帮助，下面我们列出一些比较常用的选项，并解释其含义：

-a MODULE_ARGS　　　#模块的参数，如果执行默认COMMAND的模块，即是命令参数，如： “date”，“pwd”等等
-k，--ask-pass #ask for SSH password。登录密码，提示输入SSH密码而不是假设基于密钥的验证
--ask-su-pass #ask for su password。su切换密码
-K，--ask-sudo-pass #ask for sudo password。提示密码使用sudo，sudo表示提权操作
--ask-vault-pass #ask for vault password。假设我们设定了加密的密码，则用该选项进行访问
-B SECONDS #后台运行超时时间
-C #模拟运行环境并进行预运行，可以进行查错测试
-c CONNECTION #连接类型使用
-f FORKS #并行任务数，默认为5
-i INVENTORY #指定主机清单的路径，默认为/etc/ansible/hosts
--list-hosts #查看有哪些主机组
-m MODULE_NAME #执行模块的名字，默认使用 command 模块，所以如果是只执行单一命令可以不用 -m参数
-o #压缩输出，尝试将所有结果在一行输出，一般针对收集工具使用
-S #用 su 命令
-R SU_USER #指定 su 的用户，默认为 root 用户
-s #用 sudo 命令
-U SUDO_USER #指定 sudo 到哪个用户，默认为 root 用户
-T TIMEOUT #指定 ssh 默认超时时间，默认为10s，也可在配置文件中修改
-u REMOTE_USER #远程用户，默认为 root 用户
-v #查看详细信息，同时支持-vvv，-vvvv可查看更详细信息

ansible 配置公私钥
上面我们已经提到过 ansible 是基于 ssh 协议实现的，所以其配置公私钥的方式与 ssh 协议的方式相同，具体操作步骤如下：
#1.生成私钥[root@server ~]# ssh-keygen #2.向主机分发私钥[root@server ~]# ssh-copy-id root@192.168.48.101[root@server ~]# ssh-copy-id root@192.168.48.102
192.168.48.101为node01的ip地址
这样的话，就可以实现无密码登录，我们的实验过程也会顺畅很多。
　　注意，如果出现了一下报错：
-bash: ssh-copy-id: command not found
那么就证明我们需要安装一个包：
yum -y install openssh
把包安装上即可。
注意：先ssh 192.168.48.101和ssh 192.168.48.102各节点，生成缓存信息，才能进行主机连通性测试
ansible ping模块
主机连通性测试
我们使用ansible web -m ping命令来进行主机连通性测试，效果如下：
[root@server ~]# ansible web -m ping192.168.48.101 | SUCCESS =&gt; &#123;    &quot;changed&quot;: false,     &quot;ping&quot;: &quot;pong&quot;&#125;192.168.48.102 | SUCCESS =&gt; &#123;    &quot;changed&quot;: false,     &quot;ping&quot;: &quot;pong&quot;&#125;
这样就说明我们的主机是连通状态的。接下来的操作才可以正常进行。
用户级ansible环境构建
主机分布



主机名
ip
系统
内存
硬盘




controller
192.168.48.100
Centos7.9
4G
100G


node01
192.168.48.101
Centos7.9
4G
100G


node02
192.168.48.102
Centos7.9
4G
100G



创建student用户
三台机创建student用户
#创建student用户useradd studentpasswd student123456
controller创建student用户工作目录
#切换student用户，创建工作目录，新建ansible.cfg配置文件，验证配置文件生效。su studentcd #创建资产清单 mkdir ansiblecat &gt;&gt; /home/student/ansible/inventory &lt;&lt;EOF[servers]192.168.48.101192.168.48.102EOF
controller编辑配置文件ansible.cfg
cd ansiblecat &gt;&gt; /home/student/ansible/ansible.cfg &lt;&lt;EOF[defaults]inventory=/home/student/ansible/inventoryremote_port=22remote_user=root#指定远程用户为rootask_pass=TrueEOF
验证清单主机存活（指定root）
验证清单主机存活 ,执行命令进行测试，可以看到在每次执行ansible时都会询问连接用户的密码（相 当于-K参数）
ansible all --list[student@controller ansible]$ ansible all --listSSH password:  hosts (2):    192.168.48.101    192.168.48.102[student@controller ansible]$#成功
如果不想输入密码，那需要修改配置文件：
vi /home/student/ansible/ansible.cfg······ask_pass=False

[student@controller ansible]$ ansible all --list  hosts (2):    192.168.48.101    192.168.48.102[student@controller ansible]$#无输入密码选项
实例（指定student）
远程用户指定为普通用户（student）
[student@controller ansible]$ vim ansible.cfg[defaults]inventory=/home/student/ansible/inventoryremote_port=22remote_user=student#指定远程用户为studentask_pass=False
这时候执行ping会报错
ansible all -m ping


这时候我们要配置免密登入（这里的密钥是student用户的，和root不一样，不会覆盖root用户的，这是在student用户下执行的命令）
[student@controller ansible]$ ssh-keygen#回车三次[student@controller ansible]$ ssh-copy-id student@192.168.48.101#输入yes和node1的root密码[student@controller ansible]$ ssh-copy-id student@192.168.48.102#输入yes和node2的root密码
这是执行ping命令
ansible all -m ping

测试提取文件
这个普通用户（student）并不能执行所有的操作，比如ansible以student身份登录，执行（ls /root）发普通用户没有权限，
ansible all -m shell -a &quot;ls /root&quot;

解决这个问题就需要提权：sudo 提权
在受控主机(node01、node02)上执行visudo（配置 /etc/sudoers）
node01机子
[root@node01 ~]# visudo

node02机子
[root@node02 ~]# visudo

或者(node01和node02执行以下指令)
cat &gt;&gt;/etc/sudoers.d/student &lt;&lt; EOF student          ALL=(ALL)        NOPASSWD: ALLEOF
在控制主机（controller）上(student用户)修改ansible.cfg配置文件提权
[student@controller ansible]$ vim ansible.cfg[defaults]inventory=/home/student/ansible/inventoryremote_port=22remote_user=studentask_pass=False[privilege_escalation]become=Truebecome_method=sudobecome_user=rootbecome_ask_pass=False

验证
ansible all -m shell -a &quot;ls /root&quot;

#在控制节点的student经过sudo提权之后可以读取/root目录了
Ansible-常用模块
command模块
linux命令，不支持管道、重定向等，不建议使用
ansible all -m command -a &quot;pwd&quot;ansible all -m command -a &quot;ls&quot;ansible all -m command -a &quot;cat /etc/passwd |grep student&quot; #这个不能正常使用 ansible all -m command -a &quot;echo bb &gt;&gt;/tmp/testansible&quot;ansible all -m command -a &quot;cat /tmp/testansible&quot;
#重定向也无法正常使用
课堂练习：
使用command命令查询各主机磁盘状态、查询内存状态
ansible all -m command -a &quot;df -h&quot;ansible all -m command -a &quot;free -m&quot;
shell模块
支持管道、重定向等，常用模块
ansible all -m shell -a &quot;cat /etc/passwd | grep student&quot; #支持管道ansible all -m shell -a &quot;echo bb &gt;&gt;/tmp/testansible&quot;ansible all -m shell -a &quot;cat /tmp/testansible&quot;
#支持重定向
课堂练习：
使用shell模块查看selinux状态
ansible all -m shell -a &quot;getenforce&quot;
通过shell模块批量关闭selinux
临时关闭：ansible all -m shell -a &quot;setenforce 0&quot;永久关闭：ansible all -m shell -a &quot;sed -ri &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/config&quot;ansible all -m shell -a &quot;reboot&quot;
文件模块
copy模块
从主控端复制文件到远程主机
ansible-doc copy
常用参数
src：source源路径文件/目录。即要复制到远程主机的文件在本地的地址，可以是绝对路径，也可以是 相对路径。如果路径是一个目录，它将递归复制。在这种情况下，如果路径使用&quot;/“来结尾，则只复制目录里的内容，如果没有使用”/&quot;来结尾，则包含目录在内的整个内容全部复制。
dest：destnation受管主机上的一个目标路径，即要将源文件复制到的远程主机的绝对路径，如果源文件是一个目录，那么该路径也必须是个目录（必须）
content：代替src，将本机指定内容传至远程主机并生成目标文件，相当于 echo 重定向内容到文件
mode：文件权限（chmod）
linux权限回顾

owner：文件属主（chown）
group：文件属组（chgrp）
backup：在覆盖之前将原文件备份，备份文件包含时间信息。
directory_mode： 递归地设定目录的权限，默认为系统默认权限
force： 若目标主机包含该文件，但内容不同，如果设置为yes，则强制覆盖，如果为no，则只有当目标 主机的存放位置不存在该文件时，才复制。默认为yes
使用案例


复制dir1目录及其文件到受控主机/tmp/下（无斜杠-操作整个目录）

注：关于src目录加不加/的演示
#复制dir1目录及其文件到受控主机/tmp/下su studentcd ~llmkdir ansiblecd ansible/mkdir dir1echo &quot;123&quot; &gt;dir1/file1ansible 192.168.48.101 -m copy -a &quot;src=&#x27;dir1&#x27; dest=/tmp/&quot; 

查看受控主机是否复制成功

成功

2.仅复制dir1目录下的文件（有斜杠-操作目录下的文件，不复制目录）
ansible 192.168.48.101 -m copy -a&quot;src=&#x27;dir1/&#x27; dest=/tmp/&quot;


练习

将控制主机的copyfile文件复制到受管主机的 /tmp 目录

echo &#x27;123&#x27; &gt; copyfileansible 192.168.48.101 -m copy -a &#x27;src=copyfile dest=/tmp&#x27;


直接在受管主机上生成一个指定内容的文件文件

ansible 192.168.48.101 -m copy -a &quot;content=&#x27;test copy\n&#x27; dest=/tmp/f1&quot;


在受管主机上生成指定属性和内容的文件

ansible 192.168.48.101 -m copy -a &quot;content=&#x27;test copy 2\n&#x27; dest=/tmp/f2 mode=0644 owner=student group=student&quot;


在文件覆盖前生成备份文件

ansible 192.168.48.101 -m copy -a &quot;content=&#x27;test copy 1\n&#x27; dest=/tmp/f1 backup=yes&quot;ansible 192.168.48.101 -m shell -a &quot;ls -l /tmp/f1*&quot;


script模块
在远程主机上运行ansible服务器上的脚本，优点是不需手动传送脚本至每个服务器。
其实是ansible自动传到远程主机、执行然后再删除脚本：copy+shell+delete
cat &gt;&gt; tesh.sh &lt;&lt; EOF#！/bin/bash echo helloEOFansible all -m script -a tesh.sh

执行结果显示了每台主机的执行情况。
CHANGED 表示执行过程中发生了变化，即脚本被成功执行。
rc 字段显示返回码为 0，表示执行成功。
stderr 字段显示了标准错误输出，其中包含了连接关闭的信息。
stdout 字段显示了标准输出，其中包含了脚本执行的结果，即 “hello”。
cat &gt;&gt;  ansible_ntp.sh &lt;&lt; EOF#!/bin/bashsystemctl status ntpd &gt;/dev/null 2&gt;&amp;1if [ \$? == 0 ]; then	echo &quot;ntp service has been installed&quot;	exitfiyum install -y ntp &gt;/dev/null 2&gt;&amp;1if [ \$? == 0 ]; then	systemctl start ntpd	systemctl enabled ntpd &gt;/dev/null 2&gt;&amp;1	sleep 5	ntpq -pelse	echo &quot;ntp service install failed,check network or yum&quot;fiEOF这条命令的意思就是在后台执行这个程序,并将错误输出2重定向到标准输出1,然后将标准输出1全部放 到/dev/null文件,也就是清空.所以可以看出&quot; &gt;/dev/null 2&gt;&amp;1 &quot;常用来避免shell命令或者程序等运行中有内容输出。chmod 777 ansible_ntp.shansible all -m script -a &quot;ansible_ntp.sh&quot;

fetch模块
从受管主机上，拉取文件到控制节点（目前不支持目录,可以先打包,再提取文件）
常见参数
dest：控制节点的保存路径
src：受管节点要拉取文件的路径（必须是文件，不能是目录）
flat：直接保存到目标指定位置，而不是在受管主机名下的文件路径中。
使用案例

从受管主机拉取指定文件

ansible 192.168.48.101 -m fetch -a &quot;src=/etc/hosts dest=/home/student/ansible&quot;
索取到本地目录下的文件会自动生成与目标主机的域名或IP地址的目录存放索取的文件


直接拉取受管主机文件到控制节点指定位置

flat：直接保存到目标指定位置，而不是在受管主机名下的文件路径中。
ansible 192.168.48.101 -m fetch -a &quot;src=/etc/hosts dest=/home/student/ansible/file1 flat=yes&quot;


打包目录并所有内容到控制节点指定位置

ansible 192.168.48.101 -m shell -a &#x27;pwd&#x27;ansible 192.168.48.101 -m shell -a &#x27;tar cf test.tar.gz /var/log&#x27;ansible 192.168.48.101 -m shell -a &#x27;ls -l /home/student/&#x27;ansible 192.168.48.101 -m fetch -a &quot;src=/home/student/test.tar.gz dest=/home/student/ansible/ flat=yes&quot;


file模块
file 模块可以帮助我们完成一些对文件的基本操作。比如，创建文件或目录、删除文件或目录、修改文 件权限等。
常用参数
mode： 定义文件/目录的权限,比如，如果想要将文件权限设置为&quot;rw-r-x—&quot;，则可以使用mode=650进行 设置，或者使用mode=0650，效果也是相同的。
owner： 定义文件/目录的所有者,属主对应的用户必须在远程主机中存在,否则会报错。
group： 定义文件/目录的属组,属组对应的组必须在远程主机中存在，否则会报错。
path： 必选项，定义受管主机的文件/目录的路径
recurse： 递归地设置文件的属性，只对目录有效
src： 要被链接的源文件的路径，只应用于state=hard/link的情况
dest： 被链接到的路径，只应用于state=hard/link的情况。
state： 操作方法
directory：如果目录不存在，创建目录
file：即使文件不存在，也不会被创建（只能指定已存在的文件）
touch：如果文件不存在，则会创建一个新的文件，如果文件或目录已存在，则更新其最后修改时间
link：创建软链接
hard：创建硬链接
absent：删除目录、文件或者取消链接文件，相当于rm -rf
force： 只应用于state=hard/link的情况，若需要在两种情况下强制创建软链接，一种是源文件不存在但 之后会建立的情况下；另一种是目标软链接已存在，需要先取消之前的软链，然后创建新的软链，有两个选 项：yes|no
使用案例

创建指定文件属性的空目录

ansible 192.168.48.101 -m file -a &quot;path=/tmp/dir2 state=directory owner=student group=student mode=0755&quot;ansible 192.168.48.101 -m shell -a &quot;ls -l /tmp/&quot;


创建指定文件属性的空文件

ansible 192.168.48.101 -m file -a &quot;path=/tmp/file2 state=touch owner=student group=student mode=0755&quot;ansible 192.168.48.101 -m shell -a &quot;ls -l /tmp/&quot;

注意：为何不能用state=file
file：即使文件不存在，也不会被创建（只能指定已存在的文件）
file3不存在
ansible 192.168.48.101 -m file -a &quot;path=/tmp/file3 state=file owner=student group=student mode=0755&quot;

如果指定file1（已存在）呢（将root用户属组改成student）
ansible 192.168.48.101 -m file -a &quot;path=/tmp/file1 state=file owner=student group=student mode=0755&quot;


删除目录、删除文件

ansible 192.168.48.101 -m file -a &quot;path=/tmp/dir1 state=absent&quot;ansible 192.168.48.101 -m file -a &quot;path=/tmp/file1 state=absent&quot;ansible 192.168.48.101 -m shell -a &quot;ls -l /tmp/&quot;


创建链接文件

软链接：快捷方式
file2→file1（link）
生成file1（如果已存在就忽略这步）
ansible 192.168.48.101 -m copy -a &quot;content=&#x27;123 \n&#x27; dest=/tmp/file1&quot;

生成软链接
ansible 192.168.48.101 -m file -a &quot;src=/tmp/file1 path=/tmp/file2 state=link force=true&quot;

取消软连接
ansible 192.168.48.101 -m file -a &quot;path=/tmp/file2 state=absent&quot;

硬链接：指向同一个inode
file3→file1（hard）
file3不存在
ansible 192.168.48.101 -m file -a &quot;src=/tmp/file1 path=/tmp/file3 state=hard&quot;

课堂练习
1、在/tmp/下创建目录ansiblefile,并在该文件夹下创建test.txt文件，指定属主student,赋予权限0700
ansible all -m shell -a &quot;useradd student&quot;	#创建一个用户与组ansible 192.168.48.101 -m file -a &#x27;path=/tmp/ansiblefile state=directory&#x27;ansible 192.168.48.101 -m file -a &#x27;path=/tmp/ansiblefile/test.txt state=touch owner=student mode=0700&#x27;

2、删除远程机器上的指定文件或目录(删除远程主机上的文件：/tmp/ansiblefile/test.txt
ansible 192.168.48.101 -m file -a &#x27;path=/tmp/ansiblefile/test.txt state=absent&#x27;

lineinfile模块
增加或修改文件内容（以行为单位做流式处理），该模块在自动化运维中非常重要,他可以通过正则表达 式替换指定文本;例如开启一些配置选项等可以新加一行文本,或者是删除指定的行,本命令一定认真掌握 下来.
*常见参数*
path：必须参数，远端文件路径
line：必须参数，修改后的内容（按行写入），追加
regexp：（定位）匹配正则语句,与要过滤的关键字
state：文件修改状态（present 添加生效 / absent 删除）
replace：替换文件内容
create：当文件不存在时，是否创建对应文件
backup：若文件更新时创建备份副本
insertafter：在指定位置的下一行插入（定位）
insertbefore：在指定位置的上一行插入（定位）
使用案例
假设前提：将控制节点的/etc/selinux/config文件复制到受管主机192.168.48.101，另存为/tmp/selinux文件
ansible 192.168.48.101 -m copy -a &quot;src=/etc/selinux/config dest=/tmp/selinux&quot;ansible 192.168.48.101 -m shell -a &quot;cat /tmp/selinux&quot;



修改文件内容：考虑两个问题，修改哪个部分，修改成什么内容

修改SELINUX开头的行，更新内容为：SELINUX=disabled
ansible 192.168.48.101 -m lineinfile -a &quot;path=/tmp/selinux regexp=&#x27;^SELINUX=&#x27; line=&#x27;SELINUX=disabled&#x27; &quot;ansible 192.168.48.101 -m shell -a &quot;cat /tmp/selinux&quot;

2.增加文件内容：考虑两个问题，增加什么内容，增加在哪个位置（上一行或下一行）
在SELINUX开头的行，在下一行加个注释，并且应用生效
ansible 192.168.48.101 -m lineinfile -a &quot;path=/tmp/selinux insertafter=&#x27;^SELINUX=&#x27; line=&#x27;##Disabled SELINUX&#x27; &quot;

通过正则匹配查找/tmp/selinux文本,并在文本末尾插入一行##end
ansible 192.168.48.101 -m lineinfile -a &#x27;path=/tmp/selinux regexp=&quot;EOF&quot; line=&quot;#end&quot;&#x27;

3.删除文件内容:把刚才添加的“##disabled selinux”注释去掉（删除所在行）
ansible 192.168.48.101 -m lineinfile -a &quot;path=/tmp/selinux regexp=&#x27;^##Disa&#x27; state=absent&quot;ansible 192.168.48.101 -m shell -a &quot;cat /tmp/selinux&quot;



备份文件:在SELINUX开头的行，前一行加个注释，并且生效，生成备份文件

ansible 192.168.48.101 -m lineinfile -a &quot;path=/tmp/selinux insertbefore=&#x27;^SELINUX=&#x27; line=&#x27;##Disabled SELINUX&#x27; backup=yes state=present&quot;ansible 192.168.48.101 -m shell -a &quot;ls -l /tmp/selinux*&quot;


课堂练习：
修改192.168.48.101主机的/etc/hosts文件，
1、增加内容192.168.48.102 node03,验证增加成功
ansible 192.168.48.101 -m lineinfile -a &quot;path=/etc/hosts line=&#x27;192.168.48.102 node03&#x27; &quot;ansible 192.168.48.101 -m shell -a &quot;cat /etc/hosts&quot;

2、匹配以192开头的行，修改192.168.48.102 对应的 域名为node02，验证增加成功
ansible 192.168.48.101 -m lineinfile -a &quot;path=/etc/hosts regexp=&#x27;^192&#x27; line=&#x27;192.168.48.102 node02&#x27; &quot;

3、匹配以192开头的行之前增加 192.168.48.101 node01 ，验证增加成功
ansible 192.168.48.101 -m lineinfile -a &quot;path=/etc/hosts insertbefore=&#x27;^192&#x27; line=&#x27;192.168.48.101 node01&#x27; &quot;

4、在文档结尾增加 192.168.48.100 controller
ansible 192.168.48.101 -m lineinfile -a &#x27;path=/etc/hosts regexp=&quot;EOF&quot; line=&quot;192.168.48.100 controller&quot;&#x27;

5、删除步骤1-3加入的内容
ansible 192.168.48.101 -m lineinfile -a &quot;path=/etc/hosts regexp=&#x27;^192.&#x27; state=absent&quot;ansible 192.168.48.101 -m shell -a &quot;cat /etc/hosts&quot;

软件包模块
yum模块
*常用参数*
name：软件包名称（必填）
state：
latest（更新到最新）
present（安装）
version（版本）
absent（卸载）
查看是否安装了某个服务 rpm -qa|grep httpd
*使用案例*
给node01安装httpd服务
ansible 192.168.48.101 -m yum -a &#x27;name=httpd state=present&#x27;

验证安装
ansible 192.168.48.101 -m shell -a &#x27;rpm -qa|grep httpd&#x27;
卸载 state=absent
ansible 192.168.48.101 -m yum -a &#x27;name=httpd state=absent&#x27;
更新软件state=latest
ansible 192.168.48.101 -m yum -a &#x27;name=&#x27;httpd&#x27; state=latest&#x27;
系统模块
user模块
常用参数
comment：注释信息group：主要组groups：附加组state：present/absentgenerate_ssh_key：生成SSH验证密钥name：用户名shell：Shell类型uid：UID
使用案例
1、在node1上创建用户 test_user UID=1010
ansible 192.168.48.101 -m user -a &quot;name=test_user  uid=1010 comment=&#x27;ansible_test_user&#x27; shell=/bin/bash generate_ssh_key=yes  state=present&quot;
ansible 192.168.48.101 -m shell -a &#x27;id test_user&#x27;

2、删除用户test_user（userdel test_user）
ansible 192.168.48.101 -m user -a &quot;name=test_user  state=absent force=yes&quot;ansible 192.168.48.101 -m shell -a &#x27;getent passwd |grep test_user&#x27;

group组模块
1、创建组test_group (groupadd -g 1010 test_group）
ansible 192.168.48.101 -m group -a &quot;name=test_group  gid=1010  state=present&quot;ansible 192.168.48.101 -m shell -a &#x27;getent group|grep test_group&#x27;

2、删除组test_group （groupdel test_group）
ansible 192.168.48.101 -m group -a &quot;name=test_group   state=absent&quot;ansible 192.168.48.101 -m shell -a &#x27;getent group|grep test_group&#x27;

service模块
启用/启动/停止指定的服务
常用参数
name：服务名（确定服务存在）（必选项）state：服务目标状态	（state=started/stopped/restarted/...）（必选项）enabled：是否开机启动(yes/no)
1、在node01上安装和启用httpd服务（相当于systemctl enable --now httpd）
安装
ansible 192.168.48.101 -m yum -a &#x27;name=httpd state=present&#x27;
启用
ansible 192.168.48.101 -m service -a &quot;name=httpd state=started  enabled=yes&quot;
ansible 192.168.48.101 -m shell -a &#x27;systemctl status httpd&#x27;


2、停止服务
ansible 192.168.48.101 -m service -a &#x27;name=httpd state=stopped&#x27;

3、重启服务
ansible 192.168.48.101 -m service -a &#x27;name=httpd state=restarted&#x27;

firewalld模块
常见参数
source：数据源（相当于--add-source）interface：端口（相当于--add-port）service：服务（相当于--add-service）zone：关联区域（相当于--zone）permanent：永久生效（相当于--permanent）immediate：立即生效（相当于执行了firewall-cmd --reload）state：防火墙规则状态（enabled | disabled）（必填项）rich_rule：富规则（相当于--add-rich-rule=&#x27;&#x27;）
使用案例
1、添加基本规则
在node1中将http服务进行放行，并关联到public区域中，立即生效且永久生效
ansible 192.168.48.101 -m firewalld -a &#x27;zone=public service=http permanent=yes immediate=yes state=enabled&#x27;
综合练习
（1）在node01上创建一个用户devops：
ansible node01 -m user -a &quot;name=devops state=present&quot;

（2）在node01上创建一个目录 /devops，设置所属组、权限：
ansible node01 -m file -a &quot;path=/devops state=directory owner=devops group=devops mode=0755&quot; ansible node01 -m shell -a &quot;ls -l /&quot;  

（3）安装httpd服务，设定开机自启动，验证服务状态为启动：
ansible node01 -m yum -a &quot;name=httpd state=present&quot;ansible node01 -m service -a &quot;name=httpd state=started enabled=yes&quot;


（4）创建一个文件 /devops/index.html 包含一行内容：DevOps：
ansible node01 -m copy -a &quot;content=&#x27;DevOps\n&#x27; dest=/devops/index.html&quot;

（5）创建软链接：/var/www/html/index.html 到 /devops/index.html：
ansible node01 -m file -a &quot;src=/devops/index.html dest=/var/www/html/index.html state=link force=true&quot;

（6）验证软链接：
ansible node01 -m shell -a &quot;ls -l  /var/www/html/index.html&quot;

（7）取消软链接，新建/var/www/html/index.html 文档，访问网页：
ansible node01 -m file -a &quot;path=/var/www/html/index.html state=absent&quot;ansible node01 -m copy -a &quot;content=&#x27;Hello World&#x27; dest=/var/www/html/index.html&quot;


使用浏览器访问http://192.168.48.101验证主页信息
Ansible-Playbook
介绍

playbook是由一个或多个&quot;play&quot;组成的列表
play的主要功能在于将预定义的一组主机，装扮成事先通过ansible中的task定义好的角色。
Task实际是调用ansible的一个module，将多个play组织在一个playbook中，
即可以让它们联合起来，按事先编排的机制执行预定义的动作
Playbook采用YAML语言编写

用户通过ansible命令直接调用yml语言写好的playbook,playbook由多条play组成每条play都有一个任务(task)相对应的操作,然后调用模块modules，应用在主机清单上,通过ssh远程连接从而控制远程主机或者网络设备
YAML语法
&gt; 在单一档案中，可用连续三个连字号（---）区分多个档案。  另外，还有选择性的连续三个点号( ... )用来表示档案结尾&gt; 次行开始正常写Playbook的内容，一般建议写明该Playbook的功能&gt; 使用#号注释代码&gt; 缩进必须是统一的，不能空格和tab混用，一般缩进2个空格&gt; 缩进的级别也必须是一致的，同样的缩进代表同样的级别，程序判别配置的级别是通过缩进结合换行来实现的&gt; YAML文件内容是区别大小写的，key/value的值均需大小写敏感&gt; 多个key/value可同行写也可换行写，同行使用:分隔，同一行使用 , 逗号分隔&gt; value可以是个字符串，也可是另一个列表[]&gt; 一个完整的代码块功能需最少元素需包括 name 和 task&gt; 一个name只能包括一个task&gt; YAML中不允许在双引号中出现转义符号，所以都是以单引号来避免转义符错误&gt; 使用 | 和 &gt; 来分隔多行，实际上这只是一行。 &gt; YAML文件扩展名通常为yml或yaml
三种常见的数据交换格式

YAML的语法和其他高阶语言类似，并且可以简单表达清单、散列表、标量等数据结构。序列（Sequence）里的项用&quot;-&quot;来代表，Map里的键值对（字典）用&quot;:&quot;分隔示例    name: John Smith    age: 41    gender: Male    spouse:      name: Jane Smith      age: 37      gender: Female    children:      - name: Jimmy Smith        age: 17        gender: Male      - name: Jenny Smith        age 13        gender: Female
修改vim
vim ~/.vimrcset nu       set paste    set cursorline set cursorcolumn autocmd FileType yaml setlocal ai et ts=2 sw=2 set nu       #显示行号set paste    #黏贴内容保留格式set cursorline #行定位set cursorcolumn #列定位autocmd FileType yaml setlocal ai et ts=2 sw=2 #FileType 代表文件类型,后边跟参数yaml，就是这个作用于yaml文件，编写其他文件时不起作用#ts=2是tabstop=2的缩写，表示使用2个空格自动代替tab键#et=expandtab 表示tab键的缩写#sw=2 是shiftwidth=2的缩写，表示开启自动缩进对齐，缩进宽度为2个空格#ai=auto indent   自动退格对齐\
将这段代码添加到 ~/.vimrc 文件中，以使 Vim 在启动时自动应用这些设置
playbook基础组件
一个简单的剧本模型（YAML语言）
1&gt; 缩进：用两个空格缩进
2&gt; 列表：用 -
3&gt; 字典：key: value
--- - hosts: YYY          #待操作主机集，可以不写，执行时通过 -i 调用host文件  remote_user： root  #在远端使用哪个用户执行  tasks:              #任务集（必须）  - name: task1       #只是一个文本提示，执行时会输出其中内容（例如输出Install httpd）    module1:          #真正干活的部分，其实就是前面讲过的ansible各种模块      argument1 : value1       argument2 : value2   - name: task2     module2:      argument1 : value1       argument2 : value2 ... 
解释：
Hosts：    &gt; playbook中的每一个play的目的都是为了让特定主机以某个指定的用户身份执行任务。      hosts用于指定要执行指定任务的主机，须事先定义在主机清单中
remote_user:     可用于Host和task中。    也可以通过指定其通过sudo的方式在远程主机上执行任务，其可用于play全局或某任务；    此外，甚至可以在sudo时使用sudo_user指定sudo时切换的用户    - hosts: all      remote_user: root   (可省略,默认为root)  以root身份连接      tasks:    指定任务      - name: test connection        ping:          remote_user: magedu          sudo: yes           默认sudo为root          sudo_user:wang      sudo为wang
task列表和action    任务列表task:由多个动作,多个任务组合起来的,每个任务都调用的模块,一个模块一个模块执行    1&gt; play的主体部分是task list，task list中的各任务按次序逐个在hosts中指定的所有主机上执行，       即在所有主机上完成第一个任务后，再开始第二个任务    2&gt; task的目的是使用指定的参数执行模块，而在模块参数中可以使用变量。   模块执行是幂等的，这意味着多次执行是安全的，因为其结果均一致    3&gt; 每个task都应该有其name，用于playbook的执行结果输出，建议其内容能清晰地描述任务执行步骤。   如果未提供name，则action的结果将用于输出
playbook书写风格
简单案例：
编写echo.yaml文件，内容如下
vim echo.yaml---- hosts: 192.168.48.102  tasks:  - name: 输出1    shell: echo &quot;1&quot;  - name: 输出2    shell: echo &quot;2&quot;...#执行该剧本文件：ansible-playbook echo.yaml  
命令执行返回的结果:
第一行PLAY表示执行的主机或者主机组。
第二行TASK[Gathering Facts]，在 Playbook 中并没有定义，这是Ansible自带的task 收集主机的信息，此功能非常实用，后面的任务中会详细讲解。这里仅需知晓task为Ansible自带的功能，可以通过Playbook中添加 gather_facts: no 进行关闭。
下面两个task是自行编辑的task，可以发现没有返回结果，但是当出现黄色的changed时代表执行或者修改成功。changed代表前后状态发生改变,例如使用copy模块，拷贝同一个东西，第一次执行成功的时候是changed状态，第二次再执行的时候就是ok状态。ok状态代表:Ansible检查了需要更改的内容发现前后没有变化，所以直接返回ok状态，实际上 Ansible并没有去执行该操作。最后代表状态,即 Playbook的执行结果。ok表示检查了但不需要操作的任务量。failed表示执行失败的数量，changRed代表状unreachable表示不可达的主机数态更改的数量，ok+changed 才代表执行完成的任务数量。
编写playbook
vim httpd.yaml--- ##列出第一个play - name: Install httpd package and start httpd service ##标明 该play的用途   hosts: 192.168.142.101 ##指定对其运行play中任务的主机（必填项，指定多台主机可以使用分组，或者 , 分隔）   tasks: ##play的任务列表   - name: Install httpd package ##任务1的描述     yum: ##任务1调用yum模块，模块内容往下写       name: httpd ##参数1：yum模块需要使用的软件包       state: present ##参数2：安装软件包#以上任务等同于 ansible 192.168.142.101 -m yum -a &quot;name=httpd state=present&quot;   - name: Start httpd service ##任务2的描述     service: ##任务2调用服务模块       name: httpd ##参数1：service调用的服务名称       state: started ##参数2：service调用服务要达到的目标状态       enabled: yes ##参数3：调用的服务开机启动 #以上任务等同于 ansible 192.168.142.101 -m service -a &quot;name=httpd state=started enabled=yes&quot; 

运行playbook
运行playbook的方式    ansible-playbook &lt;filename.yaml&gt; ... [options]常见选项    --check -C       只检测可能会发生的改变，但不真正执行操作                      (只检查语法,如果执行过程中出现问题,-C无法检测出来)                     (执行playbook生成的文件不存在,后面的程序如果依赖这些文件,也会导致检测失败)    --list-hosts     列出运行任务的主机    --list-tags      列出tag  (列出标签)    --list-tasks     列出task (列出任务)    --limit 主机列表 只针对主机列表中的主机执行    -v -vv -vvv      显示过程示例    ansible-playbook hello.yaml --check 只检测    ansible-playbook hello.yaml --list-hosts  显示运行任务的主机    ansible-playbook hello.yaml --limit 192.168.142.101  限制主机    ansible-playbook hello.yaml --list-tasks  显示运行任务的主机
1、在ansible工作目录运行 完整剧本：httpd.yaml
执行剧本ansible-playbook playbooks/httpd.yaml验证服务ansible 192.168.142.101 -m shell -a &#x27;rpm -qa |grep httpd&#x27;ansible 192.168.142.101 -m shell -a &#x27;netstat -ntulp |grep 80&#x27;

2.提高输出的详细程度

注：通常使用 ansible-playbook -v 即可。

3.执行空运行（冒烟运行）


handlers+notify
Handlers 实际上就是一个触发器，是task列表，这些task与前述的task并没有本质上的不同,用于当关注的资源发生变化时，才会采取一定的操作。任务都有状态changed或者ok，只有在任务执行状态为change时，才执行该任务调用的handler。
Notify此action可用于在每个play的最后被触发，这样可避免多次有改变发生时每次都执行指定的操作，仅在所有的变化发生完成后一次性地执行指定操作。在notify中列出的操作称为handler，也即notify中调用handler中定义的操作
示例1：

使用playbook安装httpd，并验证服务启动，查看httpd使用的端口

vim httpd3.yaml---- hosts: all  remote_user: root  tasks:  - name: Install httpd    yum:      name: httpd      state: present  - name: ensure apache is running    service:      name: httpd      state: started      enabled: yesansible-playbook httpd3.yamlansible all -m shell -a &#x27;systemctl status httpd&#x27;ansible all -m shell -a &#x27;netstat -tunlp|grep httpd&#x27;

在被控端（两台机）修改httpd的conf文件，监听端口改成8080

mkdir -p /root/ansible/filescp /etc/httpd/conf/httpd.conf  /root/ansible/files#如果前面httpd3运行成功，说明成功安装httpd，则httpd.conf会存在vim /root/ansible/files/httpd.conf#将Listen 80 修改为Listen 8080

修改剧本文件，增加拷贝配置文件的task，并重新执行剧本。

vim httpd4.yaml---- hosts: all  remote_user: root  tasks:     - name: Install httpd    yum:      name: httpd      state: present  - name: backup httpd.conf    shell: cp /etc/httpd/conf/httpd.conf&#123;,.bak&#125;  #备份原文件  - name: copy configure file    copy:       src: /root/ansible/files/httpd.conf       dest: /etc/httpd/conf/      backup: yes  #第二步修改了8080端口，将文件移回原处覆盖源文件  - name: ensure apache is running    service:      name: httpd      state: started      enabled: yesansible-playbook httpd4.yamlansible all -m shell -a &#x27;cat /etc/httpd/conf/httpd.conf|grep 8080&#x27;ansible all -m shell -a &#x27;systemctl status httpd&#x27;ansible all -m shell -a &#x27;netstat -tunlp|grep httpd&#x27;#发现修改配置，但没有生效，因为没有重启httpd应用

增加handlers和notify

vim httpd4.yaml---- hosts: all  remote_user: root  tasks:     - name: Install httpd    yum:      name: httpd      state: present  - name: copy configure file    copy:       src: /root/ansible/files/httpd.conf       dest: /etc/httpd/conf/      backup: yes    #调用触发列表里的 restart httpd任务，调用之后重启httpd，配置文件即刻生效    notify: restart httpd  - name: ensure apache is running    service:      name: httpd      state: started      enabled: yes#触发器列表  handlers:  - name: restart httpd    service:       name: httpd       state: restarted                ansible-playbook httpd4.yamlansible all -m shell -a &#x27;netstat -tunlp|grep httpd&#x27;发现端口是8080（即成功）
修改/root/ansible/files/httpd.conf ，将端口修改为8081，重新执行httpd4.yaml，并验证服务端口已经改变。
ansible-playbook httpd4.yamlansible all -m shell -a &#x27;netstat -tunlp|grep httpd&#x27;
发现端口变成8081，说明只有在任务执行状态为change时，才执行该任务调用的handler。
TAGS
tage: 添加标签 
可以指定某一个任务添加一个标签,添加标签以后,想执行某个动作可以做出挑选来执行,多个动作可以使用同一个标签
停止httpd服务
ansible all -m service -a &#x27;name=httpd  state=stopped&#x27;ansible all -m shell -a &#x27;ss -tln |grep :8081&#x27;
[root@controller ~]# vi httpd5.yaml---- hosts: all  remote_user: root  tasks:  - name: install httpd    yum:      name: httpd      state: present    tags: install #install标签  - name: set listen8080    shell: sed -i &quot;s/Listen 80/Listen 8080/g&quot; /etc/httpd/conf/httpd.conf    tags: set8080 #设置8080标签  - name: backup cofing    copy:      src: /etc/httpd/conf/httpd.conf      dest: /etc/httpd/conf/httpd.conf      backup: yes    notify: restart httpd    tags: backup #备份标签  - name: 配置 httpd 服务    service:      name: httpd      state: started      enabled: yes    tags: started #开启httpd服务标签  handlers:  - name: restart httpd    service:      name: httpd      state: restarted    tags: restart  #重启服务标签...
ansible-playbook -tstarted httpd5.yaml   #指定执行started 这个标签ansible all -m shell -a &#x27;ss -tln |grep :8080&#x27;ansible-playbook -t install,conf httpd.yaml   #指定执行install,backup 两个标签
管理变量

变量定义：key=value
示例：http_port=80
变量调用方式：
1&gt; 通过 调用变量，且变量名前后必须有空格，有时用&quot;&quot;才生效 (引号）
在playbook中定义
vars语句定义全局变量（该变量作用于整个Play）
vim test_vars.yaml---- name: test_var  hosts: all  vars:    username: test_user1  tasks:  - name:  create user via a variable file    user:      name: &quot;&#123;&#123; username &#125;&#125;&quot;          #冒号后面不能以&#123;开头，不然会报语法错误，需要加上引号。      state: present#创建一个test_user1的用户  ansible-playbook -v test_vars.yaml   ansible all -m shell -a &#x27;getent passwd test_user1&#x27;

执行结果：

课堂练习
通过playbook中定义用户名和组名，实现变量引用，创建用户和组。
vim var.yaml---- hosts: all  remote_user: root  vars:  - username: user1  - groupname: group1  tasks:  - name: create group    group:       name: &quot;&#123;&#123; groupname &#125;&#125;&quot;       state: present  - name: create user    user:       name: &quot;&#123;&#123; username &#125;&#125;&quot;       state: presentansible-playbook -v var.yamlansible all -m shell -a &#x27;getent passwd user1&#x27;ansible all -m shell -a &#x27;getent group group1&#x27;



开启防火墙ansible all -m shell -a &#x27;systemctl start firewalld&#x27;vim firewall.yaml---- hosts: all  remote_user: root  vars:  - http_port: 80  tasks:  - name: insert firewalld rule for httpd    firewalld:       port: &quot;&#123;&#123; http_port &#125;&#125;/tcp&quot;       permanent: true       state: enabled       immediate: yes    ansible-playbook -v firewall.yamlansible all -m shell -a &#x27;systemctl stop firewalld&#x27;ansible all -m shell -a &#x27;firewall-cmd --query-port=80/tcp&#x27;  
在独立的变量YAML文件中定义
当变量较多的时候，或者变量需要多个playbook重用的时候，可以把变量放到独立的文件中，通过关键字&quot;var_files&quot;把文件中定义的变量引用到playbook中。
vars_files 引用变量文件（只能所用于Play全局，不能在某个task中单独被引用）
vim var_file.yamlusername: test_user3vim test_var_file.yaml---- name: test_var_file  hosts: all  vars_files:  - /root/var_file.yaml  tasks:  - name:  create user via a variable file    user:      name: &quot;&#123;&#123; username &#125;&#125;&quot;      state: present...  ansible-playbook -v test_var_file.yaml  ansible all -m shell -a &#x27;getent passwd test_user3&#x27;
练习：将防火墙端口写入
将变量写进单独的配置文件中引用test_var_file.yaml---- hosts: all  remote_user: root  vars_files:  - /root/var_file.yaml  tasks:  - name: insert firewalld rule for httpd    firewalld:       port: &quot;&#123;&#123; http_port &#125;&#125;/tcp&quot;       permanent: true       state: enabled       immediate: yes...vim var_file.yamlhttp_port: 78ansible-playbook -v test_var_file.yamlansible all -m shell -a &#x27;firewall-cmd --query-port=78/tcp&#x27;  
远程主机上的系统变量（Facts事实）
ansible会通过setup模块来搜集主机的系统信息，这些搜集到的系统信息叫做Facts。

每个playbook在执行前都会默认执行setup模块，所以这些facts信息是可以以变量的形式被使用。
查看Facts变量
ansible all -m setup 能查看到node节点的所有信息ansible 192.168.48.101 -m setup |grep ansible_hostname或者可以使用filter过滤信息ansible all -m setup -a &#x27;filter=&quot;ansible_hostname&quot;&#x27;  查询主机名ansible all -m setup -a &#x27;filter=&quot;ansible_default_ipv4&quot;&#x27;  查询ipv4地址ansible all -m setup -a &quot;filter=ansible_memory_mb&quot;查询内存其他常用信息列出如下：ansible_all_ipv4_addresses：仅显示ipv4的信息。ansible_devices：仅显示磁盘设备信息。ansible_distribution：显示是什么系统，例：centos,suse等。ansible_distribution_major_version：显示是系统主版本。ansible_distribution_version：仅显示系统版本。ansible_machine：显示系统类型，例：32位，还是64位。ansible_eth0：仅显示eth0的信息。ansible_hostname：仅显示主机名。ansible_kernel：仅显示内核版本。ansible_lvm：显示lvm相关信息。ansible_memtotal_mb：显示系统总内存。ansible_memfree_mb：显示可用系统内存。ansible_memory_mb：详细显示内存情况。ansible_swaptotal_mb：显示总的swap内存。ansible_swapfree_mb：显示swap内存的可用内存。ansible_mounts：显示系统磁盘挂载情况。ansible_processor：显示cpu个数(具体显示每个cpu的型号)。ansible_processor_vcpus：显示cpu个数(只显示总的个数)。
使用Facts变量
vim var2.yaml---- hosts: all  remote_user: root  tasks:  - name: create log file    file:       name: /root/&#123;&#123; ansible_hostname &#125;&#125;       state: touchansible-playbook var2.yamlansible all -m shell -a &#x27;ls /root|grep node*&#x27;
复杂Facts变量的使用

方式1：使用中括号
&#123;&#123; ansible_date_time[&quot;date&quot;] &#125;&#125;
方式2：使用点号（推荐）
&#123;&#123; ansible_date_time.date &#125;&#125;
vim var2.yaml---- hosts: all  remote_user: root  tasks:  - name: 22    copy:      content: &quot;&#123;&#123; ansible_date_time.date &#125;&#125;&quot;      dest: /tmp/f1ansible-playbook var2.yamlansible all -m shell -a &#x27;cat /tmp/f1&#x27;
关闭Facts
搜集facts会消耗额外时间，可以在剧本中设置是否开启和关闭facts搜集。
开启gather_facts:yes，关闭gather_facts:no

在/etc/ansible/hosts(主机清单)中定义变量
普通变量：主机组中主机单独定义，优先级高于公共变量(单个主机 )
公共(组)变量：针对主机组中所有主机定义统一变量(一组主机的同一类别)
可以是主机级别或者是主机组级别的
定义主机级别变量
vim  /etc/ansible/hosts[all]192.168.48.101 username=test_user3   #主机级别变量192.168.48.102 username=test_user4  
编辑剧本文件
vim test_vars2.yaml---- name: test inventory vars  hosts: all  tasks:  - name:  create user via a variable file    user:      name: &quot;&#123;&#123; username &#125;&#125;&quot;      state: presentansible-playbook -v test_vars2.yaml  ansible all -m shell -a &#x27;getent passwd test_user3&#x27;ansible all -m shell -a &#x27;getent passwd test_user4&#x27;
主机组级别定义变量（相对于主机级别定义的变量，优先级较低）
vim  /etc/ansible/hosts[all]192.168.142.101 username=test_user3   #主机级别变量192.168.142.102   [all:vars]username=test_user5ansible-playbook -v test_vars2.yaml 发现第一台主机不变，第二台主机创建新的用户test_user5,证明主机组变量比主机变量优先级低
通过命令行指定变量
ansible-playbook -e 变量 剧本（优先级最高）ansible-playbook -v -e username=test_user10 test_vars2.yaml 

安装httpd服务
示例：test_vars3.yaml ---- hosts: all  remote_user: root  tasks:  - name: install package    yum:      name: &quot;&#123;&#123; pkname &#125;&#125;&quot;      state: present  - name: start service    service:      name: &quot;&#123;&#123; pkname &#125;&#125;&quot;      state: started      enabled: yesansible-playbook –e pkname=httpd test_vars3.yaml 
示例：test_vars3.yaml ---- hosts: all  remote_user: root  tasks:  - name: install package    yum:       name: &quot;&#123;&#123; pkname1 &#125;&#125;&quot;       state: present  - name: install package    yum:       name: &quot;&#123;&#123; pkname2 &#125;&#125;&quot;       state: present  ansible-playbook -e &#x27;pkname1=httpd pkname2=tree&#x27; -v test_vars3.yaml 
复杂变量的使用
数组
如果我们定义变量，而这些值都属于同一类型的元素，那么我们必定要用数组。
例如：
vim test_com_var.yaml---- hosts: all  vars:    user_name:    - test_user11    - test_user12    - test_user13    - test_user14  tasks:  - name: create users    user:      name: &quot;&#123;&#123; user_name[1] &#125;&#125;&quot;      state: present

验证：ansible-playbook -v test_com_var.yaml

注意：在用user模块建用户的时候，只能调用数组中的某一个值，不能全部调用，否则报错“用户名不合法”


字典（dictionary）
如果我们的变量信息中具备多种不同的元素时，采用字典。
例如：
vim test_com_var.yaml---- name: test_dict  hosts: all  vars:    user_info:      test_user20:        name: test_user20        shell: /bin/bash        comment: test_user20      test_user21:        name: test_user21        shell: /bin/bash        comment: test_user21  tasks:  - name: create users via dict    user:      name: &quot;&#123;&#123; user_info[&#x27;test_user20&#x27;][&#x27;name&#x27;] &#125;&#125;&quot;      shell: &quot;&#123;&#123; user_info[&#x27;test_user20&#x27;][&#x27;shell&#x27;] &#125;&#125;&quot;      state: present


变量引用的另一种写法：引用对象写法（python语法）

注意：以点作为分隔（引用对象）这种方式，可能会和python本身的语义引起冲突，所以不建议使用这种方式
注册变量
注册变量是指将一个任务（task）的输出结果定义到一个变量中，这个变量就可以在随后的任务中像普通变量一样使用。
很多情况下，注册变量用来收集shell的执行结果，结果中包含标准输入和标准输出。接下来使用shell模块执行命令将命令结果传入名为var_echo 的变量并使用debug进行检测。
register 的使用形如 register: varname，即 register模块后直接加变量名即可，而register这一行仅仅需要写在需要收集输出的那一行下即可。
案例：
vim test_com_var.yaml---- name: test_dict  hosts: all  vars:    user_info:      test_user20:        name: test_user20        shell: /bin/bash        comment: test_user20      test_user21:        name: test_user21        shell: /bin/bash        comment: test_user21  tasks:  - name: create users via dict    user:      name: &quot;&#123;&#123; user_info.test_user21.name &#125;&#125;&quot;      shell: &quot;&#123;&#123; user_info.test_user21.shell &#125;&#125;&quot;      state: presentansible-playbook test_com_var.yaml
执行结果如下：

修改剧本文件，加入debug模块
vim test_com_var.yaml- name: test_dict  hosts: all  vars:    user_info:      test_user20:        name: test_user20        shell: /sbin/nologin        comment: test_user20      test_user21:        name: test_user21        shell: /sbin/nologin        comment: test_user21  tasks:  - name: create users via dict    user:      name: &quot;&#123;&#123; user_info.test_user21.name &#125;&#125;&quot;      shell: &quot;&#123;&#123; user_info.test_user21.shell &#125;&#125;&quot;      state: present    register: user_result  - name: debug result of user creation    debug:      msg: &quot;&#123;&#123; user_result &#125;&#125;&quot;      ansible-playbook test_com_var.yaml
运行结果：

可以引用结果中的部分元素（user_result[‘uid’] / user_result.uid）
vim test_com_var.yaml- name: test_dict  hosts: all  vars:    user_info:      test_user20:        name: test_user20        shell: /sbin/nologin        comment: test_user20      test_user21:        name: test_user21        shell: /sbin/nologin        comment: test_user21  tasks:  - name: create users via dict    user:      name: &quot;&#123;&#123; user_info.test_user21.name &#125;&#125;&quot;      shell: &quot;&#123;&#123; user_info.test_user21.shell &#125;&#125;&quot;      state: present    register: user_result  - name: debug result of user creation    debug:      msg: &quot;&#123;&#123; user_result.uid &#125;&#125;&quot;        ansible-playbook test_com_var.yaml    
运行结果：

可以对输出结果进行迭代引用（用register存在多个变量中）
vim test_com_var.yaml---- name: test_dict  hosts: all  vars:    user_info:      test_user20:        name: test_user20        shell: /sbin/nologin        comment: test_user20      test_user21:        name: test_user21        shell: /sbin/nologin        comment: test_user21  tasks:  - name: create users via dict    user:      name: &quot;&#123;&#123; user_info.test_user21.name &#125;&#125;&quot;      shell: &quot;&#123;&#123; user_info.test_user21.shell &#125;&#125;&quot;      state: present    register: user_result  - name: debug result of user creation    debug:      msg: &quot;&#123;&#123; user_result.uid &#125;&#125;&quot;    register: shell_result    - name: debug result of shell    debug:      msg: &quot;&#123;&#123; shell_result &#125;&#125;&quot;        ansible-playbook test_com_var.yaml    

Ansible Vault（Ansible 保管箱）
作用：加密敏感的数据、密码等信息，通常情况下都是定义在变量内的敏感信息。
应用的情景：
1&gt; 加密变量文件（敏感数据、密码信息等）
2&gt; 加密证书
命令：ansible-vault
命令用法：
创建一个加密文件：
ansible-vault create sec.yaml

使用vim sec.ym或者cat sec.yaml只能看到加密后的内容

如何查看加密过的文件内容：

如何在剧本中调用加密文件
vim test_vault.yaml---- name: Create users via vault  hosts: all  vars_files:  - /root/sec.yaml  tasks:  - name: Create users    user:      name: &quot;&#123;&#123; username &#125;&#125;&quot;      state: present
执行剧本时报错

解决方法：
方法一：ansible-playbook 命令时候添加–ask-vault-pass参数
ansible-playbook --ask-vault-pass test_vault.yaml

方法二：ansible-playbook --vault-id @prompt test_vault.yaml（2.3之后使用，建议）

方法三：ansible-playbook --vault-password-file=pass.yaml  test_vault.yaml
（纯文本形式的密码存放在文件中，只能单行写一个密码，需要对该密码文件加强安全措施）
echo 123456 &gt; pass.yaml

解密

加密一个已存在的文件

重置加密文件的密码

编辑已存在的加密文件

Tips：如果我们使用加密文件保存变量、密码等敏感数据，最好采用隐藏文件来存放，增强安全性。


ansible-playbook --vault-password-file=.pass.yaml  test_vault.yaml

综合实践
编辑剧本文件实现以下功能：
0、设置主机组 all
1、设置变量 定义nginx服务端口为8081
2、关闭facts
3、调用service模块，卸载受控端的httpd
4、调用SELinux模块，关闭selinux
5、调用yum模块安装epel源
6、调用yum模块安装nginx
7、调用lineinfile模块修改nginx配置文件中的监听端口，使用自定义的服务端口变量，并将结果注册到 port_result
8、调用service模块启动nginx，并设置为开机自启动
9、调用debug模块，msg信息为port_result
10、验证受控端服务及端口
---- hosts: all  gather_facts: no  vars:     nginx_port: &quot;8081&quot;  tasks:     - name: uninstall httpd    yum:      name: httpd      state: absent  - name: stop selinux    selinux:      state: disabled  - name: install epel    yum:      name: epel-release      state: present  - name:     yum:      name: nginx      state: present  - name: set nginx_port    lineinfile:      path: /etc/nginx/nginx.conf      regexp: &quot;        listen       80;&quot;      line: &quot;        listen       &#123;&#123; nginx_port &#125;&#125;;&quot;    register: port_result     - name: start nginx    service:      name: nginx      state: started      enabled: yes  - name: debug msg    debug:      msg: &quot;&#123;&#123; port_result &#125;&#125;&quot;ansible-playbook  install_nginx.yaml -Cansible-playbook install_nginx.yaml  ansible all -m shell -a  &quot;ps aux |grep nginx &quot;  ansible all -m shell -a  &quot;netstat -lntp |grep nginx &quot; 


Ansible-templates
JINJA2语法简要介绍
Jinja2语言，支持的数据类型：
字符串：使用单引号或双引号数字：整数，浮点数列表：[item1, item2, ...]元组：(item1, item2, ...)字典：&#123;key1:value1, key2:value2, ...&#125;布尔型：true/false
支持的运算及操作：
算术运算：+, -, *, /, //, %, **比较操作：==, !=, &gt;, &gt;=, &lt;, &lt;=逻辑运算：and，or，not流表达式：For，If，When
Playbook的进阶应用
使用when实现条件判断
条件测试:如果需要根据变量、facts或此前任务的执行结果来做为某task执行与否的前提时要用到条件测试。剧本中不能使用if判断，需要使用when判断。
when语句：在task后添加when子句即可使用条件测试，可以使用facts或playbook中定义的变量，支持Jinja2表达式语法
示例：
tasks:  - name: &quot;shutdown RedHat flavored systems&quot;    command: /sbin/shutdown -h now    when: ansible_os_family == &quot;RedHat&quot;  当系统属于红帽系列,执行command模块，注意：&#x27;所有变量&#x27;都可以直接在条件语句中使用，而无需使用双大括号
也可以使用多个when进行多条件判断，等效于and。
vim test_when.yml---- hosts: all  tasks:  - name: copy file    copy:      src: /etc/hosts      dest: /root/hosts_when    when: ansible_hostname is match &quot;node1&quot;		#when支持通配符ansible all --listansible-playbook  test_when.ymlansible all -m shell -a &quot;ls -l /root/hosts_when&quot;
使用with_items实现迭代
迭代：当有需要重复性执行的任务时，可以使用迭代机制
&gt; 对迭代项的引用，固定变量名为&quot;item&quot;

要在task中使用with_items给定要迭代的元素列表，
&gt; 列表格式：
字符串
字典

示例：打印1、2、3
vim test_items.yml---- name: add serveral users  gather_facts: no  hosts: all  tasks:  - name: test loop    debug:       msg: &quot;name --- &#123;&#123; item &#125;&#125;&quot; #&#123;&#123; item &#125;&#125; 系统自定义变量    with_items:   ##with_items定义&#123;&#123; item &#125;&#125; 的值和个数,      一般放到模块的末尾，与模块同一缩进级别        - one    - two    - three
示例：创建用户
vim test_items.yml---- name: add serveral users  hosts: all  tasks:  - user:      name: &quot;&#123;&#123; item  &#125;&#125;&quot;  #&#123;&#123; item &#125;&#125; 系统自定义变量      state: present    with_items:           ##with_items定义&#123;&#123; item &#125;&#125; 的值和个数,一般放到模块的末尾，与模块同一缩进级别        - testuser1    - testuser2ansible all --listansible-playbook  test_items.ymlansible all -m shell -a &quot;getent passwd testuser1&quot;上面语句的功能等同于下面的语句：- name: add user testuser1  user: name=testuser1 state=present - name: add user testuser2  user: name=testuser2 state=present 
示例：拷贝多个文件
touch /root/1.txt /root/2.txtvim test_items2.yml---- name: copy serveral files  hosts: all  tasks:  - copy:      src: /root/&#123;&#123; item  &#125;&#125;      dest: /etc/&#123;&#123; item &#125;&#125;    with_items:              - 1.txt    - 2.txtansible-playbook  test_items2.ymlansible all -m shell -a &quot;ls -l /etc/*.txt&quot;
示例：迭代字典
with_items中可以使用元素还可为hashes示例：vim test_items3.yml---- name: add several users  gather_facts: no  hosts: all  tasks:  - user:      name: &quot;&#123;&#123; item.name &#125;&#125;&quot;      state: present      groups: &quot; &#123;&#123; item.groups &#125;&#125;&quot;    with_items:    - &#123; name: &#x27;testuser3&#x27;, groups: &#x27;wheel&#x27; &#125;    - &#123; name: &#x27;testuser4&#x27;, groups: &#x27;root&#x27; &#125;ansible-playbook  test_items3.ymlansible all -m shell -a &quot;getent passwd testuser3&quot;ansible all -m shell -a &quot;id testuser3&quot;ansible all -m shell -a &quot;getent passwd testuser4&quot;ansible all -m shell -a &quot;id testuser4&quot;
课堂作业：使用with—items拷贝多个文件
要求：item列表条目为字典类型，包含src、dest、mode3个键值对，使用with_items实现多个文件的拷贝，并赋予设定的权限。
touch /root/3.txt /root/4.txtvim test_items4.yml---- name: copy several files  hosts: all  tasks:  - copy:      src: &quot;&#123;&#123; item.src &#125;&#125;&quot;      dest: &quot;&#123;&#123; item.dest &#125;&#125;&quot;      mode: &quot;&#123;&#123; item.mode &#125;&#125;&quot;    with_items:              - &#123; src: &quot;/root/3.txt&quot;, dest: &quot;/root/&quot;, mode: &quot;0644&quot; &#125;    - &#123; src: &quot;/root/4.txt&quot;, dest: &quot;/root/&quot;, mode: &quot;0644&quot; &#125;ansible-playbook  test_items4.ymlansible all -m shell -a &quot;ls -l /root/*.txt&quot;
when和with items组合使用
当when和with_items一起使用的时候，每个项都会单独被when语句处理
vim test_when_items.yml---- hosts: all  tasks:  - command: echo &#123;&#123; item &#125;&#125;    with_items: [ 1,2,3,4,5,6,8,10]    when: item &gt; 5          ansible-playbook test_when_items.yml
templates 模板
templates功能：根据模板文件动态生成对应的配置文件，命名必须以.j2结尾，支持jinja2语法。
在呈现 JINJA2模板时，文件中引用的变量和表达式被替换为对应的值。模板中使用的变量可以在 Playbook 的 vars 部分中指定。可以将受管主机的事实用作模板中的变量。
分隔符使用规范：
&#123;% EXPR %&#125;：用于表达式或逻辑（如循环、判断等）&#123;&#123; EXPR &#125;&#125;：用于向最终用户输出表达式或变量的结果。在呈现时将被替换为一个或多个值，对最终用户可见。&#123;# COMMENT #&#125;，用于注释，不会出现在最终文件中。
templates的使用场景
在实际的工作中由于每台服务器的环境配置都可能不同，但是往往很多服务的配置文件都需要根据服务器环境进行不同的配置，比如Nginx最大进程数、Redis最大内存等。
为了解决这个问题可以使用Ansible的template模块，该模块和copy模块作用基本一样，都是把管理端的文件复制到客户端主机上，但是区别在于template模块可以通过变量来获取配置值，支持多种判断、循环、逻辑运算等，而copy只能原封不动的把文件内容复制过去。
示例：httpd.conf的templates模板
创建并编辑httpd.conf.j2文件
yum -y install httpdrpm -qa httpd cp /etc/httpd/conf/httpd.conf /root/httpd.conf.j2 vim /root/httpd.conf.j2 ---------42行----------Listen &#123;&#123;port&#125;&#125; ----------95行---------ServerName &#123;&#123;domain&#125;&#125; vim /etc/ansible/hosts [websrvs]192.168.142.101 port=80 domain=www.node1.com192.168.142.102 port=81 domain=www.node2.com
卸载受控机上的httpd服务
ansible websrvs -m shell -a &#x27;yum remove -y httpd&#x27;ansible websrvs -m shell -a &#x27;yum remove -y nginx&#x27;
新建yaml文件
cd /rootvim a.yaml---- hosts: websrvs  remote_user: root  vars:  - package: httpd  - service: httpd  tasks:  - name: install service    yum:      name: &quot;&#123;&#123; package &#125;&#125;&quot;      state: latest  - name: httpd.conf    template:      src: /root/httpd.conf.j2      dest: /etc/httpd/conf/httpd.conf    notify: restart service  - name: start service    service:      name: &quot;&#123;&#123; service &#125;&#125;&quot;      state: started      enabled: true  handlers:  - name: restart service    service:      name: &quot;&#123;&#123; service &#125;&#125;&quot;      state: restarted
执行yaml文件并验证
ansible-playbook a.yaml --syntax-checkansible-playbook a.yamlansible websrvs -a &#x27;systemctl status httpd&#x27;ansible websrvs -m shell -a &#x27;ss -ntl&#x27;ansible websrvs -m shell -a &#x27;netstat -ntlp |grep httpd&#x27;ansible websrvs -m shell -a &#x27;lsof -i:80&#x27;ansible websrvs -m shell -a &#x27;lsof -i:81&#x27;
tamplates-for(循环)
语法：
&#123;% for 变量 in 列表 %&#125;&#123;&#123; 文本内容调用变量 &#125;&#125;&#123;% endfor %&#125;
示例：使用for循环遍历调用users列表变量的元素
&#123;%  for user in users %&#125;&#123;&#123;  user &#125;&#125;&#123;% endfor %&#125;
示例：yaml文件中变量的调用
编写yaml文件 jinja2_for.yml
vim jinja2_for.yml---- name: jinja2_for example  hosts: all  remote_user: root  vars:    users:    - user1    - user2  tasks:  - name: Copy template    template:       src: /root/users.j2      dest: /root/users 
编写/root/users.j2文件
vim /root/users.j2&#123;% for user in users %&#125;username: &#123;&#123; user &#125;&#125;&#123;% endfor %&#125;
执行并验证
ansible-playbook jinjia2_for.yml --syntax-checkansible-playbook jinjia2_for.ymlansible all -m shell -a &quot;cat /root/users&quot;
扩展示例：
以下示例模板使用for语句逐一运行users变量中的所有值，将user替换为各个值，但值为root时除外。
vim users.j2&#123;# for statement #&#125;&#123;% for user in users if not user ==&quot;root&quot; %&#125;User number &#123;&#123; loop.index &#125;&#125;- &#123;&#123; user &#125;&#125;&#123;% endfor %&#125;
loop.index变量扩展至循环当前所处的索引号。它在循环第一次执行时值为1，每一次迭代递增1.
ansible-playbook jinjia2_for.yml --syntax-checkansible-playbook jinjia2_for.ymlansible all -m shell -a &quot;cat /root/users&quot;
示例：事实变量的调用
编写yaml文件  jinja2_for2.yml
vim jinja2_for2.yml---- name: jinja2_for example2  hosts: all  remote_user: root  vars:    users:    - user1    - user2  tasks:  - name: Copy template    template:       src: /root/host.j2      dest: /root/hosts 
编写/root/host.j2文件
vim /root/host.j2&#123;% for host in groups[&#x27;websrvs&#x27;] %&#125;&#123;&#123; ansible_facts.default_ipv4.address &#125;&#125;&#123;&#123; ansible_facts.fqdn &#125;&#125;&#123;% endfor %&#125;
执行并验证
ansible-playbook jinjia2_for2.yml --syntax-checkansible-playbook jinjia2_for2.ymlansible websrvs -m shell -a &quot;cat /root/hosts&quot;
tamplates-if（判断）
Jinja2使用 if 语句来提供条件控制。如果满足条件，允许在文件中添加一行内容。
语法：
&#123;% if 条件 %&#125;&#123;&#123; 语句 &#125;&#125;&#123;% endif %&#125;
示例：
编写yaml文件 jinja2_if.yml
vim jinja2_if.yml---- name: jinja2_if example  hosts: websrvs  remote_user: root  tasks:  - name: Copy template    template:       src: /root/host2.j2      dest: /root/hosts2 
编辑host2.j2文件
vim /root/host2.j2&#123;% if ansible_facts.default_ipv4.address ==&#x27;192.168.142.101&#x27; %&#125;&#123;&#123; ansible_facts.default_ipv4.address &#125;&#125;&#123;&#123; ansible_facts.fqdn &#125;&#125;&#123;% endif %&#125;
执行并验证
ansible-playbook jinja2_if.yml --syntax-checkansible-playbook jinja2_if.ymlansible all -m shell -a &quot;cat /root/hosts2&quot;
综合案例：nginx templates
0、编辑主机清单，组websrvs，包含2台受控主机
1、主控端安装ngxin、拷贝nginx配置文件为nginx.conf.j2模板文件。创建nginx首页模版，命名为html.j2,引用实事变量：主机名，文件内容格式如： Welcome to 
2、编写test_template.yaml文件，要求tasks
​      1）安装epel源 2）安装nginx 3）拷贝nginx.conf.j2模板文件为受控主机的nginx配置文件4）拷贝html.j2模板文件为受控主机的nginx首页文件4）开启服务
3、校验playbook语法并执行，验证受控主机的nginx进程数\服务端口\首页
4、修改nginx.conf.j2模板文件，配置 worker_processes数量为实事变量：受控主机处理器vcpu个数的两倍，保存
5、修改test_template.yaml文件，添加notify和handlers，在配置文件变化时，重启nginx
6、校验playbook语法并执行，验证受控主机的nginx进程数
7、修改hosts文件为每台主机定义服务端口变量 第一台 8082，第二台8083
8、修改test_template.yaml文件，修改监听端口行，增加主机端口变量的引用
9、校验playbook语法并执行，验证受控主机的nginx服务端口
10、修改test_template.yaml，增加端口变量定义，端口88
11、校验playbook语法并执行，验证受控主机的nginx服务端口
1、主控端安装ngxin、拷贝nginx配置文件为nginx.conf.j2模板文件
yum install -y nginxcp /etc/nginx/nginx.conf /root/nginx.conf.j2echo &quot;welcome to &#123;&#123; ansible_hostname  &#125;&#125;&quot; &gt; html.j2ansible all -m setup |grep hostnameansible all -m setup |grep vcpu
2、创建nginx首页模版，命名为html.j2,引用实事变量：主机名
内容格式如： Welcome to 
2、编写test_template.yaml文件
vim test_template.yaml---- hosts: websrvs  remote_user: root  tasks:  - name: install epel    yum:      name: epel-release  - name: install package    yum:      name: nginx  - name: copy template    template:      src: /root/nginx.conf.j2      dest: /etc/nginx/nginx.conf  - name: copy template    template:      src: /root/html.j2      dest: /usr/share/nginx/html  - name: copy html    template:      src: /root/html.j2      dest: /usr/share/nginx/html/index.html      - name: start service    service:      name: nginx      state: started      enabled: yes
执行并验证
ansible-playbook test_template.yaml  --syntax-checkansible-playbook test_template.yaml  ansible all -m shell -a &#x27;systemctl status nginx&#x27;ansible all -m shell -a &#x27;ss -ntpl|grep nginx&#x27;#可以查看到进程，每个cpu一个ansible all -m shell -a &#x27;ps aux|grep nginx&#x27;
3、修改template文件，修改进程数为cpu内核的2倍
ansible websrvs -m setup|grep &quot;cpu&quot;vim nginx.conf.j2修改第6行worker_processes &#123;&#123; ansible_processor_vcpus*2  &#125;&#125;    #worker_processes auto
修改test_template.yaml文件，添加notify和handlers，在配置文件变化时，重启nginx
vim test_template.yaml---- hosts: websrvs  remote_user: root  tasks:  - name: install package    yum:      name: nginx  - name: copy template    template:      src: /root/nginx.conf.j2      dest: /etc/nginx/nginx.conf    notify:    - restart service  - name: copy html    template:      src: /root/html.j2      dest: /usr/share/nginx/html/index.html      - name: start service    service:      name: nginx      state: started      enabled: yes  handlers:  - name: restart service    service:      name: nginx      state: restarted
执行并验证
ansible-playbook test_template.yaml#可以查看到进程，ansible all -m shell -a &#x27;ps aux|grep nginx&#x27; #查看nginx进程数为cpu核数的2倍
检验nginx配置文件是否存在语法错误
nginx -t
nginx和httpd服务，web服务保证只有一个运行。
4、使用主机变量，修改服务端口
修改hosts文件增加端口变量
#使用主机变量#修改nginx对应的端口vim /etc/ansible/hosts[websrvs]192.168.142.101 http_port=8083192.168.142.102 http_port=8084
修改模板文件，增加端口引用
vim nginx.conf.j2修改39、40行server&#123;    listen   &#123;&#123; http_port &#125;&#125; ;    listen   [::]:&#123;&#123;  http_port  &#125;&#125; ;&#125;
执行并验证
ansible-playbook test_template.yamlansible websrvs -m shell -a &#x27;ss -ntpl|grep nginx&#x27;
5、使用playbook变量
修改test_template.yaml，增加端口信息
vim test_template.yaml---- hosts: websrvs  remote_user: root  vars：  - http_port: 88  tasks:  - name: install package    yum:      name: nginx  - name: copy template    template:      src: /root/nginx.conf.j2      dest: /etc/nginx/nginx.conf    notify:    - restart service  - name: copy html    template:      src: /root/html.j2      dest: /usr/share/nginx/html/index.html     - name: start service    service:      name: nginx      state: started      enabled: yes  handlers:  - name: restart service    service:      name: nginx      state: restarted
执行并验证
ansible-playbook test_template.yamlansible websrvs -m shell -a &#x27;ss -ntpl|grep nginx&#x27;#发现端口变成88
6、使用命令行变量
ansible-playbook -e &quot;http_port=99&quot;  test_template.yamlansible websrvs -m shell -a &#x27;ss -ntpl|grep nginx&#x27;#发现端口变成99
Roles
·由来: ansible自动化运行，基础由AD-Hoc命令来完成，在命令变多时，产生了playbook进行管理任务，简单任务使用playcook可以轻松处理，但是有复杂任务时单个playbook不可以胜任了，这时需要把多个playbook进行组合，少量用include将剧本中任务互相关联即可完成，但是playbook还在增多的情况时就不方便管理了，这时引入roles对playbook进行有效组织就十分必要了· Roles:角色，是ansible自1.2版本开始引入的新特性·目的:用于层次性，结构化地组织playbook, roles能够根据层次型结构自动装载变量、文件、任务、模块及触发器·方法: roles通过分别将放置于变量、文件、任务、模块及触发器单独的目录中，并可以便捷地include它们的一种机制·应用:角色一般用于基于主机构建服务的场景中、但也可以是用于构建守护进程等场景中
roles默认路径设置
/etc/ansible/ansible.cfgroles_path= /etc/ansible/roles
Roles各目录结构及作用
每个角色，以特定的层级目录结构进行组织roles目录结构：playbook.yml  调用角色roles/  project/ (角色名称)    tasks/    files/    vars/    templates/    handlers/    default/ 不常用，设定默认变量时使用此目录中的main.yml文件    meta/    不常用，定义当前角色的特殊设定及其依赖关系,至少应该包含一个名为main.yml的文件；其它文件需在此文件中通过include进行包含各目录的作用：/roles/project/ :项目名称,有以下子目录，project可以是mysql\httpd\nginx\memcached等    files/ ：存放由copy或script模块等调用的文件    templates/：template模块查找所需要模板文件的目录    tasks/：定义task,role的基本元素，至少应该包含一个名为main.yml的文件, 其定义了此角色的任务列表.             在handler中使用include包含的其它的handler文件也应该位于此目录中；    handlers/：至少应该包含一个名为main.yml的文件；用于定义此角色用到的各handler；               其它的文件需要在此文件中通过include进行包含    vars/：定义变量，至少应该包含一个名为main.yml的文件,；           其它的文件需要在此文件中通过include进行包含    meta/：定义当前角色的特殊设定及其依赖关系,至少应该包含一个名为main.yml的文件，           其它文件需在此文件中通过include进行包含, ansible1.3及其以后的版本才支持；    defaults/：为当前角色设定默认变量时使用此目录；应当包含一个main.yml文件
创建role框架
您可以使用标准Linux命令创建新角色所需的所有子目录和文件。
cd /etc/ansible/roles/mkdir  httpdcd httpdmkdir  tasks  handlers  vars  meta  defaults templates files 
或者可以运行ansible-galaxy init来创建新角色的目录结构。指定角色的名称作为命令的参数，该命令在当前工作目录中为新角色创建子目录。
cd /etc/ansible/rolesansible-galaxy init httpdtree httpd[root@controller roles]# tree httpdhttpd      #具体的⾓⾊项⽬名称，   ⽐如nginx、tomcat、php  (⾃由设置)├── defaults  #⽤于为当前⾓⾊设定默认变量，  此⽬录应当包含⼀个main.yml⽂件│   └── main.yml  #类似代码中的主函数，  进⾏统⼀管理├── files      #⽤来存放由copy模块或script模块等模块调⽤的⽂件├── handlers    #⽤于定义此⾓⾊中触发条件时执⾏的动作，  此⽬录应当包含⼀个main.yml⽂件│   └── main.yml├── meta      #⽤于定义此⾓⾊的特殊设定及其依赖关系，  此⽬录应当包含⼀个main.yml⽂件│   └── main.yml├── README.md   #说明⽂件├── tasks       #⽤于定义当前⾓⾊的任务列表，  此⽬录应当包含⼀个main.yml⽂件│   └── main.yml├── templates  #⽤来存放jinjia2模板，template模块会⾃动在此⽬录中寻找jinjia2模板⽂件├── tests   #⽤于存放测试role本⾝功能的playbook和主机定义⽂件，  在开发测试阶段⽐较常⽤ ,此⽬录应当包含⼀个main.yml⽂件和⾃⾝资源设定invetory│   ├── inventory│   └── test.yml└── vars    #⽤于定义此⾓⾊⽤到的变量，  此⽬录应当包含⼀个main.yml⽂件    └── main.yml
实验任务：安装httpd服务
原始的playbook版本
1、制作主页
echo hi &gt; index.html
2、拷贝本机httpd的配置文件为httpd.conf.j2模版，并修改
cp /etc/httpd/conf/httpd.conf  httpd.conf.j2
vim  httpd.j2   #42行 修改为
&#123;% if http_port is defined %&#125;Listen &#123;&#123; ansible_facts.default_ipv4.address &#125;&#125;:&#123;&#123; http_port &#125;&#125;&#123;% endif %&#125;
3、编写playbook文件，
创建变量http_port: 8080
执行任务：
1)安装httpd
2)拷贝主页
3)拷贝配置（做触发器）
4)防火墙放通自定义的端口
- name: firewalld configuration  firewalld:      port: &quot;&#123;&#123;  http_port &#125;&#125;/tcp&quot;    permanent: yes     immediate: yes     state: enabled  when: http_port is defined
5)开启服务
---                                                                                                                                                                           - hosts: all   remote_user: root  vars:    http_port: 8080  tasks:  - name: install httpd package    yum:      name: httpd      state: present  - name: create a web content    copy:      src: index.html      dest: /var/www/html/index.html  - name: config file    template:      src: httpd.conf.j2      dest: /etc/httpd/conf/httpd.conf    notify: restart_httpd    when: http_port is defined                                                                                                                                                  - name: firewalld configuration    firewalld:                                                                         port: &quot;&#123;&#123;  http_port &#125;&#125;/tcp&quot;      permanent: yes       immediate: yes       state: enabled    when: http_port is defined  - name: start service    service:       name: httpd       state: started       enabled: yes  handlers:  - name: restart_httpd    service:      name: httpd      state: restarted
验证端口及主页
ansible all -m shell -a “ss -tunlp|grep httpd”
任务分析：
1.配置 httpd 的时候，可能存在配置文件，配置文件可能含有变量
2.必要变量的定义
3.源码文件的定义
创建httpd角色框架
ansible-galaxy init httpd
查看目录结构
[root@controller roles]# tree httpdhttpd├── defaults│  └── main.yml├── files├── handlers│   └── main.yml├── meta│   └── main.yml├── README.md├── tasks│   └── main.yml├── templates├── tests│   ├── inventory│   └── test.yml└── vars    └── main.yml
部署完善httpd角色框架
cd roles/httpd/tasks/touch install.yml conf_template.yml service.yml index.yml   httpd_firewalld.yml
定义分任务(tasks/中存放)
vim install.yml- name: install httpd package  yum:     name: httpd    state: present     vim conf_template.yml- name: config file  template:     src: httpd.conf.j2     dest: /etc/httpd/conf/httpd.conf   notify: restart_httpd  when: http_port is defined         vim service.yml- name: start service  service:     name: httpd     state: started     enabled: yes    vim  index.yml- name: create a web content  copy:    src: index.html    dest: /var/www/html/index.htmlvim httpd_firewalld.yml- name: firewalld configuration  firewalld:    port: &quot;&#123;&#123;  http_port &#125;&#125;/tcp&quot;    permanent: yes    immediate: yes    state: enabled  when: http_port is defined
定义主任务(tasks/main.yml)
创建main.yml主控文件,调用以上单独的yml文件,main.yml定义了谁先执行谁后执行的顺序vim main.yml- include: install.yml- include: index.yml- include: conf_template.yml- include: httpd_firewalld.yml- include: service.yml
定义变量（vars/main.yml）
vim /etc/ansible/roles/httpd/vars/main.yml---#vars file for httpdhttp_port: 8080
定义首页文件（files/index.html）
cd    /etc/ansible/roles/httpd/files/vim index.html&lt;h1&gt; welcome to wd home &lt;\h1&gt;
定义模板(templates/httpd.conf.j2 )
yum -y install httpdcp /etc/httpd/conf/httpd.conf  /etc/ansible/roles/httpd/templates/httpd.conf.j2 vim templates/httpd.conf.j2将LISTEN 80 行修改为以下内容&#123;% if http_port is defined %&#125;Listen &#123;&#123; ansible_facts.default_ipv4.address &#125;&#125;:&#123;&#123; http_port &#125;&#125;&#123;% endif %&#125;
定义角色处理程序（handlers/mail.yml）
vim /etc/ansible/roles/httpd/handlers/main.yml- name: restart_httpd  service:     name: httpd     state: restarted
调用角色，配置httpd服务（roles/role_httpd.yml）
cd /etc/ansidle/rolesvim role_httpd.yml---# httpd role- name: httpd deployment  hosts: websrvs  remote_user: root        roles:       #调用角色  - httpd   
检查语法及冒烟运行
ansible-playbook role_httpd.yml -C
正式执行
ansible-playbook role_httpd.yml
验证服务
ansible all -m shell -a &quot;ss -tunlp |grep httpd&quot;curl 192.168.142.101:8080

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Centos 7</tag>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title>Ceph集群分布式集群</title>
    <url>/posts/43453/</url>
    <content><![CDATA[
Ceph集群分布式集群
本篇教程针对外部ceph集群介绍教程，如果是基于K8S内部ceph请前往基于K8S1.28.2实验rook部署ceph - 严千屹博客 (qianyios.top)
机器拓扑



主机名
ip
硬盘1
硬盘2
内存
cpu
软件
OS




ceph1
192.168.48.101
100G
100G
2G
1v
Docker 24.0.7epel最新版本ceph（reef、18.2.0）
Centos stream 9


ceph2
192.168.48.102
100G
100G
2G
1v
Docker 24.0.7epel最新版本ceph（reef、18.2.0）
Centos stream 9


ceph3
192.168.48.103
100G
100G
2G
1v
Docker 24.0.7epel最新版本ceph（reef、18.2.0）
Centos stream 9



已经测试过了，最小配置，可以满足毕设需求，后面可自行根据需要调整

基础配置
系统基础配置
配置主机名
ceph1
hostnamectl set-hostname ceph1 &amp;&amp; bash
ceph2
hostnamectl set-hostname ceph2 &amp;&amp; bash
ceph3
hostnamectl set-hostname ceph3 &amp;&amp; bash
操作节点[所有节点]
#关闭防火墙和selinuxsed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsetenforce 0systemctl disable --now firewalld#设置时间同步yum install chrony -ysystemctl start chronydsystemctl enable chronydchronyc sourceschronyc sources#配置hostscat &lt;&lt; EOF &gt; /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.48.101 ceph1192.168.48.102 ceph2192.168.48.103 ceph3EOF#配置yummkdir repo.bakmv /etc/yum.repos.d/* repo.bak/cat &gt; /etc/yum.repos.d/centos9.repo&lt;&lt; &quot;EOF&quot;[BaseOS]name=Baseosbaseurl=https://mirrors.aliyun.com/centos-stream/9-stream/BaseOS/x86_64/osgpgcheck=0enabled=1[AppStream]name=AppStreambaseurl=https://mirrors.aliyun.com/centos-stream/9-stream/AppStream/x86_64/os/gpgcheck=0enabled=1[extras-common]name=extras-commonbaseurl=https://mirrors.aliyun.com/centos-stream/SIGs/9-stream/extras/x86_64/extras-common/gpgcheck=0enabled=0[ceph-reef]name=ceph-reefbaseurl=https://mirrors.aliyun.com/ceph/rpm-reef/el9/x86_64/gpgcheck=0enabled=1[epel]name=epelbaseurl=https://mirrors.aliyun.com/epel/9/Everything/x86_64/gpgcheck=0enabled=1[ceph-reef-noarch]name=ceph-reef-noarchbaseurl=https://mirrors.aliyun.com/ceph/rpm-reef/el9/noarch/gpgcheck=0enabled=1EOF# 内核参数设置：开启IP转发，允许iptables对bridge的数据进行处理 cat &lt;&lt; EOF &gt; /etc/sysctl.conf net.ipv4.ip_nonlocal_bind = 1net.ipv4.ip_forward = 1EOFcat &lt;&lt; EOF &gt; /etc/sysctl.d/ceph.conf kernel.pid_max = 4194303 vm.swappiness = 0 EOFsysctl -p#更新yum源dnf clean all &amp;&amp; dnf makecache#安装软件dnf install vim net-tools wget lsof python3 yum-utils device-mapper-persistent-data lvm2 -y
配置ssh免密
操作节点[ceph1]
yum install -y sshpass cat &gt; sshmianmi.sh &lt;&lt; &quot;EOF&quot;#!/bin/bash# 目标主机列表hosts=(&quot;ceph1&quot; &quot;ceph2&quot; &quot;ceph3&quot;)  #就改这个# 主机密码password=&quot;123456&quot;                    #就改这个# 生成 SSH 密钥对ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa# 循环遍历目标主机for host in &quot;$&#123;hosts[@]&#125;&quot;do    # 复制公钥到目标主机    sshpass -p &quot;$password&quot; ssh-copy-id -o StrictHostKeyChecking=no &quot;$host&quot;        # 验证免密登录    sshpass -p &quot;$password&quot; ssh -o StrictHostKeyChecking=no &quot;$host&quot; &quot;echo &#x27;免密登录成功&#x27;&quot;doneEOFsh sshmianmi.sh
安装docker
操作节点[所有节点]
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repodnf install -y docker-ce docker-ce-cli containerd.io docker-compose-pluginsystemctl enable --now dockermkdir -p /etc/dockertee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123;  &quot;registry-mirrors&quot;: [&quot;https://pw860av8.mirror.aliyuncs.com&quot;]&#125;EOFsystemctl daemon-reloadsystemctl restart dockerdocker info
安装cephadm
操作节点[所有节点]
dnf install --assumeyes cephadmwhich cephadm
安装ceph集群
启动一个集群
操作节点[ceph1]
在 ceph1 上启动 cephadm bootstrap
cephadm bootstrap --mon-ip 192.168.48.101
上述指令会为我们完成以下工作：

创建mon
创建ssh key并且添加到 /root/.ssh/authorized_keys 文件
将集群间通信的最小配置写入/etc/ceph/ceph.conf
将client.admin管理secret密钥的副本写入/etc/ceph/ceph.client.admin.keyring。
将公用密钥的副本写入/etc/ceph/ceph.pub

执行结果如下：
框框中的是ceph页面的账号密码

访问页面并用上面的账号密码登入，就会提示你修改密码，修改成你自己的密码
https://192.168.48.101:8443/

运行ceph查看集群健康状态
[root@ceph1 ~]# ceph -sbash: ceph: command not found
这里会显示报错，我们要在所有节点需要安装ceph-common
操作节点[所有节点]
cephadm add-repo --release reefdnf install -y liburing# 安装ceph-common包dnf install -y librbd1 ceph-common# 查看ceph-common是否正常安装ceph -v
再次在ceph1查看集群健康状况
ceph -s

这里有报错提示HEALTH_WARN暂时不用管
添加ceph节点
操作节点[ceph1]
#拷贝ceph1的Ceph 集群的公钥至其他节点，以便加入集群ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph2ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph3# 添加ceph节点ceph orch host add ceph2 192.168.48.102ceph orch host add ceph3 192.168.48.103删除可以用ceph orch host rm ceph3# 查看现在的节点情况ceph orch host ls# 添加label后可以允许该节点运行ceph cli（比如cephadm shell命令）ceph orch host label add ceph2 _adminceph orch host label add ceph3 _admin# 设置 mon 节点ceph orch apply mon 3ceph orch apply mon ceph1,ceph2,ceph3# 查看 mon 详情[root@ceph1 ~]#  ceph mon dumpepoch 3fsid 233c64c8-9e76-11ee-86fc-000c2965f5d0last_changed 2023-12-19T14:13:02.000188+0000created 2023-12-19T13:56:10.829244+0000min_mon_release 18 (reef)election_strategy: 10: [v2:192.168.48.101:3300/0,v1:192.168.48.101:6789/0] mon.ceph11: [v2:192.168.48.103:3300/0,v1:192.168.48.103:6789/0] mon.ceph32: [v2:192.168.48.102:3300/0,v1:192.168.48.102:6789/0] mon.ceph2dumped monmap epoch 3# 设置 mgr 节点ceph orch apply mgr 3ceph orch apply mgr ceph1,ceph2,ceph3[root@ceph1 ~]#  ceph -s  cluster:    id:     233c64c8-9e76-11ee-86fc-000c2965f5d0    health: HEALTH_WARN            OSD count 0 &lt; osd_pool_default_size 3  services:    mon: 3 daemons, quorum ceph1,ceph3,ceph2 (age 7m)    mgr: ceph1.gtejkn(active, since 18m), standbys: ceph2.rvzimn, ceph3.pbajtu    osd: 0 osds: 0 up, 0 in  data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   0 B used, 0 B / 0 B avail    pgs:
添加OSD
操作节点[ceph1]

ceph orch daemon add osd ceph1:/dev/nvme0n2ceph orch daemon add osd ceph2:/dev/nvme0n2ceph orch daemon add osd ceph3:/dev/nvme0n2
查看状态
[root@ceph1 ~]# ceph -s  cluster:    id:     233c64c8-9e76-11ee-86fc-000c2965f5d0    health: HEALTH_OK  services:    mon: 3 daemons, quorum ceph1,ceph3,ceph2 (age 25m)    mgr: ceph1.gtejkn(active, since 36m), standbys: ceph2.rvzimn, ceph3.pbajtu    osd: 3 osds: 3 up (since 20s), 3 in (since 33s)  data:    pools:   1 pools, 1 pgs    objects: 2 objects, 449 KiB    usage:   81 MiB used, 300 GiB / 300 GiB avail    pgs:     1 active+clean[root@ceph1 ~]#
至此ceph集群部署成果
ceph存储的使用



存储类型
特征
应用场景
典型设备




块存储（RBD）
存储速度较快 不支持共享存储 [ReadWriteOnce]
虚拟机硬盘
硬盘 Raid


文件存储（CephFS）
存储速度慢（需经操作系统处理再转为块存储） 支持共享存储 [ReadWriteMany]
文件共享
FTP NFS


对象存储（Object）
具备块存储的读写性能和文件存储的共享特性 操作系统不能直接访问，只能通过应用程序级别的API访问
图片存储 视频存储
OSS



块存储(RBD)
配置rbd
操作节点[ceph1]
------------------------------------------------------#创建名为qianyios-rbd的存储池poolceph osd pool create qianyios-rbd 128 128 [root@ceph1 ~]# ceph osd pool create qianyios-rbd 128 128pool &#x27;qianyios-rbd&#x27; created#删除pool用这个ceph osd pool delete qianyios-rbd qianyios-rbd --yes-i-really-really-mean-it------------------------------------------------------#查看列表ceph osd pool ls[root@ceph1 ~]# ceph osd pool ls.mgrqianyios-rbd------------------------------------------------------#将类型为存储池的pool（qianyios-rbd）转换为rbd类型ceph osd pool application enable qianyios-rbd rbd[root@ceph1 ~]# ceph osd pool application enable qianyios-rbd rbdenabled application &#x27;rbd&#x27; on pool &#x27;qianyios-rbd&#x27;------------------------------------------------------#初始化qianyios-rbd存储池rbd pool init qianyios-rbd------------------------------------------------------#创建一个名为 &quot;qy-rbd-img&quot; 的 RBD 镜像，存储在 &quot;qianyios-rbd&quot; 存储池中，镜像的大小为 10GB。rbg create -p qianyios-rbd --image qy-rbd-img --size 10G或者（二选一）rbd create --size 10G qianyios-rbd/qy-rbd-img#删除镜像用这个rbd rm qianyios-rbd/qy-rbd-img------------------------------------------------------#查看创建状态rbd ls qianyios-rbdrbd info qianyios-rbd/qy-rbd-img[root@ceph1 ~]# rbd ls qianyios-rbdqy-rbd-img[root@ceph1 ~]# rbd info qianyios-rbd/qy-rbd-imgrbd image &#x27;qy-rbd-img&#x27;:        size 10 GiB in 2560 objects        order 22 (4 MiB objects)        snapshot_count: 0        id: 8589e2720f1f        block_name_prefix: rbd_data.8589e2720f1f        format: 2        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten        op_features:        flags:        create_timestamp: Thu Dec 21 23:09:33 2023        access_timestamp: Thu Dec 21 23:09:33 2023        modify_timestamp: Thu Dec 21 23:09:33 2023------------------------------------------------------# 创建rbd 用户key（验证文件）#创建一个名为 &quot;qianyios-user&quot; 的客户端，并为其授予读写qianyios-rbd存储池的权限ceph auth get-or-create client.qianyios-user mon &#x27;profile rbd&#x27; osd &#x27;profile rbd pool=qianyios-rbd&#x27; mgr &#x27;profile rbd pool=qianyios-rbd&#x27;# output会输出以下内容[client.qianyios-user]        key = AQDKVYRl1nZ8ARAAvhXZojmroyy+YIb9dnTGSw==ceph auth get client.qianyios-user -o /root/ceph.client.qianyios-user.keyring[root@ceph1 ~]# cat /root/ceph.client.qianyios-user.keyring[client.qianyios-user]        key = AQDKVYRl1nZ8ARAAvhXZojmroyy+YIb9dnTGSw==        caps mgr = &quot;profile rbd pool=qianyios-rbd&quot;        caps mon = &quot;profile rbd&quot;        caps osd = &quot;profile rbd pool=qianyios-rbd&quot;------------------------------------------------------
!讲解一下这个创建用户key（验证文件）


讲解：通过这个命令，会创建一个用户，并分别指定这个用户对mon、osd、mgr的权限。如果不加pool=xxx，则这个用户能管理整个守护进程。


mon的权限必须是mon ‘profile rbd’，因为Mon服务本身就是只读的，你再限制一个只读权限，脱裤子放屁。


假设你写了 osd ‘profile rbd’，则用户能管理所有pool的osd设备。


假设你写了osd ‘profile rbd pool=xxx’，则这个用户可以读写名叫xxx的pool的osd设备。


假设你写了osd ‘profile rbd-read-only pool=xxx’，则这个用户只能读取名叫xxx的pool中的osd数据，不能写入。


mgr同理，但是最好不要写mgr ‘profile rbd’，否则A池的管理员能够关闭B池的librados接口，那就糟糕了。


举例：

创建一个volumes用户，该用户对volumes池拥有读写权限

ceph auth get-or-create client.volumes mon &#x27;profile rbd&#x27; osd &#x27;profile rbd pool=volumes&#x27; mgr &#x27;profile rbd pool=volumes&#x27;

创建一个volumes用户，该用户对volumes池拥有只读权限。

ceph auth get-or-create client.volumes mon &#x27;profile rbd&#x27; osd &#x27;profile rbd-read-only pool=volumes&#x27; mgr &#x27;profile rbd pool=volumes&#x27;
客户端挂载
这里新开一台客户机centos stream 9，这个客户端可以是你的任何一台机要挂载的机子
添加yum
操作节点[test]测试机
mkdir repo.bakmv /etc/yum.repos.d/* repo.bak/cat &gt; /etc/yum.repos.d/centos9.repo&lt;&lt; &quot;EOF&quot;[BaseOS]name=Baseosbaseurl=https://mirrors.aliyun.com/centos-stream/9-stream/BaseOS/x86_64/osgpgcheck=0enabled=1[AppStream]name=AppStreambaseurl=https://mirrors.aliyun.com/centos-stream/9-stream/AppStream/x86_64/os/gpgcheck=0enabled=1[extras-common]name=extras-commonbaseurl=https://mirrors.aliyun.com/centos-stream/SIGs/9-stream/extras/x86_64/extras-common/gpgcheck=0enabled=0[ceph-reef]name=ceph-reefbaseurl=https://mirrors.aliyun.com/ceph/rpm-reef/el9/x86_64/gpgcheck=0enabled=1[epel]name=epelbaseurl=https://mirrors.aliyun.com/epel/9/Everything/x86_64/gpgcheck=0enabled=1[ceph-reef-noarch]name=ceph-reef-noarchbaseurl=https://mirrors.aliyun.com/ceph/rpm-reef/el9/noarch/gpgcheck=0enabled=1EOFdnf clean all &amp;&amp; dnf makecache
客户端安装cephadm和ceph-common
操作节点[test]测试机
dnf install -y cephadm ceph-common
拷贝ceph1的证书至客户端
把ceph1创建的ceph.client.qianyios-user.keyring 和ceph.conf复制到客户端（192.168.48.128）
scp /root/ceph.client.qianyios-user.keyring /etc/ceph/ceph.conf 192.168.48.128:/etc/ceph/

[root@test ~]# ll /etc/ceph/total 12-rw-r--r-- 1 root root 185 Dec 21 23:45 ceph.client.qianyios-user.keyring-rw-r--r-- 1 root root 283 Dec 21 23:45 ceph.conf-rw-r--r-- 1 root root  92 Dec 12 08:01 rbdmap[root@test ~]#
开始挂载
lsblk

#挂载块存储rbd map qianyios-rbd/qy-rbd-img --keyring /etc/ceph/ceph.client.qianyios-user.keyring --id qianyios-user-----/dev/rbd0-----lsblk

此时的/dev/rbd0并不能使用，我们需要格式化，并且挂载到本机
#格式化mkfs.xfs /dev/rbd0

#创建挂载点mkdir /opt/qianyios-rbdmount /dev/rbd0 /opt/qianyios-rbdecho &quot;qianyios-rbd&quot; &gt; /opt/qianyios-rbd.txtcat /opt/qianyios-rbd.txt

取消挂载
umount /opt/qianyios-rbdrbd unmap qianyios-rbd/qy-rbd-imglsblk

文件存储(CephFS)
创建cephfs
部署元数据服务器，创建一个CephFS(文件系统), 名字为qianyios-fs
ceph fs volume create qianyios-fs --placement=&quot;3 ceph1 ceph2 ceph3&quot;# 查看状态ceph fs volume lsceph orch ps --daemon-type mdsceph fs status qianyios-fsceph mds stat

设置nfs高可用
先生成nfs和nfs_ingress配置文件。其中nfs_ingress是为了nfs高可用配置的。
cat &lt;&lt;EOF &gt; nfs.yamlservice_type: nfsservice_id: nkonfsplacement:  hosts:    - ceph1    - ceph2    - ceph3spec:  port: 2048EOFcat &lt;&lt;EOF &gt;nfs_ingress.yamlservice_type: ingressservice_id: nfs.nkonfsplacement:  count: 3spec:  backend_service: nfs.nkonfs  frontend_port: 2049  monitor_port: 9001  virtual_ip: 192.168.48.200/24#高可用vipEOF
ceph orch apply -i nfs.yamlceph orch apply -i nfs_ingress.yamlceph orch ls --service_name=nfs.nkonfsceph orch ls --service_name=ingress.nfs.nkonfsceph nfs cluster lsceph nfs cluster info nkonfs

导出nfs
ceph nfs export create cephfs --cluster-id nkonfs --pseudo-path /qy-fs --fsname qianyios-fs# 查看详情ceph nfs export info nkonfs /qy-fs

客户端挂载NFS
注意：只支持 NFS v4.0+ 的协议。
由于showmount 不支持 NFS v4，纯 NFSv4又不提供其他获取导出列表的方法，只能期望挂载的时候命令不要输错了。
在客户端执行以下命令：
dnf install -y nfs-utilsmkdir /qy-fs-datamount -t nfs -o nfsvers=4.1,proto=tcp 192.168.48.200:/qy-fs /qy-fs-data

测试文件写入
tar zcf etc.tar.gz /etc #测试文件写入cp etc.tar.gz /qy-fs-data/ll /qy-fs-data/ #生成500M大文件写入dd if=/dev/zero of=/qy-fs-data/testfile bs=1M count=500#查看分区使用df -h /fsdata

对象存储(RGW)
RGW（RADOS Gateway）是Ceph存储系统的一部分，它提供了一个对象存储服务
RGW可以作为一个独立的服务运行，也可以与Ceph集群的其他组件（如OSD和MON）一起部署。
区域（zone）: 一个ceph集群可以包含多个区域，一个区域只属于一个集群，一个区域可以有多个RGW
区域组（zonegroup）：由一个或多个区域组成，包含一个主区域（master zone），其他区域称为Secondary Zone，区域组内的所有区域之间同步数据
域（realm）: 同一个或多个区域组组成，包含一个主区域组，其他都次区域组。域中的所有rados网关都从位于主区域组和主区域中的rados网关拉取配置
设置radosgw区域和用户

在所有节点部署qianyios领域和qianyios-shenzhen 区域的rgw守护程序：
操作节点[所有节点]
1.# 如果尚未创建 realm（领域），请首先创建一个名字为qianyios的 realm（领域） ：radosgw-admin realm create --rgw-realm=qianyiosradosgw-admin realm list2.# 接下来创建一个新的 zonegroup（区域组）：radosgw-admin zonegroup create --rgw-zonegroup=qianyios-shenzhen  --masterradosgw-admin zonegroup list3.#接下来创建一个 zone：radosgw-admin zone create --rgw-zonegroup=qianyios-shenzhen --rgw-zone=qianyios-shenzhen-zone1 --master#这个命令用于创建一个名为 qianyios-shenzhen-zone1 的 RadosGW 区域（Zone），并将它归属于 qianyios-shenzhen 的区域组（Zone Group）。radosgw-admin zone list4.#这个命令用于更新指定的 RadosGW 域（Realm）的周期（Period）。--rgw-realm=qianyios 参数指定要更新的 RadosGW 域的名称为 qianyiosradosgw-admin period update --rgw-realm=qianyios --commit5.#在 Ceph 集群中部署一个名为 qianyios-rgw 的 RadosGW 实例，并将其关联到指定的域（Realm）、区域组（Zone Group）和区域（Zone），并指定了存储位置。ceph orch apply rgw qianyios-rgw --realm=qianyios --zonegroup=qianyios-shenzhen --zone=qianyios-shenzhen-zone1 --placement=&quot;3 ceph1 ceph2 ceph3&quot; --port=8080#删除实例用这个ceph orch rm rgw.qianyios-rgw6.# 查看各节点 rgw 是否启动ceph orch ps --daemon-type rgw7.# 创建创建 radosgw 用户名为admin，显示名称为admin userradosgw-admin user create --uid=&quot;admin&quot; --display-name=&quot;admin user&quot;8.# 创建完成之后需要把access_key和secret_key保存下来，也可以使用下面的命令来查看radosgw-admin user info --uid=admin&quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;admin&quot;,            &quot;access_key&quot;: &quot;LAI14X0RJVJG3QGR16FP&quot;,            &quot;secret_key&quot;: &quot;t11PVzqYtEvb22ajctD0U5tCN5d0CaDPkZENiqTb&quot;        &#125;    ],
使用s3 browser点击这里访问官网下载软件，将以上access_key,secret_key填上，可以正常连接，但是没有桶，手动创建一个桶，并上传文件，正常。

此时什么都没有，创建一个桶


在桌面创建一个test文件移进去


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>Centos 9 stream</tag>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title>GitBash笔记</title>
    <url>/posts/36392/</url>
    <content><![CDATA[
GitBash笔记
下载gitbash工具Git for Windows安装教程在**文末**
·官网下载 https://git-scm.com/downloads
·Windows系统直接用https://gitforwindows.org/
基本环境
定义用户名和邮箱
git config --global user.name &quot;xxx&quot;git config --global user.email &quot;xxx@xxx.xxx&quot;检查设对没有git config user.namegit config user.email
用户名，邮箱就是你绑定的邮箱
生成远程连接密钥
ssh-keygen -t rsa -C &quot;xxx@xxx.xxx&quot;查看密钥cat ~/.ssh/id_rsa.pub会有一串密密麻麻的文字，全部复制到这 生成就行了
全局使用

测试远程连接
ssh -T git@github.com
基本命令
官方给了一个较好的解释文档

git init
#初始化一个新的git仓库git init
README.MD文件
没有它会报错，如果你没准备可以用以下命令
echo &quot;# 123456&quot; &gt;&gt; README.md
README.md是一个文本文件，通常在Git项目的根目录中，用于向其他人介绍该项目的信息。其中，.md是Markdown（标记语言）的文件格式，在GitHub等网站中被广泛使用。
具体来说，README.md文件通常包含以下信息：

项目名称和描述
如何安装和运行该项目
项目的使用方法和注意事项
贡献者的信息和代码许可
项目的版本历史和最新更新内容等等。

通过编写README.md文件，可以提供给其他人一个简洁、清晰的项目概述，方便其他人快速了解和使用该项目。同时，也可以通过修改README.md文件来更新和维护项目的最新信息和文档。
在GitHub等网站中，README.md文件会被自动渲染为网页显示，因此对项目的宣传和文档编写具有重要意义。
git add
将文件添加到git仓库git add &lt;file&gt;例如 ：注意指令末尾小数点git add . #将项目的所有文件添加到仓库中
git commit
git commit -m &quot;message&quot;提交更改，并附上提交信息
git status
#查看Git仓库状态git status
git log
git log查看提交记录
git clone 

git clone https://github.com······#克隆一个Git仓库到本地
git push
#将本地的更改推送到远程仓库git push
git pull
#从远程仓库拉取最新更改git pull
git branch
#查看和管理分支git branch
git merge 
git merge#将一个分支合并到当前分支中
git stash：
#将当前的更改保存到“存储区”，以便以后再次使用git stash
git remote add  ：
#将远程仓库添加到本地Git仓库中关联本地仓库和远程仓库
实例操作
请先完成 1.基本环境的所有步骤
假设我要上传项目
GitHub建好一个新库

在本地也创建一个本地库
勾选显示隐藏的项目


右键空白处

初始化本地仓库
git init

将项目的所有文件添加到本地仓库中
git add .#注意小数点
添加README.md文件
要求当前文件下有这个文件，没有会报错
git add README.md
提交到仓库，附上信息备注
git commit -m &quot;上传测试文件&quot;

修改分支（名字自定）
git branch -M main
git branch -M main命令用于将当前分支的名称修改为main，并将所有已有分支指向新的主分支main。
将本地仓库关联到GitHub仓库
git remote add origin https://github.com/······
https的地址，如果https不行也可以换成ssh地址

拉取最新更改
git pull origin main##上传github之前pull一下,第一次创建的库没有main分支,所有第一次不用打这个，以后建议，习惯的pull以下
上传代码至GitHub远程仓库
git push -u origin main


克隆代码
###从远程库克隆
这是针对在本地的一个空的项目，要从远程库考代码下来，一般有两个步骤：
1.在本地想要克隆的文件夹下面创建GIT版本库，以及建立远程库的连接。（详细步骤可以查看前面章节内容）
####建好本地库，基础环境 初始化等步骤，最后pull一下
git initgit remote add origin https://github.com/·······git pull origin main

2.用git clone克隆远程库所在项目的代码，比如要克隆上一节的代码，用下面命令即可
git clone https://github.com/·······

更新代码
在本地仓库添加一个test2.txt


查看当前的git仓库状态

git status

更新test2.txt文件

git add test2.txt

对test2.txt文件注入备注信息

git commit -m &quot;上传test2.txt&quot;

拉取main分支最新代码

git pull origin main

push到远程main分支上

git push origin main
你也可以更新全部
git add *git commit -m &quot;上传所有文件&quot;git pull origin maingit push origin main
打开GitHub已经同步了

安装教程
建议新建一个文件夹，放git，作为安装路径


更换路径

按需自助选择


选择开始文件夹
方框内 Git 可改为其他名字，也可点击 “Browse…” 选择其他文件夹或者给&quot;Don’t create a Start Menu folder&quot; 打勾不要文件夹，点击 [next]

Git编辑器

决定初始化新项目(仓库)的主干名字

调整环境变量

选择SSH执行文件

选择HTTPS后端传输
注意：如果具有企业管理证书的组织中使用Git，就需要使用安全通道。

配置行尾符号转换

配置终端模拟器以与 Git Bash 一起使用

选择默认的 git pull 模式
git pull 就是获取最新的远程仓库分支到本地，并与本地分支合并。

选择一个凭证帮助程序

配置额外的选项
配置建议两个都选

配置实验性选择

安装完成！！

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>GitBash</tag>
      </tags>
  </entry>
  <entry>
    <title>巨完美的Docker镜像加速方案</title>
    <url>/posts/69efb119/</url>
    <content><![CDATA[
最完美的Docker镜像加速方案
注册账号
现在这注册一个账号 CNB - Cloud Native Build
创建组织

vscode安装cnb插件


创建令牌
按照上面的需要去权限进行创建令牌



cvXpcpkmqiNQA1c77TZOR5ke1EG
创建仓库
一定要是公开的

这时候这里就会读取到

开始同步
假设有个镜像，是拉取不到的
k8s.gcr.io/pause:3.1


因为镜像有标签，所以在最下面有个带标签的地址


下载成功！
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker笔记</title>
    <url>/posts/27092/</url>
    <content><![CDATA[
Docker笔记
结合Centos 8部署Docker - 严千屹博客 (qianyios.top)搭建的Docker实现本次笔记的所有内容及例子
也可以使用以下Centos7部署Docker CE 19.03实现本实验的所有例子
Centos7部署Docker CE 19.03
（2023年4月8日01点00分）当前最常用的 Docker 版本是 Docker CE 19.03。这是 Docker 社区版（Community Edition）的最新版本，它包括一些更新和新功能，如多阶段构建、Dockerfile 中的 ARG 和 FROM 指令以及与 Kubernetes 的更好集成等。另外，Docker CE 19.03 支持 Windows、MacOS 和 Linux 操作系统。
卸载旧版本
sudo yum -y remove docker \                  docker-client \                  docker-client-latest \                  docker-common \                  docker-latest \                  docker-latest-logrotate \                  docker-logrotate \                  docker-selinux \                  docker-engine-selinux \                  docker-enginesudo yum remove docker-ce docker-ce-cli containerd.io docker-compose docker-machine docker-swarmsudo rm /etc/yum.repos.d/docker-ce.reposudo rm -rf /var/lib/docker
安装docker
sudo yum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.reposudo yum install docker-ce docker-ce-cli containerd.io -ysudo systemctl start dockerdocker versionsudo curl -L &quot;https://github.com/docker/compose/releases/download/1.28.6/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-composedocker-compose --version
Docker的常用命令
docker version		   # 显示docker的版本信息docker info			   # 显示docker的系统信息,包括镜像和容器的数量docker 命令 --help 	   # 帮助命令
镜像命令
镜像命令
docker images		# 查看所有本地的主机上的镜像-a,--al1	# 列出所有镜像-q,--quiet	# 只显示镜像的id# 解释REPOSITORY        TAG         IMAGE ID        CREATED          SIZE镜像的仓库源      镜像的标签  镜像的id        镜像的创建时间   镜像的大小
搜索命令
[root@docker ~]# docker search centosNAME    DESCRIPTION    STARS                               OFFICIAL  AUTOMATEDcentos  DEPRECATED;    The official build of CentOS.       7537      [OK]检索Docker仓库中的Ubuntu镜像
拉取镜像
[root@docker ~]# docke pull 镜像名[:tag]		#下载镜像# 如果不写tag，默认就是latest[root@docker ~]# docker pull centosUsing default tag: latestlatest: Pulling from library/centosa1d0c7532777: Pull completeDigest: sha256:a27fd8080b517143cbbbab9dfb7c8571c40d67d534bbdee55bd6c473f432b177Status: Downloaded newer image for centos:latestdocker.io/library/centos:latest从docker库拉去centos镜像[root@docker ~]# docker imagesREPOSITORY   TAG       IMAGE ID       CREATED         SIZEcentos       latest    5d0da3dc9764   18 months ago   231MB
删除镜像
docker rmi -f 镜像名/id		# 删除指定镜像docker rmi -f 镜像名1 镜像名2 镜像名3	 # 一次删除指定多个镜像docker rmi -f $(docker images -aq)		# 删除全部容器[root@docker ~]# docker imagesREPOSITORY   TAG       IMAGE ID       CREATED         SIZEcentos       latest    5d0da3dc9764   18 months ago   231MB[root@docker ~]# docker rmi 5d0Untagged: centos:latest。。。。。。（一般情况下 删除镜像id 只需要输入前面3个字符就行了）
查看镜像分层
[root@docker ~]# docker image inspect 镜像id
镜像资源包
curl -O http://mirrors.douxuedu.com/cloud/docker.zipunzip docker.zipdocker load &lt; 镜像名.tar例如：[root@localhost ~]# docker load &lt; docker/centos_latest.tar
容器命令
新建容器并启动
docker run [可选参数] 镜像名		# 新建容器并启动--name=Name		# 容器名字用来区分容器-d				# 后台方式运行-it				# 使用交互方式运行，进入容器查看内容-p				# 指定容器的端口-p 8080:8080-P				# 随机指定端口进入容器[root@docker ~]# docker run -it --name test centos[root@4018f9ac2f33 /]# pwd/exit         # 退出Ctrl+P+Q     # 容器不停止退出
查看容器列表
查看容器列表   （若加 -a可以查看所有容器包括为运行的）[root@docker ~]# docker ps [可选参数]CONTAINER ID   IMAGE     COMMAND       CREATED              STATUS              PORTS     NAMES4018f9ac2f33   centos    &quot;/bin/bash&quot;   About a minute ago   Up About a minute             test[可选参数]-a			# 列出当前正在运行的容器+带出历史运行过的容器-n=?		# 显示最近创建的容器-q			# 只显示容器的编号
删除容器
（一般情况下 删除容器id 只需要输入前面3个字符就行了）docker rm 容器id	# 删除指定的容器，不能删除正在运行的容器，如果要强制删除 rm -fdocker rm -f $ (docker ps -aq)		# 强制删除所有的容器docker ps -a -q|xargs docker rm		# 删除所有的容器
启动和停止容器
docker start 容器id		# 启动容器docker restart 容器id		# 重启容器docker stop 容器id		# 停止当前正在运行的容器docker kill 容器id		# 强制停止当前容器
进入正在运行的容器
进入容器后开启一个新的终端，可以在里面操作docker exec -it 容器id /bin/bash进入容器正在执行的终端，不会启动新的进程docker attach 容器id
拷贝容器内的文件到主机上
docker cp 容器id:容器内路径 目的主机路径[root@docker ~]# docker exec -it 401 /bin/bash[root@4018f9ac2f33 /]# echo &quot;11&quot; &gt; test.txt[root@4018f9ac2f33 /]# cat test.txt11Ctrl+P+Q     # 容器不停止退出[root@docker ~]# docker cp 401:test.txt /root/Preparing to copy...Successfully copied 2.048kB to /root/[root@docker ~]# cat test.txt11
停止容器，将容器打包成新镜像
[root@docker ~]# docker commit [可选参数] 容器id REPOSITORY:TAG[可选参数]：-a :提交的镜像作者；-c :使用Dockerfile指令来创建镜像；-m :提交时的说明文字；-p :在commit时，将容器暂停。[root@localhost ~]# docker commit -a qianyios -m &quot;创建了test.txt&quot; 401 test/centos:v1sha256:29ffb8423a78853c5a49918c99e8d513239f9c3365cca06e1fc0027f589b7f59[root@localhost ~]# docker imagesREPOSITORY    TAG       IMAGE ID       CREATED         SIZEtest/centos   v1        29ffb8423a78   3 seconds ago   233MBcentos        latest    5d0da3dc9764   18 months ago   231MB
Docker数据卷

数据卷用途：使容器内部的文件跟容器外面的文件同步
数据卷的特点:

数据卷可在容器之间共享或重用数据
卷中的更改可以直接生效
数据卷中的更改不会包含在镜像的更新中
数据卷的生命周期一直持续到没有容器使用它为止



数据卷基本命令
docker volume ls		# 查看所有挂载的数据卷docker volume inspect 卷名	# 查看数据卷所在的路径
指定路径挂载
docker run -v 主机目录:容器内目录
卷名挂载
docker run -v 卷名(不是目录了):容器内 目录
匿名挂载
docker run -v 容器内目录
数据卷权限
docker run -v 主机目录:容器内目录:ro		# ro readonly 只读docker run -v 主机目录:容器内目录:rw		# rw readwrite 可读可写# ro只能在主机上面来操作目录，容器无法操作
数据卷容器
--volumes-from 要同步文件的容器			# 实现多个容器的数据同步与共享

例子：三个MySQL同步数据

1、数据库1使用的命令
docker run -d -p 主机端口:3306 /-v /etc/mysql/conf.d -v /var/1ib/mysql /-e MYSQL_ROOT_PASSWORD=数据库密码 --name mysql01 mysql
2、数据库2使用的命令
docker run -d -p 主机端口:3306 /--volumes-from mysql01 /-e MYSQL_ROOT_PASSWORD=数据库密码 --name mysql02 mysqlCopy
3、数据库3使用的命令
docker run -d -p 主机端口:3306 /--volumes-from mysql01 /-e MYSQL_ROOT_PASSWORD=数据库密码 --name mysql03 mysql
DockerFile
DockerFile是用来构建Docker镜像的构建文件。
!注意事项

1、每个保留关键字（指令）都是必须是大写字母
2、执行从上到下顺序执行
3、#表示注释
4、每一个指令都会创建提交一个新的镜像层，并提交


DockerFile命令
FROM          # 基础镜镜像,—切从这里开始构建MAINTAINER    # 镜像的作者,姓名&lt;邮箱&gt;RUN           # 镜像构建的时候需要运行的命令ADD           # 添加内容，添加压缩包会自动解压WORKDIR		 # 镜像的工作目录VOLUME        # 挂载的目录EXPOSE        # 保留端口配置CMD			# 指定这个容器启动的时候要运行的命令,只有最后一个会生效，可被替代ENTRYPOINT    # 指定这个容器启动的时候要运行的命令,可以追加命令ONBUILD		 # 当构建一个被继承DockerFile这个时候就会运行ONBUILD 的指令。触发指令。COPY          # 类似ADD ，将我们文件拷贝到镜像中ENV           # 构建的时候设置环境变量!
用DockerFile文件创建镜像
docker build -f DockerFile文件名 -t 镜像名:版本号 .# -f指定Dockfile文件，若Dockerfile文件名就是Dockerfile，则不用-f再指定。系统则自动判定此文件是Dockerfile文件。# 最后一个是 . 一定加上
例子：
[root@docker ~]# mkdir centos[root@docker ~]# cd centos/[root@docker centos]# touch DockerFile[root@docker centos]# vi DockerFile#基础镜像信息FROM centos:latest#维护者信息MAINTAINER qianyios xiaoohu2002@163.com#镜像操作指令RUN sed -i &#x27;s/mirrorlist/#mirrorlist/g&#x27; /etc/yum.repos.d/CentOS-*RUN sed -i &#x27;s|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g&#x27; /etc/yum.repos.d/CentOS-*RUN yum install -y wgetRUN wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repoRUN yum clean all &amp;&amp; yum makecacheRUN mkdir /test RUN echo &#x27;test&#x27; &gt; /test/test.txt[root@docker centos]# docker build -f /root/centos/DockerFile -t centos-test:v1 .[root@docker centos]# docker imagesREPOSITORY    TAG       IMAGE ID       CREATED         SIZEcentos-test   v1        b1f4c91e59ee   2 minutes ago   328MB#此时已经有一个镜像了
用这个镜像创建容器，且将容器中的/test挂载到主机目录下的/opt/test
b1f是镜像id前三个字符
docker run -it -v /opt/test:/test --name test -p 80:80 b1f /bin/bashexit退出exit之后查看/opt/test是否有test.txt文件[root@docker ~]# cat /opt/test/test.txttest成功！！！
配置镜像加速器
# 设置 Docker 镜像加速器cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123;  &quot;registry-mirrors&quot;: [    &quot;http://hub-mirror.c.163.com&quot;,    &quot;https://docker.mirrors.ustc.edu.cn/&quot;  ],  &quot;insecure-registries&quot;: [],  &quot;debug&quot;: false,  &quot;experimental&quot;: false,  &quot;features&quot;: &#123;    &quot;buildkit&quot;: true  &#125;&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker
Docker 网络
docker网络主要是解决容器联网问题，也是我们使用容器中最重要的一个环节，如果容器没有网络则无法向网络中提供服务。
网络管理命令：docker network
[root@docker ~]# docker network --helpUsage:	docker network COMMANDManage networksCommands:  connect        连接容器到网络  create          创建网络  disconnect    断开容器与网络的连接  inspect         显示一个或多个网络的详细信息  ls                 列表网络  prune           删除所有未使用的网络  rm               删除一个或多个网络
docker网络类型
创建容器的时候可以通过—network命令来指定容器的网络，网络类型有以下四种

bridge
host
none
容器网络或联盟网络

bridge
桥接网络是指容器通过桥接的方式将容器网卡桥接到宿主机的docker0网桥，然后在通过宿主机防火墙的NAT表实现与外网的联系。
宿主机docker0网桥
[root@docker ~]# ifconfig #docker0网桥docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255        inet6 fe80::42:c7ff:fe37:8e8  prefixlen 64  scopeid 0x20&lt;link&gt;        ether 02:42:c7:37:08:e8  txqueuelen 0  (Ethernet)        RX packets 6618  bytes 277975 (271.4 KiB)        RX errors 0  dropped 0  overruns 0  frame 0        TX packets 8152  bytes 24675021 (23.5 MiB)        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0.....省略了本机的网卡信息#容器网卡，每创建一个桥接网络的容器就会生成一个对应的网卡vethf75a942: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500        inet6 fe80::9085:f5ff:fe34:77b5  prefixlen 64  scopeid 0x20&lt;link&gt;        ether 92:85:f5:34:77:b5  txqueuelen 0  (Ethernet)        RX packets 2850  bytes 158484 (154.7 KiB)        RX errors 0  dropped 0  overruns 0  frame 0        TX packets 3397  bytes 11613136 (11.0 MiB)        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0                        如果想看更清楚一下  可以使用  ip  add  show命令[root@docker ~]# ip add show4: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default     link/ether 02:42:c7:37:08:e8 brd ff:ff:ff:ff:ff:ff    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0       valid_lft forever preferred_lft forever    inet6 fe80::42:c7ff:fe37:8e8/64 scope link        valid_lft forever preferred_lft forever容器网卡14: vethf75a942@if13: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default     link/ether 92:85:f5:34:77:b5 brd ff:ff:ff:ff:ff:ff link-netnsid 1    inet6 fe80::9085:f5ff:fe34:77b5/64 scope link        valid_lft forever preferred_lft forever注意：这里的vethf75a942@if13指的就是容器网卡，V代表虚拟网卡的意思，eth 以太网卡，f75a942网卡编号，if13指的是宿主机网桥(docekr0)的一个端口，对应容器的网卡编号加一。所以容器内的网卡编号应该是 eth0@if14通过在容器中执行命令  ip add show 也可以看到[root@docker ~]# docker exec a5f ip add show1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever13: eth0@if14: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default     link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0    inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0       valid_lft forever preferred_lft forever
防火墙的NAT表内容
[root@docker ~]# iptables -t nat -LChain PREROUTING (policy ACCEPT)target     prot opt source               destination         DOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCALChain INPUT (policy ACCEPT)target     prot opt source               destination         Chain OUTPUT (policy ACCEPT)target     prot opt source               destination         DOCKER     all  --  anywhere            !loopback/8           ADDRTYPE match dst-type LOCALChain POSTROUTING (policy ACCEPT)target     prot opt source               destination         MASQUERADE  all  --  172.17.0.0/16        anywhere            Chain DOCKER (2 references)target     prot opt source               destination         RETURN     all  --  anywhere             anywhere        
docker0 与容器网卡桥接
通过brctl show命令可以看到容器网卡和docker0网卡的桥接信息[root@docker ~]# brctl showbridge name	bridge id		STP enabled	interfacesdocker0		8000.0242c73708e8	no		vethf75a942
创建一个网络为bridge类型的容器，不指定默认也是这个类型
[root@docker ~]# docker run -d --network bridge --name centos1 baishuming2020/centos_nginx
host
容器和真机共用网卡及对应的端口，缺点就是同一个端口只能宿主机或者某个容器使用，其他容器不能用。
创建一个网络类型host的容器[root@docker ~]# docker run -d --network host --name centos2 baishuming2020/centos_nginx
none
容器仅有lo网卡，是一个不能联网的本地容器
创建一个网络类型为lo的容器[root@docker ~]# docker run -d --network none --name centos3 baishuming2020/centos_nginx
实现网桥网络
目的：不同的服务容器组应用不同的网桥，避免同一网络内容器太多，保持容器网络独立性。
关于新网桥联网问题：创建网桥后，宿主机会自动帮你做NAT，所以不用担心联网问题
查看网络-ls
[root@docker ~]# docker network lsNETWORK ID          NAME                DRIVER              SCOPE80982d2613cd        bridge              bridge              local40c179ab420a        docker1             bridge              local04aadb7475c0        docker100           bridge              localce79e9d7525a        host                host                local8f0358469e57        none                null                localNETWORK ID     网桥ID   NAME          	 名称DRIVER        	 网络类型  SCOPE	   	   作用范围
创建网桥-create
[root@docker ~]# docker network create -d bridge --subnet 192.168.148.0/24 --gateway 192.168.148.2 a16a410e27b66ea587142d967f7dff6f36c04ced3c27116a79831412f3743aba56[root@docker ~]# docker network lsNETWORK ID          NAME                DRIVER              SCOPE6ee1e928b710        bridge              bridge              localce79e9d7525a        host                host                local6a410e27b66e        mydocker0           bridge              local8f0358469e57        none                null                local修改docker网桥名字1、修改名字[root@docker ~]# docker network rename old_name new_name2、重启docker服务[root@docker ~]# systemctl restart docker
删除未使用的网桥-prune
[root@docker ~]# docker network prune WARNING! This will remove all networks not used by at least one container.Are you sure you want to continue? [y/N] yDeleted Networks:docker1
删除某个网桥-rm
[root@docker ~]# docker network rm docker100docker100注意：不能被活动容器占用
容器连接到网桥
前提是该容器是桥接网络
docker network connect 网卡 容器[root@docker ~]# docker network connect docker1 centos1[root@docker ~]# docker exec centos1 ifconfigeth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500        inet 172.17.0.2  netmask 255.255.0.0  broadcast 172.17.255.255        ether 02:42:ac:11:00:02  txqueuelen 0  (Ethernet)        RX packets 8  bytes 656 (656.0 B)        RX errors 0  dropped 0  overruns 0  frame 0        TX packets 0  bytes 0 (0.0 B)        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0发现centos1容器多了一块网卡，使用的正是docker1的网段eth1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500        inet 192.168.1.2  netmask 255.255.255.0  broadcast 192.168.1.255        ether 02:42:c0:a8:01:02  txqueuelen 0  (Ethernet)        RX packets 16  bytes 1312 (1.2 KiB)        RX errors 0  dropped 0  overruns 0  frame 0        TX packets 0  bytes 0 (0.0 B)        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536        inet 127.0.0.1  netmask 255.0.0.0        loop  txqueuelen 1000  (Local Loopback)        RX packets 0  bytes 0 (0.0 B)        RX errors 0  dropped 0  overruns 0  frame 0        TX packets 0  bytes 0 (0.0 B)        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
容器断开网桥
将centos1容器的网络从docker1网桥断开[root@docker ~]# docker network disconnect docker1 centos1
常见故障
FAQ1：使用改名后的新网桥的容器可能无法解析域名
原因：没有配置新网桥的DNS
解决方法：为容器手动配置一个DNS地址即可
FAQ2：Networking will not work
[root@docker ~]# docker run -d --network docker100 --name centos4 baishuming2020/centos_nginxWARNING: IPv4 forwarding is disabled. Networking will not work.67f2c276123c993cd66b9d7a99ba22402331a13f9ea8817e57324a934896b805解决方案1、打开转发[root@docker ~]# echo &quot;net.ipv4.ip_forward=1&quot; &gt;&gt;  /usr/lib/sysctl.d/00-system.conf2、重启网络[root@docker ~]# systemctl restart network
不同主机间的容器通信
macvlan
在 Docker 中，macvlan 是众多 Docker 网络模型中的一种，并且是一种跨主机的网络模型，作为一种驱动启用，Docker macvlan 只支持 bridge 模式
#macvlan 需要一块独立的网卡来进行使用，所以我们需要新添加一块网卡docker network create -d macvlan --subnet=172.16.10.0/24 --gateway=172.16.10.1  -o parent=ens224  mtacvlan-1-o parent=网卡名称  指定用来给 macvlan 网络使用的物理网卡注意，要在所有需要运行 macvlan 的主机上执行这条命令，但是要记得更改网关的地址，避免造成IP冲突docker run -itd --network macvlan-1 centos /bin/bash
overlay
在 Docker 中，overlay 是众多 Docker 网络模型中的一种，并且是一种跨主机的全局网络模型，有一个数据库专门的来存储网络分配信息，避免 IP 冲突，同时内部还有一个小型的 DNS 我们可以直接通过主机名进行访问
consul 服务端：docker run -itd -h consul --name consul --restart=always -p 8500:8500 progrium/consul -server -bootstrap-h 				主机名–name 			容器名–restart=always 重启策略progrium/consul 镜像名称-server 		以服务节点启动-bootstrap		预期的启动节点数：自举在浏览器内输入 IP地址+端口号 可以看到 web 页面在所有主机上编辑 daemon.json 文件：&#123;&quot;hosts&quot;: [&quot;tcp://0.0.0.0:2375&quot;,&quot;unix:///var/run/docker.sock&quot;]， 监听相关端口&quot;cluster-store&quot;:&quot;consul://192.168.1.150:8500&quot;,		   集群的主机地址&quot;cluster-advertise&quot;:&quot;192.168.1.150:2375”		宣告自己的地址 &#125;重启 docker 服务创建 overlay 网络（全局网络）：一台主机上创建自动同步	docker network create -d overlay overlay-1启动容器测试：	docker run -it --name docker-1 --network=overlay-1 centos /bin/bash		docker run -it --name docker-2 --network=overlay-1 centos /bin/bash	验证：ping docker-1
常见故障
如发现各容器内分配的ip之间相互ping不通
原因：可能由于防火墙问题引起的,默认forward链是drop状态，需要打开才可以解决方案:执行下面操作，保证INPUT  FORWARD链都是ACCEPT状态清除其他规则[root@docker_node1 ~]# iptables -P INPUT ACCEPT[root@docker_node1 ~]# iptables -P FORWARD ACCEPT[root@docker_node1 ~]# iptables -F[root@docker_node1 ~]# iptables -L -n[root@docker_node2 ~]# iptables -P INPUT ACCEPT[root@docker_node2 ~]# iptables -P FORWARD ACCEPT[root@docker_node2 ~]# iptables -F[root@docker_node2 ~]# iptables -L -n
Docker私有仓库
在Docker中，当我们执行 docker pull xxx 的时候 ，它实际上是从 hub.docker.com 这个地址去查找，这就是 Docker 公司为我们提供的公共仓库。在工作中，我们不可能把企业项目 push 到公有仓库进行管理。所以为了更好的管理镜像，Docker 不仅提供了一个中央仓库，同时也允许我们搭建本地私有仓库。
docker容器镜像仓库分类：

公网仓库：docker hub
私网仓库: registry、harbor

部署步骤
拉取registry镜像
docker pull registry 
创建registry仓库容器
1、创建持久化存储，将容器镜像存储目录/var/lib/registry挂载到本地/opt/qyck下：mkdir /opt/qyck2、创建 registry 容器：docker run -itd -p 5000:5000 \-v /opt/qyck:/var/lib/registry  \--restart=always registry:latest3、查看容器是否运行[root@qyck ~]# docker psCONTAINER ID   IMAGE             COMMAND                  CREATED          STATUS         PORTS                                       NAMESd1ea79cc023f   registry:latest   &quot;/entrypoint.sh /etc…&quot;   10 seconds ago   Up 9 seconds   0.0.0.0:5000-&gt;5000/tcp, :::5000-&gt;5000/tcp   trusting_gates
测试容器应用
[root@zutuanxue_manage01 ~]# curl 192.168.48.128:5000/v2/_catalog&#123;&quot;repositories&quot;:[]&#125;显示仓库中没有任何镜像
上传镜像
测试：拉取nginx镜像
docker pull nginx[root@qyck ~]# docker imagesREPOSITORY   TAG       IMAGE ID       CREATED       SIZEregistry     latest    65f3b3441f04   9 days ago    24MBnginx        latest    448a08f1d2f9   2 weeks ago   142MB
设置docker仓库为registry本地仓库
#1、修改docker进程启动文件，修改其启动方式，目的是为了让通过docker配置文件启动[root@qyck ~]# sed -i.bak &#x27;/^ExecStart=/c\ExecStart=\/usr\/bin\/dockerd&#x27; /usr/lib/systemd/system/docker.service#2、设置docker 守护进程的配置文件 /etc/docker/daemon.json,默认没有该文件[root@qyck ~]# cat /etc/docker/daemon.json &#123; &quot;insecure-registries&quot;: [&quot;http://192.168.48.128:5000&quot;]&#125;insecure-registries 指定非安全的仓库地址，多个用逗号隔开#3、重启docker生效配置文件systemctl daemon-reloadsystemctl restart docker
将镜像打tag
[root@qyck ~]# docker tag nginx:latest 192.168.48.128:5000/nginx:v1[root@qyck ~]# docker imagesREPOSITORY                  TAG       IMAGE ID       CREATED       SIZE192.168.48.128:5000/nginx   v1        448a08f1d2f9   2 weeks ago   142MB
推送镜像到仓库
[root@qyck ~]# docker push 192.168.48.128:5000/nginx:v1The push refers to repository [192.168.48.128:5000/nginx]1040838fe30e: Pushed93ee76f39c97: Pushed5684be535bf1: Pushed6bc8ae8fb3cf: Pusheda29cc9587af6: Pushed8553b91047da: Pushedv1: digest: sha256:3f01b0094e21f7d55b9eb7179d01c49fdf9c3e1e3419d315b81a9e0bae1b6a90 size: 1570#2、查看上传[root@qyck ~]# curl http://192.168.48.128:5000/v2/_catalog&#123;&quot;repositories&quot;:[&quot;nginx&quot;]&#125;#查看存储文件夹[root@qyck ~]# ls /opt/qyck/docker/registry/v2/repositories/nginx
拉取镜像
在另外一台机拉取nginx镜像
#1、设置docker启动文件[root@zutuanxue_node1 ~]# sed -i.bak &#x27;/^ExecStart=/c\ExecStart=\/usr\/bin\/dockerd&#x27; /usr/lib/systemd/system/docker.service#2、设置docker配置文件[root@zutuanxue_node1 ~]# cat  /etc/docker/daemon.json &#123; &quot;insecure-registries&quot;: [&quot;http://192.168.48.128:5000&quot;]&#125;systemctl daemon-reloadsystemctl restart docker
拉取镜像
[root@qyck ~]# docker pull 192.168.48.128:5000/nginx:v1v1: Pulling from nginx9e3ea8720c6d: Pull completebf36b6466679: Pull complete15a97cf85bb8: Pull complete9c2d6be5a61d: Pull complete6b7e4a5c7c7a: Pull complete8db4caa19df8: Pull completeDigest: sha256:3f01b0094e21f7d55b9eb7179d01c49fdf9c3e1e3419d315b81a9e0bae1b6a90Status: Downloaded newer image for 192.168.48.128:5000/nginx:v1192.168.48.128:5000/nginx:v1[root@qyck ~]# docker imagesREPOSITORY                  TAG       IMAGE ID       CREATED       SIZEregistry                    latest    65f3b3441f04   9 days ago    24MB192.168.48.128:5000/nginx   v1        448a08f1d2f9   2 weeks ago   142MB
Docker学习总结
![1](…/img/Docker node/f8242ed25c3cc4a0f16a604d1d53b5f885684b49.png)
docker run 参数解析
-d: 后台运行容器，并返回容器ID；-i: 以交互模式运行容器，通常与 -t 同时使用；-P: 随机端口映射，容器内部端口随机映射到主机的端口-p: 指定端口映射，格式为：主机(宿主)端口:容器端口-t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用；--name=&quot;nginx-lb&quot;: 为容器指定一个名称；--dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致；--dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致；-h &quot;mars&quot;: 指定容器的hostname；-e username=&quot;ritchie&quot;: 设置环境变量；--env-file=[]: 从指定文件读入环境变量；--cpuset=&quot;0-2&quot; or --cpuset=&quot;0,1,2&quot;: 绑定容器到指定CPU运行；-m :设置容器使用内存最大值；--net=&quot;bridge&quot;: 指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型；--link=[]: 添加链接到另一个容器；--expose=[]: 开放一个端口或一组端口；--volume , -v: 绑定一个卷
此笔记参考 开摆工作室/Docker基础精心整理

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop 3.3.5部署</title>
    <url>/posts/cbb23bdb/</url>
    <content><![CDATA[
Hadoop 3.3.5部署
前情提要
本次实验采用Ubuntu 24.04LTS，自行安装
Ubuntu 24.04.02 LTS 初始化安装 | 严千屹博客
本笔记分伪分布和分布式两大块，但建议从头开始观看
文章所需资源可点击这里下载

伪分布主机拓扑




主机名
ip（NAT）
内存
硬盘




qianyios
192.168.48.128
7G
100G




分布式主机拓扑




机名
ip（NAT）
内存
硬盘




master
192.168.48.128
6G
100G


slave
192.168.48.129
6G
100G



基础初始化
简单部署一个单节点的hadoop，然后打快照，后续给伪分布和分布式做基础底座
由于本系统在Ubuntu 24.04.02 LTS 初始化安装 | 严千屹博客已经进行了设置阿里源和关闭防火墙，这里就不再赘述了
切换root用户
qianyios@qianyios:~$ su -  root密码：root@qianyios:~#
基础配置
cat &gt;init.sh&lt;&lt;&quot;EOF&quot;#!/bin/bashsed -i &#x27;s/^#*PermitRootLogin.*/PermitRootLogin yes/&#x27; /etc/ssh/sshd_configsed -i &#x27;s/^#*PasswordAuthentication.*/PasswordAuthentication yes/&#x27; /etc/ssh/sshd_configsystemctl restart ssh# 添加 hostsecho &quot;192.168.48.128 qianyios&quot; &gt;&gt; /etc/hostsecho &quot;已添加 hosts 条目。&quot;# 设置主机名hostnamectl set-hostname qianyiosecho &quot;主机名已设置为 qianyios。&quot;# 安装 sshpassapt install -y sshpass || &#123; echo &quot;安装 sshpass 失败&quot;; exit 1; &#125;echo &quot;sshpass 安装完成。&quot;# 目标主机列表hosts=(&quot;qianyios&quot;)# 密码password=&quot;123456&quot;# 生成 SSH 密钥对ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsaecho &quot;SSH 密钥对已生成。&quot;# 循环遍历目标主机for host in &quot;$&#123;hosts[@]&#125;&quot;do    echo &quot;正在为 $host 配置免密登录...&quot;    sshpass -p &quot;$password&quot; ssh-copy-id -o StrictHostKeyChecking=no &quot;$host&quot; || &#123; echo &quot;复制公钥到 $host 失败&quot;; exit 1; &#125;    sshpass -p &quot;$password&quot; ssh -o StrictHostKeyChecking=no &quot;$host&quot; &quot;echo &#x27;免密登录成功&#x27;&quot; || &#123; echo &quot;验证免密登录失败&quot;; exit 1; &#125;donerebootEOFbash init.sh
测试免密登入
root@qianyios:~# ssh qianyiosWelcome to Ubuntu 24.04.2 LTS (GNU/Linux 6.11.0-17-generic x86_64)......Last login: Tue Feb 25 00:02:32 2025 from 127.0.0.1root@qianyios:~#
下载所需资源并解压到/root/hadoop/下，如下图

安装java环境和hadoop
cd /root/hadoopmkdir /usr/lib/jvm#安装java8tar -xf /root/hadoop/jdk-8u371-linux-x64.tar.gz  -C /usr/lib/jvmecho &quot;export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_371&quot; &gt;&gt; /etc/profileecho &quot;export PATH=\$JAVA_HOME/bin:\$PATH&quot; &gt;&gt; /etc/profilesource /etc/profilejava -version#安装hadoop3.3.5tar -zxf hadoop-3.3.5.tar.gz -C /usr/localmv /usr/local/hadoop-3.3.5/ /usr/local/hadoopecho &quot;export HADOOP_HOME=/usr/local/hadoop&quot; &gt;&gt; /etc/profileecho &quot;export PATH=\$HADOOP_HOME/bin/:\$HADOOP_HOME/sbin/:\$PATH&quot; &gt;&gt; /etc/profilesource /etc/profilehadoop version
成功图

这时候关机
poweroff
打个快照，方便做分布式部署,如果你要做分布式的直接跳到4.分布式

伪分布
开机吧！
编写配置文件
编写cort-site.yaml文件
修改下面hdfs://qianyios:9000中的qianyios为你的主机名
cat &gt; /usr/local/hadoop/etc/hadoop/core-site.xml&lt;&lt; &quot;EOF&quot;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://qianyios:9000&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;EOF
编写hdfs-site.xml
cat &gt;/usr/local/hadoop/etc/hadoop/hdfs-site.xml&lt;&lt;&quot;EOF&quot;&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;dfs.replication&lt;/name&gt;        &lt;value&gt;1&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;EOF
启动hdfs服务
hadoop初始化
hdfs namenode -format
这条命令只需要运行一次，以后都不要再运行了！！！！！！
这条命令只需要运行一次，以后都不要再运行了！！！！！！
这条命令只需要运行一次，以后都不要再运行了！！！！！！

添加环境变量
echo &quot;export HDFS_NAMENODE_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export HDFS_DATANODE_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export HDFS_SECONDARYNAMENODE_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export YARN_RESOURCEMANAGER_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export YARN_NODEMANAGER_USER=root&quot; &gt;&gt; /etc/profilesource /etc/profile
修改hadoop配置文件
echo &quot;export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_371&quot; &gt;&gt; /usr/local/hadoop/etc/hadoop/hadoop-env.sh
启动hadoop
#启动服务start-all.sh
#关闭服务stop-all.sh


localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.

这是一个 SSH 的警告信息，表明 SSH 客户端首次连接到 localhost 时，将 localhost 的主机密钥（使用 ED25519 算法生成）添加到了 known_hosts 文件中。
这是 SSH 的正常行为，用于防止中间人攻击。每次 SSH 客户端连接到一个新主机时，都会将主机的密钥记录下来。


启动historyserver服务
#启动hadoopstart-all.shmapred --daemon start historyserver
关闭用
mapred --daemon stop historyserver
正常启动hadoop你会看到如下服务
root@qianyios:~# jps14050 NodeManager10245 JobHistoryServer13894 ResourceManager13255 NameNode13449 DataNode13673 SecondaryNameNode14606 Jps
访问网页ip:9870查看hdfs

访问网页ip:8088查看hadoop

至此伪分布hadoop就搞定了，这时候你要在你这里打上一个伪分布的快照
分布式
前情提要



机名
ip（NAT）
内存
硬盘




master
192.168.48.128
6G
100G


slave
192.168.48.129
6G
100G



由于前面不是做了一个hadoop的一个基础快照吗，这时候你就对那个基础快照进行完整克隆两个出来，分别命名为master和slave


这时候先开slave，master不要开
vim /etc/netplan/01-network-manager-all.yaml
ip改成192.168.48.129
# Let NetworkManager manage all devices on this systemnetwork:  ethernets:    ens33:      addresses: [192.168.48.129/24]      dhcp4: false      nameservers:          addresses: [192.168.48.2, 114.114.114.114]      routes:        - to: default          via: 192.168.48.2  version: 2  renderer: NetworkManager
重启网卡
netplan apply
这时候再把master开机，接着就可以进行基础操作了
基础操作
以下我会提前告诉你哪些是哪个节点要操作的命令
操作节点：=master和slave=
cat &gt;fbsnit.sh &lt;&lt;&quot;EOF&quot;#!/bin/bashif [ $# -eq 1 ];then  echo &quot;设置主机名为：$1&quot;else  echo  &quot;使用方法：sh $0 主机名&quot;  exit 2fisudo sed -i &#x27;/qianyios/d&#x27; /etc/hosts#这里你要改成你的ipgrep -q &quot;^192\.168\.48\.128\s\+master&quot; /etc/hosts || echo &quot;192.168.48.128 master&quot; &gt;&gt; /etc/hostsgrep -q &quot;^192\.168\.48\.129\s\+slave&quot; /etc/hosts || echo &quot;192.168.48.129 slave&quot; &gt;&gt; /etc/hostshostnamectl set-hostname $1#设置免密apt install -y sshpass || &#123; echo &quot;安装 sshpass 失败&quot;; exit 1; &#125;echo &quot;sshpass 安装完成。&quot;# 目标主机列表hosts=(&quot;master&quot; &quot;slave&quot;)# 密码password=&quot;123456&quot;# 生成 SSH 密钥对ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsaecho &quot;SSH 密钥对已生成。&quot;# 循环遍历目标主机for host in &quot;$&#123;hosts[@]&#125;&quot;do    echo &quot;正在为 $host 配置免密登录...&quot;    sshpass -p &quot;$password&quot; ssh-copy-id -o StrictHostKeyChecking=no &quot;$host&quot; || &#123; echo &quot;复制公钥到 $host 失败&quot;; exit 1; &#125;    sshpass -p &quot;$password&quot; ssh -o StrictHostKeyChecking=no &quot;$host&quot; &quot;echo &#x27;免密登录成功&#x27;&quot; || &#123; echo &quot;验证免密登录失败&quot;; exit 1; &#125;donerebootEOF
#master执行这个bash fbsnit.sh master
#slave执行这个bash fbsnit.sh slave
修改配置文件
操作节点：master
1.修改workers文件
在Hadoop集群中，workers文件是一个非常重要的配置文件，它用于指定Hadoop集群中所有从节点（DataNode和TaskTracker/NodeManager）的主机名或IP地址
将slave修改成你自己的从节点的主机名
cat&gt; /usr/local/hadoop/etc/hadoop/workers &lt;&lt;&quot;EOF&quot;slaveEOF
2.修改core-site.xml
操作节点：master
将master修改成你自己的主节点的主机名


作用：配置Hadoop的核心参数，主要涉及文件系统的访问和临时目录的设置。

fs.defaultFS：指定HDFS的默认访问路径，格式为hdfs://&lt;namenode-host&gt;:&lt;port&gt;。这是Hadoop客户端访问HDFS的入口。
hadoop.tmp.dir：指定Hadoop的临时目录，用于存储运行时的临时文件。




cat&gt; /usr/local/hadoop/etc/hadoop/core-site.xml&lt;&lt;&quot;EOF&quot;&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;fs.defaultFS&lt;/name&gt;                &lt;value&gt;hdfs://master:9000&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;                &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;                &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;        &lt;/property&gt;&lt;/configuration&gt;EOF
3.修改hdfs-site.xml
操作节点：master
将master修改成你自己的主节点的主机名


作用：配置HDFS（Hadoop Distributed File System）的高级参数。

dfs.namenode.secondary.http-address：指定Secondary NameNode的HTTP地址。
dfs.replication：设置HDFS数据块的副本数量，默认为3，这里设置为1（适合单节点测试环境）。
dfs.namenode.name.dir：指定NameNode存储元数据的目录。
dfs.datanode.data.dir：指定DataNode存储数据块的目录。




cat&gt; /usr/local/hadoop/etc/hadoop/hdfs-site.xml&lt;&lt;&quot;EOF&quot;&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;                &lt;value&gt;master:50090&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.replication&lt;/name&gt;                &lt;value&gt;1&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;                &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;                &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;EOF
4.修改mapred-site.xml配置文件
操作节点：master
将master修改成你自己的主节点的主机名


作用：配置MapReduce作业的运行参数。

mapreduce.framework.name：指定MapReduce作业运行的框架（这里是YARN）。
mapreduce.jobhistory.address 和 mapreduce.jobhistory.webapp.address：指定MapReduce作业历史服务器的地址和Web界面地址。
环境变量配置：设置MapReduce作业运行时的环境变量，例如HADOOP_MAPRED_HOME。




cat&gt; /usr/local/hadoop/etc/hadoop/mapred-site.xml&lt;&lt;&quot;EOF&quot;&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;                &lt;value&gt;yarn&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;                &lt;value&gt;master:10020&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;                &lt;value&gt;master:19888&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;&lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;&lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.map.env&lt;/name&gt;&lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.reduce.env&lt;/name&gt;&lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;EOF
5.修改yarn-site.xml文件
操作节点：master
将master修改成你自己的主节点的主机名


作用：配置YARN（Yet Another Resource Negotiator）的参数。

yarn.resourcemanager.hostname：指定ResourceManager的主机名，用于资源管理和作业调度。
yarn.nodemanager.aux-services：启用MapReduce的Shuffle服务，这是MapReduce作业运行的必要配置。




cat&gt; /usr/local/hadoop/etc/hadoop/yarn-site.xml&lt;&lt;&quot;EOF&quot;&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;                &lt;value&gt;master&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;EOF
将上述配置拷贝到slave
操作节点：master
cd /usr/local/hadoop/etc/hadoop/scp core-site.xml slave:/usr/local/hadoop/etc/hadoop/scp hdfs-site.xml slave:/usr/local/hadoop/etc/hadoop/scp mapred-site.xml slave:/usr/local/hadoop/etc/hadoop/scp workers slave:/usr/local/hadoop/etc/hadoop/scp yarn-site.xml slave:/usr/local/hadoop/etc/hadoop/cd
这里是不用输入密码，如果提示你要输入密码，说明你前面4.2的ssh免密没做好
修改环境变量拷贝到slave
操作节点：master
echo &quot;export HDFS_NAMENODE_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export HDFS_DATANODE_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export HDFS_SECONDARYNAMENODE_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export YARN_RESOURCEMANAGER_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export YARN_NODEMANAGER_USER=root&quot; &gt;&gt; /etc/profilesource /etc/profilescp /etc/profile slave:/etc/profilesource /etc/profile
修改hadoop环境配置文件
操作节点：master
并将配置文件拷贝到slave
echo &quot;export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_371&quot; &gt;&gt; /usr/local/hadoop/etc/hadoop/hadoop-env.shscp /usr/local/hadoop/etc/hadoop/hadoop-env.sh slave:/usr/local/hadoop/etc/hadoop/hadoop-env.sh
集群启动
操作节点：master
master初始化
hdfs namenode -format

启动hadoop
操作节点：master
#启动hadoopstart-all.sh
#关闭服务stop-all.sh
启动的时候如果有这些没关系

启动historyserver
操作节点：master
mapred --daemon start historyserver
关闭用
mapred --daemon stop historyserver
查看进程
两个节点说运行的服务如下
root@master:~# jps31364 ResourceManager31140 SecondaryNameNode30856 NameNode32282 Jps28046 JobHistoryServerroot@slave:~# jps6304 DataNode6444 NodeManager6605 Jps
访问hadoop页面
http://192.168.48.128:8088/

http://192.168.48.128:9870/

至此分布式hadoop集群构建成功
这时候就你要给你这两台机，打上hadoop集群部署成功的快照，以便你后期做项目不报错可以恢复
HBase
HBase 是一个面向列式存储的分布式数据库，其设计思想来源于 Google 的 BigTable 论文。HBase 底层存储基于 HDFS 实现，集群的管理基于 ZooKeeper 实现。HBase 良好的分布式架构设计为海量数据的快速存储、随机访问提供了可能，基于数据副本机制和分区机制可以轻松实现在线扩容、缩容和数据容灾，是大数据领域中 Key-Value 数据结构存储最常用的数据库方案。
本实验部署在伪分布机子上
安装
tar -xf /root/hadoop/hbase-2.5.4-bin.tar.gz -C /usr/local/mv /usr/local/hbase-2.5.4 /usr/local/hbaseecho &quot;export HBASE_HOME=/usr/local/hbase&quot; &gt;&gt; /etc/profileecho &quot;export PATH=\$PATH:\$HBASE_HOME/bin&quot; &gt;&gt; /etc/profilesource /etc/profilesed -i &quot;s/CLASSPATH=\$&#123;CLASSPATH&#125;:\$JAVA_HOME\/lib\/tools.jar/CLASSPATH=\$&#123;CLASSPATH&#125;:\$JAVA_HOME\/lib\/tools.jar:\/usr\/local\/hbase\/lib\/*/g&quot; /usr/local/hbase/bin/hbasehbase version

HBase配置
文件里的qianyios要改成你的主机名,运行 HDFS NameNode 的主机名。
hbase.cluster.distributed

设置为 true 表示 HBase 以分布式模式运行。
如果设置为 false，HBase 将以单机模式运行（通常用于测试）。

HBASE_MANAGES_ZK=true

HBASE_MANAGES_ZK=true ：

表示 HBase 将启动并管理自己的嵌入式 ZooKeeper 实例。
这种模式通常用于单机环境或小型测试环境，简化了配置和管理。


HBASE_MANAGES_ZK=false ：

表示 HBase 不会启动自己的 ZooKeeper 实例，而是依赖外部独立的 ZooKeeper 集群。
这种模式适用于生产环境，推荐使用独立的 ZooKeeper 集群以提高稳定性和性能。



echo &quot;export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_371&quot; &gt;&gt; $HBASE_HOME/conf/hbase-env.shecho &quot;export HBASE_CLASSPATH=/usr/local/hbase/conf&quot; &gt;&gt; $HBASE_HOME/conf/hbase-env.shecho &quot;export HBASE_MANAGES_ZK=true&quot; &gt;&gt; $HBASE_HOME/conf/hbase-env.shecho &quot;export HBASE_DISABLE_HADOOP_CLASSPATH_LOOKUP=true&quot; &gt;&gt; $HBASE_HOME/conf/hbase-env.shcat &gt;$HBASE_HOME/conf/hbase-site.xml&lt;&lt;&quot;EOF&quot;&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;hbase.rootdir&lt;/name&gt;                &lt;value&gt;hdfs://qianyios:9000/hbase&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;                &lt;value&gt;true&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt;                &lt;value&gt;false&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;EOF
启动hbase
start-all.sh start-hbase.sh
然后输入jps,有以下三个个就安装成功

测试hbase
hbase shelllist
能运行没报错就行

访问hbase网页
http://192.168.48.128:16010/

关机备份打快照
关机顺序
stop-hbase.shstop-all.shpoweroff
开机顺序
start-all.shstart-hbase.sh
实例测试1



学号（S_No）
姓名（S_Name）
性别（S_Sex）
年龄（S_Age）




2015001
zhangsan
male
23


2015002
Mary
female
22


2015003
Lisi
male
24



创建学生表
hbase shellcreate &#x27;student&#x27;,&#x27;no&#x27;,&#x27;name&#x27;,&#x27;sex&#x27;,&#x27;age&#x27;#查看表结构describe &#x27;student&#x27;

添加数据
s001为行键,行键可以自定义,但是要注意区别,按照前面的学生表,输入第一行s001的学生信息,我这里就简单输入一些信息，做例子用
#查看表的信息scan &#x27;student&#x27;put &#x27;student&#x27;,&#x27;s001&#x27;,&#x27;no&#x27;,&#x27;2015001&#x27;put &#x27;student&#x27;,&#x27;s001&#x27;,&#x27;name&#x27;,&#x27;zhangsan&#x27;scan &#x27;student&#x27;

查看整行
get &#x27;student&#x27;,&#x27;s001&#x27;

查看单元格
get &#x27;student&#x27;,&#x27;s001&#x27;,&#x27;name&#x27;

实例测试2
这是一个订单表

创建order表
创建一个order表，出现两列族userinfo,orderinfo
你看这次的行是1就和上个实例的s001，是不一样，都是可以自定义的
然后在列族下创建列userinfo:name，userinfo:age，orderinfo:id，orderinfo:money
在创建列的同时附带值
create &#x27;order&#x27;,&#x27;userinfo&#x27;,&#x27;orderinfo&#x27;listput &#x27;order&#x27;,&#x27;1&#x27;,&#x27;userinfo:name&#x27;,&#x27;sw&#x27;put &#x27;order&#x27;,&#x27;1&#x27;,&#x27;userinfo:age&#x27;,&#x27;24&#x27;put &#x27;order&#x27;,&#x27;1&#x27;,&#x27;orderinfo:id&#x27;,&#x27;23333&#x27;put &#x27;order&#x27;,&#x27;1&#x27;,&#x27;orderinfo:money&#x27;,&#x27;30&#x27;scan &#x27;order&#x27;

修改数据
put &#x27;order&#x27;,&#x27;1&#x27;,&#x27;userinfo:name&#x27;,&#x27;zhangxiaosan&#x27;get &#x27;order&#x27;,&#x27;1&#x27;,&#x27;userinfo:name&#x27;scan &#x27;order&#x27;

时间戳
数据添加到HBase的时候都会被记录一个时间戳，这个时间戳被我们当做一个版本。
当修改某一条的时候，本质上是往里边新增一条数据，记录的版本加一。

现在要把这条记录的值改为40，实际上就是多添加一条记录，在读的时候按照时间戳读最新的记录

get &#x27;order&#x27;,&#x27;1&#x27;,&#x27;orderinfo:money&#x27;put &#x27;order&#x27;,&#x27;1&#x27;,&#x27;orderinfo:money&#x27;,&#x27;40&#x27;get &#x27;order&#x27;,&#x27;1&#x27;,&#x27;orderinfo:money&#x27;

删除数据
name后一定要加个:
scan &#x27;student&#x27;delete &#x27;student&#x27;,&#x27;s001&#x27;,&#x27;name:&#x27;get &#x27;student&#x27;,&#x27;s001&#x27;,&#x27;name&#x27;

删除表
disable &#x27;student&#x27;describe &#x27;student&#x27;drop &#x27;student&#x27;

Hive
hive 是一种底层封装了Hadoop 的数据仓库处理工具，使用类SQL 的hiveSQL 语言实现数据查询，所有hive 的数据都存储在Hadoop 兼容的文件系统、(例如，[Amazon S3](https://baike.baidu.com/item/Amazon S3/10809744)、HDFS)中。hive 在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS 中hive 设定的目录下，因此，hive 不支持对数据的改写和添加，所有的数据都是在加载的时候确定的。
用户接口Client
用户接口主要有三个：CLI，Client 和 WUI。其中最常用的是 Cli，Cli启动的时候，会同时启动一个 hive 副本。Client 是 hive 的客户端，用户连接至 hive Server。在启动 Client 模式的时候，需要指出 hive Server 所在节点，并且在该节点启动 hive Server。 WUI 是通过浏览器访问 hive。
元数据存储 Metastore
hive 将元数据存储在数据库中，如 mysql、derby。hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。
驱动器：Driver 解释器、编译器、优化器、执行器
解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行。
Hadoop
hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（不包含 * 的查询，比如 select * from tbl 不会生成 MapReduce 任务）。


安装hive
qianyios:3306这里要改成你的主机名
root是数据库的root用户
qianyios666是数据库密码
tar -xf /root/hadoop/apache-hive-3.1.3-bin.tar.gzmv apache-hive-3.1.3-bin /usr/local/hiveecho &quot;export HIVE_HOME=/usr/local/hive&quot; &gt;&gt; /etc/profileecho &quot;export PATH=\$HIVE_HOME/bin:\$PATH&quot; &gt;&gt; /etc/profilesource /etc/profilecat &gt;/usr/local/hive/conf/hive-site.xml&lt;&lt;&quot;EOF&quot;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;    &lt;value&gt;jdbc:mysql://qianyios:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;    &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt;    &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;    &lt;value&gt;root&lt;/value&gt;    &lt;description&gt;username to use against metastore database&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;    &lt;value&gt;qianyios666&lt;/value&gt;    &lt;description&gt;password to use against metastore database&lt;/description&gt;  &lt;/property&gt; 	&lt;property&gt;    &lt;name&gt;hive.exec.mode.local.auto&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;EOF
安装mysql
apt remove mariadb* -yapt install -y net-tools wget mysql-serversystemctl enable --now mysql#mysql初始化mysql_secure_installation
#输入noPress y|Y for Yes, any other key for No: noRemove anonymous users?: yesDisallow root login remotely?: noRemove test database and access to it?: yesReload privilege tables now?: yes
mysql -uroot#创建用户 &#x27;root&#x27;@&#x27;localhost&#x27;CREATE USER IF NOT EXISTS &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;qianyios666&#x27;;GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;localhost&#x27; WITH GRANT OPTION;#将 &#x27;root&#x27;@&#x27;localhost&#x27; 的认证插件切换为 mysql_native_passwordALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;qianyios666&#x27;;#创建用户 &#x27;root&#x27;@&#x27;qianyios&#x27;CREATE USER IF NOT EXISTS &#x27;root&#x27;@&#x27;qianyios&#x27; IDENTIFIED BY &#x27;qianyios666&#x27;;GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;qianyios&#x27; WITH GRANT OPTION;FLUSH PRIVILEGES;create database hive;exitsudo sed -i &#x27;s/^bind-address\s*=\s*127.0.0.1/bind-address = 0.0.0.0/&#x27; /etc/mysql/mysql.conf.d/mysqld.cnfsudo sed -i &#x27;s/^mysqlx-bind-address\s*=\s*127.0.0.1/mysqlx-bind-address = 0.0.0.0/&#x27; /etc/mysql/mysql.conf.d/mysqld.cnfsystemctl restart mysql
启动hive
cdwget -P /root/hadoop/ https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-java-8.0.11.tar.gztar -xf /root/hadoop/mysql-connector-java-8.0.11.tar.gzcp mysql-connector-java-8.0.11/mysql-connector-java-8.0.11.jar /usr/local/hive/lib/mv /usr/local/hive/lib/guava-19.0.jar&#123;,.bak&#125;cp /usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar  /usr/local/hive/libstart-all.sh #hive初始化，这个命令只需要运行一次schematool -dbType mysql -initSchema#启动hivehive
hive初始化有这个说明成功初始化如果失败，检查一下配置文件或者数据库


要退出就按两下ctrl+C
Hive数据类型



类型
描述
示例




TINYINT（tinyint）
一个字节（8位）有符号整数，  -128~127
1


SMALLINT（smallint）
2字节（16位）有符号整数，-32768~32767
1


INT（int）
4字节（32位）有符号整数
1


BIGINT（bigint）
8字节（64位）有符号整数
1


FLOAT（float）
4字节（32位）单精度浮点数
1


DOUBLE（double）
8字节（64位）双精度浮点数
1


DECIMAL(decimal)
任意精度的带符号小数
1


BOOLEAN（boolean）
true/false
true/false


STRING（string）
字符串，变长
‘a’,‘b’,‘1’


VARCHAR（varchar）
变长字符串
‘a’


CHAR（char）
固定长度字符串
‘a’


BINANY（binany）
字节数组
无法表示


TIMESTAMP（timestamp）
时间戳，纳秒精度
1.22327E+11


DATE（date）
日期
‘2016-03-29’



hive的集合数据类型



类型
描述
示例




ARRAY
有序数组，字段的类型必须相同
Array（1，2）


MAP
一组无序的键值对，键的类型必须是原始数据类型，他的值可以是任何类型，同一个映射的键的类型必须相同，值得类型也必须相同
Map（‘a’,1）


STRUCT
一组命名的字段,字段类型可以不同
Struct（‘a’,1,2.0


UNION
UNION则类似于C语言中的UNION结构，在给定的任何一个时间点，UNION类型可以保存指定数据类型中的任意一种




基本命令
以下在hive数据仓库了运行,输入以下命令进入，可能启动有点慢
hive
创建数据库和表
create database hive;use hive;create table usr(id int,name string,age int);
查看和描述数据库和表
show databases;show tables;USE hive;describe database hive;describe hive.usr;

向表中装载数据
insert into usr values(1,&#x27;sina&#x27;,20);#从linux读取数据[root@qianyios555 ~]# echo &quot;2,zhangsan,22&quot; &gt;&gt; /opt/data#从hive导入数据hive&gt; use hive;create table usr1(id int,name string,age int) row format delimited fields terminated by &quot;,&quot;;load data local inpath &#x27;/opt/data&#x27; overwrite into table usr1;
从hdfs中读取数据
#从linux读取数据echo &quot;3,lisi,25&quot; &gt; /opt/test.txthdfs dfs -put /opt/test.txt /hiveuse hive;load data inpath &#x27;hdfs://qianyios:9000/test.txt&#x27; overwrite into table usr1;
从别的表中读取数据
hive&gt; select * from usr;OK1       sina    20hive&gt; select * from usr1;OK3       lisi    25#读取usr1的id=3的数据到usrinsert overwrite table usr select * from usr1 where id=3;hive&gt; select * from usr;OK3       lisi    25
查询表中数据
select * from usr1;
Hive实验：词频统计
在linux上创建输入目录：/opt/input；
mkdir /opt/input
在以上输入目录中添加多个文本文件，其中文件中包含单词：姓名学号，例如：qianyios555；
echo &quot;hello1 qianyios555&quot; &gt; /opt/input/text1.txtecho &quot;hello2 qianyios555&quot; &gt; /opt/input/text2.txtecho &quot;hello3 qianyios555&quot; &gt; /opt/input/text3.txt
在Hive中创建表“docs”，并把输入目录的文件数据加载到该表中；
hiveuse hive;create table docs(line string);load data local inpath &#x27;/opt/input&#x27; overwrite into table docs;select * from docs;
编写HiveQL语句对输入目录的文本进行词频统计，统计单词“姓名学号”出现的次数。
create table word_count asselect word,count(1) as count from(select explode(split(line,&#x27; &#x27;)) as word from docs) wgroup by wordorder by word;select * from word_count;describe word_count;


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop部署</title>
    <url>/posts/32436/</url>
    <content><![CDATA[
Hadoop部署
本笔记分伪分布和分布式两大块，但建议从头开始观看
文章所需资源可点击这里下载
伪分布
单节点 masteryjx48 （Centos 7.9）



名称
ip1（NAT）
内存
硬盘




masteryjx48
192.168.48.11
5G
100G



基本配置
本地yum配置，自行挂载本地Centos7.9镜像
mkdir repo.bakmv /etc/yum.repos.d/* repo.bak/mount /dev/cdrom /mntcat &gt;&gt;/etc/yum.repos.d/local.repo&lt;&lt;EOF[local]name=localbaseurl=file:///mntgpgcheck=0enabled=1EOFyum clean all &amp;&amp; yum makecachesystemctl disable firewalld --nowsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/sysconfig/selinux
更改hosts
echo &quot;192.168.48.11 masteryjx48&quot; &gt;&gt; /etc/hosts
配置主机名
hostnamectl set-hostname masteryjx48 &amp;&amp; bash
配置ssh免密登入
ssh-keygen -t rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keysssh-copy-id masteryjx48ssh masteryjx48exit
安装JAVA环境
mkdir /usr/lib/jvmtar -xf /root/jdk-8u162-linux-x64.tar.gz -C /usr/lib/jvmecho &quot;export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162&quot; &gt;&gt; /etc/profileecho &quot;export PATH=\$JAVA_HOME/bin:\$PATH&quot; &gt;&gt; /etc/profilesource /etc/profilejava -version
安装Hadoop
tar -zxf hadoop-3.1.3.tar.gz -C /usr/localmv /usr/local/hadoop-3.1.3/ /usr/local/hadoopecho &quot;export HADOOP_HOME=/usr/local/hadoop&quot; &gt;&gt; /etc/profileecho &quot;export PATH=\$HADOOP_HOME/bin/:\$HADOOP_HOME/sbin/:\$PATH&quot; &gt;&gt; /etc/profilesource /etc/profilehadoop version
打个快照，方便做分布式部署,做分布式的直接跳到2.分布式
编写配置文件
编写cort-site.yaml文件
[root@masteryjx48 ~]# cat /usr/local/hadoop/etc/hadoop/core-site.xml&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://masteryjx48:9000&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;
编写hdfs-site.xml
[root@masteryjx48 ~]# cat /usr/local/hadoop/etc/hadoop/hdfs-site.xml&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;dfs.replication&lt;/name&gt;        &lt;value&gt;1&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;
启动hdfs服务
hdfs namenode -format
添加环境变量
echo &quot;export HDFS_NAMENODE_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export HDFS_DATANODE_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export HDFS_SECONDARYNAMENODE_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export YARN_RESOURCEMANAGER_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export YARN_NODEMANAGER_USER=root&quot; &gt;&gt; /etc/profilesource /etc/profile
修改hadoop配置文件
echo &quot;export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162&quot; &gt;&gt; /usr/local/hadoop/etc/hadoop/hadoop-env.sh
启动hadoop服务
[root@masteryjx48 ~]# start-all.shStarting namenodes on [masteryjx48]Last login: Thu Mar  9 07:12:02 CST 2023 on pts/0Starting datanodesLast login: Thu Mar  9 07:14:08 CST 2023 on pts/0Starting secondary namenodes [masteryjx48]Last login: Thu Mar  9 07:14:11 CST 2023 on pts/0Starting resourcemanagerLast login: Thu Mar  9 07:14:15 CST 2023 on pts/0Starting nodemanagersLast login: Thu Mar  9 07:14:20 CST 2023 on pts/0
关闭hadoop服务  stop-dfs.sh
启动historyserver服务
mr-jobhistory-daemon.sh start historyserver
查看java进程
[root@masteryjx48 ~]# jps9280 ResourceManager8785 DataNode9443 NodeManager9014 SecondaryNameNode8599 NameNode11127 Jps11034 JobHistoryServer
访问网页ip:9870查看hdfs

访问网页ip:8088查看hadoop

分布式
❗❗❗❗这里就克隆前面创建好的快照（1.6步骤），修改好ip



主机
ip
系统和软件
内存




master
192.168.48.11
Centos7.9、Hadoop
5G


slave1
192.168.48.12
Centos7.9、Hadoop
5G



环境配置
修改hosts 把之前添加的删掉
master
hostnamectl set-hostname masterbash
slave1
hostnamectl set-hostname slave1bashecho &quot;192.168.48.11 master&quot; &gt;&gt; /etc/hostsecho &quot;192.168.48.12 slave1&quot; &gt;&gt; /etc/hostsscp /etc/hosts root@master:/etc/hosts
SSH免密登入设置
master
ssh-keygen -t rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keysssh masterexitssh-copy-id -i root@slave1ssh slave1llexit
slave1
ssh-keygen -t rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keysssh slave1exitssh-copy-id -i root@masterssh masterllexit
检查java和hadoop环境
master[root@master ~]# java -versionjava version &quot;1.8.0_162&quot;Java(TM) SE Runtime Environment (build 1.8.0_162-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.162-b12, mixed mode)[root@master ~]# hadoop versionHadoop 3.1.3Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579Compiled by ztang on 2019-09-12T02:47ZCompiled with protoc 2.5.0From source with checksum ec785077c385118ac91aadde5ec9799This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.1.3.jarslave1[root@slave1 ~]# java -versionjava version &quot;1.8.0_162&quot;Java(TM) SE Runtime Environment (build 1.8.0_162-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.162-b12, mixed mode)[root@slave1 ~]# hadoop versionHadoop 3.1.3Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579Compiled by ztang on 2019-09-12T02:47ZCompiled with protoc 2.5.0From source with checksum ec785077c385118ac91aadde5ec9799This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.1.3.jar
修改配置文件
修改workers配置文件
[root@master hadoop]# cd /usr/local/hadoop/etc/hadoop[root@master hadoop]# cat workersslave1
修改core-site.xml
[root@master hadoop]# cat core-site.xml&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;fs.defaultFS&lt;/name&gt;                &lt;value&gt;hdfs://master:9000&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;                &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;                &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;        &lt;/property&gt;&lt;/configuration&gt;
修改hdfs-site.xml
[root@master hadoop]# cat hdfs-site.xml&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;                &lt;value&gt;master:50090&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.replication&lt;/name&gt;                &lt;value&gt;1&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;                &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;                &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;
修改mapred-site.xml配置文件
[root@master hadoop]# cat mapred-site.xml&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;                &lt;value&gt;yarn&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;                &lt;value&gt;master:10020&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;                &lt;value&gt;master:19888&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;&lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;&lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.map.env&lt;/name&gt;&lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.reduce.env&lt;/name&gt;&lt;value&gt;HADOOP_MAPRED_HOME=/usr/local/hadoop&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;
修改yarn-site.xml文件
[root@master hadoop]# cat yarn-site.xml&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;                &lt;value&gt;master&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;
将上述配置拷贝到slave1上
master
cd /usr/local/hadoop/etc/hadoop/scp core-site.xml slave1:/usr/local/hadoop/etc/hadoop/scp hdfs-site.xml slave1:/usr/local/hadoop/etc/hadoop/scp mapred-site.xml slave1:/usr/local/hadoop/etc/hadoop/scp workers slave1:/usr/local/hadoop/etc/hadoop/scp yarn-site.xml slave1:/usr/local/hadoop/etc/hadoop/
修改环境变量拷贝到slave1
master
echo &quot;export HDFS_NAMENODE_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export HDFS_DATANODE_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export HDFS_SECONDARYNAMENODE_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export YARN_RESOURCEMANAGER_USER=root&quot; &gt;&gt; /etc/profileecho &quot;export YARN_NODEMANAGER_USER=root&quot; &gt;&gt; /etc/profilesource /etc/profilescp /etc/profile slave1:/etc/profilesource /etc/profile
修改hadoop环境配置文件并将配置文件拷贝到slave1
echo &quot;export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162&quot; &gt;&gt; /usr/local/hadoop/etc/hadoop/hadoop-env.shscp /usr/local/hadoop/etc/hadoop/hadoop-env.sh slave1:/usr/local/hadoop/etc/hadoop/hadoop-env.sh
集群配置启动
master初始化
hdfs namenode -format
启动hadoop
start-all.sh
启动historyserver
mr-jobhistory-daemon.sh start historyserver
查看java进程
[root@master hadoop]# jps35863 ResourceManager2841 JobHistoryServer39065 NameNode39771 Jps35597 SecondaryNameNode
[root@slave1 ~]# jps5587 NodeManager5492 DataNode6215 Jps



 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Centos 7</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop组件部署</title>
    <url>/posts/4909/</url>
    <content><![CDATA[
Hadoop组件部署
Hadoop部署 - 严千屹 (qianyios.top)本笔记建立在Hadoop伪分布机子上，可以前往查看安装机子
Zookeeper
你需要克隆出三台hadoop的基础模版机，这里有教程和说明，然后再进行开始操作，这个Zookeeper是hadoop的一个组件，独立出来了的，也就是说是一个独立的Zookeeper集群，Hbase是需要基于Zookeeper运行的，如果你不需要独立的Zookeeper可以不用做这个，hbase有自带的Zookeeper



名称
ip




zk01
192.168.48.11


zk02
192.168.48.12


zk03
192.168.48.13



设置hosts
[root@localhost ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.48.11 zk01192.168.48.12 zk02192.168.48.13 zk03
更改主机名
hostnamectl set-hostname zk01 &amp;&amp; bash
检查java版本
[root@zk01 ~]# java -versionjava version &quot;1.8.0_162&quot;Java(TM) SE Runtime Environment (build 1.8.0_162-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.162-b12, mixed mode)
检查hadoop版本
[root@zk01 ~]# hadoop versionHadoop 3.1.3Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579Compiled by ztang on 2019-09-12T02:47ZCompiled with protoc 2.5.0From source with checksum ec785077c385118ac91aadde5ec9799This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.1.3.jar
安装zookeeper
tar -xf apache-zookeeper-3.8.0-bin.tar.gz -C /optecho &quot;export ZOOKEEPER_HOME=/opt/apache-zookeeper-3.8.0-bin&quot; &gt;&gt; /etc/profileecho &quot;export PATH=\$ZOOKEEPER_HOME/bin:\$PATH&quot; &gt;&gt; /etc/profilesource /etc/profilecd /opt/apache-zookeeper-3.8.0-bin/cp conf/zoo_sample.cfg conf/zoo.cfgvi conf/zoo.cfg    tickTime=2000    dataDir=/opt/apache-zookeeper-3.8.0-bin/data    clientPort=2181    initLimit=10    syncLimit=5    maxClientCnxns=60    server.1=zk01:2888:3888    server.2=zk02:2888:3888    server.3=zk03:2888:3888mkdir /opt/apache-zookeeper-3.8.0-bin/dataecho 1 &gt; /opt/apache-zookeeper-3.8.0-bin/data/myidcat /opt/apache-zookeeper-3.8.0-bin/data/myid
关机克隆出两台机 zk02 zk03
zk02  192.168.48.12
vi /opt/apache-zookeeper-3.8.0-bin/data/myid2
zk03  192.168.48.13
vi /opt/apache-zookeeper-3.8.0-bin/data/myid3
互相ping测试连通性
ping zk01ping zk02ping zk03
能互通说明成功
开启zookeeper服务
需开启两台才能看见Mode: follower
zkServer.sh startzkServer.sh statuszkServer.sh stop
HBase安装
安装hbase
[root@hadoop ~]# ll-rw-r--r--  1 root root 232190985 3月  17 19:37 hbase-2.2.2-bin.tar.gztar -xf hbase-2.2.2-bin.tar.gz -C /usr/local/mv /usr/local/hbase-2.2.2 /usr/local/hbaseecho &quot;export HBASE_HOME=/usr/local/hbase&quot; &gt;&gt; /etc/profileecho &quot;export PATH=\$PATH:\$HBASE_HOME/bin&quot; &gt;&gt; /etc/profilesource /etc/profile------------------------------------------------------------------------[root@hadoop ~]# vi /usr/local/hbase/bin/hbaseCLASSPATH=$&#123;CLASSPATH&#125;:$JAVA_HOME/lib/tools.jar:/usr/local/hbase/lib/*或[root@hadoop ~]# sed -i &quot;s/CLASSPATH=\$&#123;CLASSPATH&#125;:\$JAVA_HOME\/lib\/tools.jar/CLASSPATH=\$&#123;CLASSPATH&#125;:\$JAVA_HOME\/lib\/tools.jar:\/usr\/local\/hbase\/lib\/*/g&quot; /usr/local/hbase/bin/hbase------------------------------------------------------------------------[root@hadoop ~]# hbase version                                                       SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hbase/lib/client-facing-thirdparty/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]HBase 2.2.2Source code repository git://6ad68c41b902/opt/hbase-rm/output/hbase revision=e6513a76c91cceda95dad7af246ac81d46fa2589Compiled by hbase-rm on Sat Oct 19 10:10:12 UTC 2019From source with checksum 4d23f97701e395c5d34db1882ac5021b
HBase配置
HBASE_MANAGES_ZK=true设置为true就是说用hbase自带的Zookeeper，如果你有独立的Zookeeper集群，自行设置
echo &quot;export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162&quot; &gt;&gt; $HBASE_HOME/conf/hbase-env.shecho &quot;export HBASE_CLASSPATH=/usr/local/hbase/conf&quot; &gt;&gt; $HBASE_HOME/conf/hbase-env.shecho &quot;export HBASE_MANAGES_ZK=true&quot; &gt;&gt; $HBASE_HOME/conf/hbase-env.shvi $HBASE_HOME/conf/hbase-site.xml&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;hbase.rootdir&lt;/name&gt;                &lt;value&gt;hdfs://yjx48:9000/hbase&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;                &lt;value&gt;true&lt;/value&gt;#是否分布式运行，false即为单机        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt;                &lt;value&gt;false&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;
HBase启动
start-all.sh start-hbase.sh[root@hadoop hbase]# jps16532 ResourceManager22502 HMaster-----------15799 NameNode16697 NodeManager23097 Jps15962 DataNode22666 HRegionServer-----------16223 SecondaryNameNode22431 HQuorumPeer[root@hadoop ~]# hbase shellhbase(main):001:0&gt; listTABLE0 row(s)Took 0.3118 seconds=&gt; []hbase(main):002:0&gt; exit
访问网页
ip:16010
HBase管理



学号（S_No）
姓名（S_Name）
性别（S_Sex）
年龄（S_Age）




2015001
zhangsan
male
23


2015002
Mary
female
22


2015003
Lisi
male
24



创建学生表
hbase(main):004:0&gt; create &#x27;student&#x27;,&#x27;no&#x27;,&#x27;name&#x27;,&#x27;sex&#x27;,&#x27;age&#x27;Created table studentTook 1.3125 seconds=&gt; Hbase::Table - studenthbase(main):005:0&gt; listTABLEstudent1 row(s)Took 0.0074 seconds=&gt; [&quot;student&quot;]#查看表结构hbase(main):001:0&gt; describe &#x27;student&#x27;Table student is ENABLEDstudentCOLUMN FAMILIES DESCRIPTION&#123;NAME =&gt; &#x27;age&#x27;, VERSIONS =&gt; &#x27;1&#x27;, EVICT_BLOCKS_ON_CLOSE =&gt; &#x27;false&#x27;.......
添加数据
s001为行键
hbase(main):001:0&gt; scan &#x27;student&#x27;ROW                        COLUMN+CELL0 row(s)Took 0.2712 secondshbase(main):002:0&gt; put &#x27;student&#x27;,&#x27;s001&#x27;,&#x27;no&#x27;,&#x27;2015001&#x27;Took 0.0236 secondshbase(main):003:0&gt; put &#x27;student&#x27;,&#x27;s001&#x27;,&#x27;name&#x27;,&#x27;zhangsan&#x27;Took 0.0057 secondshbase(main):004:0&gt; scan &#x27;student&#x27;ROW                        COLUMN+CELL s001                      column=name:, timestamp=1679058447572, value=zhangsan s001                      column=no:, timestamp=1679058447550, value=20150011 row(s)Took 0.0179 seconds
查看整行
hbase(main):001:0&gt; get &#x27;student&#x27;,&#x27;s001&#x27;COLUMN                     CELL name:                     timestamp=1679058447572, value=zhangsan no:                       timestamp=1679058447550, value=20150011 row(s)Took 0.2910 seconds
查看单元格
hbase(main):008:0&gt; get &#x27;student&#x27;,&#x27;s001&#x27;,&#x27;name&#x27;COLUMN                     CELL name:                     timestamp=1679058447572, value=zhangsan1 row(s)Took 0.0053 seconds
订单例子

创建order表
create &#x27;order&#x27;,&#x27;userinfo&#x27;,&#x27;orderinfo&#x27;listput &#x27;order&#x27;,&#x27;1&#x27;,&#x27;userinfo:name&#x27;,&#x27;sw&#x27;put &#x27;order&#x27;,&#x27;1&#x27;,&#x27;userinfo:age&#x27;,&#x27;24&#x27;put &#x27;order&#x27;,&#x27;1&#x27;,&#x27;orderinfo:id&#x27;,&#x27;23333&#x27;put &#x27;order&#x27;,&#x27;1&#x27;,&#x27;orderinfo:money&#x27;,&#x27;30&#x27;scan &#x27;order&#x27;-----------------------------------------------------------hbase(main):017:0* create &#x27;order&#x27;,&#x27;userinfo&#x27;,&#x27;orderinfo&#x27;Created table orderTook 2.3102 seconds=&gt; Hbase::Table - orderhbase(main):018:0&gt; listTABLEorderstudent2 row(s)Took 0.0104 seconds=&gt; [&quot;order&quot;, &quot;student&quot;]hbase(main):019:0&gt; put &#x27;order&#x27;,&#x27;1&#x27;,&#x27;userinfo:name&#x27;,&#x27;sw&#x27;Took 0.0326 secondshbase(main):020:0&gt; put &#x27;order&#x27;,&#x27;1&#x27;,&#x27;userinfo:age&#x27;,&#x27;24&#x27;Took 0.0031 secondshbase(main):021:0&gt; put &#x27;order&#x27;,&#x27;1&#x27;,&#x27;orderinfo:id&#x27;,&#x27;23333&#x27;Took 0.0036 secondshbase(main):022:0&gt; put &#x27;order&#x27;,&#x27;1&#x27;,&#x27;orderinfo:money&#x27;,&#x27;30&#x27;Took 0.0031 secondshbase(main):023:0&gt; scan &#x27;order&#x27;ROW                        COLUMN+CELL 1                         column=orderinfo:id, timestamp=1679060732699, value=23333 1                         column=orderinfo:money, timestamp=1679060732711, value=30 1                         column=userinfo:age, timestamp=1679060732685, value=24 1                         column=userinfo:name, timestamp=1679060732667, value=sw1 row(s)Took 0.0116 seconds
修改数据
hbase(main):001:0&gt; put &#x27;student&#x27;,&#x27;s001&#x27;,&#x27;name&#x27;,&#x27;zhangxiaosan&#x27;Took 0.2879 secondshbase(main):002:0&gt; get &#x27;student&#x27;,&#x27;s001&#x27;,&#x27;name&#x27;COLUMN                     CELL name:                     timestamp=1679061655288, value=zhangxiaosan1 row(s)Took 0.0280 seconds
时间戳
#数据添加到HBase的时候都会被记录一个时间戳，这个时间戳被我们当做一个版本。
当修改某一条的时候，本质上是往里边新增一条数据，记录的版本加一。

#现在要把这条记录的值改为40，实际上就是多添加一条记录，在读的时候按照时间戳读最新的记录

put &#x27;order&#x27;,&#x27;1&#x27;,&#x27;orderinfo:money&#x27;,&#x27;40&#x27;get &#x27;order&#x27;,&#x27;1&#x27;,&#x27;orderinfo:money&#x27;hbase(main):008:0&gt; put &#x27;order&#x27;,&#x27;1&#x27;,&#x27;orderinfo:money&#x27;,&#x27;40&#x27;Took 0.0190 secondshbase(main):009:0&gt; get &#x27;order&#x27;,&#x27;1&#x27;,&#x27;orderinfo:money&#x27;COLUMN                     CELL orderinfo:money           timestamp=1679064515487, value=401 row(s)Took 0.0096 seconds
删除数据
name一定要加个:
scan &#x27;student&#x27;delete &#x27;student&#x27;,&#x27;s001&#x27;,&#x27;name:&#x27;get &#x27;student&#x27;,&#x27;s001&#x27;,&#x27;name&#x27;
删除表
disable &#x27;student&#x27;describe &#x27;student&#x27;drop &#x27;student&#x27;
访问网页
ip:16010
NoSQL数据库安装
（Redis键值对非关系型数据库）
安装redis
tar -xf redis-5.0.5.tar.gzmv redis-5.0.5 /opt/rediscd /opt/redisyum install -y gcc automake autoconf libtool#编译安装make &amp;&amp; make installcd src[root@yjx48 src]# ./redis-server5861:C 30 Mar 2023 08:49:48.699 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo5861:C 30 Mar 2023 08:49:48.699 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=5861, just started5861:C 30 Mar 2023 08:49:48.699 # Warning: no config file specified, using the default config. In order to specify a config file use ./redis-server /path/to/redis.conf5861:M 30 Mar 2023 08:49:48.699 * Increased maximum number of open files to 10032 (it was originally set to 1024).                _._           _.-``__ &#x27;&#x27;-._      _.-``    `.  `_.  &#x27;&#x27;-._           Redis 5.0.5 (00000000/0) 64 bit  .-`` .-```.  ```\/    _.,_ &#x27;&#x27;-._ (    &#x27;      ,       .-`  | `,    )     Running in standalone mode |`-._`-...-` __...-.``-._|&#x27;` _.-&#x27;|     Port: 6379 |    `-._   `._    /     _.-&#x27;    |     PID: 5861  `-._    `-._  `-./  _.-&#x27;    _.-&#x27; |`-._`-._    `-.__.-&#x27;    _.-&#x27;_.-&#x27;| |    `-._`-._        _.-&#x27;_.-&#x27;    |           http://redis.io  `-._    `-._`-.__.-&#x27;_.-&#x27;    _.-&#x27; |`-._`-._    `-.__.-&#x27;    _.-&#x27;_.-&#x27;| |    `-._`-._        _.-&#x27;_.-&#x27;    |  `-._    `-._`-.__.-&#x27;_.-&#x27;    _.-&#x27;      `-._    `-.__.-&#x27;    _.-&#x27;          `-._        _.-&#x27;              `-.__.-&#x27;5861:M 30 Mar 2023 08:49:48.700 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
#另开一个会话[root@yjx48 ~]# cd /opt/redis/src[root@yjx48 src]# ./redis-cli127.0.0.1:6379&gt; set hello worldOK127.0.0.1:6379&gt; get hello&quot;world&quot;127.0.0.1:6379&gt; exit[root@yjx48 src]#
数据库管理

redis语法
#插入数据set student:2015001:sname zhangsanget student:2015001:snameset student:2015001:sex maleget student:2015001:sex#修改数据set student:2015001:sname zhangxiaosanget student:2015001:sname#删除数据get set student:2015001:snamedel set student:2015001:snameget set student:2015001:sname#没数据了
Hash数据库
student表
2015001=&#123;	name=zhangsan	sex=male	age=23&#125;
插入和查询数据
hset student:2015001 name zhangsanhset student:2015001 sex malehset student:2015001 age 23hget student:2015001 name hget student:2015001 sexhgetall student:2015001
修改数据
hset student:2015001 sex femalehget student:2015001 sex female
删除数据
hdel student:2015001 sexhget student:2015001 sex#无数据
MongoDB
​		Mongodb是一个基于分布式文件存储的文档数据库，介于关系数据库和非关系数据库之间，是非关系数
据库当中功能最丰富、最像关系数据库的一种 NOSQL数据库。
​		Mongo最大的特点是支持的查询语言非常强大，语法有点类似于面向对象的查询语言，几乎可以实现类
似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。
​		Mongodb支持的数据结构非常松散，是类似json的bson格式，因此可以存储比较复杂的数据类型。
JSON语法
JSON 语法是 JavaScript 语法的子集。
JSON 数字
JSON 数字可以是整型或者浮点型：
{ “age”:30 }
JSON 对象
JSON 对象在大括号 {} 中书写：
对象可以包含多个名称/值对：
JSON 数组
JSON 数组在中括号 [] 中书写：
数组可包含多个对象：
[&#123; key1 : value1-1 , key2:value1-2 &#125;,&#123; key1 : value2-1 , key2:value2-2 &#125;,&#123; key1 : value3-1 , key2:value3-2 &#125;,...&#123; key1 : valueN-1 , key2:valueN-2 &#125;,]&#123;	&quot;sites&quot;: [        &#123; &quot;name&quot;:&quot;菜鸟教程&quot; , &quot;url&quot;:&quot;www.runoob.com&quot; &#125;,        &#123; &quot;name&quot;:&quot;google&quot; , &quot;url&quot;:&quot;www.google.com&quot; &#125;,        &#123; &quot;name&quot;:&quot;微博&quot; , &quot;url&quot;:&quot;www.weibo.com&quot; &#125;       ]&#125;
在上面的例子中，对象 sites 是包含三个对象的数组。每个对象代表一条关于某个网站（name、url）
的记录。
JSON 布尔值
JSON 布尔值可以是 true 或者 false：
&#123; &quot;flag&quot;:true &#125;
JSON null
JSON 可以设置 null 值：
&#123; &quot;runoob&quot;:null &#125;
MongoDB安装
tar -xf mongodb-linux-x86_64-rhel70-5.0.5.tgz mv mongodb-linux-x86_64-rhel70-5.0.5 /opt/mongodbcd /opt/mongodb/bin./mongo -version#默认情况下 MongoDB 启动后会初始化以下两个目录，事先创建好：#数据存储目录：/var/lib/mongodb#日志文件目录：/var/log/mongodbmkdir -p /var/lib/mongomkdir -p /var/log/mongodb#启动mongodb服务cd /opt/mongodb/bin./mongod --dbpath /var/lib/mongo --logpath /var/log/mongodb/mongod.log --forkps ax | grep mongod./mongo
数据库管理
常用命令
#列出所有数据库
&gt;show dbsadmin 0.000GBconfig 0.000GBlocal 0.000GB
#切换数据库
&gt;use adminswitched to db admin
#显示当前数据库的所有集合
&gt;show collectionssystem.version
#显示集合的所有数据
&gt;db.system.version.find()&#123; &quot;_id&quot; : &quot;featureCompatibilityVersion&quot;, &quot;version&quot; : &quot;5.0&quot; &#125;
创建数据库和集合
#mongodb没有创建数据库命令&gt; use schoolswitched to db school#创建集合，同时会自动创建以上的数据库&gt; db.createCollection(&#x27;student&#x27;)&#123; &quot;ok&quot; : 1 &#125;&gt; show dbsadmin 0.000GBconfig 0.000GBlocal 0.000GBschool 0.000GB&gt; show collectionsStudent
插入数据
#两种方法插入数据：insert和save#_id可以手动输入，否则会自动生成&gt;db.student.insert(&#123;  sno: 2015001,  name: &quot;zhangsan&quot;,  sex: &quot;male&quot;,  age: 23&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt; db.student.find()&#123; &quot;_id&quot; : ObjectId(&quot;642e21279c9d145e592fda70&quot;), &quot;sno&quot; : 2015001, &quot;name&quot; : &quot;zhangsan&quot;, &quot;sex&quot; : &quot;male&quot;, &quot;age&quot; : 23 &#125;&gt; db.student.save(&#123;sno:2015002,name:&#x27;marry&#x27;,sex:&#x27;female&#x27;,age:22&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt; db.student.find()&#123; &quot;_id&quot; : ObjectId(&quot;642e259014c45ed3f90756c0&quot;), &quot;sno&quot; : 2015001, &quot;name&quot; : &quot;zhangsan&quot;, &quot;sex&quot; : &quot;male&quot;, &quot;age&quot; : 23 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;642e259614c45ed3f90756c1&quot;), &quot;sno&quot; : 2015002, &quot;name&quot; : &quot;marry&quot;, &quot;sex&quot; : &quot;female&quot;, &quot;age&quot; : 22 &#125;
#insert和save区别：手动插入一行时，如_id已经存在，insert则出错，save则替代原值。&gt; db.student.insert(&#123;&quot;_id&quot;: ObjectId(&quot;642e259014c45ed3f90756c0&quot;),   &quot;sno&quot;: 2015001,   &quot;name&quot;: &quot;zhangsan&quot;,   &quot;sex&quot;: &quot;male&quot;,   &quot;age&quot;: 23 &#125;)WriteResult(&#123;        &quot;nInserted&quot; : 0,        &quot;writeError&quot; : &#123;                &quot;code&quot; : 11000,                &quot;errmsg&quot; : &quot;E11000 duplicate key error collection: test.student index: _id_ dup key: &#123; _id: ObjectId(&#x27;642e21279c9d145e592fda70&#x27;) &#125;&quot;        &#125;&#125;)#更改年龄23→24&gt; db.student.save(&#123;&quot;_id&quot;: ObjectId(&quot;642e259014c45ed3f90756c0&quot;),   &quot;sno&quot;: 2015001,   &quot;name&quot;: &quot;zhangsan&quot;,   &quot;sex&quot;: &quot;male&quot;,   &quot;age&quot;: 24 &#125;)WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)&gt; db.student.find()&#123; &quot;_id&quot; : ObjectId(&quot;642e259014c45ed3f90756c0&quot;), &quot;sno&quot; : 2015001, &quot;name&quot; : &quot;zhangsan&quot;, &quot;sex&quot; : &quot;male&quot;, &quot;age&quot; : 24 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;642e259614c45ed3f90756c1&quot;), &quot;sno&quot; : 2015002, &quot;name&quot; : &quot;marry&quot;, &quot;sex&quot; : &quot;female&quot;, &quot;age&quot; : 22 &#125;
查找数据
#查询#查询格式：find([query],[fields]，类似于sql的select语句，query相当于where，fields相当于显示的列#查询名字为zhangsan的数据&gt; db.student.find(&#123;name:&#x27;zhangsan&#x27;&#125;)&#123; &quot;_id&quot; : ObjectId(&quot;642e259014c45ed3f90756c0&quot;), &quot;sno&quot; : 2015001, &quot;name&quot; : &quot;zhangsan&quot;, &quot;sex&quot; : &quot;male&quot;, &quot;age&quot; : 24 &#125;#查询名字为zhangsan的人的性别&gt; db.student.find(&#123;name:&#x27;zhangsan&#x27;&#125;,&#123;name:1,sex:1&#125;)&#123; &quot;_id&quot; : ObjectId(&quot;642e259014c45ed3f90756c0&quot;), &quot;name&quot; : &quot;zhangsan&quot;, &quot;sex&quot; : &quot;male&quot; &#125;#不显示_id&gt; db.student.find(&#123;name:&#x27;zhangsan&#x27;&#125;,&#123;_id:0,name:1,sex:1&#125;)&#123; &quot;name&quot; : &quot;zhangsan&quot;, &quot;sex&quot; : &quot;male&quot; &#125;#查询指定列&gt; db.student.find(&#123;&#125;,&#123;name:1&#125;)&#123; &quot;_id&quot; : ObjectId(&quot;642e259014c45ed3f90756c0&quot;), &quot;name&quot; : &quot;zhangsan&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;642e259614c45ed3f90756c1&quot;), &quot;name&quot; : &quot;marry&quot; &#125;#and查询条件 &gt; db.student.find(&#123;name:&#x27;zhangsan&#x27;,sex:&#x27;female&#x27;&#125;)&gt; db.student.find(&#123;name:&#x27;zhangsan&#x27;,sex:&#x27;male&#x27;&#125;)&#123; &quot;_id&quot; : ObjectId(&quot;642e259014c45ed3f90756c0&quot;), &quot;sno&quot; : 2015001, &quot;name&quot; : &quot;zhangsan&quot;, &quot;sex&quot; : &quot;male&quot;, &quot;age&quot; : 24 &#125;&gt;db.student.find(&#123;$or:[&#123;age:24&#125;,&#123;age:22&#125;]&#125;)#or查询&gt; db.student.find(&#123;  $or:[&#123;age:24&#125;,&#123;age:22&#125;]  &#125;)&#123; &quot;_id&quot; : ObjectId(&quot;642e259014c45ed3f90756c0&quot;), &quot;sno&quot; : 2015001, &quot;name&quot; : &quot;zhangsan&quot;, &quot;sex&quot; : &quot;male&quot;, &quot;age&quot; : 24 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;642e259614c45ed3f90756c1&quot;), &quot;sno&quot; : 2015002, &quot;name&quot; : &quot;marry&quot;, &quot;sex&quot; : &quot;female&quot;, &quot;age&quot; : 22 &#125;
修改数据
#格式：update( query, [, upsert_bool, multi_bool] )
#query : update的查询条件，类似sql update查询内where后面的。
#update : update的对象和一些更新的操作符（如$,$inc…）等，也可以理解为sql update查询内
set后面的
#upsert : 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是
false，不插入。
#multi : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查
出来多条记录全部更新。
&gt; db.student.update(&#123;name:&#x27;zhangsan&#x27;&#125;,&#123;$set:&#123;age:23&#125;&#125;)WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)&gt; db.student.find()&#123; &quot;_id&quot; : ObjectId(&quot;642e259014c45ed3f90756c0&quot;), &quot;sno&quot; : 2015001, &quot;name&quot; : &quot;zhangsan&quot;, &quot;sex&quot; : &quot;male&quot;, &quot;age&quot; : 23 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;642e259614c45ed3f90756c1&quot;), &quot;sno&quot; : 2015002, &quot;name&quot; : &quot;marry&quot;, &quot;sex&quot; : &quot;female&quot;, &quot;age&quot; : 22 &#125;
删除数据
&gt; db.student.remove(&#123;name:&#x27;zhangsan&#x27;&#125;)WriteResult(&#123; &quot;nRemoved&quot; : 1 &#125;)&gt; db.student.find()&#123; &quot;_id&quot; : ObjectId(&quot;642e259614c45ed3f90756c1&quot;), &quot;sno&quot; : 2015002, &quot;name&quot; : &quot;marry&quot;, &quot;sex&quot; : &quot;female&quot;, &quot;age&quot; : 22 &#125;
删除集合
&gt; db.createCollection(&#x27;course&#x27;)&#123; &quot;ok&quot; : 1 &#125;&gt; show collectionscoursestudent&gt; db.course.drop()true&gt; show collectionsstudent
Hive数据仓库安装
hive 是一种底层封装了Hadoop 的数据仓库处理工具，使用类SQL 的hiveSQL 语言实现数据查询，所有hive 的数据都存储在Hadoop 兼容的文件系统、(例如，[Amazon S3](https://baike.baidu.com/item/Amazon S3/10809744)、HDFS)中。hive 在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS 中hive 设定的目录下，因此，hive 不支持对数据的改写和添加，所有的数据都是在加载的时候确定的。
用户接口Client
用户接口主要有三个：CLI，Client 和 WUI。其中最常用的是 Cli，Cli启动的时候，会同时启动一个 hive 副本。Client 是 hive 的客户端，用户连接至 hive Server。在启动 Client 模式的时候，需要指出 hive Server 所在节点，并且在该节点启动 hive Server。 WUI 是通过浏览器访问 hive。
元数据存储 Metastore
hive 将元数据存储在数据库中，如 mysql、derby。hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。
驱动器：Driver 解释器、编译器、优化器、执行器
解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行。
Hadoop
hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（不包含 * 的查询，比如 select * from tbl 不会生成 MapReduce 任务）。


安装hive
tar -xf apache-hive-3.1.2-bin.tar.gzmv apache-hive-3.1.2-bin /usr/local/hiveecho &quot;export HIVE_HOME=/usr/local/hive&quot; &gt;&gt; /etc/profileecho &quot;export PATH=\$HIVE_HOME/bin:\$PATH&quot; &gt;&gt; /etc/profilesource /etc/profilecd /usr/local/hive/conf/cp hive-default.xml.template hive-default.xmlvi hive-site.xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;    &lt;value&gt;jdbc:mysql://yjx48:3306/hive?useSSL=false&lt;/value&gt;    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;    &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;    &lt;value&gt;root&lt;/value&gt;    &lt;description&gt;username to use against metastore database&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;    &lt;value&gt;Yjx@666.&lt;/value&gt;    &lt;description&gt;password to use against metastore database&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.exec.mode.local.auto&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;
其中Yjx@666.是mysql密码
安装mysql
cdyum remove mariadb-libs.x86_64 -yyum install -y net-toolsmkdir mysqltar -xf mysql-5.7.37-1.el7.x86_64.rpm-bundle.tar -C mysqlcd mysqlrpm -ivh mysql-community-common-5.7.37-1.el7.x86_64.rpmrpm -ivh mysql-community-libs-5.7.37-1.el7.x86_64.rpmrpm -ivh mysql-community-libs-compat-5.7.37-1.el7.x86_64.rpmrpm -ivh mysql-community-client-5.7.37-1.el7.x86_64.rpmrpm -ivh mysql-community-devel-5.7.37-1.el7.x86_64.rpmrpm -ivh mysql-community-server-5.7.37-1.el7.x86_64.rpmsystemctl enable --now mysqldgrep  &#x27;temporary password&#x27; /var/log/mysqld.logmysqladmin -uroot -p&#x27;darm4hb.2Rsy&#x27; password &#x27;Yjx@666.&#x27;mysql -uroot -pYjx@666.#给root用户授权grant all privileges on *.* to &#x27;root&#x27;@&#x27;localhost&#x27; identified by &#x27;Yjx@666.&#x27; with grant option;grant all privileges on *.* to &#x27;root&#x27;@&#x27;%&#x27; identified by &#x27;Yjx@666.&#x27; with grant option;flush privileges;create database hive;exit
配置和启动hive
cdtar -xf mysql-connector-java-5.1.40.tar.gzcp mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar /usr/local/hive/lib/mv /usr/local/hive/lib/guava-19.0.jar&#123;,.bak&#125;cp /usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar  /usr/local/hive/libstart-all.sh schematool -dbType mysql -initSchemahive
Hive数据类型



类型
描述
示例




TINYINT（tinyint）
一个字节（8位）有符号整数，  -128~127
1


SMALLINT（smallint）
2字节（16位）有符号整数，-32768~32767
1


INT（int）
4字节（32位）有符号整数
1


BIGINT（bigint）
8字节（64位）有符号整数
1


FLOAT（float）
4字节（32位）单精度浮点数
1


DOUBLE（double）
8字节（64位）双精度浮点数
1


DECIMAL(decimal)
任意精度的带符号小数
1


BOOLEAN（boolean）
true/false
true/false


STRING（string）
字符串，变长
‘a’,‘b’,‘1’


VARCHAR（varchar）
变长字符串
‘a’


CHAR（char）
固定长度字符串
‘a’


BINANY（binany）
字节数组
无法表示


TIMESTAMP（timestamp）
时间戳，纳秒精度
1.22327E+11


DATE（date）
日期
‘2016-03-29’



hive的集合数据类型



类型
描述
示例




ARRAY
有序数组，字段的类型必须相同
Array（1，2）


MAP
一组无序的键值对，键的类型必须是原始数据类型，他的值可以是任何类型，同一个映射的键的类型必须相同，值得类型也必须相同
Map（‘a’,1）


STRUCT
一组命名的字段,字段类型可以不同
Struct（‘a’,1,2.0


UNION
UNION则类似于C语言中的UNION结构，在给定的任何一个时间点，UNION类型可以保存指定数据类型中的任意一种




基本命令
创建数据库和表
create database hive;use hive;create table usr(id int,name string,age int);
查看和描述数据库和表
show databases;show tables;describe database hive;describe hive.usr;
向表中装载数据
insert into usr values(1,&#x27;sina&#x27;,20);#从linux读取数据[root@yjx48 ~]# echo &quot;2,zhangsan,22&quot; &gt;&gt; /opt/datahive&gt; use hive;create table usr1(id int,name string,age int) row format delimited fields terminated by &quot;,&quot;;load data local inpath &#x27;/opt/data&#x27; overwrite into table usr1;
从hdfs中读取数据
echo &quot;3,lisi,25&quot; &gt;&gt; /opt/test.txthdfs dfs -put /opt/test.txt /hiveload data inpath &#x27;hdfs://yjx48:9000/test.txt&#x27; overwrite into table usr1;
从别的表中读取数据
hive&gt; select * from usr;OK1       sina    20hive&gt; select * from usr1;OK3       lisi    25#读取usr1的id=3的数据到usrinsert overwrite table usr select * from usr1 where id=3;hive&gt; select * from usr;OK3       lisi    25
查询表中数据
select * from usr1;
Hive实验：词频统计
在linux上创建输入目录：/opt/input；
mkdir /opt/input
在以上输入目录中添加多个文本文件，其中文件中包含单词：姓名学号，例如：yjx48；
echo &quot;hello1 yjx48&quot; &gt;&gt; /opt/input/text1.txtecho &quot;hello2 yjx48&quot; &gt;&gt; /opt/input/text2.txtecho &quot;hello3 yjx48&quot; &gt;&gt; /opt/input/text3.txt
在Hive中创建表“docs”，并把输入目录的文件数据加载到该表中；
hiveuse hive;create table docs(line string);load data local inpath &#x27;/opt/input&#x27; overwrite into table docs;select * from docs;
编写HiveQL语句对输入目录的文本进行词频统计，统计单词“姓名学号”出现的次数。
create table word_count asselect word,count(1) as count from(select explode(split(line,&#x27; &#x27;)) as word from docs) wgroup by wordorder by word;select * from word_count;describe word_count;


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Zookeeper</tag>
        <tag>HBase</tag>
        <tag>NoSQL</tag>
        <tag>MongoDB</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>JavaScript</title>
    <url>/posts/cc1b9611/</url>
    <content><![CDATA[
JavaScript 基础
本笔记整理自黑马程序员前端JavaScript入门到精通全套视频教程，javascript核心进阶ES6语法、API、js高级等基础知识和实战教程
介绍

掌握 JavaScript 的引入方式，初步认识 JavaScript 的作用

引入方式
JavaScript 程序不能独立运行，它需要被嵌入 HTML 中，然后浏览器才能执行 JavaScript 代码。通过 script 标签将 JavaScript 代码引入到 HTML 中，有两种方式：
内部方式
通过 script 标签包裹 JavaScript 代码
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 引入方式&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;!-- 内联形式：通过 script 标签包裹 JavaScript 代码 --&gt;  &lt;script&gt;    alert(&#x27;嗨，欢迎来严千屹博客学习前端技术！&#x27;)  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
外部形式
一般将 JavaScript 代码写在独立的以 .js 结尾的文件中，然后通过 script 标签的 src 属性引入
// demo.jsdocument.write(&#x27;嗨，欢迎来严千屹博客学习前端技术！&#x27;)
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 引入方式&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;!-- 外部形式：通过 script 的 src 属性引入独立的 .js 文件 --&gt;  &lt;script src=&quot;demo.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
如果 script 标签使用 src 属性引入了某 .js 文件，那么 标签的代码会被忽略！！！如下代码所示：
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 引入方式&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;!-- 外部形式：通过 script 的 src 属性引入独立的 .js 文件 --&gt;  &lt;script src=&quot;demo.js&quot;&gt;    // 此处的代码会被忽略掉！！！！  	alert(666);    &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
注释和结束符
通过注释可以屏蔽代码被执行或者添加备注信息，JavaScript 支持两种形式注释语法：
单行注释
使用 //  注释单行代码
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 注释&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;script&gt;    // 这种是单行注释的语法    // 一次只能注释一行    // 可以重复注释    document.write(&#x27;嗨，欢迎来严千屹博客学习前端技术！&#x27;);  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
多行注释
使用 /* */ 注释多行代码
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 注释&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;script&gt;    /* 这种的是多行注释的语法 */    /*    	更常见的多行注释是这种写法    	在些可以任意换行    	多少行都可以      */    document.write(&#x27;嗨，欢迎来严千屹博客学习前端技术！&#x27;)  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
注：编辑器中单行注释的快捷键为 ctrl + /
结束符
在 JavaScript 中 ; 代表一段代码的结束，多数情况下可以省略 ; 使用回车（enter）替代。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 结束符&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;script&gt;     alert(1);    alert(2);    alert(1)    alert(2)  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
实际开发中有许多人主张书写 JavaScript 代码时省略结束符 ;
输入和输出
输出和输入也可理解为人和计算机的交互，用户通过键盘、鼠标等向计算机输入信息，计算机处理后再展示结果给用户，这便是一次输入和输出的过程。
举例说明：如按键盘上的方向键，向上/下键可以滚动页面，按向上/下键这个动作叫作输入，页面发生了滚动了这便叫输出。
输出
JavaScript 可以接收用户的输入，然后再将输入的结果输出：
alert()、document.wirte()
以数字为例，向 alert() 或 document.write()输入任意数字，他都会以弹窗形式展示（输出）给用户。
输入
向 prompt() 输入任意内容会以弹窗形式出现在浏览器中，一般提示用户输入一些内容。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 输入输出&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;script&gt;     // 1. 输入的任意数字，都会以弹窗形式展示    document.write(&#x27;要输出的内容&#x27;)    alert(&#x27;要输出的内容&#x27;);    // 2. 以弹窗形式提示用户输入姓名，注意这里的文字使用英文的引号    prompt(&#x27;请输入您的姓名:&#x27;)  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
变量

了解变量、数据类型、运算符等基础概念，能够实现数据类型的转换，结合四则运算体会如何编程。


体会现实世界中的事物与计算机的关系
理解什么是数据并知道数据的分类
理解变量存储数据的“容器”
掌握常见运算符的使用，了解优先级关系
知道 JavaScript 数据类型隐式转换的特征


理解变量是计算机存储数据的“容器”，掌握变量的声明方式

变量是计算机中用来存储数据的“容器”，它可以让计算机变得有记忆，通俗的理解变量就是使用【某个符号】来代表【某个具体的数值】（数据）
&lt;script&gt;  // x 符号代表了 5 这个数值  x = 5  // y 符号代表了 6 这个数值  y = 6      //举例： 在 JavaScript 中使用变量可以将某个数据（数值）记录下来！  // 将用户输入的内容保存在 num 这个变量（容器）中  num = prompt(&#x27;请输入一数字!&#x27;)  // 通过 num 变量（容器）将用户输入的内容输出出来  alert(num)  document.write(num)&lt;/script&gt;
声明
声明(定义)变量有两部分构成：声明关键字、变量名（标识）
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 声明和赋值&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;script&gt;     // let 变量名    // 声明(定义)变量有两部分构成：声明关键字、变量名（标识）    // let 即关键字，所谓关键字是系统提供的专门用来声明（定义）变量的词语    // age 即变量的名称，也叫标识符    let age  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
关键字是 JavaScript 中内置的一些英文词汇（单词或缩写），它们代表某些特定的含义，如 let 的含义是声明变量的，看到 let  后就可想到这行代码的意思是在声明变量，如 let age;
let 和 var 都是 JavaScript 中的声明变量的关键字，推荐使用 let 声明变量！！！
赋值
声明（定义）变量相当于创造了一个空的“容器”，通过赋值向这个容器中添加数据。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 声明和赋值&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;script&gt;     // 声明(定义)变量有两部分构成：声明关键字、变量名（标识）    // let 即关键字，所谓关键字是系统提供的专门用来声明（定义）变量的词语    // age 即变量的名称，也叫标识符    let age    // 赋值，将 18 这个数据存入了 age 这个“容器”中    age = 18    // 这样 age 的值就成了 18    document.write(age)        // 也可以声明和赋值同时进行    let str = &#x27;hello world!&#x27;    alert(str);  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
关键字
JavaScript 使用专门的关键字 let 和 var 来声明（定义）变量，在使用时需要注意一些细节：
以下是使用 let 时的注意事项：

允许声明和赋值同时进行
不允许重复声明
允许同时声明多个变量并赋值
JavaScript 中内置的一些关键字不能被当做变量名

以下是使用 var 时的注意事项：

允许声明和赋值同时进行
允许重复声明
允许同时声明多个变量并赋值

大部分情况使用 let 和 var 区别不大，但是 let 相较 var 更严谨，因此推荐使用 let，后期会更进一步介绍二者间的区别。
变量名命名规则
关于变量的名称（标识符）有一系列的规则需要遵守：

只能是字母、数字、下划线、$，且不能能数字开头
字母区分大小写，如 Age 和 age 是不同的变量
JavaScript 内部已占用于单词（关键字或保留字）不允许使用
尽量保证变量具有一定的语义，见字知义

注：所谓关键字是指 JavaScript 内部使用的词语，如 let 和var，保留字是指 JavaScript 内部目前没有使用的词语，但是将来可能会使用词语。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 变量名命名规则&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;script&gt;     let age = 18 // 正确    let age1 = 18 // 正确    let _age = 18 // 正确    // let 1age = 18; // 错误，不可以数字开头    let $age = 18 // 正确    let Age = 24 // 正确，它与小写的 age 是不同的变量    // let let = 18; // 错误，let 是关键字    let int = 123 // 不推荐，int 是保留字  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
常量
概念：使用 const 声明的变量称为“常量”。
使用场景：当某个变量永远不会改变的时候，就可以使用 const 来声明，而不是let。
命名规范：和变量一致
const PI = 3.14

注意： 常量不允许重新赋值,声明的时候必须赋值（初始化）

数据类型

计算机世界中的万事成物都是数据。

计算机程序可以处理大量的数据，为了方便数据的管理，将数据分成了不同的类型：
注：通过 typeof 关键字检测数据类型
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 数据类型&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;script&gt;     // 检测 1 是什么类型数据，结果为 number    document.write(typeof 1)  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
数值类型
即我们数学中学习到的数字，可以是整数、小数、正数、负数
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 数据类型&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;script&gt;     let score = 100 // 正整数    let price = 12.345 // 小数    let temperature = -40 // 负数    document.write(typeof score) // 结果为 number    document.write(typeof price) // 结果为 number    document.write(typeof temperature) // 结果为 number  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
JavaScript 中的数值类型与数学中的数字是一样的，分为正数、负数、小数等。
字符串类型
通过单引号（ ''） 、双引号（ &quot;&quot;）或反引号包裹的数据都叫字符串，单引号和双引号没有本质上的区别，推荐使用单引号。
注意事项：

无论单引号或是双引号必须成对使用
单引号/双引号可以互相嵌套，但是不以自已嵌套自已
必要时可以使用转义符 \，输出单引号或双引号

&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 数据类型&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;script&gt;     let user_name = &#x27;小明&#x27; // 使用单引号    let gender = &quot;男&quot; // 使用双引号    let str = &#x27;123&#x27; // 看上去是数字，但是用引号包裹了就成了字符串了    let str1 = &#x27;&#x27; // 这种情况叫空字符串		    documeent.write(typeof user_name) // 结果为 string    documeent.write(typeof gender) // 结果为 string    documeent.write(typeof str) // 结果为 string  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
布尔类型
表示肯定或否定时在计算机中对应的是布尔类型数据，它有两个固定的值 true 和 false，表示肯定的数据用 true，表示否定的数据用 false。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 数据类型&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;script&gt;     //  pink老师帅不帅？回答 是 或 否    let isCool = true // 是的，摔死了！    isCool = false // 不，套马杆的汉子！    document.write(typeof isCool) // 结果为 boolean  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
undefined
未定义是比较特殊的类型，只有一个值 undefined，只声明变量，不赋值的情况下，变量的默认值为 undefined，一般很少【直接】为某个变量赋值为 undefined。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 数据类型&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;script&gt;     // 只声明了变量，并末赋值    let tmp;    document.write(typeof tmp) // 结果为 undefined  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
注：JavaScript 中变量的值决定了变量的数据类型。
类型转换

理解弱类型语言的特征，掌握显式类型转换的方法

在 JavaScript 中数据被分成了不同的类型，如数值、字符串、布尔值、undefined，在实际编程的过程中，不同数据类型之间存在着转换的关系。
隐式转换
某些运算符被执行时，系统内部自动将数据类型进行转换，这种转换称为隐式转换。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 隐式转换&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;script&gt;     let num = 13 // 数值    let num2 = &#x27;2&#x27; // 字符串    // 结果为 132    // 原因是将数值 num 转换成了字符串，相当于 &#x27;13&#x27;    // 然后 + 将两个字符串拼接到了一起    console.log(num + num2)    // 结果为 11    // 原因是将字符串 num2 转换成了数值，相当于 2    // 然后数值 13 减去 数值 2    console.log(num - num2)    let a = prompt(&#x27;请输入一个数字&#x27;)    let b = prompt(&#x27;请再输入一个数字&#x27;)    alert(a + b);  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
注：数据类型的隐式转换是 JavaScript 的特征，后续学习中还会遇到，目前先需要理解什么是隐式转换。
补充介绍模板字符串的拼接的使用
显式转换
编写程序时过度依靠系统内部的隐式转换是不严禁的，因为隐式转换规律并不清晰，大多是靠经验总结的规律。为了避免因隐式转换带来的问题，通常根逻辑需要对数据进行显示转换。
Number
通过 Number 显示转换成数值类型，当转换失败时结果为 NaN（Not a Number）即不是一个数字。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 隐式转换&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;script&gt;    let t = &#x27;12&#x27;    let f = 8    // 显式将字符串 12 转换成数值 12    t = Number(t)    // 检测转换后的类型    // console.log(typeof t);    console.log(t + f) // 结果为 20    // 并不是所有的值都可以被转成数值类型    let str = &#x27;hello&#x27;    // 将 hello 转成数值是不现实的，当无法转换成    // 数值时，得到的结果为 NaN （Not a Number）    console.log(Number(str))  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
运算符

理解什么是流程控制，知道条件控制的种类并掌握其对应的语法规则，具备利用循环编写简易ATM取款机程序能力


运算符
语句
综合案例

算术运算符
数字是用来计算的，比如：乘法 * 、除法 / 、加法 + 、减法 - 等等，所以经常和算术运算符一起。
算术运算符：也叫数学运算符，主要包括加、减、乘、除、取余（求模）等



运算符
作用




+
求和


-
求差


*
求积


/
求商


%
取模（取余数），开发中经常用于作为某个数字是否被整除




注意：在计算失败时，显示的结果是 NaN （not a number）

// 算术运算符console.log(1 + 2 * 3 / 2) //  4 let num = 10console.log(num + 10)  // 20console.log(num + num)  // 20// 1. 取模(取余数)  使用场景：  用来判断某个数是否能够被整除console.log(4 % 2) //  0  console.log(6 % 3) //  0console.log(5 % 3) //  2console.log(3 % 5) //  3// 2. 注意事项 : 如果我们计算失败，则返回的结果是 NaN (not a number)console.log(&#x27;pink老师&#x27; - 2)console.log(&#x27;pink老师&#x27; * 2)console.log(&#x27;pink老师&#x27; + 2)   // pink老师2
赋值运算符
赋值运算符：对变量进行赋值的运算符
=     将等号右边的值赋予给左边, 要求左边必须是一个容器



运算符
作用




+=
加法赋值


-+
减法赋值


*=
乘法赋值


/=
除法赋值


%=
取余赋值



&lt;script&gt;let num = 1// num = num + 1// 采取赋值运算符// num += 1num += 3console.log(num)&lt;/script&gt;
自增/自减运算符



符号
作用
说明




++
自增
变量自身的值加1，例如: x++


–
自减
变量自身的值减1，例如: x–




在前和在后在单独使用时二者并没有差别，而且一般开发中我们都是独立使用
++在后（后缀式）我们会使用更多


注意：

只有变量能够使用自增和自减运算符
、-- 可以在变量前面也可以在变量后面，比如: x  或者  ++x


&lt;script&gt;    // let num = 10    // num = num + 1    // num += 1    // // 1. 前置自增    // let i = 1    // ++i    // console.log(i)    // let i = 1    // console.log(++i + 1)    // 2. 后置自增    // let i = 1    // i++    // console.log(i)    // let i = 1    // console.log(i++ + 1)    // 了解     let i = 1    console.log(i++ + ++i + i)  &lt;/script&gt;
比较运算符
使用场景：比较两个数据大小、是否相等，根据比较结果返回一个布尔值（true / false）



运算符
作用




&gt;
左边是否大于右边


&lt;
左边是否小于右边


&gt;=
左边是否大于或等于右边


&lt;=
左边是否小于或等于右边


===
左右两边是否类型和值都相等（重点）


==
左右两边值是否相等


!=
左右值不相等


!==
左右两边是否不全等



&lt;script&gt;  console.log(3 &gt; 5)  console.log(3 &gt;= 3)  console.log(2 == 2)  // 比较运算符有隐式转换 把&#x27;2&#x27; 转换为 2  双等号 只判断值  console.log(2 == &#x27;2&#x27;)  // true  // console.log(undefined === null)  // === 全等 判断 值 和 数据类型都一样才行  // 以后判断是否相等 请用 ===    console.log(2 === &#x27;2&#x27;)  console.log(NaN === NaN) // NaN 不等于任何人，包括他自己  console.log(2 !== &#x27;2&#x27;)  // true    console.log(2 != &#x27;2&#x27;) // false   console.log(&#x27;-------------------------&#x27;)  console.log(&#x27;a&#x27; &lt; &#x27;b&#x27;) // true  console.log(&#x27;aa&#x27; &lt; &#x27;ab&#x27;) // true  console.log(&#x27;aa&#x27; &lt; &#x27;aac&#x27;) // true  console.log(&#x27;-------------------------&#x27;)&lt;/script&gt;
逻辑运算符
使用场景：可以把多个布尔值放到一起运算，最终返回一个布尔值



符号
名称
日常读法
特点
口诀




&amp;&amp;
逻辑与
并且
符号两边有一个假的结果为假
一假则假


||
逻辑或
或者
符号两边有一个真的结果为真
一真则真


!
逻辑非
取反
true变false  false变true
真变假，假变真






A
B
A &amp;&amp; B
A || B
!A




false
false
false
false
true


false
true
false
true
true


true
false
false
true
false


true
true
true
true
false



&lt;script&gt;    // 逻辑与 一假则假    console.log(true &amp;&amp; true)    console.log(false &amp;&amp; true)    console.log(3 &lt; 5 &amp;&amp; 3 &gt; 2)    console.log(3 &lt; 5 &amp;&amp; 3 &lt; 2)    console.log(&#x27;-----------------&#x27;)    // 逻辑或 一真则真    console.log(true || true)    console.log(false || true)    console.log(false || false)    console.log(&#x27;-----------------&#x27;)    // 逻辑非  取反    console.log(!true)    console.log(!false)    console.log(&#x27;-----------------&#x27;)    let num = 6    console.log(num &gt; 5 &amp;&amp; num &lt; 10)    console.log(&#x27;-----------------&#x27;)  &lt;/script&gt;
运算符优先级


逻辑运算符优先级： ！&gt; &amp;&amp; &gt;  ||

语句
表达式和语句

分支语句
分支语句可以根据条件判定真假，来选择性的执行想要的代码
分支语句包含：

if分支语句（重点）
三元运算符
switch语句

if 分支语句
语法：
if(条件表达式) &#123;  // 满足条件要执行的语句&#125;
小括号内的条件结果是布尔值，为 true 时，进入大括号里执行代码；为false，则不执行大括号里面代码
小括号内的结果若不是布尔类型时，会发生类型转换为布尔值，类似Boolean()
如果大括号只有一个语句，大括号可以省略，但是，俺们不提倡这么做~
&lt;script&gt;    // 单分支语句    // if (false) &#123;    //   console.log(&#x27;执行语句&#x27;)    // &#125;    // if (3 &gt; 5) &#123;    //   console.log(&#x27;执行语句&#x27;)    // &#125;    // if (2 === &#x27;2&#x27;) &#123;    //   console.log(&#x27;执行语句&#x27;)    // &#125;    //  1. 除了0 所有的数字都为真    //   if (0) &#123;    //     console.log(&#x27;执行语句&#x27;)    //   &#125;    // 2.除了 &#x27;&#x27; 所有的字符串都为真 true    // if (&#x27;pink老师&#x27;) &#123;    //   console.log(&#x27;执行语句&#x27;)    // &#125;    // if (&#x27;&#x27;) &#123;    //   console.log(&#x27;执行语句&#x27;)    // &#125;    // // if (&#x27;&#x27;) console.log(&#x27;执行语句&#x27;)    // 1. 用户输入    let score = +prompt(&#x27;请输入成绩&#x27;)    // 2. 进行判断输出    if (score &gt;= 700) &#123;      alert(&#x27;恭喜考入黑马程序员&#x27;)    &#125;    console.log(&#x27;-----------------&#x27;)  &lt;/script&gt;
if双分支语句
如果有两个条件的时候，可以使用 if else 双分支语句
if (条件表达式)&#123;  // 满足条件要执行的语句&#125; else &#123;  // 不满足条件要执行的语句&#125;
例如：
&lt;script&gt;   // 1. 用户输入   let uname = prompt(&#x27;请输入用户名:&#x27;)   let pwd = prompt(&#x27;请输入密码:&#x27;)   // 2. 判断输出   if (uname === &#x27;pink&#x27; &amp;&amp; pwd === &#x27;123456&#x27;) &#123;     alert(&#x27;恭喜登录成功&#x27;)   &#125; else &#123;     alert(&#x27;用户名或者密码错误&#x27;)   &#125; &lt;/script&gt;
if 多分支语句
使用场景： 适合于有多个条件的时候
&lt;script&gt;   // 1. 用户输入   let score = +prompt(&#x27;请输入成绩：&#x27;)   // 2. 判断输出   if (score &gt;= 90) &#123;     alert(&#x27;成绩优秀，宝贝，你是我的骄傲&#x27;)   &#125; else if (score &gt;= 70) &#123;     alert(&#x27;成绩良好，宝贝，你要加油哦~~&#x27;)   &#125; else if (score &gt;= 60) &#123;     alert(&#x27;成绩及格，宝贝，你很危险~&#x27;)   &#125; else &#123;     alert(&#x27;成绩不及格，宝贝，我不想和你说话，我只想用鞭子和你说话~&#x27;)   &#125; &lt;/script&gt;
三元运算符（三元表达式）
使用场景： 一些简单的双分支，可以使用  三元运算符（三元表达式），写起来比 if  else双分支 更简单
符号：? 与 : 配合使用
语法：
条件 ? 表达式1 ： 表达式2
例如：
// 三元运算符（三元表达式）// 1. 语法格式// 条件 ? 表达式1 : 表达式2 // 2. 执行过程 // 2.1 如果条件为真，则执行表达式1// 2.2 如果条件为假，则执行表达式2// 3. 验证// 5 &gt; 3 ? &#x27;真的&#x27; : &#x27;假的&#x27;console.log(5 &lt; 3 ? &#x27;真的&#x27; : &#x27;假的&#x27;)// let age = 18 // age = age + 1//  age++// 1. 用户输入 let num = prompt(&#x27;请您输入一个数字:&#x27;)// 2. 判断输出- 小于10才补0// num = num &lt; 10 ? 0 + num : numnum = num &gt;= 10 ? num : 0 + numalert(num)
switch语句（了解）
使用场景： 适合于有多个条件的时候，也属于分支语句，大部分情况下和 if多分支语句 功能相同
注意：

switch case语句一般用于等值判断, if适合于区间判断
switchcase一般需要配合break关键字使用 没有break会造成case穿透
if 多分支语句开发要比switch更重要，使用也更多

例如：
// switch分支语句// 1. 语法// switch (表达式) &#123;//   case 值1://     代码1//     break//   case 值2://     代码2//     break//   ...//   default://     代码n// &#125;&lt;script&gt;  switch (2) &#123;    case 1:    console.log(&#x27;您选择的是1&#x27;)    break  // 退出switch    case 2:    console.log(&#x27;您选择的是2&#x27;)    break  // 退出switch    case 3:    console.log(&#x27;您选择的是3&#x27;)    break  // 退出switch    default:    console.log(&#x27;没有符合条件的&#x27;)  &#125;&lt;/script&gt;
断点调试
**作用：**学习时可以帮助更好的理解代码运行，工作时可以更快找到bug
浏览器打开调试界面

按F12打开开发者工具
点到源代码一栏 （ sources ）
选择代码文件

**断点：**在某句代码上加的标记就叫断点，当程序执行到这句有标记的代码时会暂停下来
循环语句
使用场景：重复执行 指定的一段代码，比如我们想要输出10次 ‘我学的很棒’
学习路径：
1.while循环
2.for 循环（重点）
while循环
while :  在…. 期间， 所以 while循环 就是在满足条件期间，重复执行某些代码。
语法：
while (条件表达式) &#123;   // 循环体    &#125;
例如：
// while循环: 重复执行代码// 1. 需求: 利用循环重复打印3次 &#x27;月薪过万不是梦，毕业时候见英雄&#x27;let i = 1while (i &lt;= 3) &#123;  document.write(&#x27;月薪过万不是梦，毕业时候见英雄~&lt;br&gt;&#x27;)  i++   // 这里千万不要忘了变量自增否则造成死循环&#125;
循环三要素：
1.初始值 （经常用变量）
2.终止条件
3.变量的变化量
例如：
&lt;script&gt;  // // 1. 变量的起始值  // let i = 1  // // 2. 终止条件  // while (i &lt;= 3) &#123;  //   document.write(&#x27;我要循环三次 &lt;br&gt;&#x27;)  //   // 3. 变量的变化量  //   i++  // &#125;  // 1. 变量的起始值  let end = +prompt(&#x27;请输入次数:&#x27;)let i = 1// 2. 终止条件while (i &lt;= end) &#123;  document.write(&#x27;我要循环三次 &lt;br&gt;&#x27;)  // 3. 变量的变化量  i++&#125;&lt;/script&gt;
中止循环
break   中止整个循环，一般用于结果已经得到, 后续的循环不需要的时候可以使用（提高效率）
continue  中止本次循环，一般用于排除或者跳过某一个选项的时候
&lt;script&gt;    // let i = 1    // while (i &lt;= 5) &#123;    //   console.log(i)    //   if (i === 3) &#123;    //     break  // 退出循环    //   &#125;    //   i++    // &#125;    let i = 1    while (i &lt;= 5) &#123;      if (i === 3) &#123;        i++        continue      &#125;      console.log(i)      i++    &#125;  &lt;/script&gt;
无限循环
1.while(true) 来构造“无限”循环，需要使用break退出循环。（常用）
2.for(;;) 也可以来构造“无限”循环，同样需要使用break退出循环。
// 无限循环  // 需求： 页面会一直弹窗询问你爱我吗？// (1). 如果用户输入的是 &#x27;爱&#x27;，则退出弹窗// (2). 否则一直弹窗询问// 1. while(true) 无限循环// while (true) &#123;//   let love = prompt(&#x27;你爱我吗?&#x27;)//   if (love === &#x27;爱&#x27;) &#123;//     break//   &#125;// &#125;// 2. for(;;) 无限循环for (; ;) &#123;  let love = prompt(&#x27;你爱我吗?&#x27;)  if (love === &#x27;爱&#x27;) &#123;    break  &#125;&#125;
总结

if 多分支语句和 switch的区别：


共同点

都能实现多分支选择， 多选1
大部分情况下可以互换



区别：

switch…case语句通常处理case为比较确定值的情况，而if…else…语句更加灵活，通常用于范围判断(大于，等于某个范围)。
switch 语句进行判断后直接执行到程序的语句，效率更高，而if…else语句有几种判断条件，就得判断多少次
switch 一定要注意 必须是 ===  全等，一定注意 数据类型，同时注意break否则会有穿透效果
结论：

当分支比较少时，if…else语句执行效率高。
当分支比较多时，switch语句执行效率高，而且结构更清晰。






综合案例-ATM存取款机

分析：
①：提示输入框写到循环里面（无限循环）
②：用户输入4则退出循环 break
③：提前准备一个金额预先存储一个数额 money
④：根据输入不同的值，做不同的操作
​     (1)  取钱则是减法操作， 存钱则是加法操作，查看余额则是直接显示金额
​     (2) 可以使用 if else if 多分支 来执行不同的操作
完整代码：
&lt;script&gt;  // 1. 开始循环 输入框写到 循环里面  // 3. 准备一个总的金额  let money = 100while (true) &#123;  let re = +prompt(`请您选择操作：1.存钱2.取钱3.查看余额4.退出`)  // 2. 如果用户输入的 4 则退出循环， break  写到if 里面，没有写到switch里面， 因为4需要break退出循环  if (re === 4) &#123;    break  &#125;  // 4. 根据输入做操作  switch (re) &#123;    case 1:      // 存钱      let cun = +prompt(&#x27;请输入存款金额&#x27;)      money = money + cun      break      case 2:      // 存钱      let qu = +prompt(&#x27;请输入取款金额&#x27;)      money = money - qu      break      case 3:      // 存钱      alert(`您的银行卡余额是$&#123;money&#125;`)      break  &#125;&#125;&lt;/script&gt;
for 语句

掌握 for 循环语句，让程序具备重复执行能力

for 是 JavaScript 提供的另一种循环控制的话句，它和 while 只是语法上存在差异。
for语句的基本使用

实现循环的 3 要素

&lt;script&gt;  // 1. 语法格式  // for(起始值; 终止条件; 变化量) &#123;  //   // 要重复执行的代码  // &#125;  // 2. 示例：在网页中输入标题标签  // 起始值为 1  // 变化量 i++  // 终止条件 i &lt;= 6  for(let i = 1; i &lt;= 6; i++) &#123;    document.write(`&lt;h$&#123;i&#125;&gt;循环控制，即重复执行&lt;h$&#123;i&#125;&gt;`)  &#125;&lt;/script&gt;


变化量和死循环，for 循环和 while 一样，如果不合理设置增量和终止条件，便会产生死循环。


跳出和终止循环


&lt;script&gt;    // 1. continue     for (let i = 1; i &lt;= 5; i++) &#123;        if (i === 3) &#123;            continue  // 结束本次循环，继续下一次循环        &#125;        console.log(i)    &#125;    // 2. break    for (let i = 1; i &lt;= 5; i++) &#123;        if (i === 3) &#123;            break  // 退出结束整个循环        &#125;        console.log(i)    &#125;&lt;/script&gt;
结论：

JavaScript 提供了多种语句来实现循环控制，但无论使用哪种语句都离不开循环的3个特征，即起始值、变化量、终止条件，做为初学者应着重体会这3个特征，不必过多纠结三种语句的区别。
起始值、变化量、终止条件，由开发者根据逻辑需要进行设计，规避死循环的发生。
当如果明确了循环的次数的时候推荐使用for循环,当不明确循环的次数的时候推荐使用while循环


注意：for 的语法结构更简洁，故 for 循环的使用频次会更多。

循环嵌套
利用循环的知识来对比一个简单的天文知识，我们知道地球在自转的同时也在围绕太阳公转，如果把自转和公转都看成是循环的话，就相当于是循环中又嵌套了另一个循环。

实际上 JavaScript 中任何一种循环语句都支持循环的嵌套，如下代码所示：

// 1. 外面的循环 记录第n天 for (let i = 1; i &lt; 4; i++) &#123;    document.write(`第$&#123;i&#125;天 &lt;br&gt;`)    // 2. 里层的循环记录 几个单词    for (let j = 1; j &lt; 6; j++) &#123;        document.write(`记住第$&#123;j&#125;个单词&lt;br&gt;`)    &#125;&#125;
记住，外层循环循环一次，里层循环循环全部
倒三角
 // 外层打印几行for (let i = 1; i &lt;= 5; i++) &#123;    // 里层打印几个星星    for (let j = 1; j &lt;= i; j++) &#123;        document.write(&#x27;★&#x27;)    &#125;    document.write(&#x27;&lt;br&gt;&#x27;)&#125;

九九乘法表
样式css
span &#123;    display: inline-block;    width: 100px;    padding: 5px 10px;    border: 1px solid pink;    margin: 2px;    border-radius: 5px;    box-shadow: 2px 2px 2px rgba(255, 192, 203, .4);    background-color: rgba(255, 192, 203, .1);    text-align: center;    color: hotpink;&#125;
javascript
 // 外层打印几行for (let i = 1; i &lt;= 9; i++) &#123;    // 里层打印几个星星    for (let j = 1; j &lt;= i; j++) &#123;        // 只需要吧 ★ 换成  1 x 1 = 1           document.write(`		&lt;div&gt; $&#123;j&#125; x $&#123;i&#125; = $&#123;j * i&#125; &lt;/div&gt;     `)    &#125;    document.write(&#x27;&lt;br&gt;&#x27;)&#125;

数组

知道什么是数组及其应用的场景，掌握数组声明及访问的语法。

数组是什么？
数组：(Array)是一种可以按顺序保存数据的数据类型
**使用场景：**如果有多个数据可以用数组保存起来，然后放到一个变量中，管理非常方便
数组的基本使用
定义数组和数组单元
&lt;script&gt;  // 1. 语法，使用 [] 来定义一个空数组  // 定义一个空数组，然后赋值给变量 classes  // let classes = [];  // 2. 定义非空数组  let classes = [&#x27;小明&#x27;, &#x27;小刚&#x27;, &#x27;小红&#x27;, &#x27;小丽&#x27;, &#x27;小米&#x27;]&lt;/script&gt;
通过 [] 定义数组，数据中可以存放真正的数据，如小明、小刚、小红等这些都是数组中的数据，我们这些数据称为数组单元，数组单元之间使用英文逗号分隔。
访问数组和数组索引
使用数组存放数据并不是最终目的，关键是能够随时的访问到数组中的数据（单元）。其实 JavaScript 为数组中的每一个数据单元都编了号，通过数据单元在数组中的编号便可以轻松访问到数组中的数据单元了。
我们将数据单元在数组中的编号称为索引值，也有人称其为下标。
索引值实际是按着数据单元在数组中的位置依次排列的，注意是从 0 开始的，如下图所示：

观察上图可以数据单元【小明】对应的索引值为【0】，数据单元【小红】对应的索引值为【2】
&lt;script&gt;  let classes = [&#x27;小明&#x27;, &#x27;小刚&#x27;, &#x27;小红&#x27;, &#x27;小丽&#x27;, &#x27;小米&#x27;]    // 1. 访问数组，语法格式为：变量名[索引值]  document.write(classes[0] // 结果为：小明  document.write(classes[1] // 结果为：小刚  document.write(classes[4] // 结果为：小米    // 2. 通过索引值还可以为数组单重新赋值  document.write(classes[3] // 结果为：小丽  // 重新为索引值为 3 的单元赋值  classes[3] = &#x27;小小丽&#x27;  document.wirte(classes[3]; // 结果为： 小小丽&lt;/script&gt;
数据单元值类型
数组做为数据的集合，它的单元值可以是任意数据类型
&lt;script&gt;  // 6. 数组单值类型可以是任意数据类型  // a) 数组单元值的类型为字符类型  let list = [&#x27;HTML&#x27;, &#x27;CSS&#x27;, &#x27;JavaScript&#x27;]  // b) 数组单元值的类型为数值类型  let scores = [78, 84, 70, 62, 75]  // c) 混合多种类型  let mixin = [true, 1, false, &#x27;hello&#x27;]&lt;/script&gt;
数组长度属性
重申一次，数组在 JavaScript 中并不是新的数据类型，它属于对象类型。
&lt;script&gt;  // 定义一个数组  let arr = [&#x27;html&#x27;, &#x27;css&#x27;, &#x27;javascript&#x27;]  // 数组对应着一个 length 属性，它的含义是获取数组的长度  console.log(arr.length) // 3&lt;/script&gt;
操作数组
数组做为对象数据类型，不但有 length 属性可以使用，还提供了许多方法：

push 动态向数组的尾部添加一个单元
unshit 动态向数组头部添加一个单元
pop 删除最后一个单元
shift 删除第一个单元
splice 动态删除任意单元

使用以上4个方法时，都是直接在原数组上进行操作，即成功调任何一个方法，原数组都跟着发生相应的改变。并且在添加或删除单元时 length 并不会发生错乱。
&lt;script&gt;  // 定义一个数组  let arr = [&#x27;html&#x27;, &#x27;css&#x27;, &#x27;javascript&#x27;]  // 1. push 动态向数组的尾部添加一个单元  arr.push(&#x27;Nodejs&#x27;)  console.log(arr)  arr.push(&#x27;Vue&#x27;)  // 2. unshit 动态向数组头部添加一个单元  arr.unshift(&#x27;VS Code&#x27;)  console.log(arr)  // 3. splice 动态删除任意单元  arr.splice(2, 1) // 从索引值为2的位置开始删除1个单元  console.log(arr)  // 4. pop 删除最后一个单元  arr.pop()  console.log(arr)  // 5. shift 删除第一个单元  arr.shift()  console.log(arr)&lt;/script&gt;
函数

理解封装的意义，能够通过函数的声明实现逻辑的封装，知道对象数据类型的特征，结合数学对象实现简单计算功能。


理解函数的封装的特征
掌握函数声明的语法
理解什么是函数的返回值
知道并能使用常见的内置函数


理解函数的封装特性，掌握函数的语法规则

声明和调用
函数可以把具有相同或相似逻辑的代码“包裹”起来，通过函数调用执行这些被“包裹”的代码逻辑，这么做的优势是有利于精简代码方便复用。
声明（定义）
声明（定义）一个完整函数包括关键字、函数名、形式参数、函数体、返回值5个部分

调用
声明（定义）的函数必须调用才会真正被执行，使用 () 调用函数。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 声明和调用&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;script&gt;    // 声明（定义）了最简单的函数，既没有形式参数，也没有返回值    function sayHi() &#123;      console.log(&#x27;嗨~&#x27;)    &#125;    // 函数调用，这些函数体内的代码逻辑会被执行    // 函数名()            sayHi()    // 可以重复被调用，多少次都可以    sayHi()  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;

注：函数名的命名规则与变量是一致的，并且尽量保证函数名的语义。

小案例： 小星星
&lt;script&gt;        // 函数声明        function sayHi() &#123;            // document.write(&#x27;hai~&#x27;)            document.write(`*&lt;br&gt;`)            document.write(`**&lt;br&gt;`)            document.write(`***&lt;br&gt;`)            document.write(`****&lt;br&gt;`)            document.write(`*****&lt;br&gt;`)            document.write(`******&lt;br&gt;`)            document.write(`*******&lt;br&gt;`)            document.write(`********&lt;br&gt;`)            document.write(`*********&lt;br&gt;`)        &#125;        // 函数调用        sayHi()        sayHi()        sayHi()        sayHi()        sayHi()    &lt;/script&gt;
参数
通过向函数传递参数，可以让函数更加灵活多变，参数可以理解成是一个变量。
声明（定义）一个功能为打招呼的函数

传入数据列表
声明这个函数需要传入几个数据
多个数据用逗号隔开

&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 函数参数&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;script&gt;    // 声明（定义）一个功能为打招呼的函数    // function sayHi() &#123;    //   console.log(&#x27;嗨~&#x27;)    // &#125;    // 调用函数    // sayHi()	    // 这个函数似乎没有什么价值，除非能够向不同的人打招呼    // 这就需要借助参数来实现了    function sayHi(name) &#123;      // 参数 name 可以被理解成是一个变量      console.log(name)      console.log(&#x27;嗨~&#x27; + name)    &#125;    // 调用 sayHi 函数，括号中多了 &#x27;小明&#x27;    // 这时相当于为参数 name 赋值了    sayHi(&#x27;小明&#x27;)// 结果为 小明    // 再次调用 sayHi 函数，括号中多了 &#x27;小红&#x27;    // 这时相当于为参数 name 赋值了    sayHi(&#x27;小红&#x27;) // 结果为 小红  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
总结：

声明（定义）函数时的形参没有数量限制，当有多个形参时使用 , 分隔
调用函数传递的实参要与形参的顺序一致

形参和实参
形参：声明函数时写在函数名右边小括号里的叫形参（形式上的参数）
实参：调用函数时写在函数名右边小括号里的叫实参（实际上的参数）
形参可以理解为是在这个函数内声明的变量（比如 num1 = 10）实参可以理解为是给这个变量赋值
开发中尽量保持形参和实参个数一致
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 函数参数&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;script&gt;    // 声明（定义）一个计算任意两数字和的函数    // 形参 x 和 y 分别表示任意两个数字，它们是两个变量    function count(x, y) &#123;      console.log(x + y);    &#125;    // 调用函数，传入两个具体的数字做为实参    // 此时 10 赋值给了形参 x    // 此时 5  赋值给了形参 y    count(10, 5); // 结果为 15  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
返回值
函数的本质是封装（包裹），函数体内的逻辑执行完毕后，函数外部如何获得函数内部的执行结果呢？要想获得函数内部逻辑的执行结果，需要通过 return 这个关键字，将内部执行结果传递到函数外部，这个被传递到外部的结果就是返回值。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 函数返回值&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;script&gt;    // 定义求和函数    function count(a, b) &#123;      let s = a + b      // s 即为 a + b 的结果      // 通过 return 将 s 传递到外部      return s    &#125;    // 调用函数，如果一个函数有返回值    // 那么可将这个返回值赋值给外部的任意变量    let total = count(5, 12)  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
总结：

在函数体中使用return 关键字能将内部的执行结果交给函数外部使用
函数内部只能出现1 次 return，并且 return 下一行代码不会再被执行，所以return 后面的数据不要换行写
return会立即结束当前函数
函数可以没有return，这种情况默认返回值为 undefined

作用域
通常来说，一段程序代码中所用到的名字并不总是有效和可用的，而限定这个名字的可用性的代码范围就是这个名字的作用域。
作用域的使用提高了程序逻辑的局部性，增强了程序的可靠性，减少了名字冲突。
全局作用域
作用于所有代码执行的环境(整个 script 标签内部)或者一个独立的 js 文件
处于全局作用域内的变量，称为全局变量
局部作用域
作用于函数内的代码环境，就是局部作用域。 因为跟函数有关系，所以也称为函数作用域。
处于局部作用域内的变量称为局部变量

如果函数内部，变量没有声明，直接赋值，也当全局变量看，但是强烈不推荐
但是有一种情况，函数内部的形参可以看做是局部变量。

匿名函数
函数可以分为具名函数和匿名函数
匿名函数：没有名字的函数,无法直接使用。
函数表达式
// 声明let fn = function() &#123;    console.log(&#x27;函数表达式&#x27;)&#125;// 调用fn()
立即执行函数
(function()&#123; xxx  &#125;)();(function()&#123;xxxx&#125;());

无需调用，立即执行，其实本质已经调用了
多个立即执行函数之间用分号隔开

在能够访问到的情况下 先局部 局部没有在找全局
对象

知道对象数据类型的特征，能够利用数组对象渲染页面


理解什么是对象，掌握定义对象的语法
掌握数学对象的使用


对象是 JavaScript 数据类型的一种，之前已经学习了数值类型、字符串类型、布尔类型、undefined。对象数据类型可以被理解成是一种数据集合。它由属性和方法两部分构成。

语法
声明一个对象类型的变量与之前声明一个数值或字符串类型的变量没有本质上的区别。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 对象语法&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;script&gt;    // 声明字符串类型变量    let str = &#x27;hello world!&#x27;        // 声明数值类型变量    let num = 199    // 声明对象类型变量，使用一对花括号    // user 便是一个对象了，目前它是一个空对象    let user = &#123;&#125;  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
属性和访问
数据描述性的信息称为属性，如人的姓名、身高、年龄、性别等，一般是名词性的。

属性都是成 对出现的，包括属性名和值，它们之间使用英文 : 分隔
多个属性之间使用英文 , 分隔
属性就是依附在对象上的变量
属性名可以使用 &quot;&quot; 或 ''，一般情况下省略，除非名称遇到特殊符号如空格、中横线等

&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 对象语法&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;script&gt;    // 通过对象描述一个人的数据信息    // person 是一个对象，它包含了一个属性 name    // 属性都是成对出现的，属性名 和 值，它们之间使用英文 : 分隔    let person = &#123;      name: &#x27;小明&#x27;, // 描述人的姓名      age: 18, // 描述人的年龄      stature: 185, // 描述人的身高      gender: &#x27;男&#x27;, // 描述人的性别    &#125;  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
声明对象，并添加了若干属性后，可以使用 . 或 [] 获得对象中属性对应的值，我称之为属性访问。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 对象语法&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;script&gt;    // 通过对象描述一个人的数据信息    // person 是一个对象，它包含了一个属性 name    // 属性都是成对出现的，属性名 和 值，它们之间使用英文 : 分隔    let person = &#123;      name: &#x27;小明&#x27;, // 描述人的姓名      age: 18, // 描述人的年龄      stature: 185, // 描述人的身高      gender: &#x27;男&#x27;, // 描述人的性别    &#125;;        // 访问人的名字    console.log(person.name) // 结果为 小明    // 访问人性别    console.log(person.gender) // 结果为 男    // 访问人的身高    console.log(person[&#x27;stature&#x27;] // 结果为 185   // 或者    console.log(person.stature) // 结果同为 185  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
扩展：也可以动态为对象添加属性，动态添加与直接定义是一样的，只是语法上更灵活。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 对象语法&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;script&gt;    // 声明一个空的对象（没有任何属性）	let user = &#123;&#125;    // 动态追加属性    user.name = &#x27;小明&#x27;    user[&#x27;age&#x27;] = 18        // 动态添加与直接定义是一样的，只是语法上更灵活  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
方法和调用
数据行为性的信息称为方法，如跑步、唱歌等，一般是动词性的，其本质是函数。

方法是由方法名和函数两部分构成，它们之间使用 : 分隔
多个属性之间使用英文 , 分隔
方法是依附在对象中的函数
方法名可以使用 &quot;&quot; 或 ''，一般情况下省略，除非名称遇到特殊符号如空格、中横线等

&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 对象方法&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;script&gt;    // 方法是依附在对象上的函数    let person = &#123;      name: &#x27;小红&#x27;,      age: 18,      // 方法是由方法名和函数两部分构成，它们之间使用 : 分隔      singing: function () &#123;        console.log(&#x27;两只老虎，两只老虎，跑的快，跑的快...&#x27;)      &#125;,      run: function () &#123;        console.log(&#x27;我跑的非常快...&#x27;)      &#125;    &#125;  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
声明对象，并添加了若干方法后，可以使用 . 或 [] 调用对象中函数，我称之为方法调用。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 对象方法&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;script&gt;    // 方法是依附在对象上的函数    let person = &#123;      name: &#x27;小红&#x27;,      age: 18,      // 方法是由方法名和函数两部分构成，它们之间使用 : 分隔      singing: function () &#123;        console.log(&#x27;两只老虎，两只老虎，跑的快，跑的快...&#x27;)      &#125;,      run: function () &#123;        console.log(&#x27;我跑的非常快...&#x27;)      &#125;    &#125;        // 调用对象中 singing 方法    person.singing()    // 调用对象中的 run 方法    person.run()  &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
扩展：也可以动态为对象添加方法，动态添加与直接定义是一样的，只是语法上更灵活。
&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;title&gt;JavaScript 基础 - 对象方法&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;script&gt;    // 声明一个空的对象（没有任何属性，也没有任何方法）	let user = &#123;&#125;    // 动态追加属性    user.name = &#x27;小明&#x27;    user.[&#x27;age&#x27;] = 18        // 动态添加方法    user.move = function () &#123;      console.log(&#x27;移动一点距离...&#x27;)    &#125;      &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;
注：无论是属性或是方法，同一个对象中出现名称一样的，后面的会覆盖前面的。
null
null 也是 JavaScript 中数据类型的一种，通常只用它来表示不存在的对象。使用 typeof 检测类型它的类型时，结果为 object。
遍历对象
let obj = &#123;      uname: &#x27;pink&#x27;,      age: 18,      sex: &#x27;男&#x27;    &#125;    console.log(obj[&#x27;uname&#x27;];//输出某个属性名的值        for(let i in obj) &#123;      // i 属性名  字符串  带引号    obj.&#x27;uname&#x27;     i ===  &#x27;uname&#x27;      // obj[k]  属性值    obj[&#x27;uname&#x27;]   obj[i]      console.log(i)  //输出所有属性名    &#125;    for(let i in obj) &#123;      // i 属性名  字符串  带引号    obj.&#x27;uname&#x27;     i ===  &#x27;uname&#x27;      // obj[i]  属性值    obj[&#x27;uname&#x27;]   obj[i]      console.log(obj[i]  //输出所有属性的值    &#125;
for in 不提倡遍历数组 因为 k 是 字符串

案例
&lt;script&gt;    let student = [      &#123; name: &#x27;小米&#x27;, age: 18, sex: &#x27;男&#x27;, homedown: &#x27;河南&#x27; &#125;,      &#123; name: &#x27;小美&#x27;, age: 12, sex: &#x27;女&#x27;, homedown: &#x27;北京&#x27; &#125;,      &#123; name: &#x27;小三&#x27;, age: 13, sex: &#x27;男&#x27;, homedown: &#x27;湖北&#x27; &#125;,      &#123; name: &#x27;小四&#x27;, age: 15, sex: &#x27;女&#x27;, homedown: &#x27;广东&#x27; &#125;    ]    for (let i = 0; i &lt;= student.length; i++) &#123;        // console.log(i) //下标索引号        // console.log(student[i] //数组元素=每个对象        console.log(student[i].name) //输出每个对象的名字        //console.log(student[i].age) //输出每个对象的年龄      &#125;  &lt;/script&gt;

内置对象
回想一下我们曾经使用过的 console.log，console其实就是 JavaScript 中内置的对象，该对象中存在一个方法叫 log，然后调用 log 这个方法，即 console.log()。
除了 console 对象外，JavaScritp 还有其它的内置的对象
Math
Math 是 JavaScript 中内置的对象，称为数学对象，这个对象下即包含了属性，也包含了许多的方法。
属性

Math.PI，获取圆周率

// 圆周率console.log(Math.PI);
方法

Math.random，生成 0 到 1 间的随机数

// 0 ~ 1 之间的随机数, 包含 0 不包含 1Math.random()

Math.ceil，数字向上取整

// 舍弃小数部分，整数部分加1Math.ceil(3.4)

Math.floor，数字向下取整

// 舍弃小数部分，整数部分不变Math.floor(4.68)

Math.round，四舍五入取整

// 取整，四舍五入原则Math.round(5.46539)Math.round(4.849)

Math.max，在一组数中找出最大的

// 找出最大值Math.max(10, 21, 7, 24, 13)

Math.min，在一组数中找出最小的

// 找出最小值Math.min(24, 18, 6, 19, 21)

Math.pow，幂方法

// 求某个数的多少次方Math.pow(4, 2) // 求 4 的 2 次方Math.pow(2, 3) // 求 2 的 3 次方

Math.sqrt，平方根

// 求某数的平方根Math.sqrt(16)
数学对象提供了比较多的方法，这里不要求强记，通过演示数学对象的使用，加深对对象的理解。
]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title>Harbor共享存储高可用</title>
    <url>/posts/40301/</url>
    <content><![CDATA[
Harbor共享存储高可用
主机拓扑



角色
主机名
ip
系统
资源最低要求




Harbor1nginxKeepalived1
harbor1
192.168.48.106
OpenEuler22.03LTS
CPU：4核 内存：2G 硬盘：40G


Harbor2nginxKeepalived2
harbor2
192.168.48.107
OpenEuler22.03LTS
CPU：4核 内存：2G 硬盘：40G


postgresqlRedisNFS共享
zujian
192.168.48.108
OpenEuler22.03LTS
CPU：4核 内存：2G 硬盘：40G


高可用ip
192.168.48.100






系统架构图

基本配置
操作节点：[harbor1，harbour2，zujian]
vi jichu_init.sh
将以下脚本内容添加进去
#!/bin/bashif [ $# -eq 2 ];then  echo &quot;设置主机名为：$1&quot;  echo &quot;ens160设置IP地址为：192.168.48.$2&quot;else  echo  &quot;使用方法：sh $0 主机名 主机位&quot;  exit 2fiecho &quot;--------------------------------------&quot;echo &quot;1.正在设置主机名：$1&quot;hostnamectl set-hostname $1echo &quot;2.正在关闭firewalld、dnsmasq、selinux&quot;systemctl disable firewalld &amp;&gt; /dev/nullsystemctl disable dnsmasq &amp;&gt; /dev/nullsystemctl stop firewalldsystemctl stop dnsmasqsed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsetenforce 0echo &quot;3.正在设置ens160：192.168.48.$2&quot;cat &gt; /etc/sysconfig/network-scripts/ifcfg-ens160 &lt;&lt;EOFTYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noNAME=ens160UUID=53b402ff-5865-47dd-a853-7afcd6521738DEVICE=ens160ONBOOT=yesIPADDR=192.168.48.$2GATEWAY=192.168.48.2PREFIX=24DNS1=192.168.48.2DNS2=114.114.114.114nmcli c reloadnmcli c up ens 160echo &quot;4.优化ssh&quot;sed -i &quot;s#\#UseDNS yes#UseDNS no#g&quot; /etc/ssh/sshd_configsed -i &quot;s#GSSAPIAuthentication yes#GSSAPIAuthentication no#g&quot; /etc/ssh/sshd_configsystemctl restart sshdecho &quot;5.更改欧拉源为华为云源，速度快一点&quot;sed -i &#x27;s/\$basearch/x86_64/g&#x27; /etc/yum.repos.d/openEuler.reposed -i &#x27;s/http\:\/\/repo.openeuler.org/https\:\/\/mirrors.huaweicloud.com\/openeuler/g&#x27; /etc/yum.repos.d/openEuler.repoecho &quot;6.更新yum源软件包缓存&quot;yum clean all &amp;&amp; yum makecachednf update -yecho &quot;7.修改history格式及记录数&quot;sed -i &quot;s#HISTSIZE=1000##g&quot; /etc/profilecat &gt;&gt; /etc/profile &lt;&lt;EOFshopt -s histappendUSER_IP=`who -u am i 2&gt;/dev/null| awk &#x27;&#123;print $NF&#125;&#x27;|sed -e &#x27;s/[()]//g&#x27;`export HISTFILE=~/.commandline_warriorexport HISTTIMEFORMAT=&quot;%Y-%m-%d %H:%M:%S  `whoami`@$&#123;USER_IP&#125;: &quot;export HISTSIZE=200000export HISTFILESIZE=1000000export PROMPT_COMMAND=&quot;history -a&quot;EOFsource /etc/profileecho &quot;8.添加hosts解析&quot;cat &gt; /etc/hosts &lt;&lt;EOF127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.48.106 harbor1192.168.48.107 harbor2192.168.48.108 zujianEOFecho &quot;10.安装chrony服务，并同步时间&quot;dnf install chrony -ysystemctl start chronydsystemctl enable chronydchronyc sourceschronyc sourcesecho &quot;11、安装依赖包&quot;dnf install -y cmake gcc gcc-c++ perl readline readline-devel openssl openssl-devel zlib zlib-devel ncurses-devel readline readline-devel zlib zlib-develreboot
执行脚本命令格式：sh jichu_init.sh 主机名 主机位[harbor1] sh jichu_init.sh harbor1 106[harbor2] sh jichu_init.sh harbor2 107[zujian] sh jichu_init.sh zujian 108
配置ssh免密
操作节点：[harbor1，harbour2，zujian]
dnf install -y sshpass cat &gt; sshmianmi.sh &lt;&lt; &quot;EOF&quot;#!/bin/bash# 目标主机列表hosts=(&quot;harbor1&quot; &quot;harbor2&quot; &quot;zujian&quot;)# 密码password=&quot;Lj201840.&quot;# 生成 SSH 密钥对ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa# 循环遍历目标主机for host in &quot;$&#123;hosts[@]&#125;&quot;do    # 复制公钥到目标主机    sshpass -p &quot;$password&quot; ssh-copy-id -o StrictHostKeyChecking=no &quot;$host&quot;        # 验证免密登录    sshpass -p &quot;$password&quot; ssh -o StrictHostKeyChecking=no &quot;$host&quot; &quot;echo &#x27;免密登录成功&#x27;&quot;doneEOFsh sshmianmi.sh
安装高可用组件
操作节点:[harbor1，harbour2]
dnf install -y keepalived nginx
安装nginx
操作节点:[harbor1，harbour2]
cat &gt; /etc/nginx/nginx.conf &lt;&lt;&quot;EOF&quot;user nginx;worker_processes auto;error_log /var/log/nginx/error.log notice;pid /run/nginx.pid;include /usr/share/nginx/modules/*.conf;events &#123;    worker_connections 1024;&#125;stream &#123;    log_format  main  &#x27;$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent&#x27;;    access_log  /var/log/nginx/harbor-access.log  main;    upstream harbor&#123;       server 192.168.48.106:8081;   #harbor1       server 192.168.48.107:8081;   #harbor2    &#125;    server &#123;       listen  80;       proxy_pass harbor;    &#125;&#125;http &#123;    log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;                      &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;                      &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;    access_log  /var/log/nginx/access.log  main;    sendfile            on;    tcp_nopush          on;    keepalive_timeout   65;    types_hash_max_size 4096;    include             /etc/nginx/mime.types;    default_type        application/octet-stream;        include /etc/nginx/conf.d/*.conf;    server &#123;        listen       8888 default_server;        server_name  _;        location / &#123;        &#125;    &#125;&#125;EOFsystemctl enable --now nginxnginx -s reload
安装安装keepalived
操作节点:[harbor1]
cat &gt;/etc/keepalived/keepalived.conf &lt;&lt; &quot;EOF&quot;! Configuration File for keepalivedglobal_defs &#123;   notification_email &#123;     qianyios@qq.com   &#125;   router_id harbor1&#125;vrrp_instance zh &#123;    state MASTER    interface ens160    mcast_src_ip 192.168.48.106    virtual_router_id 107    priority 100    advert_int 1    nopreempt    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        192.168.48.100/24    &#125;    track_script &#123;        chk_nginx    &#125;&#125;vrrp_script chk_nginx &#123;    script &quot;/etc/keepalived/check_nginx.sh&quot;    interval 2    weight -20&#125;EOF
操作节点:[harbor2]
cat &gt;/etc/keepalived/keepalived.conf &lt;&lt; &quot;EOF&quot;! Configuration File for keepalivedglobal_defs &#123;   notification_email &#123;     qianyios@qq.com   &#125;   router_id harbor2&#125;vrrp_instance zh &#123;    state BACKUP    interface ens160    mcast_src_ip 192.168.48.107    virtual_router_id 107    priority 99    advert_int 1    nopreempt    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        192.168.48.100/24    &#125;    track_script &#123;        chk_nginx    &#125;&#125;vrrp_script chk_nginx &#123;    script &quot;/etc/keepalived/check_nginx.sh&quot;    interval 2    weight -20&#125;EOF
配置检查脚本
操作节点:[harbor1，harbour2]
cat &gt;/etc/keepalived/check_nginx.sh &lt;&lt;&quot;EOF&quot;#!/bin/bashcounter=`ps -C nginx --no-header | wc -l`if [ $counter -eq 0 ]; then    systemctl start nginx    sleep 2    counter=`ps -C nginx --no-header | wc -l`    if [ $counter -eq 0 ]; then        systemctl stop keepalived    fifiEOF
启动服务
systemctl enable --now nginx keepalivednginx -s reload
查看VIP虚拟ip
[root@harbor1 ~]# ip a.............2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000    link/ether 00:0c:29:af:76:8c brd ff:ff:ff:ff:ff:ff    inet 192.168.48.106/24 brd 192.168.48.255 scope global noprefixroute ens160       valid_lft forever preferred_lft forever    inet 192.168.48.100/24 scope global secondary ens160   ###192.168.48.100/24就是刚刚设置的高可用ip       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:feaf:768c/64 scope link noprefixroute       valid_lft forever preferred_lft forever
安装postgresql
操作节点：[zujian]
安装
#创建postgres用户useradd postgrespasswd postgres#设置密码123456#编译安装postgresqlwget https://ftp.postgresql.org/pub/source/v16.2/postgresql-16.2.tar.gztar zxvf postgresql-16.2.tar.gz -C /usr/local/bin/cd /usr/local/bin/postgresql-16.2/./configure --prefix=/usr/local/postgresqlmake &amp;&amp; make install#建立数据目录mkdir -p /data/postgresql/data#创建日志目录mkdir -p /data/postgresql/log#创建socket目录mkdir -p /data/postgresql/tmp#授权chown -R postgres:postgres /usr/local/postgresql/chown -R postgres:postgres /data/postgresql#设置postgres环境su - postgrescdcat &lt;&lt; &quot;EOF&quot; &gt;&gt; ~/.bash_profilePGHOME=/usr/local/postgresqlexport PGHOMEPGDATA=/data/postgresql/dataexport PGDATAPATH=$PATH:$HOME/bin:$HOME/.local/bin:$PGHOME/binexport PATHEOFsource ~/.bash_profilepsql -V#初始化数据库initdb --username=postgres -D /data/postgresql/data#会有Success. You can now start the database server using:      #表示初始化成功#修改初始化的配置文件cat &gt; /data/postgresql/data/postgresql.conf &lt;&lt; &quot;EOF&quot;max_connections = 100                   # 允许最大连接数shared_buffers = 128MB                  # 内存大小dynamic_shared_memory_type = posix      # the default is usually the first optionmax_wal_size = 1GBmin_wal_size = 80MBlog_timezone = &#x27;Asia/Shanghai&#x27;datestyle = &#x27;iso, mdy&#x27;timezone = &#x27;Asia/Shanghai&#x27;lc_messages = &#x27;en_US.UTF-8&#x27;             # locale for system error messagelc_monetary = &#x27;en_US.UTF-8&#x27;             # locale for monetary formattinglc_numeric = &#x27;en_US.UTF-8&#x27;              # locale for number formattinglc_time = &#x27;en_US.UTF-8&#x27;                 # locale for time formattingdefault_text_search_config = &#x27;pg_catalog.english&#x27;listen_addresses = &#x27;*&#x27; #监听所有地址data_directory = &#x27;/data/postgresql/data&#x27;  # 数据目录指定port = 5432unix_socket_directories = &#x27;/data/postgresql/tmp&#x27;unix_socket_group = &#x27;&#x27;unix_socket_permissions = 0777logging_collector = onlog_directory = &#x27;/data/postgresql/log&#x27;log_rotation_size = 1GBlog_timezone = &#x27;Asia/Shanghai&#x27;log_min_duration_statement = 100EOF#设置远程连接cat &gt;&gt; /data/postgresql/data/pg_hba.conf &lt;&lt; EOFlocal   all             all                                     trusthost    all             all             0.0.0.0/0               passwordhost    all             all             ::1/128                 passwordhost    all             postgres             0.0.0.0/0           trustEOF#启动PostgreSQLpg_ctl -D /data/postgresql/data -l logfile start
进入数据库
psql -h 127.0.0.1 -p 5432 -U postgrespostgres=# \passwordEnter new password for user &quot;postgres&quot;:Enter it again:#输入密码123456postgres=# exit#重新启动pg_ctl -D /data/postgresql/data -l /data/postgresql/data/postgresql.log restart#提示一下信息成功（不是命令哈，不要去运行）pg_ctl: old server process (PID: 25461) seems to be gonestarting server anywaywaiting for server to start.... doneserver startedpsql -h 127.0.0.1 -p 5432 -U postgres#输入密码123456CREATE DATABASE registry;CREATE DATABASE notary_signer;CREATE DATABASE notary_servers;\lcreate user server with password &#x27;123456&#x27;;create user signer with password &#x27;123456&#x27;;\duGRANT ALL PRIVILEGES ON DATABASE registry to postgres;   GRANT ALL PRIVILEGES ON DATABASE notary_signer to postgres;   GRANT ALL PRIVILEGES ON DATABASE notary_servers to postgres;   exit


设置启动服务
操作节点[zujian]
#回到root用户下执行su - rootcat &gt;/etc/init.d/PG-start.sh&lt;&lt; &quot;EOF&quot;sudo -u postgres /usr/local/postgresql/bin/pg_ctl -D /data/postgresql/data startEOFcat &gt;/etc/init.d/PG-stop.sh&lt;&lt; &quot;EOF&quot;sudo -u postgres /usr/local/postgresql/bin/pg_ctl -D /data/postgresql/data stopEOFcat &gt;/etc/init.d/PG-restart.sh&lt;&lt;&quot;EOF&quot;sudo -u postgres /usr/local/postgresql/bin/pg_ctl -D /data/postgresql/data restartEOFsudo chmod +x /etc/init.d/PG-start.shsudo chmod +x /etc/init.d/PG-stop.shsudo chmod +x /etc/init.d/PG-restart.shcat &gt;/etc/systemd/system/postgresql.service &lt;&lt; &quot;EOF&quot;[Unit]Description=postgresql Service[Service]Type=oneshotuser=rootRemainAfterExit=trueExecStart=/usr/bin/sudo /etc/init.d/PG-start.shExecStop=/usr/bin/sudo /etc/init.d/PG-stop.shExecRestart=/usr/bin/sudo /etc/init.d/PG-restart.sh[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable --now postgresql
错误积累
#每次我启动pgsql的时候就有这个东西2024-09-14 14:34:03.196 CST [17746] HINT:  Is another postmaster (PID 16042) running in data directory &quot;/data/postgresql/data&quot;? stopped waitingpg_ctl: could not start server#意思是有个pid进程在运行，杀掉它就行了sudo kill -9 16042
安装redis
操作节点：[zujian]
安装redis
wget https://download.redis.io/releases/redis-7.2.4.tar.gztar zxvf redis-7.2.4.tar.gz mv redis-7.2.4 /usr/local/bin/cd /usr/local/bin/redis-7.2.4make &amp;&amp; make install
修改配置文件
vi /usr/local/bin/redis-7.2.4/redis.conf#bind 127.0.0.1 -::1  #注释掉bind的行，允许任何主机连接；daemonize yes       #将no修改为yes，使redis可以使用守护进程方式启动；requirepass 123456   #添加这行，设置redis连接的auth密码（123456）protected-mode no  #禁用保护模式以下是一步到位将以上四个命令全部实现sed -i &#x27;s/^bind 127.0.0.1 -::1/#bind 127.0.0.1 -::1/&#x27; /usr/local/bin/redis-7.2.4/redis.confsed -i &#x27;s/^daemonize no/daemonize yes/&#x27; /usr/local/bin/redis-7.2.4/redis.confecho -e &quot;\nrequirepass 123456&quot; &gt;&gt; /usr/local/bin/redis-7.2.4/redis.confsed -i &#x27;s/^protected-mode yes/protected-mode no/&#x27; /usr/local/bin/redis-7.2.4/redis.conf
启动服务
redis-server redis.conf
[root@zujian redis-7.2.4]# redis-server redis.conf2226:C 20 Apr 2024 17:10:45.039 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add &#x27;vm.overcommit_memory = 1&#x27; to /etc/sysctl.conf and then reboot or run the command &#x27;sysctl vm.overcommit_memory=1&#x27; for this to take effect.
Redis在启动时可能会出现这样的日志：在分析这个问题之前， 首先要弄清楚什么是overcommit？ Linux操作系统对大部分申请内存的请求都回复yes， 以便能运行更多的程序。 因为申请内存后， 并不会马上使用内存， 这种技术叫做overcommit。如果Redis在启动时有上面的日志， 说明vm.overcommit_memory=0， Redis提示把它设置为1。
vm.overcommit_memory用来设置内存分配策略， 有三个可选值， 如表：可用内存代表物理内存与swap之和

解决办法：
echo &quot;vm.overcommit_memory=1&quot; &gt;&gt; /etc/sysctl.confsysctl vm.overcommit_memory=1redis-server redis.conf
再重新启动就可以查看版本和端口号了
[root@zujian redis-7.2.4]# redis-cli -vredis-cli 7.2.4[root@zujian redis-7.2.4]# ps aux |grep 6379root        2227  0.0  0.3  68412 10888 ?        Ssl  17:10   0:00 redis-server *:6379root        5427  0.0  0.0  22096  2300 pts/0    S+   17:19   0:00 grep --color=auto 6379#redis-server有这个就行
关闭服务（只是普及知识，测试可用，不要随意关闭）
redis-cli shutdown
客户端连接redis
操作节点：[zujian]
将redis-cli的工具复制到Harbor1，harbor2
查看redis-cli工具位置
[root@zujian]# which redis-cli/usr/local/bin/redis-cli
复制
which redis-cliscp /usr/local/bin/redis-cli harbor1:/usr/local/bin/scp /usr/local/bin/redis-cli harbor2:/usr/local/bin/
操作节点：[harbor1,harbor2]
redis-cli -h 192.168.48.108 -p 6379 -a 123456


到此redis安装成功
设置启动服务
cat &gt;/etc/init.d/redis-start.sh&lt;&lt; &quot;EOF&quot;/usr/local/bin/redis-server /usr/local/bin/redis-7.2.4/redis.confEOFcat &gt;/etc/init.d/redis-stop.sh&lt;&lt; &quot;EOF&quot;/usr/local/bin/redis-cli -a 123456 shutdownEOFsudo chmod +x /etc/init.d/redis-start.shsudo chmod +x /etc/init.d/redis-stop.shcat &gt;/etc/systemd/system/redis.service &lt;&lt; &quot;EOF&quot;[Unit]Description=redis Service[Service]Type=oneshotuser=rootRemainAfterExit=trueExecStart=/usr/bin/sudo /etc/init.d/redis-start.shExecStop=/usr/bin/sudo /etc/init.d/redis-stop.sh[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable --now redis
NFS共享存储安装
操作节点：[zujian]
dnf install -y nfs-utilssystemctl enable --now nfs#创建远程共享目录mkdir -p /data/harbor_datacat &gt;&gt; /etc/exports &lt;&lt; &quot;EOF&quot;/data/harbor_data 192.168.48.0/24(rw,no_root_squash)EOF#使配置生效exportfs -arv#生效结果[root@zujian ~]# showmount -eExport list for zujian:/data/harbor_data 192.168.48.0/24
操作节点：[harbor1,harbor2]
Harbor1、harbor2机器上安装nfs-utils客户端并挂载共享存储
dnf install -y nfs-utilssystemctl enable --now nfsmkdir -p /data/harbor_datacat &gt;&gt;/etc/fstab&lt;&lt;&quot;EOF&quot;192.168.48.108:/data/harbor_data /data/harbor_data nfs defaults 0 0EOFmount -adf -h | grep harbor#以下是挂载成功[root@harbor1 ~]# df -h | grep harbor192.168.48.108:/data/harbor_data   63G  3.0G   57G   6% /data/harbor_data[root@harbor2 ~]# df -h | grep harbor192.168.48.108:/data/harbor_data   63G  3.0G   57G   6% /data/harbor_data
harbor仓库安装
操作节点：[harbor1,harbor2]
安装docker
wget https://download.docker.com/linux/static/stable/x86_64/docker-26.0.1.tgztar xf docker-*.tgzcp docker/* /usr/bin/#创建containerd的service文件,并且启动cat &gt;/etc/systemd/system/containerd.service &lt;&lt;EOF[Unit]Description=containerd container runtimeDocumentation=https://containerd.ioAfter=network.target local-fs.target[Service]ExecStartPre=-/sbin/modprobe overlayExecStart=/usr/bin/containerdType=notifyDelegate=yesKillMode=processRestart=alwaysRestartSec=5LimitNPROC=infinityLimitCORE=infinityLimitNOFILE=1048576TasksMax=infinityOOMScoreAdjust=-999[Install]WantedBy=multi-user.targetEOFsystemctl enable --now containerd.service#准备docker的service文件cat &gt; /etc/systemd/system/docker.service &lt;&lt;EOF[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.service containerd.serviceWants=network-online.targetRequires=docker.socket containerd.service[Service]Type=notifyExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sockExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=alwaysStartLimitBurst=3StartLimitInterval=60sLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTasksMax=infinityDelegate=yesKillMode=processOOMScoreAdjust=-500[Install]WantedBy=multi-user.targetEOF#准备docker的socket文件cat &gt; /etc/systemd/system/docker.socket &lt;&lt;EOF[Unit]Description=Docker Socket for the API[Socket]ListenStream=/var/run/docker.sockSocketMode=0660SocketUser=rootSocketGroup=docker[Install]WantedBy=sockets.targetEOFgroupadd dockersystemctl enable --now docker.socket  &amp;&amp; systemctl enable --now docker.service#验证mkdir /etc/dockercat &gt;/etc/docker/daemon.json &lt;&lt;EOF&#123;  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],  &quot;registry-mirrors&quot;: [    &quot;https://docker.mirrors.ustc.edu.cn&quot;,    &quot;http://hub-mirror.c.163.com&quot;,    &quot;https://pw860av8.mirror.aliyuncs.com&quot;  ],  &quot;max-concurrent-downloads&quot;: 10,  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-level&quot;: &quot;warn&quot;,  &quot;log-opts&quot;: &#123;    &quot;max-size&quot;: &quot;500m&quot;,    &quot;max-file&quot;: &quot;3&quot;    &#125;,  &quot;data-root&quot;: &quot;/var/lib/docker&quot;&#125;EOFsystemctl restart dockerdocker -v
安装docker-compose
操作节点：[harbor1,harbor2]
wget https://github.com/docker/compose/releases/download/v2.26.1/docker-compose-linux-x86_64mv docker-compose-linux-x86_64 /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-composedocker-compose version
配置内核参数并使之生效
操作节点：[harbor1,harbor2]
modprobe br_netfiltercat &gt;&gt; /etc/sysctl.conf &lt;&lt;EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1 #路由转发EOFsysctl -p
下载harbor包并配置文件
操作节点：[harbor1,harbor2]
下载离线包offline字样
wget https://github.com/goharbor/harbor/releases/download/v2.9.4/harbor-offline-installer-v2.9.4.tgztar zxvf harbor-offline-installer-v2.9.4.tgzmv harbor /var/cd /var/harbor/[root@harbor1 harbor]# lscommon.sh  harbor.v2.9.4.tar.gz  harbor.yml.tmpl  install.sh  LICENSE  preparecp harbor.yml.tmpl harbor.yml
配置harbor文件
操作节点：[harbor1]
vi /var/harbor/harbor.yml
hostname: 192.168.48.106  #harbor1http:  port: 8081  #https:       #先注释https协议，后面再实现 # port: 443 # certificate: /your/certificate/path # private_key: /your/private/key/path## 启用外部代理，启用后hostname将不再使用external_url: https://192.168.48.100#harbor页面密码harbor_admin_password: Harbor12345#配置NFS共享存储data_volume: /data/harbor_data_version: 2.9.0#配置数据库external_database:  harbor:    host: 192.168.48.108  # 数据库主机地址    port: 5432              # 数据库端口    db_name: registry    # 数据库名称    username: postgres        # 连接该数据库的用户名    password: 123456    # 连接数据库的密码    ssl_mode: disable    max_idle_conns: 50    max_open_conns: 100  notary_server:    host: 192.168.48.108    port: 5432    db_name: notary_server    username: postgres    password: 123456    ssl_mode: disable  notary_signer:    host: 192.168.48.108    port: 5432    db_name: notary_signer    username: postgres    password: 123456    ssl_mode: disable #配置redisexternal_redis:  host: 192.168.48.108:6379 #redis服务IP地址和端口号  password: 123456   #连接外部redis服务的密码  registry_db_index: 1    jobservice_db_index: 2 #job服务的数据库索引  chartmuseum_db_index: 3  #chartmuseum插件的Redis索引  trivy_db_index: 5   #Trivy扫描器的数据索引  idle_timeout_seconds: 30  #超时时间#启用metrics数据采集插件metric:  enabled: false  port: 9090  path: /metricstrivy:  ignore_unfixed: false  skip_update: false  skip_java_db_update: false  offline_scan: false  security_check: vuln  insecure: falsejobservice:  max_job_workers: 10  job_loggers:    - STD_OUTPUT    - FILE  logger_sweeper_duration: 1 #daysnotification:  webhook_job_max_retry: 3  webhook_job_http_client_timeout: 3 #secondslog:  level: info  local:    rotate_count: 50    rotate_size: 200M    location: /var/log/harborproxy:  http_proxy:  https_proxy:  no_proxy:  components:    - core    - jobservice    - trivyupload_purging:  enabled: true  age: 168h  interval: 24h  dryrun: falsecache:  enabled: false  expire_hours: 24
操作节点：[harbor2]
vi /var/harbor/harbor.yml
hostname: 192.168.48.107  #harbor2http:  port: 8081  #https:       #先注释https协议，后面再实现 # port: 443 # certificate: /your/certificate/path # private_key: /your/private/key/path## 启用外部代理，启用后hostname将不再使用external_url: https://192.168.48.100#harbor页面密码harbor_admin_password: Harbor12345#配置NFS共享存储data_volume: /data/harbor_data_version: 2.9.0#配置数据库external_database:  harbor:    host: 192.168.48.108  # 数据库主机地址    port: 5432              # 数据库端口    db_name: registry    # 数据库名称    username: postgres        # 连接该数据库的用户名    password: 123456    # 连接数据库的密码    ssl_mode: disable    max_idle_conns: 2    max_open_conns: 0notary_server:  host: 192.168.48.108  port: 5432  db_name: notary_server  username: postgres  password: 123456  ssl_mode: disablenotary_signer:  host: 192.168.48.108  port: 5432  db_name: notary_signer  username: postgres  password: 123456  ssl_mode: disable #配置redisexternal_redis:  host: 192.168.48.108:6379 #redis服务IP地址和端口号  password: 123456   #连接外部redis服务的密码  registry_db_index: 1    jobservice_db_index: 2 #job服务的数据库索引  chartmuseum_db_index: 3  #chartmuseum插件的Redis索引  trivy_db_index: 5   #Trivy扫描器的数据索引  idle_timeout_seconds: 30  #超时时间#启用metrics数据采集插件metric:  enabled: false  port: 9090  path: /metricstrivy:  ignore_unfixed: false  skip_update: false  skip_java_db_update: false  offline_scan: false  security_check: vuln  insecure: falsejobservice:  max_job_workers: 10  job_loggers:    - STD_OUTPUT    - FILE  logger_sweeper_duration: 1 #daysnotification:  webhook_job_max_retry: 3  webhook_job_http_client_timeout: 3 #secondslog:  level: info  local:    rotate_count: 50    rotate_size: 200M    location: /var/log/harborproxy:  http_proxy:  https_proxy:  no_proxy:  components:    - core    - jobservice    - trivyupload_purging:  enabled: true  age: 168h  interval: 24h  dryrun: falsecache:  enabled: false  expire_hours: 24
将配置文件注入到各级件中并安装
先提前下载镜像吧，这里用博主构建的镜像速度会快一点
docker pull registry.cn-hangzhou.aliyuncs.com/qianyios/prepare:v2.9.4docker tag registry.cn-hangzhou.aliyuncs.com/qianyios/prepare:v2.9.4 goharbor/prepare:v2.9.4
开始注入
cd /var/harbor/./prepare

开始安装
cd /var/harbor/./install.sh

配置自启动
cat &gt;/usr/lib/systemd/system/harbor.service &lt;&lt; &quot;EOF&quot;[Unit]Description=HarborAfter=docker.service systemd-networkd.service systemd-resolved.service nfs-server.serviceRequires=docker.serviceDocumentation=http://github.com/vmware/harbor[Service]Type=simpleRestart=on-failureRestartSec=5ExecStart=/usr/local/bin/docker-compose -f /var/harbor/docker-compose.yml upExecStop=/usr/local/bin/docker-compose -f /var/harbor/docker-compose.yml down[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable harbor --now
大坑来了，之前找了两个星期都没解决的
到此，harbor安装完成，但是你在网页可能会出现不能用admin登入，会显示密码错误你需要进行下一步安装证书
原因：首先pg数据库在安装harbor时创建的admin是用sha256协议加密的

而在我们harbor页面，我们并没有配置ssl证书，是http方式访问并没有sha256加密协议，意味着harbor再登入的时候，会出现密码错误，就是：网页登入验证===/===pg数据库验证，配置openssl证书，可以解决此问题,openssl包含sha256协议，这样就可以登入了。
OpenSSL是一个强大的加密库，广泛应用于互联网的各个角落，用于保护数据传输的安全。它实现了SSL和TLS协议，这些协议是现代网络安全的基石。
配置ssl证书
操作节点：[harbor1,harbor2]
生成ca证书
创建一个放置证书相关的目录，并使用cd进入该目录
mkdir /var/harbor/cert&amp;&amp; cd  /var/harbor/cert## 1. 生成CA证书私钥openssl genrsa -out ca.key 4096## 2. 生成CA证书，可调整 -subj 选项来表明域名名称等信息openssl req -x509 -new -nodes -sha512 -days 3650 \ -subj &quot;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=192.168.48.100&quot; \ -key ca.key \ -out ca.crt
生成服务器证书
认证证书通常包含证书请求.csr文件、签名证书.crt文件及私钥.key文件，我这里harbor配置的hostname是192.168.48.100，所以最终需要生成192.168.48.100.crt、192.168.48.100.csr、192.168.48.100.key三个文件。

key：证书私钥，一般利用rsa等算法生成
csr：证书请求文件，利用证书私钥生成证书请求文件，该文件包含了服务器和地址等信息，申请人将该文件提交给CA机构，CA机构会根据该文件所携带的私钥信息来进行签名生成证书
crt：证书文件

## 1. 生成私钥openssl genrsa -out 192.168.48.100.key 4096## 2. 生成csr文件openssl req -sha512 -new \    -subj &quot;/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=192.168.48.100&quot; \    -key 192.168.48.100.key \    -out 192.168.48.100.csr## 3. 生成ssl匹配多域名文，例如既想使用域名又需要通过127.0.0.1本地地址登陆测试，可使用subjectAltName参数来进行配置cat &gt; v3.ext &lt;&lt;-EOFauthorityKeyIdentifier=keyid,issuerbasicConstraints=CA:FALSEkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEnciphermentextendedKeyUsage = serverAuthsubjectAltName = @alt_names[alt_names]DNS.1=192.168.48.100DNS.2=127.0.0.1IP.1=192.168.48.100EOF## 4. 根据v3.ext及csr文件请求生成crt证书文件openssl x509 -req -sha512 -days 3650 \    -extfile v3.ext \    -CA ca.crt -CAkey ca.key -CAcreateserial \    -in 192.168.48.100.csr \    -out 192.168.48.100.crt
修改harbor配置文件
cat &gt;&gt; /var/harbor/harbor.yml &lt;&lt; &quot;EOF&quot;https:  port: 443  certificate: /var/harbor/cert/192.168.48.100.crt  private_key: /var/harbor/cert/192.168.48.100.keyEOF
重新启动
cd /var/harbordocker-compose down -v./preparedocker-compose up -d
镜像上传及拉取测试
操作节点：[harbor1,harbor2,zujian,qianyios（测试客户端）]
找一台客户端装好docker进行测试
[root@qianyios ~]# docker -vDocker version 26.0.1, build d260a54
新建私有镜像仓库

客户端免https登陆
# 此时直接使用docker login登陆到harbor中，会报错，下面hostname和port是harbor的配置文件中设置的名称及端口#以下是格式[root@xxxx harbor]# docker login [hostname]:[port]可能会出现以下情况#192.168.48.100是高可用vip[root@harbor1 ~]# docker login 192.168.48.100Username: adminPassword:Error response from daemon: Get &quot;https://192.168.48.100/v2/&quot;: tls: failed to verify certificate: x509: certificate signed by unknown authority[root@qianyios ~]# docker login 192.168.48.106:8081Username: adminPassword:Error response from daemon: Get &quot;https://192.168.48.106:8081/v2/&quot;: http: server gave HTTP response to HTTPS client[root@qianyios ~]## 客户端默认使用的是https协议，所以需要对docker做以下修改,在文件末尾添加insecure-registries[root@qianyios ~]# vim /etc/docker/daemon.json&#123;   ................  &quot;registry-mirrors&quot;: [],#无关紧要，不用看,  &quot;insecure-registries&quot;: [ &quot;192.168.48.100&quot; ],#重要加这行，别忘了如果他不是最后一行一定要在末尾加逗号 ................&#125;# 修改后，重启docker使其生效systemctl daemon-reloadsystemctl restart docker# 利用docker info查看是否添加上[root@qianyios ~]# docker infoContainers: 10 Running: 1 Paused: 0 Stopped: 9Images: 37... Experimental: false Insecure Registries:  192.168.48.100   ###要确保有这个才行  127.0.0.0/8 Registry Mirrors:
扩展知识-containerd私有仓库配置（可略过）
在今后的K8s版本可能也会用containerd做为k8s的容器运行时，那么配置私有仓库也是一个头疼的事情。
在/etc/containerd/config.toml会有以下两个信息，可以定位
[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs][plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors]
要在以上两个信息下配置东西如下：（理解，我下面有一步到位命令，不用手动加）
[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs]  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;192.168.48.100&quot;.tls]    insecure_skip_verify = true  # 是否跳过安全认证  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;192.168.48.100&quot;.auth]    username = &quot;admin&quot;    password = &quot;Harbor12345&quot;[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors]  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]    endpoint = [&quot;https://registry-1.docker.io&quot;]  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;192.168.48.100&quot;]    endpoint = [&quot;http://192.168.48.100&quot;]

添加Harbor信息
sed -i &#x27;/\[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs\]/a \        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;192.168.48.100&quot;.tls]\          insecure_skip_verify = true  # 是否跳过安全认证\        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.configs.&quot;192.168.48.100&quot;.auth]\          username = &quot;admin&quot;\          password = &quot;Harbor12345&quot;&#x27; /etc/containerd/config.tomlsed -i &#x27;/\[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors\]/a \        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]\          endpoint = [&quot;https://registry-1.docker.io&quot;]\        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;192.168.48.100&quot;]\          endpoint = [&quot;http://192.168.48.100&quot;]&#x27; /etc/containerd/config.toml
最后尝试下载镜像
crictl pull 192.168.48.100/cicd/jenkins:latest
这个是我自己上传的镜像，已经在harbor仓库了，我现在在有containerd的客户端进行拉取看看能不能成功

显然已经成功
进行登入测试
docker login 192.168.48.100[root@harbor1 ~]# docker login 192.168.48.100Username: adminPassword:WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded[root@qianyios ~]# docker login 192.168.48.100Username: adminPassword:WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded##经过测试，通过添加&quot;insecure-registries&quot;: [ &quot;192.168.48.100&quot; ]可以免除https登入
上传镜像测试
#下载一个nginx镜像，然后tag，再上传[root@qianyios ~]# docker pull nginxUsing default tag: latestlatest: Pulling from library/nginxa2abf6c4d29d: Pull completea9edb18cadd1: Pull complete589b7251471a: Pull complete186b1aaa4aa6: Pull completeb4df32aa5a72: Pull completea0bcbecc962e: Pull completeDigest: sha256:0d17b565c37bcbd895e9d92315a05c1c3c9a29f762b011a10c54a66cd53c9b31Status: Downloaded newer image for nginx:latestdocker.io/library/nginx:latest#在此下载了最新版的nginx镜像tag为lastest
我们进入当刚刚创建的仓库，点推送指令
#推送镜像命令格式#docker tag 源镜像名[:TAG] 192.168.48.100/qianyios/新镜像名[:TAG]docker tag SOURCE_IMAGE[:TAG] 192.168.48.100/qianyios/REPOSITORY[:TAG]

#我们将nginx镜像打上tag   [root@qianyios ~]# docker imagesREPOSITORY   TAG       IMAGE ID       CREATED       SIZEnginx        latest    605c77e624dd   2 years ago   141MB#605的意思是镜像id的前三位数字，我们指定为V1标签，相当于版本号。[root@qianyios ~]# docker tag 605 192.168.48.100/qianyios/nginx:V1[root@qianyios ~]# docker imagesREPOSITORY                      TAG       IMAGE ID       CREATED       SIZE192.168.48.100/qianyios/nginx   V1        605c77e624dd   2 years ago   141MBnginx                           latest    605c77e624dd   2 years ago   141MB#开始上传[root@qianyios ~]# docker push 192.168.48.100/qianyios/nginx:V1The push refers to repository [192.168.48.100/qianyios/nginx]d874fd2bc83b: Pushed32ce5f6a5106: Pushedf1db227348d0: Pushedb8d6e692a25e: Pushede379e8aedd4d: Pushed2edcec3590a4: PushedV1: digest: sha256:ee89b00528ff4f02f2405e4ee221743ebc3f8e8dd0bfd5c4c20a2fa2aaa7ede3 size: 1570#去页面查看


我们去harbor1测试拉取镜像，会发现下载数变成了1
[root@harbor1 ~]# docker pull 192.168.48.100/qianyios/nginx:V1V1: Pulling from qianyios/nginxa2abf6c4d29d: Pull completea9edb18cadd1: Pull complete589b7251471a: Pull complete186b1aaa4aa6: Pull completeb4df32aa5a72: Pull completea0bcbecc962e: Pull completeDigest: sha256:ee89b00528ff4f02f2405e4ee221743ebc3f8e8dd0bfd5c4c20a2fa2aaa7ede3Status: Downloaded newer image for 192.168.48.100/qianyios/nginx:V1192.168.48.100/qianyios/nginx:V1[root@harbor1 ~]#


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>OpenEuler</tag>
        <tag>Harbor</tag>
      </tags>
  </entry>
  <entry>
    <title>K8S高可用集群（内部etcd）</title>
    <url>/posts/7158/</url>
    <content><![CDATA[
K8S高可用集群（内部etcd）
以下会用到的资源K8s1.28.2（内外部etcd高可用）所需资源
主机拓扑



主机名
ip1（NAT）
系统
磁盘
内存




master1
192.168.48.101
Centos7.9
100G
4G


master2
192.168.48.102
Centos7.9
100G
4G


master3
192.168.48.103
Centos7.9
100G
4G


node01
192.168.48.104
Centos7.9
100G
8G



基础配置
centos通过单独安装，非克隆。安装完后进行基本环境的配置，配置一下几个方面：

设置主机名
关闭NetworkManager、firewalld、dnsmasq、selinux
设置ens33
优化ssh
备份并新增清华yum源、epel源、docker-ce源、k8s源
更新yum源软件包缓存
修改history格式及记录数
添加hosts解析
关闭swap分区
安装chrony服务，并同步时间
配置limits.conf
安装必备工具
升级系统并重启

操作主机：[master1,master2,master3,node01]
#!/bin/bashif [ $# -eq 2 ];then  echo &quot;设置主机名为：$1&quot;  echo &quot;ens33设置IP地址为：192.168.48.$2&quot;else  echo  &quot;使用方法：sh $0 主机名 主机位&quot;  exit 2fiecho &quot;--------------------------------------&quot;echo &quot;1.正在设置主机名：$1&quot;hostnamectl set-hostname $1echo &quot;2.正在关闭NetworkManager、firewalld、dnsmasq、selinux&quot;systemctl disable firewalld &amp;&gt; /dev/nullsystemctl disable NetworkManager &amp;&gt; /dev/nullsystemctl disable dnsmasq &amp;&gt; /dev/nullsystemctl stop firewalldsystemctl stop NetworkManagersystemctl stop dnsmasqsed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsetenforce 0echo &quot;3.正在设置ens33：192.168.48.$2&quot;cat &gt; /etc/sysconfig/network-scripts/ifcfg-ens33 &lt;&lt;EOFTYPE=ensernetBOOTPROTO=staticDEFROUTE=yesNAME=ens33DEVICE=ens33ONBOOT=yesIPADDR=192.168.48.$2NETMASK=255.255.255.0GATEWAY=192.168.48.2DNS1=114.114.114.114EOFsystemctl restart networkecho &quot;4.优化ssh&quot;sed -i &quot;s#\#UseDNS yes#UseDNS no#g&quot; /etc/ssh/sshd_configsed -i &quot;s#GSSAPIAuthentication yes#GSSAPIAuthentication no#g&quot; /etc/ssh/sshd_configsystemctl restart sshdecho &quot;5.备份并新增清华yum源、epel源、docker-ce源、k8s源&quot;rm -rf /etc/yum.repos.d/*cat &gt; /etc/yum.repos.d/Centos-Base.repo &lt;&lt;EOF[base]name=CentOS-$releasever - Basebaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/\$releasever/os/\$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[updates]name=CentOS-$releasever - Updatesbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/\$releasever/updates/\$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[extras]name=CentOS-$releasever - Extrasbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/\$releasever/extras/\$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[centosplus]name=CentOS-$releasever - Plusbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/\$releasever/centosplus/\$basearch/gpgcheck=1enabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7EOFcat &gt; /etc/yum.repos.d/epel.repo &lt;&lt;EOF[epel]name=Extra Packages for Enterprise Linux 7 - \$basearchbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/\$basearchfailovermensod=priorityenabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7[epel-debuginfo]name=Extra Packages for Enterprise Linux 7 - \$basearch - Debugbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/\$basearch/debugfailovermensod=priorityenabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7gpgcheck=1[epel-source]name=Extra Packages for Enterprise Linux 7 - \$basearch - Sourcebaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/SRPMSfailovermensod=priorityenabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7gpgcheck=1EOFcat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFcurl -o /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoecho &quot;6.更新yum源软件包缓存&quot; yum clean all &amp;&amp; yum makecacheecho &quot;7.修改history格式及记录数&quot;sed -i &quot;s#HISTSIZE=1000##g&quot; /etc/profilecat &gt;&gt; /etc/profile &lt;&lt;EOFshopt -s histappendUSER_IP=`who -u am i 2&gt;/dev/null| awk &#x27;&#123;print $NF&#125;&#x27;|sed -e &#x27;s/[()]//g&#x27;`export HISTFILE=~/.commandline_warriorexport HISTTIMEFORMAT=&quot;%Y-%m-%d %H:%M:%S  `whoami`@$&#123;USER_IP&#125;: &quot;export HISTSIZE=200000export HISTFILESIZE=1000000export PROMPT_COMMAND=&quot;history -a&quot;EOFsource /etc/profileecho &quot;8.添加hosts解析&quot;cat &gt;&gt; /etc/hosts &lt;&lt;EOF192.168.48.101 master1192.168.48.102 master2192.168.48.103 master3192.168.48.104 node01EOFecho &quot;9.关闭swap分区&quot;swapoff -a &amp;&amp; sysctl -w vm.swappiness=0 &amp;&gt; /dev/nullsed -ri &#x27;/^[^#]*swap/s@^@#@&#x27; /etc/fstabecho &quot;10.安装ntpdate服务，并同步时间&quot;yum install chrony -ysystemctl start chronydsystemctl enable chronydchronyc sourceschronyc sourcesecho &quot;11.配置limits.conf&quot;ulimit -SHn 65535cat &gt;&gt; /etc/security/limits.conf &lt;&lt;EOF* soft nofile 65536* hard nofile 131072* soft nproc 65535* hard nproc 655350* soft memlock unlimited* hard memlock unlimitedEOFecho &quot;12.必备工具安装&quot;yum install wget psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -yecho &quot;13.升级系统并重启&quot;yum update -y --exclude=kernel* &amp;&amp; reboot
sh k8s_system_init.sh 主机名 主机位[master1] sh k8s_system_init.sh master1 101[master2] sh k8s_system_init.sh master2 102[master3] sh k8s_system_init.sh master3 103[node01] sh k8s_system_init.sh node01 104
配置ssh免密
操作节点[master1]
yum install -y sshpass cat &gt; sshmianmi.sh &lt;&lt; &quot;EOF&quot;#!/bin/bash# 目标主机列表hosts=(&quot;master1&quot; &quot;master2&quot; &quot;master3&quot; &quot;node01&quot;)# 密码password=&quot;123456&quot;# 生成 SSH 密钥对ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa# 循环遍历目标主机for host in &quot;$&#123;hosts[@]&#125;&quot;do    # 复制公钥到目标主机    sshpass -p &quot;$password&quot; ssh-copy-id -o StrictHostKeyChecking=no &quot;$host&quot;        # 验证免密登录    sshpass -p &quot;$password&quot; ssh -o StrictHostKeyChecking=no &quot;$host&quot; &quot;echo &#x27;免密登录成功&#x27;&quot;doneEOFsh sshmianmi.sh
内核及ipvs模块配置
此步骤是升级内核、配置ipvs模块，开启一些k8s集群中必须的内核参数。配置一下几个方面：

下载安装包到/server/soft
安装kernel
更改内核启动顺序
安装ipvsadm
配置ipvs模块
开启k8s集群必须的内核参数
配置完内核，重启服务器

操作主机：[master1,master2,master3,node01]
vi kernel_update.sh#!/bin/bashecho &quot;1.下载安装包到/server/soft&quot;mkdir -p /server/soft ; cd /server/softwget https://mirrors.aliyun.com/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-6.6.3-1.el7.elrepo.x86_64.rpmwget https://mirrors.aliyun.com/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-6.6.3-1.el7.elrepo.x86_64.rpmecho &quot;2.正在安装kernel&quot;yum localinstall -y kernel-ml*echo &quot;3.更改内核启动顺序&quot;grub2-set-default  0 &amp;&amp; grub2-mkconfig -o /etc/grub2.cfggrubby --args=&quot;user_namespace.enable=1&quot; --update-kernel=&quot;$(grubby --default-kernel)&quot;echo &quot;4.输出现在内核版本信息&quot;grubby --default-kernelecho &quot;5.安装ipvsadm&quot;yum install ipvsadm ipset sysstat conntrack libseccomp -y &amp;&gt; /dev/nullecho &quot;6.配置ipvs模块&quot;modprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrackcat &gt;&gt; /etc/modules-load.d/ipvs.conf &lt;&lt;EOFip_vsip_vs_lcip_vs_wlcip_vs_rrip_vs_wrrip_vs_lblcip_vs_lblcrip_vs_dhip_vs_ship_vs_foip_vs_nqip_vs_sedip_vs_ftpip_vs_shnf_conntrackip_tablesip_setxt_setipt_setipt_rpfilteript_REJECTipipEOFsystemctl enable --now systemd-modules-load.service &amp;&gt; /dev/nullecho &quot;7.开启k8s集群必须的内核参数&quot;cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1fs.may_detach_mounts = 1net.ipv4.conf.all.route_localnet = 1vm.overcommit_memory=1vm.panic_on_oom=0fs.inotify.max_user_watches=89100fs.file-max=52706963fs.nr_open=52706963net.netfilter.nf_conntrack_max=2310720net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_probes = 3net.ipv4.tcp_keepalive_intvl =15net.ipv4.tcp_max_tw_buckets = 36000net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_max_orphans = 327680net.ipv4.tcp_orphan_retries = 3net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.ip_conntrack_max = 65536net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.tcp_timestamps = 0net.core.somaxconn = 16384EOFsysctl --systemecho &quot;8.配置完内核，重启服务器！&quot;reboot
sh kernel_update.sh
检查ipvs加载、内核版本验证
lsmod | grep --color=auto -e ip_vs -e nf_conntrackuname -a

高可用组件安装
haproxy配置
操作节点：[master1，master2,master3]
yum install keepalived haproxy -y
所有Master节点配置HAProxy，所有Master节点的HAProxy配置相同。
操作节点：[master1，master2]cat &gt; /etc/haproxy/haproxy.cfg &lt;&lt;&quot;EOF&quot;global  maxconn  2000  ulimit-n  16384  log  127.0.0.1 local0 err  stats timeout 30sdefaults  log global  mode  http  option  httplog  timeout connect 5000  timeout client  50000  timeout server  50000  timeout http-request 15s  timeout http-keep-alive 15sfrontend monitor-in  bind *:33305  mode http  option httplog  monitor-uri /monitorfrontend k8s-master  bind 0.0.0.0:16443  bind 127.0.0.1:16443  mode tcp  option tcplog  tcp-request inspect-delay 5s  default_backend k8s-masterbackend k8s-master  mode tcp  option tcplog  option tcp-check  balance roundrobin  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100  server master1   192.168.48.101:6443  check  server master2   192.168.48.102:6443  check  server master3   192.168.48.103:6443  checkEOF
Keepalived配置
操作节点：[master1，master2,master3]
所有Master节点配置Keepalived，以下三个Master节点配置注意ip和网卡。
操作节点：[master1]cat &gt;/etc/keepalived/keepalived.conf  &lt;&lt;&quot;EOF&quot;! Configuration File for keepalivedglobal_defs &#123;    router_id LVS_DEVELscript_user root    enable_script_security&#125;vrrp_script chk_apiserver &#123;    script &quot;/etc/keepalived/check_apiserver.sh&quot;    interval 5    weight -5    fall 2  rise 1&#125;vrrp_instance VI_1 &#123;    state MASTER    interface ens33    mcast_src_ip 192.168.48.101    virtual_router_id 51    priority 101    advert_int 2    authentication &#123;        auth_type PASS        auth_pass K8SHA_KA_AUTH    &#125;    virtual_ipaddress &#123;        192.168.48.200    &#125;    track_script &#123;       chk_apiserver    &#125;&#125;EOF
操作节点：[master2]cat &gt;/etc/keepalived/keepalived.conf  &lt;&lt;&quot;EOF&quot;! Configuration File for keepalivedglobal_defs &#123;    router_id LVS_DEVELscript_user root    enable_script_security&#125;vrrp_script chk_apiserver &#123;    script &quot;/etc/keepalived/check_apiserver.sh&quot;    interval 5    weight -5    fall 2  rise 1&#125;vrrp_instance VI_1 &#123;    state BACKUP    interface ens33    mcast_src_ip 192.168.48.102    virtual_router_id 51    priority 100    advert_int 2    authentication &#123;        auth_type PASS        auth_pass K8SHA_KA_AUTH    &#125;    virtual_ipaddress &#123;        192.168.48.200    &#125;    track_script &#123;       chk_apiserver    &#125;&#125;EOF
操作节点：[master3]cat &gt;/etc/keepalived/keepalived.conf  &lt;&lt;&quot;EOF&quot;! Configuration File for keepalivedglobal_defs &#123;    router_id LVS_DEVELscript_user root    enable_script_security&#125;vrrp_script chk_apiserver &#123;    script &quot;/etc/keepalived/check_apiserver.sh&quot;    interval 5    weight -5    fall 2  rise 1&#125;vrrp_instance VI_1 &#123;    state BACKUP    interface ens33    mcast_src_ip 192.168.48.103    virtual_router_id 51    priority 99    advert_int 2    authentication &#123;        auth_type PASS        auth_pass K8SHA_KA_AUTH    &#125;    virtual_ipaddress &#123;        192.168.48.200    &#125;    track_script &#123;       chk_apiserver    &#125;&#125;EOF
配置Keepalived健康检查文件
操作节点：[master1，master2,master3]
cat &gt; /etc/keepalived/check_apiserver.sh &lt;&lt;&quot;EOF&quot; #!/bin/bash err=0 for k in $(seq 1 3) do    check_code=$(pgrep haproxy)    if [[ $check_code == &quot;&quot; ]]; then        err=$(expr $err + 1)        sleep 1        continue    else        err=0        break    fi done  if [[ $err != &quot;0&quot; ]]; then    echo &quot;systemctl stop keepalived&quot;    /usr/bin/systemctl stop keepalived    exit 1 else    exit 0 fiEOFchmod +x /etc/keepalived/check_apiserver.sh
启动haproxy和keepalived
操作节点：[master，master2,master3]systemctl daemon-reloadsystemctl enable --now haproxysystemctl enable --now keepalived
测试集群负载均衡高可用
查看master1的vip
ip a

模拟master1的宕机测试，看看vip会不会漂移到master2去
[master1] poweroff

这时候查看master2的ip列表
[master2] ip a

结论：这时可以知道，负载均衡集群成功，当master1出现宕机情况，vip会从master1漂移到master2
docker安装
安装docker
操作节点[master1，master2，master3,node01]
wget https://download.docker.com/linux/static/stable/x86_64/docker-24.0.7.tgztar xf docker-*.tgzcp -rf docker/* /usr/bin/#创建containerd的service文件,并且启动cat &gt;/etc/systemd/system/containerd.service &lt;&lt;EOF[Unit]Description=containerd container runtimeDocumentation=https://containerd.ioAfter=network.target local-fs.target[Service]ExecStartPre=-/sbin/modprobe overlayExecStart=/usr/bin/containerdType=notifyDelegate=yesKillMode=processRestart=alwaysRestartSec=5LimitNPROC=infinityLimitCORE=infinityLimitNOFILE=1048576TasksMax=infinityOOMScoreAdjust=-999[Install]WantedBy=multi-user.targetEOFsystemctl enable --now containerd.service#准备docker的service文件cat &gt; /etc/systemd/system/docker.service &lt;&lt;EOF[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.service containerd.serviceWants=network-online.targetRequires=docker.socket containerd.service[Service]Type=notifyExecStart=/usr/bin/dockerd --config-file=/etc/docker/daemon.json -H fd://containerd=/run/containerd/containerd.sockExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=alwaysStartLimitBurst=3StartLimitInterval=60sLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTasksMax=infinityDelegate=yesKillMode=processOOMScoreAdjust=-500[Install]WantedBy=multi-user.targetEOF#准备docker的socket文件cat &gt; /etc/systemd/system/docker.socket &lt;&lt;EOF[Unit]Description=Docker Socket for the API[Socket]ListenStream=/var/run/docker.sockSocketMode=0660SocketUser=rootSocketGroup=docker[Install]WantedBy=sockets.targetEOFgroupadd dockersystemctl enable --now docker.socket  &amp;&amp; systemctl enable --now docker.service#验证mkdir /etc/dockersudo tee /etc/docker/daemon.json &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;&#123;  &quot;registry-mirrors&quot;: [    &quot;https://docker.xuanyuan.me&quot;,    &quot;https://docker.m.daocloud.io&quot;,    &quot;https://docker.1ms.run&quot;,    &quot;https://docker.1panel.live&quot;,    &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,		&quot;https://docker.qianyios.top&quot;  ],  &quot;max-concurrent-downloads&quot;: 10,  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-level&quot;: &quot;warn&quot;,  &quot;log-opts&quot;: &#123;    &quot;max-size&quot;: &quot;10m&quot;,    &quot;max-file&quot;: &quot;3&quot;  &#125;,  &quot;data-root&quot;: &quot;/var/lib/docker&quot;&#125;EOFsystemctl daemon-reloadsystemctl restart docker
安装cri-docker
操作节点[master1，master2，master3,node01]
wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.7/cri-dockerd-0.3.7.amd64.tgztar -zxvf cri-dockerd-0.3.7.amd64.tgzcp cri-dockerd/cri-dockerd  /usr/bin/chmod +x /usr/bin/cri-dockerd#写入启动配置文件cat &gt;  /usr/lib/systemd/system/cri-docker.service &lt;&lt;EOF[Unit]Description=CRI Interface for Docker Application Container EngineDocumentation=https://docs.mirantis.comAfter=network-online.target firewalld.service docker.serviceWants=network-online.targetRequires=cri-docker.socket [Service]Type=notifyExecStart=/usr/bin/cri-dockerd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9ExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=always StartLimitBurst=3 StartLimitInterval=60s LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity TasksMax=infinityDelegate=yesKillMode=process [Install]WantedBy=multi-user.targetEOF#写入socket配置文件cat &gt; /usr/lib/systemd/system/cri-docker.socket &lt;&lt;EOF[Unit]Description=CRI Docker Socket for the APIPartOf=cri-docker.service [Socket]ListenStream=%t/cri-dockerd.sockSocketMode=0660SocketUser=rootSocketGroup=docker [Install]WantedBy=sockets.targetEOFsystemctl daemon-reload &amp;&amp; systemctl enable cri-docker --now
K8S集群安装
安装k8s所需的工具
操作节点[master1，master2，master3,node01]yum -y install  kubeadm kubelet kubectl#为了实现docker使用的cgroupdriver与kubelet使用的cgroup的一致性，配置如下：sed -i &#x27;s/^KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=&quot;--cgroup-driver=systemd&quot;/g&#x27; /etc/sysconfig/kubelet#设置kubelet为开机自启动即可，由于没有生成配置文件，集群初始化后自动启动systemctl enable kubeletsystemctl enable kubelet.service
集群初始化
操作节点[master1]cat &gt; kubeadm-config.yaml &lt;&lt; EOFapiVersion: kubeadm.k8s.io/v1beta3bootstrapTokens:- groups:  - system:bootstrappers:kubeadm:default-node-token  token: abcdef.0123456789abcdef  ttl: 24h0m0s  usages:  - signing  - authenticationkind: InitConfigurationlocalAPIEndpoint:  advertiseAddress: 192.168.48.101  bindPort: 6443nodeRegistration:  criSocket: unix:///var/run/cri-dockerd.sock  imagePullPolicy: IfNotPresent  taints: null---apiServer:  timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta3certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: &#123;&#125;dns: &#123;&#125;etcd:  local:    dataDir: /var/lib/etcdimageRepository: registry.aliyuncs.com/google_containerskind: ClusterConfigurationkubernetesVersion: 1.28.2networking:  dnsDomain: cluster.local  podSubnet: 10.244.0.0/16  serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125;controlPlaneEndpoint: &quot;192.168.48.200:16443&quot;---apiVersion: kubeproxy.config.k8s.io/v1alpha1bindAddress: 0.0.0.0bindAddressHardFail: falseclientConnection:  acceptContentTypes: &quot;&quot;  burst: 0  contentType: &quot;&quot;  kubeconfig: /var/lib/kube-proxy/kubeconfig.conf  qps: 0clusterCIDR: &quot;&quot;configSyncPeriod: 0sconntrack:  maxPerCore: null  min: null  tcpCloseWaitTimeout: null  tcpEstablishedTimeout: nulldetectLocal:  bridgeInterface: &quot;&quot;  interfaceNamePrefix: &quot;&quot;detectLocalMode: &quot;&quot;enableProfiling: falsehealthzBindAddress: &quot;&quot;hostnameOverride: &quot;&quot;iptables:  localhostNodePorts: null  masqueradeAll: false  masqueradeBit: null  minSyncPeriod: 0s  syncPeriod: 0sipvs:  excludeCIDRs: null  minSyncPeriod: 0s  scheduler: &quot;&quot;  strictARP: false  syncPeriod: 0s  tcpFinTimeout: 0s  tcpTimeout: 0s  udpTimeout: 0skind: KubeProxyConfigurationlogging:  flushFrequency: 0  options:    json:      infoBufferSize: &quot;0&quot;  verbosity: 0metricsBindAddress: &quot;&quot;mode: &quot;&quot;nodePortAddresses: nulloomScoreAdj: nullportRange: &quot;&quot;showHiddenMetricsForVersion: &quot;&quot;winkernel:  enableDSR: false  forwardHealthCheckVip: false  networkName: &quot;&quot;  rootHnsEndpointName: &quot;&quot;  sourceVip: &quot;&quot;---apiVersion: kubelet.config.k8s.io/v1beta1authentication:  anonymous:    enabled: false  webhook:    cacheTTL: 0s    enabled: true  x509:    clientCAFile: /etc/kubernetes/pki/ca.crtauthorization:  mode: Webhook  webhook:    cacheAuthorizedTTL: 0s    cacheUnauthorizedTTL: 0scgroupDriver: systemdclusterDNS:- 10.96.0.10clusterDomain: cluster.localcontainerRuntimeEndpoint: &quot;&quot;cpuManagerReconcilePeriod: 0sevictionPressureTransitionPeriod: 0sfileCheckFrequency: 0shealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 0simageMinimumGCAge: 0skind: KubeletConfigurationlogging:  flushFrequency: 0  options:    json:      infoBufferSize: &quot;0&quot;  verbosity: 0memorySwap: &#123;&#125;nodeStatusReportFrequency: 0snodeStatusUpdateFrequency: 0srotateCertificates: trueruntimeRequestTimeout: 0sshutdownGracePeriod: 0sshutdownGracePeriodCriticalPods: 0sstaticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 0ssyncFrequency: 0svolumeStatsAggPeriod: 0sEOFkubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml
准备k8s所需的镜像
操作节点[master1,master2,master3]kubeadm config images pull --config /root/new.yaml 

master1节点初始化
操作节点[master1]
kubeadm init --config /root/new.yaml  --upload-certs
会生成信息

记录信息后面会用到
初始化成功以后，会产生Token值，用于其他节点加入时使用，因此要记录下初始化成功生成的token值（令牌值），有效期24小时，后续需要操作可以重新生成Token
操作节点[master1]
kubeadm join 192.168.48.200:16443 --token abcdef.0123456789abcdef \        --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f \        --control-plane --certificate-key 45eeadfc9135a368d08582a90c779c0934d24c54f56134c65d67516f5e6f981dkubeadm join 192.168.48.200:16443 --token abcdef.0123456789abcdef \        --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f
操作kubect报错：

此时通过kubectl操作，会出现失败，因为还没有将集群的&quot;钥匙&quot;交给root用户。/etc/kubernetes/admin.conf 文件是 Kubernetes（K8s）集群中的管理员配置文件，它包含了用于管理集群的身份验证和访问信息。所以下面进行配置环境变量，用于访问Kubernetes集群：
添加环境变量
操作节点[master1]
mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config
添加其他master节点至集群中
操作节点[master2,master3]
操作节点[master2,master3]kubeadm join 192.168.48.200:16443 --token abcdef.0123456789abcdef \        --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f \        --control-plane --certificate-key 45eeadfc9135a368d08582a90c779c0934d24c54f56134c65d67516f5e6f981d \        --cri-socket unix:///var/run/cri-dockerd.sock 
注意：这里末尾添加了--cri-socket unix:///var/run/cri-dockerd.sock 
接着给master2添加环境变量
操作节点[master2,master3]mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config

这里没有展示master3的图片，但是步骤一样的
模拟Token过期重新生成并加入Node节点
假设加入集群的token过期了。node01无法加入了，这里就模拟一下这种情况

Token过期后生成新的token：

kubeadm token create --print-join-command
[root@master1 ~]# kubeadm token create --print-join-commandkubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f
其中，192.168.48.200:16443 是你的 Kubernetes API 服务器的地址和端口，tn5q1b.7w1jj77ewup7k2in 是新的令牌，sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f 是令牌的 CA 证书哈希值。

Master需要生成–certificate-key：

kubeadm init phase upload-certs --upload-certs
[root@master1 ~]# kubeadm init phase upload-certs --upload-certsW1110 18:12:03.426245   10185 version.go:104] could not fetch a Kubernetes version from the internet: unable to get URL &quot;https://dl.k8s.io/release/stable-1.txt&quot;: Get &quot;https://dl.k8s.io/release/stable-1.txt&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)W1110 18:12:03.426320   10185 version.go:105] falling back to the local client version: v1.28.2[upload-certs] Storing the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace[upload-certs] Using certificate key:5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128
其中，5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128 是证书密钥。

生成新的Token用于集群添加新Node节点

操作节点[node01]
kubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f  \        --cri-socket unix:///var/run/cri-dockerd.sock 
注意：这里末尾添加了--cri-socket unix:///var/run/cri-dockerd.sock 

这时在master查看node状态（显示为notready不影响）

模拟新加master节点的加入K8S集群中
假设我们新加master节点的话，就拼接token，从刚刚生成的token拼接
[root@master1 ~]# kubeadm token create --print-join-commandkubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f
这里提取信息1
kubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f
接着
[root@master1 ~]# kubeadm init phase upload-certs --upload-certsW1110 18:12:03.426245   10185 version.go:104] could not fetch a Kubernetes version from the internet: unable to get URL &quot;https://dl.k8s.io/release/stable-1.txt&quot;: Get &quot;https://dl.k8s.io/release/stable-1.txt&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)W1110 18:12:03.426320   10185 version.go:105] falling back to the local client version: v1.28.2[upload-certs] Storing the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace[upload-certs] Using certificate key:5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128
这里提取信息2：这里前面要加上--control-plane --certificate-key
--control-plane --certificate-key 5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128
合成
kubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in \        --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f  \        --control-plane --certificate-key 5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128 \        --cri-socket unix:///var/run/cri-dockerd.sock                kubeadm join 192.168.48.200:16443 --token lnkno8.u4v1l8n9pahzf0kj \        --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f \        --control-plane --certificate-key 41e441fe56cb4bdfcc2fc0291958e6b1da54d01f4649b6651471c07583f85cdf \        --cri-socket unix:///var/run/cri-dockerd.sock
注意：这里末尾添加了--cri-socket unix:///var/run/cri-dockerd.sock 
图示

安装calico网络插件
操作节点[master1]
添加解析记录，否则无法访问
echo &#x27;185.199.108.133 raw.githubusercontent.com&#x27; &gt;&gt; /etc/hosts
应用operator资源清单文件
网络组件有很多种，只需要部署其中一个即可，推荐Calico。
Calico是一个纯三层的数据中心网络方案，Calico支持广泛的平台，包括Kubernetes、OpenStack等。
Calico 在每一个计算节点利用 Linux Kernel 实现了一个高效的虚拟路由器（ vRouter） 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息向整个 Calico 网络内传播。
此外，Calico 项目还实现了 Kubernetes 网络策略，提供ACL功能。
curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/calico.yaml -O
[root@master1 ~]# vim calico.yaml#添加两行- name: IP_AUTODETECTION_METHOD  value: interface=ens33#ens33是你的网卡

sed -i &#x27;s| docker.io/calico/| registry.cn-hangzhou.aliyuncs.com/qianyios/|&#x27; calico.yamlkubectl apply -f calico.yaml

监视kube-system命名空间中pod运行情况
等待估计20分钟左右吧(确保全部running)
kubectl get pods -n kube-system

拿掉master节点的污点
节点 master1 和 master2 都有一个名为 node-role.kubernetes.io/control-plane:NoSchedule 的污点。这个污点的作用是阻止普通的 Pod 被调度到这些节点上，只允许特定的控制平面组件（如 kube-apiserver、kube-controller-manager 和 kube-scheduler）在这些节点上运行。
这种设置有助于确保控制平面节点专门用于运行 Kubernetes 的核心组件，而不会被普通的工作负载占用。通过将污点添加到节点上，可以确保只有被授权的控制平面组件才能在这些节点上运行。
kubectl describe node master1 | grep -i taintkubectl describe node master2 | grep -i taintkubectl describe node master3 | grep -i taint

去除污点
kubectl taint node master1 node-role.kubernetes.io/control-plane:NoSchedule-kubectl taint node master2 node-role.kubernetes.io/control-plane:NoSchedule-kubectl taint node master3 node-role.kubernetes.io/control-plane:NoSchedule-

安装dashboard
操作节点[master1]
下载文件
https://github.com/kubernetes/dashboard/releases/tag/v2.7.0
目前最新版本v2.7.0
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yamlsed -i &#x27;s/kubernetesui\/dashboard:v2.7.0/registry.cn-hangzhou.aliyuncs.com\/qianyios\/dashboard:v2.7.0/g&#x27; recommended.yamlsed -i &#x27;s/kubernetesui\/metrics-scraper:v1.0.8/registry.cn-hangzhou.aliyuncs.com\/qianyios\/metrics-scraper:v1.0.8/g&#x27; recommended.yaml

修改配置文件
vim recommended.yaml---kind: ServiceapiVersion: v1metadata:  labels:    app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboardspec:  ports:    - port: 443      targetPort: 8443      nodePort: 30001  type: NodePort  selector:    app: kubernetes-dashboard---

运行dashboard
kubectl apply -f recommended.yaml

检查运行状态
kubectl get pods -n kubernetes-dashboardkubectl get pod,svc -o wide -n kubernetes-dashboard

创建cluster-admin用户
创建service account并绑定默认cluster-admin管理员群角色#创建用户kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard#用户授权kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin#获取用户Tokenkubectl create token dashboard-admin -n kubernetes-dashboard

记录token
eyJhbGciOiJSUzI1NiIsImtpZCI6IjJxb21sRTZSckhEZ09FMnlvWU5IY3dfTTRIWDEzRUpsQ000MThhSWxYNDgifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjk5NjQ2Mjg3LCJpYXQiOjE2OTk2NDI2ODcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJkYXNoYm9hcmQtYWRtaW4iLCJ1aWQiOiIyNTBhNTQ2MS03MGJlLTRhZTItOWY2Yi1hMDQwOWE1NWJhMTMifX0sIm5iZiI6MTY5OTY0MjY4Nywic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.U-lH9_sRU4TVrpAznqS60INNSev9NxHu61igGUTzTBoaWo7WjPG7vzTnpZYhvsoglTzMEKhfranJkkkn95pe-prkvSasaAL6kXHw0jQjlSMzcYiF7DoLdkOtDJSukuALEubidf9eIwZXFZ-sezZdZHm4hnk5nWme5YtdOmYOJPh5sv1dzRvM1XuOHknJPTA1BbdZuVAtSGHSjkhwx-wl-41uuQoROW5GjJs0bz4zLBFn1w_pWaSMCn7pjGJNcbr6IuDV41km_etpwwxacWyfAcxNykzCtIiE1abJj7m-e944GvAn_eqxz3wCZD6Bgt41FWRzyMHjrppJfDjk7FaHNw
登录浏览器访问
https://192.168.48.200:30001输入token：----eyJhbGciOiJSUzI1NiIsImtpZCI6IjJxb21sRTZSckhEZ09FMnlvWU5IY3dfTTRIWDEzRUpsQ000MThhSWxYNDgifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjk5NjQ2Mjg3LCJpYXQiOjE2OTk2NDI2ODcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJkYXNoYm9hcmQtYWRtaW4iLCJ1aWQiOiIyNTBhNTQ2MS03MGJlLTRhZTItOWY2Yi1hMDQwOWE1NWJhMTMifX0sIm5iZiI6MTY5OTY0MjY4Nywic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.U-lH9_sRU4TVrpAznqS60INNSev9NxHu61igGUTzTBoaWo7WjPG7vzTnpZYhvsoglTzMEKhfranJkkkn95pe-prkvSasaAL6kXHw0jQjlSMzcYiF7DoLdkOtDJSukuALEubidf9eIwZXFZ-sezZdZHm4hnk5nWme5YtdOmYOJPh5sv1dzRvM1XuOHknJPTA1BbdZuVAtSGHSjkhwx-wl-41uuQoROW5GjJs0bz4zLBFn1w_pWaSMCn7pjGJNcbr6IuDV41km_etpwwxacWyfAcxNykzCtIiE1abJj7m-e944GvAn_eqxz3wCZD6Bgt41FWRzyMHjrppJfDjk7FaHNw----

部署一个nginx测试
操作节点[master1]
vim web.yamlkind: Deployment#apiVersion: extensions/v1beta1apiVersion: apps/v1metadata:  labels:    app: web-deployment-label  name: web-deployment  namespace: defaultspec:  replicas: 3  selector:    matchLabels:      app: web-selector  template:    metadata:      labels:        app: web-selector    spec:      containers:      - name: web-container        image: nginx:latest        imagePullPolicy: Always        ports:        - containerPort: 80          protocol: TCP          name: http        - containerPort: 443          protocol: TCP          name: https---kind: ServiceapiVersion: v1metadata:  labels:    app: web-service-label  name: web-service  namespace: defaultspec:  type: NodePort  ports:  - name: http    port: 80    protocol: TCP    targetPort: 80    nodePort: 30080  - name: https    port: 443    protocol: TCP    targetPort: 443    nodePort: 30443  selector:    app: web-selector    kubectl apply -f web.yaml 

### 查看nginx的pod 的详细信息kubectl get deploy,svc,pod -o wide

访问nginx网站
http://192.168.48.200:30080


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>Centos 7</tag>
        <tag>K8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Centos 8 stream 部署 kvm</title>
    <url>/posts/17511/</url>
    <content><![CDATA[Centos 8 stream 部署 kvm
环境部署



网卡模式
NAT模式




ip
192.168.48.10


内存
4G


核心
4


硬盘
100G


功能
AMD-V



设置主机名
[root@localhost ~]# hostnamectl set-hostname KVM &amp;&amp; bash[root@KVM ~]#
设置网络
设置虚拟机网络（ens160是NAT模式）
[root@KVM ~]# vi /etc/sysconfig/network-scripts/ifcfg-ens160BOOTPROTO=staticONBOOT=yesIPADDR=192.168.48.10PREFIX=24GATEWAY=192.168.48.2DNS1=192.168.48.2DOMAIN=114.114.114.114[root@KVM ~]# nmcli c reload[root@KVM ~]# nmcli c up ens160Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/4)[root@KVM ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000    link/ether 00:0c:29:ce:a9:48 brd ff:ff:ff:ff:ff:ff    altname enp3s0    inet 192.168.48.10/24 brd 192.168.48.255 scope global noprefixroute ens160       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fece:a948/64 scope link noprefixroute       valid_lft forever preferred_lft forever[root@KVM ~]# ping -c 4 jd.comPING jd.com (111.13.149.108) 56(84) bytes of data.64 bytes from 111.13.149.108 (111.13.149.108): icmp_seq=1 ttl=48 time=47.8 ms64 bytes from 111.13.149.108 (111.13.149.108): icmp_seq=2 ttl=48 time=48.2 ms64 bytes from 111.13.149.108 (111.13.149.108): icmp_seq=3 ttl=48 time=48.2 ms64 bytes from 111.13.149.108 (111.13.149.108): icmp_seq=4 ttl=48 time=48.3 ms--- jd.com ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 6077msrtt min/avg/max/mdev = 47.845/48.146/48.297/0.235 ms
检查虚拟机是否支持虚拟化
勾选cpu虚拟化（宿主机是amd芯片）

查看虚拟化是否支持
[root@KVM ~]# lscpuVirtualization:      AMD-V说明已支持虚拟化
设置阿里yum源
[root@KVM ~]# cp /etc/yum.repos.d/CentOS-Stream-BaseOS.repo /etc/yum.repos.d/CentOS-Stream-BaseOS.repo.bak[root@KVM ~]# sed -i &#x27;s/mirrorlist/#mirrorlist/&#x27; /etc/yum.repos.d/CentOS-Stream-BaseOS.repo[root@KVM ~]# sed -i &#x27;s/#baseurl=http:\/\/mirror.centos.org/baseurl=http:\/\/mirrors.aliyun.com/g&#x27; /etc/yum.repos.d/CentOS-Stream-BaseOS.repo[root@KVM ~]# yum clean all &amp;&amp; yum makecache0 files removedCentOS Stream 8 - AppStream                                        14 MB/s |  27 MB     00:01CentOS Stream 8 - BaseOS                                          4.0 MB/s |  26 MB     00:06CentOS Stream 8 - Extras                                           28 kB/s |  18 kB     00:00CentOS Stream 8 - Extras common packages                          8.2 kB/s | 5.2 kB     00:00Metadata cache created.[root@KVM ~]#
安装kvm
安装kvm及其工具
[root@KVM ~]# yum install qemu-kvm qemu-img  virt-manager libvirt virt-manager libvirt-client virt-install virt-viewer -yWaiting for process with pid 8674 to finish.Last metadata expiration check: 0:00:05 ago on Sun 08 Jan 2023 06:03:34 AM EST.Package qemu-kvm-15:6.2.0-20.module_el8.7.0+1218+f626c2ff.1.x86_64 is already installed.Package qemu-img-15:6.2.0-20.module_el8.7.0+1218+f626c2ff.1.x86_64 is already installed.Package virt-manager-3.2.0-4.el8.noarch is already installed.Package libvirt-8.0.0-10.module_el8.7.0+1218+f626c2ff.x86_64 is already installed.Package libvirt-client-8.0.0-10.module_el8.7.0+1218+f626c2ff.x86_64 is already installed.Package virt-install-3.2.0-4.el8.noarch is already installed.Package virt-viewer-9.0-11.el8.x86_64 is already installed.Dependencies resolved.Nothing to do.Complete!
启动libvirtd服务
启动服务并设置韦开机自启动，查看状态
[root@KVM ~]# systemctl enable --now libvirtd[root@KVM ~]# systemctl status libvirtd● libvirtd.service - Virtualization daemon   Loaded: loaded (/usr/lib/systemd/system/libvirtd.service; enabled; vendor preset: enabled)   Active: active (running) since Sun 2023-01-08 06:05:12 EST; 12s ago     Docs: man:libvirtd(8)           https://libvirt.org
验证是否已加载kvm模块
[root@KVM ~]# lsmod | grep kvmkvm_amd               143360  0ccp                   106496  1 kvm_amdkvm                   942080  1 kvm_amdirqbypass              16384  1 kvm
[root@KVM ~]# virsh list Id   Name   State--------------------说明kvm安装成功
设置宿主机网络（kvm这个机子）
查看网络情况
KVN软件安装后默认以NAT方式实现网络通信。为了让KVM虚拟机能够与宿主机、本地主机、互联网相互通信，需将宿主机（KVM这个机子）网络设置为bridge方式。
ip a
lo为回环接口，该接口不从外界接收和发送数据包，仅在操作系统内部接收和发送数据包
ens160是以太网接口，与网卡对应，每个硬件网卡对应一个以太网接口
virbr0为虚拟网络接口，由kvm创建，为连接其上的kvm虚拟机网络提供访问外部网络的功能
创建bridge
创建 bridge 时需使用nmcli命令创建 br0，并将其绑定到可以正常工作的网络接口上，同时让br0成为连接宿主机与互联网的接口。
[root@KVM ~]# nmcli connection add type bridge con-name br0 ifname br0 autoconnect yesConnection &#x27;br0&#x27; (ac3429bc-907c-4ad1-bd54-bbf39d853a53) successfully added.查看是否创建成功[root@KVM ~]# nmcli cNAME    UUID                                  TYPE      DEVICEbr0     ac3429bc-907c-4ad1-bd54-bbf39d853a53  bridge    br0ens160  0d45e631-b256-4e08-b8d8-2c42b9481594  ethernet  ens160virbr0  7075d19e-a20c-43da-b161-e7c7519febdb  bridge    virbr0网桥创建成功后会自动生成配置文件[root@KVM ~]# ls -l /etc/sysconfig/network-scripts/-rw-r--r--. 1 root root 312 Jan  8 06:20 ifcfg-br0-rw-r--r--. 1 root root 365 Jan  8 05:40 ifcfg-ens160
设置br0和ens160网卡
将br0桥接到ens160
[root@KVM ~]# vi /etc/sysconfig/network-scripts/ifcfg-br0BOOTPROTO=staticIPADDR=192.168.48.10GATEWAY=192.168.48.2PREFIX=24DNS=114.114.114.114[root@KVM ~]# vi /etc/sysconfig/network-scripts/ifcfg-ens160BOOTPROTO=noneBRIDGE=br0[root@KVM ~]# nmcli c reload[root@KVM ~]# nmcli c drow br0r此时如果你是用ssh工具连着的，你会断开，此时你要去vm那里输入[root@KVM ~]# nmcli c up br0[root@KVM ~]# nmcli c up ens160然后等1分钟左右，就可以连上ssh工具了
测试网络连通性[root@KVM ~]# ping -c 2 jd.comPING jd.com (211.144.24.218) 56(84) bytes of data.64 bytes from 211.144.24.218 (211.144.24.218): icmp_seq=1 ttl=128 time=50.7 ms64 bytes from 211.144.24.218 (211.144.24.218): icmp_seq=2 ttl=128 time=51.9 ms--- jd.com ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1002msrtt min/avg/max/mdev = 50.683/51.286/51.890/0.644 ms[root@KVM ~]#
创建kvm虚拟机



存储池
存放目录
内容规划




disk
/opt/disk
存放KVM磁盘文件


存放iso
/opt/iso
存放待安装的ISO文件



创建iso存储池
[root@KVM ~]# mkdir -p /opt/iso[root@KVM ~]# chown root:root /opt/iso/[root@KVM ~]# chmod 777 /opt/iso/[root@KVM ~]# virsh pool-define-as iso --type dir --target /opt/iso/ Pool iso--type defined#名称为iso的存储池定义成功如果名字打错 用virsh pool-destroy [名字]virsh pool-undefine [名字] 删除[root@KVM ~]# virsh pool-list --all Name   State      Autostart------------------------------ iso    inactive   no[root@KVM ~]# virsh pool-build isoPool iso built#创建名为iso的存储池[root@KVM ~]# virsh pool-start isoPool iso started#启动iso存储池[root@KVM ~]# virsh pool-autostart isoPool iso marked as autostarted#设置iso存储池自启动查看iso信息[root@KVM ~]# virsh pool-info isoName:           isoUUID:           a966995e-2722-4a3f-a318-e158a642439eState:          runningPersistent:     yesAutostart:      yesCapacity:       63.84 GiBAllocation:     2.75 GiBAvailable:      61.09 GiB
创建disk存储池
[root@KVM ~]# mkdir -p /opt/disk[root@KVM ~]# chown root:root /opt/disk/[root@KVM ~]# chmod 777 /opt/disk/[root@KVM ~]# virsh pool-define-as disk --type dir --target /opt/disk/ Pool disk defined[root@KVM ~]# virsh pool-build diskPool disk built[root@KVM ~]# virsh pool-start diskPool disk started[root@KVM ~]# virsh pool-autostart diskPool disk marked as autostarted[root@KVM ~]# virsh pool-info diskName:           diskUUID:           30d784dc-386b-46e6-a22b-79a3b3447354State:          runningPersistent:     yesAutostart:      yesCapacity:       63.84 GiBAllocation:     2.75 GiBAvailable:      61.09 GiB
获取Centos7
下载Centos7最小化版本到ISO目录下（网络方式）
[root@KVM ~]# yum install -y wget[root@KVM ~]# wget -O /opt/iso/Centos7.iso https://mirrors.aliyun.com/centos/7.9.2009/isos/x86_64/CentOS-7-x86_64-Minimal-2009.iso--2023-01-08 08:09:00--  https://mirrors.aliyun.com/centos/7.9.2009/isos/x86_64/CentOS-7-x86_64-Minimal-2009.isoResolving mirrors.aliyun.com (mirrors.aliyun.com)... 120.241.238.243, 120.241.238.242, 120.241.238.241, ...Connecting to mirrors.aliyun.com (mirrors.aliyun.com)|120.241.238.243|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 1020264448 (973M) [application/x-cd-image]Saving to: ‘/opt/iso/Centos7.iso’/opt/iso/Centos7.iso     100%[================================&gt;] 973.00M  1.53MB/s    in 10m 54s2023-01-08 08:19:54 (1.49 MB/s) - ‘/opt/iso/Centos7.iso’ saved [1020264448/1020264448][root@KVM ~]# ll /opt/isototal 996352-rw-r--r--. 1 root root 1020264448 Nov  3  2020 Centos7.iso
也可以本地下载好后，上传至iso目录（本地方式）
下载连接：https://mirrors.aliyun.com/centos/7.9.2009/isos/x86_64/CentOS-7-x86_64-Minimal-2009.iso
[root@KVM ~]# ll /opt/isototal 996352-rw-r--r--. 1 root root 1020264448 Nov  3  2020 Centos7.iso
安装Centos7
无界面安装Centos7
[root@KVM ~]# virt-install --virt-type=kvm --name=Centos7 --vcpus=1 --memory=2048 --network bridge=br0,model=virtio --os-type=linux --os-variant=rhel7.7 --location=/opt/iso/Centos7.iso --disk /opt/disk/Centos7.qcow2,format=qcow2,size=10 --console=pty,target_type=serial --graphics=none --extra-args=&quot;console=tty0 console=ttyS0&quot;Starting install...Retrieving file vmlinuz...                                                 | 6.5 MB  00:00:00Retrieving file initrd.img...                                              |  53 MB  00:00:00Allocating &#x27;Centos7.qcow2&#x27;                                                 |  10 GB  00:00:00Running text console command: virsh --connect qemu:///system console Centos7Connected to domain &#x27;Centos7&#x27;......进入安装界面Connected to domain &#x27;Centos7&#x27;  Please make your choice from above [&#x27;q&#x27; to quit | &#x27;b&#x27; to begin installation |  &#x27;r&#x27; to refresh]:   Please make your choice from above [&#x27;q&#x27; to quit | &#x27;b&#x27; to begin installation |================================================================================================================================================================Installation 1) [x] Language settings                 2) [!] Time settings        (English (United States))                (Timezone is not set.) 3) [x] Installation source               4) [x] Software selection        (Local media)                            (Minimal Install) 5) [!] Installation Destination          6) [x] Kdump        (No disks selected)                      (Kdump is enabled) 7) [ ] Network configuration             8) [!] Root password        (Not connected)                          (Password is not set.) 9) [!] User creation        (No user will be created)  Please make your choice from above [&#x27;q&#x27; to quit | &#x27;b&#x27; to begin installation |  &#x27;r&#x27; to refresh]:   Please make your choice from above [&#x27;q&#x27; to quit | &#x27;b&#x27; to begin installation |
输入2设置时区
================================================================================================================================================================Time settingsTimezone: not setNTP servers:not configured 1)  Set timezone 2)  Configure NTP servers  Please make your choice from above [&#x27;q&#x27; to quit | &#x27;c&#x27; to continue |  &#x27;r&#x27; to refresh]:================================================================================================================================================================按1根据提示选择Asia/shanghai   然后按c
输入5设置磁盘（Installation Destination ）
Probing storage...Installation Destination[x] 1) : 10 GiB (vda)1 disk selected; 10 GiB capacity; 10 GiB free ...  Please make your choice from above [&#x27;q&#x27; to quit | &#x27;c&#x27; to continue |  &#x27;r&#x27; to refresh]:Autopartitioning Options再按c================================================================================[ ] 1) Replace Existing Linux system(s)[x] 2) Use All Space[ ] 3) Use Free SpaceInstallation requires partitioning of your hard drive. Select what space to usefor the install target.  Please make your choice from above [&#x27;q&#x27; to quit | &#x27;c&#x27; to continue |  再按c================================================================================Partition Scheme Options[ ] 1) Standard Partition[ ] 2) Btrfs[x] 3) LVM[ ] 4) LVM Thin ProvisioningSelect a partition scheme configuration.  Please make your choice from above [&#x27;q&#x27; to quit | &#x27;c&#x27; to continue |  &#x27;r&#x27; to refresh]: cGenerating updated storage configurationChecking storage configuration...
按8设置root密码
  &#x27;r&#x27; to refresh]: 8================================================================================        (Not connected) 9) [!] User creation================================================================================Please select new root password. You will have to type it twice.Password:Password (confirm):================================================================================================================================================================QuestionThe password you have provided is weak: The password fails the dictionary check- it is too simplistic/systematic.Would you like to use it anyway?Please respond &#x27;yes&#x27; or &#x27;no&#x27;: yes
全部设置完成后按b确认
  &#x27;r&#x27; to refresh]: b================================================================================================================================================================ProgressSetting up the installation environment.Creating disklabel on /dev/vda.Creating xfs on /dev/vda1.Creating lvmpv on /dev/vda2.Creating swap on /dev/mapper/centos-swap.Creating xfs on /dev/mapper/centos-root.Running pre-installation scripts.Starting package installation process就会开始安装
进入登入界面
CentOS Linux 7 (Core)Kernel 3.10.0-1160.el7.x86_64 on an x86_64localhost login:
管理kvm虚拟机
连接KVM虚拟机

[root@KVM ~]# virsh console Centos7Connected to domain &#x27;Centos7&#x27;Escape character is ^] (Ctrl + ]error: operation failed: Active console session exists for this domain如果你之前连接过，没有退出，但是你与宿主机的ssh断了，就会出现这种问题，也就是你之前的连接控制台的连接还存在；每次连接kvm之后要退出 ctrl+] 就可以退出解决办法：[root@KVM ~]# ps  -ef  |grep virshroot       65402   61750  0 02:26 pts/0    00:00:00 virsh --connect qemu:///system console Centos7root       68540   66569  0 02:35 pts/2    00:00:00 grep --color=auto virsh[root@KVM ~]# kill -9 61750[root@KVM ~]# kill -9 65402

查看KVM虚拟机状态
[root@KVM ~]# virsh list Id   Name      State------------------------- 2    Centos7   running
连接虚拟机
[root@KVM ~]# virsh console Centos7Connected to domain &#x27;Centos7&#x27;Escape character is ^] (Ctrl + ][root@localhost ~]#
初始化KVM虚拟机
设置名字
[root@localhost ~]# hostnamectl set-hostname Centos7 &amp;&amp; bash[root@centos7 ~]#
设置网络
查看状态
[root@centos7 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 52:54:00:b2:b9:d1 brd ff:ff:ff:ff:ff:ff
编辑配置文件
[root@centos7 ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=static         ---DEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=eth0UUID=b7053508-5183-4ee4-8af5-8d5241f57116DEVICE=eth0ONBOOT=yes                  ---IPADDR=192.168.48.11        ---PREFIX=24                   ---GATEWAY=192.168.48.2        ---DNS1=114.114.114.114        ---
查看KVM网络信息
[root@centos7 ~]# systemctl restart network[root@centos7 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 52:54:00:b2:b9:d1 brd ff:ff:ff:ff:ff:ff    inet 192.168.48.11/24 brd 192.168.48.255 scope global noprefixroute eth0       valid_lft forever preferred_lft forever    inet6 fe80::85d6:dde0:25ad:9c43/64 scope link noprefixroute       valid_lft forever preferred_lft forever[root@centos7 ~]#
测试一下ssh工具是不是也可以连（cmd、MobaXterm）
测试连通性
ping KVM机子[root@centos7 ~]# ping 192.168.48.10 -c 2PING 192.168.48.10 (192.168.48.10) 56(84) bytes of data.64 bytes from 192.168.48.10: icmp_seq=1 ttl=64 time=0.481 ms64 bytes from 192.168.48.10: icmp_seq=2 ttl=64 time=0.254 ms--- 192.168.48.10 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1000msrtt min/avg/max/mdev = 0.254/0.367/0.481/0.115 msping 百度[root@centos7 ~]# ping baidu.com -c 2PING baidu.com (39.156.66.10) 56(84) bytes of data.64 bytes from 39.156.66.10 (39.156.66.10): icmp_seq=1 ttl=128 time=43.5 ms64 bytes from 39.156.66.10 (39.156.66.10): icmp_seq=2 ttl=128 time=44.9 ms--- baidu.com ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1001msrtt min/avg/max/mdev = 43.599/44.258/44.917/0.659 ms[root@centos7 ~]#
设置Centos7虚拟机随KVM宿主机开机自启动
[root@KVM ~]# virsh autostart Centos7Domain &#x27;Centos7&#x27; marked as autostarted开机自启动配置后，会在/etc/libvirt/qemu/autostart/目录中增加XML格式[root@KVM ~]# ls /etc/libvirt/qemu/autostart/Centos7.xml
virsh autostart --disable 关闭开机自启动[root@KVM ~]# virsh autostart --disable Centos7Domain &#x27;Centos7&#x27; unmarked as autostarted
为KVM虚拟机增加CPU
查看Centos7配置信息
[root@KVM ~]# virsh dominfo Centos7Id:             1Name:           Centos7UUID:           735f1ca1-c82a-4734-81ec-ea3e82e98baeOS Type:        hvmState:          runningCPU(s):         1CPU time:       17.3sMax memory:     2097152 KiBUsed memory:    2097152 KiBPersistent:     yesAutostart:      enableManaged save:   noSecurity model: selinuxSecurity DOI:   0Security label: system_u:system_r:svirt_t:s0:c202,c740 (enforcing)
关闭虚拟机
[root@KVM ~]# virsh shutdown Centos7Domain &#x27;Centos7&#x27; is being shutdown[root@KVM ~]# virsh list --all Id   Name      State-------------------------- -    Centos7   shut off
修改Centos7 CPU核心数量
[root@KVM ~]# vi /etc/libvirt/qemu/Centos7.xml&lt;domain type=&#x27;kvm&#x27;&gt;  &lt;name&gt;Centos7&lt;/name&gt;  &lt;uuid&gt;735f1ca1-c82a-4734-81ec-ea3e82e98bae&lt;/uuid&gt;  &lt;metadata&gt;    &lt;libosinfo:libosinfo xmlns:libosinfo=&quot;http://libosinfo.org/xmlns/libvirt/domain/1.0&quot;&gt;      &lt;libosinfo:os id=&quot;http://redhat.com/rhel/7.7&quot;/&gt;    &lt;/libosinfo:libosinfo&gt;  &lt;/metadata&gt;  &lt;memory unit=&#x27;KiB&#x27;&gt;2097152&lt;/memory&gt;  &lt;currentMemory unit=&#x27;KiB&#x27;&gt;2097152&lt;/currentMemory&gt;  &lt;vcpu placement=&#x27;static&#x27;&gt;2&lt;/vcpu&gt;    修改为2  &lt;os&gt;    &lt;type arch=&#x27;x86_64&#x27; machine=&#x27;pc-q35-rhel8.6.0&#x27;&gt;hvm&lt;/type&gt;
启动虚拟机并查看配置文件
[root@KVM ~]# virsh start Centos7Domain &#x27;Centos7&#x27; started[root@KVM ~]# virsh dominfo Centos7Id:             1Name:           Centos7UUID:           735f1ca1-c82a-4734-81ec-ea3e82e98baeOS Type:        hvmState:          runningCPU(s):         2CPU time:       13.4sMax memory:     2097152 KiBUsed memory:    2097152 KiBPersistent:     yesAutostart:      enableManaged save:   noSecurity model: selinuxSecurity DOI:   0Security label: system_u:system_r:svirt_t:s0:c414,c924 (enforcing)[root@KVM ~]#
维护虚拟机
挂起/恢复虚拟机
[root@KVM ~]# virsh suspend Centos7Domain &#x27;Centos7&#x27; suspended[root@KVM ~]# virsh list --all Id   Name      State------------------------ 1    Centos7   paused[root@KVM ~]# virsh resume Centos7Domain &#x27;Centos7&#x27; resumed[root@KVM ~]# virsh list --all Id   Name      State------------------------- 1    Centos7   running
克隆KVM虚拟机
关闭Centos7 并且克隆Centos7为C7
[root@KVM ~]# virsh shutdown Centos7Domain &#x27;Centos7&#x27; is being shutdown[root@KVM ~]# virsh list --all Id   Name      State-------------------------- -    Centos7   shut off[root@KVM ~]# virt-clone -o Centos7 -n C7 -f /opt/disk/C7.qcow2Allocating &#x27;C7.qcow2&#x27;                                                      |  10 GB  00:00:37Clone &#x27;C7&#x27; created successfully.克隆成功[root@KVM ~]# virsh list --all Id   Name      State-------------------------- -    C7        shut off -    Centos7   shut off
开启C7，登入并且修改主机名为C7
[root@KVM ~]# virsh start C7Domain &#x27;C7&#x27; started[root@KVM ~]# virsh console C7Connected to domain &#x27;C7&#x27;Escape character is ^] (Ctrl + ]CentOS Linux 7 (Core)Kernel 3.10.0-1160.el7.x86_64 on an x86_64centos7 login: rootPassword:Last login: Mon Jan  9 15:56:16 from 192.168.48.250[root@centos7 ~]# hostnamectl set-hostname C7 &amp;&amp; bash[root@c7 ~]#克隆后ip回合Centos7一样，记得后期修改
设置Kvm虚拟机快照
登入C7 创建/opt/dev ，关闭虚拟机，创建快照
[root@c7 ~]# mkdir /opt/dev[root@c7 ~]# ls /optdev按ctrl + ] 退回到KVM宿主机[root@KVM ~]# virsh shutdown C7Domain &#x27;C7&#x27; is being shutdown创建快照[root@KVM ~]# virsh snapshot-create C7Domain snapshot 1673252946 created查看C7的快照列表[root@KVM ~]# virsh snapshot-list C7 Name         Creation Time               State--------------------------------------------------- 1673252946   2023-01-09 03:29:06 -0500   shutoff
开启C7 删除/opt/dev ，恢复快照
[root@KVM ~]# virsh start C7Domain &#x27;C7&#x27; started[root@KVM ~]# virsh console C7Connected to domain &#x27;C7&#x27;[root@c7 ~]# rm -rf /opt/dev[root@c7 ~]# ll /opttotal 0按ctrl + ] 退回到KVM宿主机[root@KVM ~]# virsh snapshot-list C7 Name         Creation Time               State--------------------------------------------------- 1673252946   2023-01-09 03:29:06 -0500   shutoff[root@KVM ~]# virsh snapshot-revert C7 1673252946virsh snapshot-revert 虚拟机名字 快照名字 
开启C7,查看是否恢复成功
[root@KVM ~]# virsh start C7Domain &#x27;C7&#x27; started[root@KVM ~]# virsh console C7Connected to domain &#x27;C7&#x27;Escape character is ^] (Ctrl + ]CentOS Linux 7 (Core)Kernel 3.10.0-1160.el7.x86_64 on an x86_64c7 login: rootPassword:Last login: Mon Jan  9 16:27:07 on ttyS0[root@c7 ~]# ls /optdev
virsh 命令
virsh shutdown 关闭KVM虚拟机 virsh destroy 强制关闭KVM虚拟机，不删除虚拟机磁盘，virsh list列表里面看不见，但是磁盘还在，还是可以启动，通过virsh list --all可以查看virsh undefine 彻底删除虚拟机，包括虚拟机存储所在的位置virsh start 开启KVM虚拟机virsh suspend 虚拟机名称 #挂起virsh resume 虚拟机名称 #恢复被挂起的

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Centos 8 stream</tag>
        <tag>KVM</tag>
      </tags>
  </entry>
  <entry>
    <title>K8S高可用集群（外部etcd）</title>
    <url>/posts/59349/</url>
    <content><![CDATA[
K8S高可用集群（外部etcd）
以下会用到的资源K8s1.28.2（内外部etcd高可用）所需资源
主机拓扑



主机名
ip1（NAT）
系统
磁盘
内存




master1
192.168.48.101
Centos7.9
100G
4G


master2
192.168.48.102
Centos7.9
100G
4G


master3
192.168.48.103
Centos7.9
100G
4G


node01
192.168.48.104
Centos7.9
100G
8G



基础配置
centos通过单独安装，非克隆。安装完后进行基本环境的配置，配置一下几个方面：

设置主机名
关闭NetworkManager、firewalld、dnsmasq、selinux
设置ens33
优化ssh
备份并新增清华yum源、epel源、docker-ce源、k8s源
更新yum源软件包缓存
修改history格式及记录数
添加hosts解析
关闭swap分区
安装chrony服务，并同步时间
配置limits.conf
安装必备工具
升级系统并重启

操作主机：[master1,master2,master3,node01]
vim k8s_system_init.sh
#!/bin/bashif [ $# -eq 2 ];then  echo &quot;设置主机名为：$1&quot;  echo &quot;ens33设置IP地址为：192.168.48.$2&quot;else  echo  &quot;使用方法：sh $0 主机名 主机位&quot;  exit 2fiecho &quot;--------------------------------------&quot;echo &quot;1.正在设置主机名：$1&quot;hostnamectl set-hostname $1echo &quot;2.正在关闭NetworkManager、firewalld、dnsmasq、selinux&quot;systemctl disable firewalld &amp;&gt; /dev/nullsystemctl disable NetworkManager &amp;&gt; /dev/nullsystemctl disable dnsmasq &amp;&gt; /dev/nullsystemctl stop firewalldsystemctl stop NetworkManagersystemctl stop dnsmasqsed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsetenforce 0echo &quot;3.正在设置ens33：192.168.48.$2&quot;cat &gt; /etc/sysconfig/network-scripts/ifcfg-ens33 &lt;&lt;EOFTYPE=ensernetBOOTPROTO=staticDEFROUTE=yesNAME=ens33DEVICE=ens33ONBOOT=yesIPADDR=192.168.48.$2NETMASK=255.255.255.0GATEWAY=192.168.48.2DNS1=114.114.114.114EOFsystemctl restart networkecho &quot;4.优化ssh&quot;sed -i &quot;s#\#UseDNS yes#UseDNS no#g&quot; /etc/ssh/sshd_configsed -i &quot;s#GSSAPIAuthentication yes#GSSAPIAuthentication no#g&quot; /etc/ssh/sshd_configsystemctl restart sshdecho &quot;5.备份并新增清华yum源、epel源、docker-ce源、k8s源&quot;rm -rf /etc/yum.repos.d/*cat &gt; /etc/yum.repos.d/Centos-Base.repo &lt;&lt;EOF[base]name=CentOS-$releasever - Basebaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/\$releasever/os/\$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[updates]name=CentOS-$releasever - Updatesbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/\$releasever/updates/\$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[extras]name=CentOS-$releasever - Extrasbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/\$releasever/extras/\$basearch/gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[centosplus]name=CentOS-$releasever - Plusbaseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/\$releasever/centosplus/\$basearch/gpgcheck=1enabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7EOFcat &gt; /etc/yum.repos.d/epel.repo &lt;&lt;EOF[epel]name=Extra Packages for Enterprise Linux 7 - \$basearchbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/\$basearchfailovermensod=priorityenabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7[epel-debuginfo]name=Extra Packages for Enterprise Linux 7 - \$basearch - Debugbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/\$basearch/debugfailovermensod=priorityenabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7gpgcheck=1[epel-source]name=Extra Packages for Enterprise Linux 7 - \$basearch - Sourcebaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/SRPMSfailovermensod=priorityenabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7gpgcheck=1EOFcat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFcurl -o /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoecho &quot;6.更新yum源软件包缓存&quot; yum clean all &amp;&amp; yum makecacheecho &quot;7.修改history格式及记录数&quot;sed -i &quot;s#HISTSIZE=1000##g&quot; /etc/profilecat &gt;&gt; /etc/profile &lt;&lt;EOFshopt -s histappendUSER_IP=`who -u am i 2&gt;/dev/null| awk &#x27;&#123;print $NF&#125;&#x27;|sed -e &#x27;s/[()]//g&#x27;`export HISTFILE=~/.commandline_warriorexport HISTTIMEFORMAT=&quot;%Y-%m-%d %H:%M:%S  `whoami`@$&#123;USER_IP&#125;: &quot;export HISTSIZE=200000export HISTFILESIZE=1000000export PROMPT_COMMAND=&quot;history -a&quot;EOFsource /etc/profileecho &quot;8.添加hosts解析&quot;cat &gt;&gt; /etc/hosts &lt;&lt;EOF192.168.48.101 master1192.168.48.102 master2192.168.48.103 master3192.168.48.104 node01EOFecho &quot;9.关闭swap分区&quot;swapoff -a &amp;&amp; sysctl -w vm.swappiness=0 &amp;&gt; /dev/nullsed -ri &#x27;/^[^#]*swap/s@^@#@&#x27; /etc/fstabecho &quot;10.安装ntpdate服务，并同步时间&quot;yum install chrony -ysystemctl start chronydsystemctl enable chronydchronyc sourceschronyc sourcesecho &quot;11.配置limits.conf&quot;ulimit -SHn 65535cat &gt;&gt; /etc/security/limits.conf &lt;&lt;EOF* soft nofile 65536* hard nofile 131072* soft nproc 65535* hard nproc 655350* soft memlock unlimited* hard memlock unlimitedEOFecho &quot;12.必备工具安装&quot;yum install wget psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git -yecho &quot;13.升级系统并重启&quot;yum update -y --exclude=kernel* &amp;&amp; reboot
sh k8s_system_init.sh 主机名 主机位[master1] sh k8s_system_init.sh master1 101[master2] sh k8s_system_init.sh master2 102[master3] sh k8s_system_init.sh master3 103[node01] sh k8s_system_init.sh node01 104
配置ssh免密
操作节点[master1]
yum install -y sshpass cat &gt; sshmianmi.sh &lt;&lt; &quot;EOF&quot;#!/bin/bash# 目标主机列表hosts=(&quot;master1&quot; &quot;master2&quot; &quot;master3&quot; &quot;node01&quot;)# 密码password=&quot;123456&quot;# 生成 SSH 密钥对ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa# 循环遍历目标主机for host in &quot;$&#123;hosts[@]&#125;&quot;do    # 复制公钥到目标主机    sshpass -p &quot;$password&quot; ssh-copy-id -o StrictHostKeyChecking=no &quot;$host&quot;        # 验证免密登录    sshpass -p &quot;$password&quot; ssh -o StrictHostKeyChecking=no &quot;$host&quot; &quot;echo &#x27;免密登录成功&#x27;&quot;doneEOFsh sshmianmi.sh
内核及ipvs模块配置
此步骤是升级内核、配置ipvs模块，开启一些k8s集群中必须的内核参数。配置一下几个方面：

下载安装包到/server/soft
安装kernel
更改内核启动顺序
安装ipvsadm
配置ipvs模块
开启k8s集群必须的内核参数
配置完内核，重启服务器

操作主机：[master1,master2,master3,node01]
vi kernel_update.sh#!/bin/bashecho &quot;1.下载安装包到/server/soft&quot;mkdir -p /server/soft ; cd /server/softwget https://mirrors.aliyun.com/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-6.6.3-1.el7.elrepo.x86_64.rpmwget https://mirrors.aliyun.com/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-6.6.3-1.el7.elrepo.x86_64.rpmecho &quot;2.正在安装kernel&quot;yum localinstall -y kernel-ml*echo &quot;3.更改内核启动顺序&quot;grub2-set-default  0 &amp;&amp; grub2-mkconfig -o /etc/grub2.cfggrubby --args=&quot;user_namespace.enable=1&quot; --update-kernel=&quot;$(grubby --default-kernel)&quot;echo &quot;4.输出现在内核版本信息&quot;grubby --default-kernelecho &quot;5.安装ipvsadm&quot;yum install ipvsadm ipset sysstat conntrack libseccomp -y &amp;&gt; /dev/nullecho &quot;6.配置ipvs模块&quot;modprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrackcat &gt;&gt; /etc/modules-load.d/ipvs.conf &lt;&lt;EOFip_vsip_vs_lcip_vs_wlcip_vs_rrip_vs_wrrip_vs_lblcip_vs_lblcrip_vs_dhip_vs_ship_vs_foip_vs_nqip_vs_sedip_vs_ftpip_vs_shnf_conntrackip_tablesip_setxt_setipt_setipt_rpfilteript_REJECTipipEOFsystemctl enable --now systemd-modules-load.service &amp;&gt; /dev/nullecho &quot;7.开启k8s集群必须的内核参数&quot;cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1fs.may_detach_mounts = 1net.ipv4.conf.all.route_localnet = 1vm.overcommit_memory=1vm.panic_on_oom=0fs.inotify.max_user_watches=89100fs.file-max=52706963fs.nr_open=52706963net.netfilter.nf_conntrack_max=2310720net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_probes = 3net.ipv4.tcp_keepalive_intvl =15net.ipv4.tcp_max_tw_buckets = 36000net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_max_orphans = 327680net.ipv4.tcp_orphan_retries = 3net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.ip_conntrack_max = 65536net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.tcp_timestamps = 0net.core.somaxconn = 16384EOFsysctl --systemecho &quot;8.配置完内核，重启服务器！&quot;reboot
sh kernel_update.sh
检查ipvs加载、内核版本验证
lsmod | grep --color=auto -e ip_vs -e nf_conntrackuname -a

部署ETCD集群
本次在master1、master2、master3上进行etcd集群部署
安装etcd
下载安装包
wget https://github.com/etcd-io/etcd/releases/download/v3.5.10/etcd-v3.5.10-linux-amd64.tar.gz --no-check-certificate

解压
tar xf etcd-v3.5.10-linux-amd64.tar.gzmv etcd-v3.5.10-linux-amd64 /tmp/etcdcp /tmp/etcd/etcd* /usr/local/bin/


添加环境变量
将文件夹中etcd和etcdctl两个文件添加到环境变量中
mkdir -p /var/lib/etcd/mkdir -p /etc/etcd/chmod 700 /var/lib/etcd

创建默认配置文件
cat &lt;&lt;EOF | sudo tee /etc/etcd/etcd.conf#节点名称ETCD_NAME=$(hostname -s)#数据存放位置ETCD_DATA_DIR=/var/lib/etcdEOF

创建etcd服务
cat &lt;&lt;EOF | sudo tee /etc/systemd/system/etcd.service [Unit]Description=Etcd ServerDocumentation=https://github.com/coreos/etcdAfter=network.target [Service]User=rootType=notifyEnvironmentFile=-/etc/etcd/etcd.confExecStart=/usr/local/bin/etcdRestart=on-failureRestartSec=10sLimitNOFILE=40000 [Install]WantedBy=multi-user.targetEOF

开启服务
systemctl daemon-reload &amp;&amp; systemctl enable etcd &amp;&amp; systemctl start etcd

查看版本信息
etcd -version

在master1节点上生成etcd配置文件
vim etcd_install.shetcd1=192.168.48.101etcd2=192.168.48.102etcd3=192.168.48.103TOKEN=smartgoETCDHOSTS=($etcd1 $etcd2 $etcd3)NAMES=(&quot;master1&quot; &quot;master2&quot; &quot;master3&quot;)for i in &quot;$&#123;!ETCDHOSTS[@]&#125;&quot;; doHOST=$&#123;ETCDHOSTS[$i]&#125;NAME=$&#123;NAMES[$i]&#125;cat &lt;&lt; EOF &gt; /tmp/$NAME.conf# [member]ETCD_NAME=$NAMEETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;http://$HOST:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;http://$HOST:2379,http://127.0.0.1:2379&quot;#[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;http://$HOST:2380&quot;ETCD_INITIAL_CLUSTER=&quot;$&#123;NAMES[0]&#125;=http://$&#123;ETCDHOSTS[0]&#125;:2380,$&#123;NAMES[1]&#125;=http://$&#123;ETCDHOSTS[1]&#125;:2380,$&#123;NAMES[2]&#125;=http://$&#123;ETCDHOSTS[2]&#125;:2380&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;$TOKEN&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;http://$HOST:2379&quot;EOFdonels /tmp/master*scp /tmp/master2.conf $etcd2:/etc/etcd/etcd.confscp /tmp/master3.conf $etcd3:/etc/etcd/etcd.confcp /tmp/master1.conf /etc/etcd/etcd.confrm -f /tmp/master*.conf

sh etcd_install.sh

在k8s集群master节点上启动etcd
systemctl restart etcdsystemctl enable --now etcd

检查etcd集群是否正常
etcdctl member listetcdctl endpoint health



高可用组件安装
haproxy配置
操作节点：[master1，master2,master3]
yum install keepalived haproxy -y
所有Master节点配置HAProxy，所有Master节点的HAProxy配置相同。
操作节点：[master1，master2]cat &gt; /etc/haproxy/haproxy.cfg &lt;&lt;&quot;EOF&quot;global  maxconn  2000  ulimit-n  16384  log  127.0.0.1 local0 err  stats timeout 30sdefaults  log global  mode  http  option  httplog  timeout connect 5000  timeout client  50000  timeout server  50000  timeout http-request 15s  timeout http-keep-alive 15sfrontend monitor-in  bind *:33305  mode http  option httplog  monitor-uri /monitorfrontend k8s-master  bind 0.0.0.0:16443  bind 127.0.0.1:16443  mode tcp  option tcplog  tcp-request inspect-delay 5s  default_backend k8s-masterbackend k8s-master  mode tcp  option tcplog  option tcp-check  balance roundrobin  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100  server master1   192.168.48.101:6443  check  server master2   192.168.48.102:6443  check  server master3   192.168.48.103:6443  checkEOF
Keepalived配置
操作节点：[master1，master2,master3]
所有Master节点配置Keepalived，以下三个Master节点配置注意ip和网卡。
操作节点：[master1]cat &gt;/etc/keepalived/keepalived.conf  &lt;&lt;&quot;EOF&quot;! Configuration File for keepalivedglobal_defs &#123;    router_id LVS_DEVELscript_user root    enable_script_security&#125;vrrp_script chk_apiserver &#123;    script &quot;/etc/keepalived/check_apiserver.sh&quot;    interval 5    weight -5    fall 2  rise 1&#125;vrrp_instance VI_1 &#123;    state MASTER    interface ens33    mcast_src_ip 192.168.48.101    virtual_router_id 51    priority 101    advert_int 2    authentication &#123;        auth_type PASS        auth_pass K8SHA_KA_AUTH    &#125;    virtual_ipaddress &#123;        192.168.48.200    &#125;    track_script &#123;       chk_apiserver    &#125;&#125;EOF
操作节点：[master2]cat &gt;/etc/keepalived/keepalived.conf  &lt;&lt;&quot;EOF&quot;! Configuration File for keepalivedglobal_defs &#123;    router_id LVS_DEVELscript_user root    enable_script_security&#125;vrrp_script chk_apiserver &#123;    script &quot;/etc/keepalived/check_apiserver.sh&quot;    interval 5    weight -5    fall 2  rise 1&#125;vrrp_instance VI_1 &#123;    state BACKUP    interface ens33    mcast_src_ip 192.168.48.102    virtual_router_id 51    priority 100    advert_int 2    authentication &#123;        auth_type PASS        auth_pass K8SHA_KA_AUTH    &#125;    virtual_ipaddress &#123;        192.168.48.200    &#125;    track_script &#123;       chk_apiserver    &#125;&#125;EOF
操作节点：[master3]cat &gt;/etc/keepalived/keepalived.conf  &lt;&lt;&quot;EOF&quot;! Configuration File for keepalivedglobal_defs &#123;    router_id LVS_DEVELscript_user root    enable_script_security&#125;vrrp_script chk_apiserver &#123;    script &quot;/etc/keepalived/check_apiserver.sh&quot;    interval 5    weight -5    fall 2  rise 1&#125;vrrp_instance VI_1 &#123;    state BACKUP    interface ens33    mcast_src_ip 192.168.48.103    virtual_router_id 51    priority 99    advert_int 2    authentication &#123;        auth_type PASS        auth_pass K8SHA_KA_AUTH    &#125;    virtual_ipaddress &#123;        192.168.48.200    &#125;    track_script &#123;       chk_apiserver    &#125;&#125;EOF
配置Keepalived健康检查文件
操作节点：[master1，master2,master3]
cat &gt; /etc/keepalived/check_apiserver.sh &lt;&lt;&quot;EOF&quot; #!/bin/bash err=0 for k in $(seq 1 3) do    check_code=$(pgrep haproxy)    if [[ $check_code == &quot;&quot; ]]; then        err=$(expr $err + 1)        sleep 1        continue    else        err=0        break    fi done  if [[ $err != &quot;0&quot; ]]; then    echo &quot;systemctl stop keepalived&quot;    /usr/bin/systemctl stop keepalived    exit 1 else    exit 0 fiEOFchmod +x /etc/keepalived/check_apiserver.sh
启动haproxy和keepalived
操作节点：[master，master2,master3]systemctl daemon-reloadsystemctl enable --now haproxysystemctl enable --now keepalived
测试集群负载均衡高可用
查看master1的vip
ip a

模拟master1的宕机测试，看看vip会不会漂移到master2去
[master1] poweroff

这时候查看master2的ip列表
[master2] ip a

结论：这时可以知道，负载均衡集群成功，当master1出现宕机情况，vip会从master1漂移到master2
docker安装
安装docker
操作节点[master1，master2，master3,node01]
wget https://download.docker.com/linux/static/stable/x86_64/docker-24.0.7.tgztar xf docker-*.tgzcp -rf docker/* /usr/bin/#创建containerd的service文件,并且启动cat &gt;/etc/systemd/system/containerd.service &lt;&lt;EOF[Unit]Description=containerd container runtimeDocumentation=https://containerd.ioAfter=network.target local-fs.target[Service]ExecStartPre=-/sbin/modprobe overlayExecStart=/usr/bin/containerdType=notifyDelegate=yesKillMode=processRestart=alwaysRestartSec=5LimitNPROC=infinityLimitCORE=infinityLimitNOFILE=1048576TasksMax=infinityOOMScoreAdjust=-999[Install]WantedBy=multi-user.targetEOFsystemctl enable --now containerd.service#准备docker的service文件cat &gt; /etc/systemd/system/docker.service &lt;&lt;EOF[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.service containerd.serviceWants=network-online.targetRequires=docker.socket containerd.service[Service]Type=notifyExecStart=/usr/bin/dockerd --config-file=/etc/docker/daemon.json -H fd://containerd=/run/containerd/containerd.sockExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=alwaysStartLimitBurst=3StartLimitInterval=60sLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTasksMax=infinityDelegate=yesKillMode=processOOMScoreAdjust=-500[Install]WantedBy=multi-user.targetEOF#准备docker的socket文件cat &gt; /etc/systemd/system/docker.socket &lt;&lt;EOF[Unit]Description=Docker Socket for the API[Socket]ListenStream=/var/run/docker.sockSocketMode=0660SocketUser=rootSocketGroup=docker[Install]WantedBy=sockets.targetEOFgroupadd dockersystemctl enable --now docker.socket  &amp;&amp; systemctl enable --now docker.service#验证mkdir /etc/dockersudo tee /etc/docker/daemon.json &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;&#123;  &quot;registry-mirrors&quot;: [    &quot;https://docker.xuanyuan.me&quot;,    &quot;https://docker.m.daocloud.io&quot;,    &quot;https://docker.1ms.run&quot;,    &quot;https://docker.1panel.live&quot;,    &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,		&quot;https://docker.qianyios.top&quot;  ],  &quot;max-concurrent-downloads&quot;: 10,  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-level&quot;: &quot;warn&quot;,  &quot;log-opts&quot;: &#123;    &quot;max-size&quot;: &quot;10m&quot;,    &quot;max-file&quot;: &quot;3&quot;  &#125;,  &quot;data-root&quot;: &quot;/var/lib/docker&quot;&#125;EOFsystemctl daemon-reloadsystemctl restart docker
安装cri-docker
操作节点[master1，master2，master3,node01]
wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.7/cri-dockerd-0.3.7.amd64.tgztar -zxvf cri-dockerd-0.3.7.amd64.tgzcp cri-dockerd/cri-dockerd  /usr/bin/chmod +x /usr/bin/cri-dockerd#写入启动配置文件cat &gt;  /usr/lib/systemd/system/cri-docker.service &lt;&lt;EOF[Unit]Description=CRI Interface for Docker Application Container EngineDocumentation=https://docs.mirantis.comAfter=network-online.target firewalld.service docker.serviceWants=network-online.targetRequires=cri-docker.socket [Service]Type=notifyExecStart=/usr/bin/cri-dockerd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9ExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=always StartLimitBurst=3 StartLimitInterval=60s LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity TasksMax=infinityDelegate=yesKillMode=process [Install]WantedBy=multi-user.targetEOF#写入socket配置文件cat &gt; /usr/lib/systemd/system/cri-docker.socket &lt;&lt;EOF[Unit]Description=CRI Docker Socket for the APIPartOf=cri-docker.service [Socket]ListenStream=%t/cri-dockerd.sockSocketMode=0660SocketUser=rootSocketGroup=docker [Install]WantedBy=sockets.targetEOFsystemctl daemon-reload &amp;&amp; systemctl enable cri-docker --now
K8S集群安装
安装k8s所需的工具
操作节点[master1，master2，master3,node01]yum -y install  kubeadm kubelet kubectl#为了实现docker使用的cgroupdriver与kubelet使用的cgroup的一致性，配置如下：sed -i &#x27;s/^KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=&quot;--cgroup-driver=systemd&quot;/g&#x27; /etc/sysconfig/kubelet#设置kubelet为开机自启动即可，由于没有生成配置文件，集群初始化后自动启动systemctl enable kubeletsystemctl enable kubelet.service
初始化集群
cat &gt; kubeadm-config.yaml &lt;&lt; EOF---apiVersion: kubeadm.k8s.io/v1beta3bootstrapTokens:- groups:  - system:bootstrappers:kubeadm:default-node-token  token: abcdef.0123456789abcdef  ttl: 24h0m0s  usages:  - signing  - authenticationkind: InitConfigurationlocalAPIEndpoint:  advertiseAddress: 192.168.48.101  bindPort: 6443nodeRegistration:  criSocket: unix:///var/run/cri-dockerd.sock---apiVersion: kubeadm.k8s.io/v1beta3kind: ClusterConfigurationkubernetesVersion: 1.28.2imageRepository: registry.aliyuncs.com/google_containersnetworking:  dnsDomain: cluster.local  podSubnet: 10.244.0.0/16  serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125;apiServerCertSANs:- 192.168.48.200controlPlaneEndpoint: &quot;192.168.48.200:16443&quot;etcd:  external:    endpoints:      - http://192.168.48.101:2379      - http://192.168.48.102:2379      - http://192.168.48.103:2379---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationfeatureGates:  # SupportIPVSProxyMode: falsemode: ipvs---apiVersion: kubelet.config.k8s.io/v1beta1authentication:  anonymous:    enabled: false  webhook:    cacheTTL: 0s    enabled: true  x509:    clientCAFile: /etc/kubernetes/pki/ca.crtauthorization:  mode: Webhook  webhook:    cacheAuthorizedTTL: 0s    cacheUnauthorizedTTL: 0scgroupDriver: systemdclusterDNS:- 10.96.0.10clusterDomain: cluster.localcpuManagerReconcilePeriod: 0sevictionPressureTransitionPeriod: 0sfileCheckFrequency: 0shealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 0simageMinimumGCAge: 0skind: KubeletConfigurationlogging:  flushFrequency: 0  options:    json:      infoBufferSize: &quot;0&quot;  verbosity: 0memorySwap: &#123;&#125;nodeStatusReportFrequency: 0snodeStatusUpdateFrequency: 0srotateCertificates: trueruntimeRequestTimeout: 0sshutdownGracePeriod: 0sshutdownGracePeriodCriticalPods: 0sstaticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 0ssyncFrequency: 0svolumeStatsAggPeriod: 0sEOF
准备k8s所需的镜像
操作节点[master1]kubeadm config images pull --config kubeadm-config.yaml

master1节点初始化
操作节点[master1]
kubeadm init --config kubeadm-config.yaml --upload-certs --v=9
会生成信息

记录信息后面会用到
初始化成功以后，会产生Token值，用于其他节点加入时使用，因此要记录下初始化成功生成的token值（令牌值），有效期24小时，后续需要操作可以重新生成Token
操作节点[master1]
kubeadm join 192.168.48.200:16443 --token abcdef.0123456789abcdef \        --discovery-token-ca-cert-hash sha256:bc27d821932d27bb4b49c187ddd412569ead01b14009e03b6d03aa3ea54e4612 \        --control-plane --certificate-key 37b1370abd3584c354517603bf97e10c795e3367e05e5ff3f2f6ca14288d8054kubeadm join 192.168.48.200:16443 --token abcdef.0123456789abcdef \        --discovery-token-ca-cert-hash sha256:bc27d821932d27bb4b49c187ddd412569ead01b14009e03b6d03aa3ea54e4612
操作kubect报错：

此时通过kubectl操作，会出现失败，因为还没有将集群的&quot;钥匙&quot;交给root用户。/etc/kubernetes/admin.conf 文件是 Kubernetes（K8s）集群中的管理员配置文件，它包含了用于管理集群的身份验证和访问信息。所以下面进行配置环境变量，用于访问Kubernetes集群：
添加环境变量
操作节点[master1]
mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config
添加其他master节点至集群中
操作节点[master2,master3]
操作节点[master2,master3]kubeadm join 192.168.48.200:16443 --token abcdef.0123456789abcdef \        --discovery-token-ca-cert-hash sha256:bc27d821932d27bb4b49c187ddd412569ead01b14009e03b6d03aa3ea54e4612 \        --control-plane --certificate-key 37b1370abd3584c354517603bf97e10c795e3367e05e5ff3f2f6ca14288d8054 \        --cri-socket unix:///var/run/cri-dockerd.sock 
注意：这里末尾添加了--cri-socket unix:///var/run/cri-dockerd.sock 
接着给master2添加环境变量
操作节点[master2,master3]mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config

这里没有展示master3的图片，但是步骤一样的
模拟Token过期重新生成并加入Node节点
假设加入集群的token过期了。node01无法加入了，这里就模拟一下这种情况

Token过期后生成新的token：

kubeadm token create --print-join-command
[root@master1 ~]# kubeadm token create --print-join-commandkubeadm join 192.168.48.200:16443 --token ke773y.6hv9utk33to4vwfy --discovery-token-ca-cert-hash sha256:bc27d821932d27bb4b49c187ddd412569ead01b14009e03b6d03aa3ea54e4612[root@master1 ~]#
其中，192.168.48.200:16443 是你的 Kubernetes API 服务器的地址和端口，ke773y.6hv9utk33to4vwfy 是新的令牌，sha256:bc27d821932d27bb4b49c187ddd412569ead01b14009e03b6d03aa3ea54e4612 是令牌的 CA 证书哈希值。

Master需要生成–certificate-key：

kubeadm init phase upload-certs --upload-certs
[root@master1 ~]# kubeadm init phase upload-certs --upload-certsW1110 18:12:03.426245   10185 version.go:104] could not fetch a Kubernetes version from the internet: unable to get URL &quot;https://dl.k8s.io/release/stable-1.txt&quot;: Get &quot;https://dl.k8s.io/release/stable-1.txt&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)W1110 18:12:03.426320   10185 version.go:105] falling back to the local client version: v1.28.2[upload-certs] Storing the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace[upload-certs] Using certificate key:5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128
其中，5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128 是证书密钥。

生成新的Token用于集群添加新Node节点

操作节点[node01]
kubeadm join 192.168.48.200:16443 \        --token ke773y.6hv9utk33to4vwfy \        --discovery-token-ca-cert-hash sha256:bc27d821932d27bb4b49c187ddd412569ead01b14009e03b6d03aa3ea54e4612  \        --cri-socket unix:///var/run/cri-dockerd.sock 
注意：这里末尾添加了--cri-socket unix:///var/run/cri-dockerd.sock 

这时在master查看node状态（显示为notready不影响）

模拟新加master节点的加入K8S集群中
假设我们新加master节点的话，就拼接token，从刚刚生成的token拼接
[root@master1 ~]# kubeadm token create --print-join-commandkubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f
这里提取信息1
kubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f
接着
[root@master1 ~]# kubeadm init phase upload-certs --upload-certsW1110 18:12:03.426245   10185 version.go:104] could not fetch a Kubernetes version from the internet: unable to get URL &quot;https://dl.k8s.io/release/stable-1.txt&quot;: Get &quot;https://dl.k8s.io/release/stable-1.txt&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)W1110 18:12:03.426320   10185 version.go:105] falling back to the local client version: v1.28.2[upload-certs] Storing the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace[upload-certs] Using certificate key:5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128
这里提取信息2：这里前面要加上--control-plane --certificate-key
--control-plane --certificate-key 5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128
合成
kubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in \        --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f  \        --control-plane --certificate-key 5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128 \        --cri-socket unix:///var/run/cri-dockerd.sock                kubeadm join 192.168.48.200:16443 --token lnkno8.u4v1l8n9pahzf0kj \        --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f \        --control-plane --certificate-key 41e441fe56cb4bdfcc2fc0291958e6b1da54d01f4649b6651471c07583f85cdf \        --cri-socket unix:///var/run/cri-dockerd.sock
注意：这里末尾添加了--cri-socket unix:///var/run/cri-dockerd.sock 
图示

安装calico网络插件
操作节点[master1]
添加解析记录，否则无法访问
echo &#x27;185.199.108.133 raw.githubusercontent.com&#x27; &gt;&gt; /etc/hosts
应用operator资源清单文件
网络组件有很多种，只需要部署其中一个即可，推荐Calico。
Calico是一个纯三层的数据中心网络方案，Calico支持广泛的平台，包括Kubernetes、OpenStack等。
Calico 在每一个计算节点利用 Linux Kernel 实现了一个高效的虚拟路由器（ vRouter） 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息向整个 Calico 网络内传播。
此外，Calico 项目还实现了 Kubernetes 网络策略，提供ACL功能。
curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/calico.yaml -O
[root@master1 ~]# vim calico.yaml#添加两行- name: WAIT_FOR_DATASTORE  value: &quot;true&quot;- name: IP_AUTODETECTION_METHOD  value: interface=ens33#ens33是你的网卡

sed -i &#x27;s| docker.io/calico/| registry.cn-hangzhou.aliyuncs.com/qianyios/|&#x27; calico.yamlkubectl apply -f calico.yaml

监视kube-system命名空间中pod运行情况
等待估计20分钟左右吧(确保全部running)
kubectl get pods -n kube-system

拿掉master节点的污点
节点 master1 和 master2 都有一个名为 node-role.kubernetes.io/control-plane:NoSchedule 的污点。这个污点的作用是阻止普通的 Pod 被调度到这些节点上，只允许特定的控制平面组件（如 kube-apiserver、kube-controller-manager 和 kube-scheduler）在这些节点上运行。
这种设置有助于确保控制平面节点专门用于运行 Kubernetes 的核心组件，而不会被普通的工作负载占用。通过将污点添加到节点上，可以确保只有被授权的控制平面组件才能在这些节点上运行。
kubectl describe node master1 | grep -i taintkubectl describe node master2 | grep -i taintkubectl describe node master3 | grep -i taint

去除污点
kubectl taint node master1 node-role.kubernetes.io/control-plane:NoSchedule-kubectl taint node master2 node-role.kubernetes.io/control-plane:NoSchedule-kubectl taint node master3 node-role.kubernetes.io/control-plane:NoSchedule-

安装dashboard
操作节点[master1]
下载文件
https://github.com/kubernetes/dashboard/releases/tag/v2.7.0
目前最新版本v2.7.0
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yamlsed -i &#x27;s/kubernetesui\/dashboard:v2.7.0/registry.cn-hangzhou.aliyuncs.com\/qianyios\/dashboard:v2.7.0/g&#x27; recommended.yamlsed -i &#x27;s/kubernetesui\/metrics-scraper:v1.0.8/registry.cn-hangzhou.aliyuncs.com\/qianyios\/metrics-scraper:v1.0.8/g&#x27; recommended.yaml

修改配置文件
vim recommended.yaml---kind: ServiceapiVersion: v1metadata:  labels:    app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboardspec:  ports:    - port: 443      targetPort: 8443      nodePort: 30001  type: NodePort  selector:    app: kubernetes-dashboard---

运行dashboard
kubectl apply -f recommended.yaml

检查运行状态
kubectl get pods -n kubernetes-dashboardkubectl get pod,svc -o wide -n kubernetes-dashboard

创建cluster-admin用户
创建service account并绑定默认cluster-admin管理员群角色#创建用户kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard#用户授权kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin#获取用户Tokenkubectl create token dashboard-admin -n kubernetes-dashboard

记录token
eyJhbGciOiJSUzI1NiIsImtpZCI6IjJxb21sRTZSckhEZ09FMnlvWU5IY3dfTTRIWDEzRUpsQ000MThhSWxYNDgifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjk5NjQ2Mjg3LCJpYXQiOjE2OTk2NDI2ODcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJkYXNoYm9hcmQtYWRtaW4iLCJ1aWQiOiIyNTBhNTQ2MS03MGJlLTRhZTItOWY2Yi1hMDQwOWE1NWJhMTMifX0sIm5iZiI6MTY5OTY0MjY4Nywic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.U-lH9_sRU4TVrpAznqS60INNSev9NxHu61igGUTzTBoaWo7WjPG7vzTnpZYhvsoglTzMEKhfranJkkkn95pe-prkvSasaAL6kXHw0jQjlSMzcYiF7DoLdkOtDJSukuALEubidf9eIwZXFZ-sezZdZHm4hnk5nWme5YtdOmYOJPh5sv1dzRvM1XuOHknJPTA1BbdZuVAtSGHSjkhwx-wl-41uuQoROW5GjJs0bz4zLBFn1w_pWaSMCn7pjGJNcbr6IuDV41km_etpwwxacWyfAcxNykzCtIiE1abJj7m-e944GvAn_eqxz3wCZD6Bgt41FWRzyMHjrppJfDjk7FaHNw
登录浏览器访问
https://192.168.48.200:30001输入token：----eyJhbGciOiJSUzI1NiIsImtpZCI6IjJxb21sRTZSckhEZ09FMnlvWU5IY3dfTTRIWDEzRUpsQ000MThhSWxYNDgifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjk5NjQ2Mjg3LCJpYXQiOjE2OTk2NDI2ODcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJkYXNoYm9hcmQtYWRtaW4iLCJ1aWQiOiIyNTBhNTQ2MS03MGJlLTRhZTItOWY2Yi1hMDQwOWE1NWJhMTMifX0sIm5iZiI6MTY5OTY0MjY4Nywic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.U-lH9_sRU4TVrpAznqS60INNSev9NxHu61igGUTzTBoaWo7WjPG7vzTnpZYhvsoglTzMEKhfranJkkkn95pe-prkvSasaAL6kXHw0jQjlSMzcYiF7DoLdkOtDJSukuALEubidf9eIwZXFZ-sezZdZHm4hnk5nWme5YtdOmYOJPh5sv1dzRvM1XuOHknJPTA1BbdZuVAtSGHSjkhwx-wl-41uuQoROW5GjJs0bz4zLBFn1w_pWaSMCn7pjGJNcbr6IuDV41km_etpwwxacWyfAcxNykzCtIiE1abJj7m-e944GvAn_eqxz3wCZD6Bgt41FWRzyMHjrppJfDjk7FaHNw----

部署一个nginx测试
操作节点[master1]
vim web.yamlkind: Deployment#apiVersion: extensions/v1beta1apiVersion: apps/v1metadata:  labels:    app: web-deployment-label  name: web-deployment  namespace: defaultspec:  replicas: 3  selector:    matchLabels:      app: web-selector  template:    metadata:      labels:        app: web-selector    spec:      containers:      - name: web-container        image: nginx:latest        imagePullPolicy: Always        ports:        - containerPort: 80          protocol: TCP          name: http        - containerPort: 443          protocol: TCP          name: https---kind: ServiceapiVersion: v1metadata:  labels:    app: web-service-label  name: web-service  namespace: defaultspec:  type: NodePort  ports:  - name: http    port: 80    protocol: TCP    targetPort: 80    nodePort: 30080  - name: https    port: 443    protocol: TCP    targetPort: 443    nodePort: 30443  selector:    app: web-selector    kubectl apply -f web.yaml 

### 查看nginx的pod 的详细信息kubectl get deploy,svc,pod -o wide

访问nginx网站
http://192.168.48.200:30080


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>Centos 7</tag>
        <tag>K8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux安装Mysql8.4.2 LTS</title>
    <url>/posts/32ce47c9/</url>
    <content><![CDATA[
Linux安装Mysql8.4.2 LTS
下载安装包
下载地址：MySQL
根据自己的系统选择哦！
我的系统是OpenEuler22.03LTS兼容红帽安装包

下载之后上传到root目录下

创建安装目录并解压安装包到此目录下
mkdir mysql8.4.2ltstar -xvf mysql-8.4.2-1.el8.x86_64.rpm-bundle.tar -C mysql8.4.2ltscd mysql8.4.2lts/ls
已经解压完成了哦

开始安装
删除旧版本的mysql和mariadb
# 卸载 mariadb 相关的包yum remove mariadb mariadb-config mariadb-libs -y# 如果之前安装过 MySQL 社区版，也需要一并移除yum remove mysql-community-common mysql-community-icu-data-files mysql-community-client-plugins mysql-community-libs mysql-community-client mysql-community-server mysql-community-libs-compat -y
最好按照以下顺序按照，不然会报错
#全局的依赖（common）rpm -ivh mysql-community-common-8.4.2-1.el8.x86_64.rpmrpm -ivh mysql-community-icu-data-files-8.4.2-1.el8.x86_64.rpmrpm -ivh mysql-community-client-plugins-8.4.2-1.el8.x86_64.rpmrpm -ivh mysql-community-libs-8.4.2-1.el8.x86_64.rpmrpm -ivh mysql-community-client-8.4.2-1.el8.x86_64.rpmrpm -ivh mysql-community-server-8.4.2-1.el8.x86_64.rpm#php依赖文件rpm -ivh mysql-community-libs-compat-8.4.2-1.el8.x86_64.rpm
注意：如果需要搭载php使用，需要安装7.7，因为rpm -ivh mysql-community-libs-compat是php的依赖。；如果不安装php，则无需安装7.7的依赖。
数据库基础配置
启动数据库并且设置自启动
systemctl enable mysqld --now
查看启动进程
ps -ef | grep mysql

查看初始密码
grep &#x27;temporary password&#x27; /var/log/mysqld.log
2024-09-08T06:44:47.204524Z 6 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: zo+&amp;q(qYG21X密码是zo+&amp;q(qYG21X
修改密码
mysql -uroot -p&quot;zo+&amp;q(qYG21X&quot;alter user &#x27;root&#x27;@&#x27;localhost&#x27; identified by &#x27;Qianyios007@&#x27;;
以后你就可以用以下目录登入mysql了
mysql -uroot -p&quot;Qianyios007@&quot;

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>OpenEuler</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux配置DNS服务</title>
    <url>/posts/b29b8cd3/</url>
    <content><![CDATA[
Linux配置DNS服务
DNS 简介
- 什么是域名

域名(DomainName)，简称域名、网域，是由一串用点分隔的名字组成的Internet上某一台计算机或计算机组的名称，用于在数据传输时标识计算机的电子方位。具有独一无二，不可重复的特性。

- 域名的关系和组成
常见域名：www.baidu.com
完整域名：www.baidu.com.
注意com 后面有一点
. ：根域，可省略不写。
com：顶级域，由ICANN组织指定和管理。
分类：1、国家地区域名: (cn(中国)、hk(香港)、sg (新加坡)等。2、通用项级域名: com (商业机构)、org (非营利组织)、edu (教育机构)等。3、新通用顶级域名: red (红色、热情)、top (顶级、高端)等。4、com.cn属于“二级域名”，是cn项级域的子域。
baidu：级域(注册域) ，可由·个人或组织申请注册。
www：三级域(子域)，服务器网站名代表。（www.baidu.com）

- 什么是DNS?
域名系统(Domain Name System,缩写: DNS)是互联网的一项服务。域名解析是把域名指向网站空间IP,让人们通过注册的域名可以方便地访问到网站的一种服务。IP地址是网络上标识站点的数字地址，为了方便记忆，采用域名来代替IP地址标识站点地址。域名解析就是域名到IP地址的转换过程。域名的解析工作由DNS服务器完成。可以理解为DNS就是翻译官。
正向解析：域名 --&gt; IP地址。
反向解析：IP地址 --&gt; 域名。
DNS 工作过程


客户端在浏览器输入一个域名：www.baidu.com，浏览器自动补充域名：www.baidu.com：80。80端口是web服务器的端口

1.在从自己本机中查询host文件，是否有此域名的解析记录，如果有则返回给浏览器
2.如果host文件没有域名的解析记录，则会在本机上继续查询是否有DNS的解析缓存，如果有则返回给浏览器
3.如果本机没有DNS的解析记录，则会在网卡设置的DNS服务器上，查询域名的解析结果
4.如果DNS服务器上也没有查询到，则会从别人询问的结果的缓存中查找
5.就迭代查询，顶级域名，二级域名，三级域名
DNS 配置文件

/ etc / named.conf ：主配置文件



/ etc / named.rfc1912.zones：区域配置文件



/ var / named / ：数据配置文件

named.ca：记录了13台根域服务器的位置
named.localhost：正向代理
named.loopback：反向代理

serial：主DNS每次改完zone必须加1，触发slave来同步；可填：纯数字、YYYYMMDDnn、Unix时间戳。
refresh：slave隔多久去查一次serial是否更新；可填：秒数或带单位的时间（如3600、1H、1D）。
retry：refresh失败后隔多久再试；可填：同上（如1800、30M）。
expire：多久拉不到数据就把zone视为失效；可填：同上（如604800、1W）。
minimum：否定/无记录结果在缓存里的最短生存时间；可填：同上（如10800、3H）。



类型
描述




A
地址记录，用来指定域名的 IPv4 地址的记录。


CNAME
将域名指向另一个域名，再由另一个域名提供 ip 地址，就需要添加 CNAME 记录。


TXT
可填写任何东西，长度限制 255。绝大多数的 TXT 记录是用来做 SPF 的（反垃圾邮件）。


NS
域名服务器记录，如果需要把子域名交给其他 DNS 服务商解析，就需要添加 NS 记录。


AAAA
地址记录，用来指定域名的 IPv6 地址的记录。


MX
邮件交换记录，如果需要设置邮箱，让邮箱能收到邮件，就需要添加 MX 记录。


PTR
PoinTeR，IP --&gt; FQDN




软件名称：bind
服务名称：named
软件端口：


UDP 53 数据通信(域名解析)
TCP 53 数据同步(主从同步)

DNS 服务搭建
配置DNS地址：/etc/resolv.conf



主机名
ip
内存
硬盘
cpu
OS




master
192.168.48.101
2g
100g
2v
Centos7


slave
192.168.48.102
2g
100g
2v
Centos7


client
192.168.48.103
2g
100g
2v
Centos7




前情提要：以下是三台机的网卡dns配置,自行设置
matser: DNS1=192.168.48.101
slave: DNS1=192.168.48.102
client: DNS1=192.168.48.101 , DNS2=192.168.48.102

服务端配置
一、安装
yum install -y bind bind-utils
二、配置文件
操作节点：[master]
1.配置主文件 vim /etc/named.conf
// listen-on port 53 &#123; 127.0.0.1; &#125;;// allow-query     &#123; localhost; &#125;;
listen-on port 53 &#123; 192.168.48.101; &#125;;
→ 只在 192.168.48.101 这台机器的 53 端口上提供 DNS 服务。如若注释掉：BIND 不再限定监听地址，会在服务器所有可用 IP 的 53 端口上提供 DNS 服务。这样就可以在多ip地址下进行dns了
allow-query &#123; any; &#125;;
→ 允许任何来源向这台 DNS 服务器发起查询请求。如果不注释掉，只要网络可达（没有防火墙、路由等限制），任何 IP 地址（公网、私网、本机），这些设备把 DNS 指向你的服务器 IP 都可以向这台 DNS 服务器发起查询，如果注释掉了，他只会允许这台机同网段下局域网内的机子可以向这台dns服务器发起查询
这次实验主要是实现企业内网dns，所以这两项我都注释掉
这次实验主要是实现企业内网dns，所以这两项我都注释掉
这次实验主要是实现企业内网dns，所以这两项我都注释掉
2、配置区域文件 vim /etc/named.rfc1912.zones
cat &gt;&gt; /etc/named.rfc1912.zones &lt;&lt;&quot;EOF2&quot;zone &quot;aaa.com&quot; IN &#123;        type master;        file &quot;aaa.com.zone&quot;;&#125;;zone &quot;48.168.192.in-addr.arpa&quot; IN &#123;        type master;        file &quot;aaa.loopback&quot;;&#125;;EOF2
3、编辑正向解析数据文件 vim /var/named/aaa.com.zone
cat &gt; /var/named/aaa.com.zone &lt;&lt;&quot;EOF&quot;$TTL 1D@       IN SOA  aaa.com. admin@qianyios.top. ( 0 1D 1H 1W 3H )        NS      mastermaster  A       192.168.48.101www     A       192.168.48.128EOF
@：代表本域名的根域（zone 本身）；
IN：记录类别为Internet（标准 TCP/IP）；
SOA：这是一条起始授权记录（Start of Authority）；
aaa.com.：该域的**主 DNS 服务器（primary nameserver）**主机名；（只是描述信息）
admin@qianyios.top.：管理员邮箱地址（把第一个点换成 @），如果不需要则用 .invalid 表示示例/无效地址。（只是描述信息）
NS  master：声明本域的权威 DNS 服务器名字叫 master（相对名，实际全名是 master.aaa.com.）。
master A  192.168.48.101：给NS的名字为 master 绑定真实 IP 192.168.48.101，让别人能解析到这台权威服务器。

检查zone文件语法用：
[root@master ~]# named-checkzone aaa.com /var/named/aaa.com.zone
zone aaa.com/IN: loaded serial 0
OK

4、编辑反向解析数据文件 vim /var/named/aaa.loopback
举例了两条反向记录
cat &gt; /var/named/aaa.loopback &lt;&lt;&quot;EOF&quot;$TTL 1D@       IN SOA  aaa.com. admin@qianyios.top. ( 0 1D 1H 1W 3H )        NS      master.aaa.com.129     PTR     www1.aaa.com.128     PTR     www.aaa.com.EOF
NS master.aaa.com.:在正向zone不是配置了NS  master吗，那反向这里就要写全称域名后面要加小数点
5.启动named服务
systemctl enable --now named systemctl restart named
重启无报错
客户端配置
操作节点：[client]
一、安装
yum install -y bind bind-utils
二、配置网卡dns
这是我的网卡名称是ens33你自己按你自己的改
vim /etc/sysconfig/network-scripts/ifcfg-ens33
DNS1=192.168.48.101

重启网卡
systemctl restart network
查看是否配置成功
[root@client ~]# cat /etc/resolv.conf# Generated by NetworkManagernameserver 192.168.48.101
三、 测试 nslookup

正向解析测试

[root@client ~]# nslookup www.aaa.comServer:         192.168.48.101Address:        192.168.48.101#53Name:   www.aaa.comAddress: 192.168.48.128  #自己查看正向解析文件

反向解析测试

[root@client ~]# host 192.168.48.128 &amp;&amp; host 192.168.48.129128.48.168.192.in-addr.arpa domain name pointer www.aaa.com.129.48.168.192.in-addr.arpa domain name pointer www1.aaa.com.#自行查看反向解析文件
成功
主从DNS服务器搭建
减轻主服务器的压力，数据从 主服务器上复制到 从服务器上
主服务器配置
操作节点：[master]
1.配置主文件 vim /etc/named.conf
listen-on port 53 &#123; 192.168.48.101; &#125;; //masterIPallow-query     &#123; any; &#125;;
listen-on port 53 &#123; 192.168.48.101; &#125;;
→ 只在 192.168.48.101 这台机器的 53 端口上提供 DNS 服务。如若注释掉：BIND 不再限定监听地址，会在服务器所有可用 IP 的 53 端口上提供 DNS 服务。这样就可以在多ip地址下进行dns了
allow-query &#123; any; &#125;;
→ 允许任何来源向这台 DNS 服务器发起查询请求。如果不注释掉，只要网络可达（没有防火墙、路由等限制），任何 IP 地址（公网、私网、本机），这些设备把 DNS 指向你的服务器 IP 都可以向这台 DNS 服务器发起查询，如果注释掉了，他只会允许这台机同网段下局域网内的机子可以向这台dns服务器发起查询
这次实验主要是实现企业内网dns，所以这两项我都注释掉
这次实验主要是实现企业内网dns，所以这两项我都注释掉
这次实验主要是实现企业内网dns，所以这两项我都注释掉
2、配置区域文件 vim /etc/named.rfc1912.zones
allow-update（可选）：只允许 192.168.48.102 这台主机向本 DNS 服务器发送动态更新
allow-update &#123; any; &#125;;含义：允许任何主机向本 DNS 服务器提交动态更新请求，即谁都可以增删改该区域的资源记录。安全风险极大，生产环境务必改成 &#123; none; &#125; 或只列出可信 IP/密钥，否则任何人都能篡改你的域名解析。
zone &quot;aaa.com&quot; IN &#123;        type master;        file &quot;aaa.com.zone&quot;;        allow-update &#123; none; &#125;;&#125;;zone &quot;48.168.192.in-addr.arpa&quot; IN &#123;        type master;        file &quot;aaa.loopback&quot;;        allow-update &#123; none; &#125;;&#125;;
3、编辑正向解析数据文件 vim /var/named/aaa.com.zone
cat &gt; /var/named/aaa.com.zone &lt;&lt;&quot;EOF&quot;$TTL 1D@       IN SOA  aaa.com. admin@qianyios.top. ( 0 1D 1H 1W 3H )        NS      mastermaster  A       192.168.48.101www     A       192.168.48.128EOF
4、编辑反向解析数据文件 vim /var/named/aaa.loopback
cat &gt; /var/named/aaa.loopback &lt;&lt;&quot;EOF&quot;$TTL 1D@       IN SOA  aaa.com. admin@qianyios.top. ( 0 1D 1H 1W 3H )        NS      master.aaa.com.129     PTR     www1.aaa.com.128     PTR     www.aaa.com.EOF
从服务器配置
操作节点：[slave]
#安装yum install -y bind bind-utils
1.配置主文件 vim /etc/named.conf
#注释掉// listen-on port 53 &#123; 127.0.0.1; &#125;;// allow-query     &#123; localhost; &#125;;
2、配置区域文件 vim /etc/named.rfc1912.zones
cat &gt;&gt; /etc/named.rfc1912.zones &lt;&lt; &quot;EOF&quot;zone &quot;aaa.com&quot; IN &#123;        type slave;        file &quot;slaves/aaa.com.zone&quot;;        masters &#123; 192.168.48.101; &#125;; //主服务器的IP&#125;;zone &quot;48.168.192.in-addr.arpa&quot; IN &#123;        type slave;        file &quot;slaves/aaa.loopback&quot;;        masters &#123; 192.168.48.101; &#125;; //主服务器的IP&#125;;EOF

如果主从同步之后区域数据文件会同步到/var/named/slaves这个目录下，现在还是空的
[root@slave ~]# ll /var/named/slavestotal 0[root@slave ~]#
两台重启named服务
操作节点：[master,slave]
systemctl restart named
这时候经过两台服务器重启named之后，master里的两个文件就已经同步到slave的/var/named/slaves这个目录下

客户端配置并测试正反向解析
操作节点：[client]
设置dns添加slave的IP地址
DNS2=192.168.48.102 然后重启网卡
修改网卡dns添加一个从服务器的ip
vim /etc/sysconfig/network-scripts/ifcfg-ens33

重启网卡
systemctl restart network
#检查是否配置成功[root@client ~]# cat /etc/resolv.conf# Generated by NetworkManagernameserver 192.168.48.101nameserver 192.168.48.102
测试主从服务
当我们给master模拟named服务故障时，由从服务器进行接管
测试前，是由master提供服务
[root@client ~]# nslookup www.aaa.comServer:         192.168.48.101Address:        192.168.48.101#53Name:   www.aaa.comAddress: 192.168.48.128[root@client ~]# host 192.168.48.128 &amp;&amp; host 192.168.48.129128.48.168.192.in-addr.arpa domain name pointer www.aaa.com.129.48.168.192.in-addr.arpa domain name pointer www1.aaa.com.
断开master的named的服务
[root@master ~]# systemctl stop named[root@master ~]#
再次进行测试
nslookup www.aaa.comhost 192.168.48.128 &amp;&amp; host 192.168.48.129

这时重启master的named，客户机又从主服务器查询域名了
systemctl start named
[root@client ~]# host 192.168.48.128 &amp;&amp; host 192.168.48.129128.48.168.192.in-addr.arpa domain name pointer www.aaa.com.129.48.168.192.in-addr.arpa domain name pointer www1.aaa.com.[root@client ~]# nslookup www.aaa.comServer:         192.168.48.101Address:        192.168.48.101#53Name:   www.aaa.comAddress: 192.168.48.128
自动主从同步
我们已经配置了主从服务器了，如果这时主服务器的区域数据文件中又添加了新的解析条目，怎么实现从服务器也能自动同步这个数据呢？很简单
操作节点：[matser]
1.配置正向区域文件
cat &gt; /var/named/aaa.com.zone &lt;&lt;&quot;EOF&quot;$TTL 1D                                   @       IN SOA  aaa.com. admin@qianyios.top. ( 1 1D 1H 1W 3H )        NS      master        NS      slavemaster  A       192.168.48.101slave   A       192.168.48.102    ;增加slave解析记录www     A       192.168.48.128abc     A       192.168.48.130EOF
( 1 1D 1H 1W 3H );第一个数字，原先是0要增加到1，就是说dns要确定这个文件已经更新过了，靠这个数字来判断的，递增一次就是更新一次了，才可以进行同步
2.配置反向解析文件
cat &gt; /var/named/aaa.loopback &lt;&lt;&quot;EOF&quot;$TTL 1D@       IN SOA  aaa.com. admin@qianyios.top. ( 1 1D 1H 1W 3H )        NS      master.aaa.com.        NS      slave.aaa.com.129     PTR     www1.aaa.com.128     PTR     www.aaa.com.130     PTR     abc.aaa.com.EOF
( 1 1D 1H 1W 3H );第一个数字，原先是0要增加到1，就是说dns要确定这个文件已经更新过了，靠这个数字来判断的，递增一次就是更新一次了，才可以进行同步

如果说你不想加NS   slave.aaa.com.
你就要在vim /etc/named.rfc1912.zones里配置该域名下的添加
notify yes;,also-notify &#123; 192.168.48.102; &#125;

notify yes;
主 DNS 变更后自动给 NS 列表里的辅 DNS 发“快更新”通知。
also-notify &#123; 192.168.48.102; &#125;
在“自动推导的 NS 列表”之外，额外再通知一个/一批 IP。就是说如果你没加NS   slave.aaa.com.，那么他会额外再通知192.168.48.102;

如下图


3、测试
测试前查看slave数据的文件时间

重启master的named的服务
systemctl restart named

这时候已经更新了，再次在客户端进行测试
[root@client ~]# nslookup abc.aaa.comServer:         192.168.48.101Address:        192.168.48.101#53Name:   abc.aaa.comAddress: 192.168.48.130[root@client ~]# host 192.168.48.130130.48.168.192.in-addr.arpa domain name pointer abc.aaa.com.
都能成功进行解析，主从自动同步成功
失败案例
我有测试没有修改序号就添加数据顺便重启，是没有同步成功的
序号为1，没增加1，本身应该是2的
cat &gt; /var/named/aaa.loopback &lt;&lt;&quot;EOF&quot;$TTL 1D@       IN SOA  aaa.com. admin@qianyios.top. ( 1 1D 1H 1W 3H )        NS      master.aaa.com.        NS      slave.aaa.com.129     PTR     www1.aaa.com.128     PTR     www.aaa.com.130     PTR     abc.aaa.com.131     PTR     qy.aaa.com.EOFcat &gt; /var/named/aaa.com.zone &lt;&lt;&quot;EOF&quot;$TTL 1D@       IN SOA  aaa.com. admin@qianyios.top. ( 1 1D 1H 1W 3H )        NS      master        NS      slavemaster  A       192.168.48.101slave   A       192.168.48.102www     A       192.168.48.128abc     A       192.168.48.130qy      A       192.168.48.131EOF
重启之前查看时间
[root@slave ~]# ll /var/named/slaves/total 8-rw-r--r-- 1 named named 329 Jul 22 08:46 aaa.com.zone-rw-r--r-- 1 named named 393 Jul 22 08:46 aaa.loopback
重启之后
[root@master ~]# systemctl restart named[root@master ~]#
slave下的文件还是和原来一样
[root@slave ~]# ll /var/named/slaves/total 8-rw-r--r-- 1 named named 329 Jul 22 08:46 aaa.com.zone-rw-r--r-- 1 named named 393 Jul 22 08:46 aaa.loopback
并且客户端解析不到我新添加的数据
[root@client ~]# host 192.168.48.131131.48.168.192.in-addr.arpa domain name pointer qy.aaa.com.[root@client ~]# nslookup qy.aaa.comServer:         192.168.48.101Address:        192.168.48.101#53Name:   qy.aaa.comAddress: 192.168.48.131#好奇怪这不是解析到了吗，因为dns里不是设置了两个dns吗，我们删掉master的dns，重启网卡[root@client ~]# cat /etc/resolv.conf# Generated by NetworkManagernameserver 192.168.48.102#再次进行解析[root@client ~]# host 192.168.48.131Host 131.48.168.192.in-addr.arpa. not found: 3(NXDOMAIN)[root@client ~]# nslookup qy.aaa.comServer:         192.168.48.102Address:        192.168.48.102#53** server can&#x27;t find qy.aaa.com: NXDOMAIN#就说明192.168.48.102并没有实现同步
只有当我修改序号为2时，再进行重启，才能进行同步
cat &gt; /var/named/aaa.loopback &lt;&lt;&quot;EOF&quot;$TTL 1D@       IN SOA  aaa.com. admin@qianyios.top. ( 2 1D 1H 1W 3H )        NS      master.aaa.com.        NS      slave.aaa.com.129     PTR     www1.aaa.com.128     PTR     www.aaa.com.130     PTR     abc.aaa.com.131     PTR     qy.aaa.com.EOFcat &gt; /var/named/aaa.com.zone &lt;&lt;&quot;EOF&quot;$TTL 1D@       IN SOA  aaa.com. admin@qianyios.top. ( 2 1D 1H 1W 3H )        NS      master        NS      slavemaster  A       192.168.48.101slave   A       192.168.48.102www     A       192.168.48.128abc     A       192.168.48.130qy      A       192.168.48.131EOFsystemctl	restart named
slave查看是否更新
[root@slave ~]# ll /var/named/slaves/total 8-rw-r--r-- 1 named named 367 Jul 22 08:56 aaa.com.zone-rw-r--r-- 1 named named 456 Jul 22 08:56 aaa.loopback#之前是8:46的，更新之后就是8:56
client查看解析
[root@client ~]# host 192.168.48.131131.48.168.192.in-addr.arpa domain name pointer qy.aaa.com.[root@client ~]# nslookup qy.aaa.comServer:         192.168.48.102    #slave成功解析Address:        192.168.48.102#53Name:   qy.aaa.comAddress: 192.168.48.131
ssh免密
此步骤是为了方便以下自动脚本
操作节点：【三台服务器】
这是我的ip和主机名，你根据需要自行修改
cat &gt;&gt; /etc/hosts &lt;&lt; &quot;EOF&quot;192.168.48.101 master192.168.48.102 slave192.168.48.103 clientEOF
操作节点：【master】

脚本里password=&quot;123456&quot;是我三台机的密码，我三台都一样，如何你是不一样，你自己自行进行手动免密了
[master] yum install -y sshpass
[master] ssh-keygen -t rsa -N “” -f ~/.ssh/id_rsa
以下这个命令的主机名，三个主机名都要输一遍，也就是三条命令了
[master] sshpass -p “密码” ssh-copy-id -o StrictHostKeyChecking=no “主机名”
测试免密，三个主机名都要输一遍，也就是三条命令了
[master] sshpass -p “密码” ssh -o StrictHostKeyChecking=no “主机名” “echo ‘免密登录成功’”

yum install -y sshpass cat &gt; sshmianmi.sh &lt;&lt; &quot;EOF&quot;#!/bin/bash# 目标主机列表hosts=(&quot;master&quot; &quot;slave&quot; &quot;client&quot;)# 密码password=&quot;123456&quot;# 生成 SSH 密钥对ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa# 循环遍历目标主机for host in &quot;$&#123;hosts[@]&#125;&quot;do    # 复制公钥到目标主机    sshpass -p &quot;$password&quot; ssh-copy-id -o StrictHostKeyChecking=no &quot;$host&quot;        # 验证免密登录    sshpass -p &quot;$password&quot; ssh -o StrictHostKeyChecking=no &quot;$host&quot; &quot;echo &#x27;免密登录成功&#x27;&quot;doneEOFsh sshmianmi.sh
自动添加解析脚本
你要做的就是修改脚本中的SLAVE_IP=&quot;192.168.48.102&quot;这是我的从节点的ip
你要把你的主从节点做好免密之后，才可以进行以下操作
操作节点:[master]
vim jx.sh
#!/bin/bash# 检查参数数量if [ &quot;$#&quot; -ne 2 ]; then    echo &quot;Usage: $0 &lt;IP_ADDRESS&gt; &lt;DOMAIN&gt;&quot;    exit 1fiIP=&quot;$1&quot;DOMAIN=&quot;$2&quot;# 获取本机 IP 地址（假设主机只有一个有效 IP）MASTER_IP=$(hostname -I | awk &#x27;&#123;print $1&#125;&#x27;)# 从服务器的 IP 地址SLAVE_IP=&quot;192.168.48.102&quot;# 解析域名和反向 IPBASE_DOMAIN=$(echo &quot;$DOMAIN&quot; | awk -F. &#x27;&#123;print $(NF-1)&quot;.&quot;$NF&#125;&#x27;)ZONE_FILE=&quot;/var/named/$BASE_DOMAIN.zone&quot;REV_ZONE_FILE=&quot;/var/named/$(echo $IP | awk -F. &#x27;&#123;print $3&quot;.&quot;$2&quot;.&quot;$1&quot;.in-addr.arpa&quot;&#125;&#x27;).zone&quot;ZONE_CONFIG=&quot;/etc/named.rfc1912.zones&quot;# 解析反向 IP 和主机名REV_IP=$(echo $IP | awk -F. &#x27;&#123;print $3&quot;.&quot;$2&quot;.&quot;$1&#125;&#x27;)REV_HOST=$(echo $IP | awk -F. &#x27;&#123;print $4&#125;&#x27;)SHORT_NAME=$(echo &quot;$DOMAIN&quot; | awk -F. &#x27;&#123;print $1&#125;&#x27;)# 确保 /var/named 目录存在mkdir -p /var/named/touch &quot;$ZONE_FILE&quot; &quot;$REV_ZONE_FILE&quot;chown named:named &quot;$ZONE_FILE&quot; &quot;$REV_ZONE_FILE&quot;chmod 644 &quot;$ZONE_FILE&quot; &quot;$REV_ZONE_FILE&quot;# 检查是否已存在相同的解析记录if grep -q &quot;^$SHORT_NAME[[:space:]]*A[[:space:]]*$IP&quot; &quot;$ZONE_FILE&quot;; then    echo &quot;记录 $DOMAIN -&gt; $IP 已存在，无需添加&quot;    exit 0fi# **递增 serial 号**increment_serial() &#123;    # 提取当前 serial 值    current_serial=$(awk &#x27;/serial/&#123;print $1&#125;&#x27; &quot;$1&quot;)        if [[ -z &quot;$current_serial&quot; ]]; then        # 如果没有找到 serial（空），初始化为 1        current_serial=1    fi    # 递增 serial    new_serial=$((current_serial + 1))    # 替换 serial 行，保留前导空格和注释    sed -i &quot;s/\([[:space:]]*\)\([0-9]\+\)\([[:space:]]*; serial\)/\1$new_serial\3/&quot; &quot;$1&quot;&#125;# 添加正向解析区域配置（如果不存在）if ! grep -q &quot;zone \&quot;$BASE_DOMAIN\&quot; IN&quot; &quot;$ZONE_CONFIG&quot;; then    cat &gt;&gt; &quot;$ZONE_CONFIG&quot; &lt;&lt;EOFzone &quot;$BASE_DOMAIN&quot; IN &#123;        type master;        file &quot;$ZONE_FILE&quot;;        notify yes;        also-notify &#123; $SLAVE_IP; &#125;;        allow-update &#123; $SLAVE_IP; &#125;;&#125;;EOFfi# 添加反向解析区域配置（如果不存在）if ! grep -q &quot;zone \&quot;$REV_IP.in-addr.arpa\&quot; IN&quot; &quot;$ZONE_CONFIG&quot;; then    cat &gt;&gt; &quot;$ZONE_CONFIG&quot; &lt;&lt;EOFzone &quot;$REV_IP.in-addr.arpa&quot; IN &#123;        type master;        file &quot;$REV_ZONE_FILE&quot;;        notify yes;        also-notify &#123; $SLAVE_IP; &#125;;        allow-update &#123; $SLAVE_IP; &#125;;&#125;;EOFfi# **初始化 Zone 文件（正向解析）**if [ ! -s &quot;$ZONE_FILE&quot; ]; then    cat &gt; &quot;$ZONE_FILE&quot; &lt;&lt;EOF\$TTL 1D@       IN SOA  $BASE_DOMAIN rname.invalid. (                                        1       ; serial                                        1D      ; refresh                                        1H      ; retry                                        1W      ; expire                                        3H )    ; minimum        NS      $BASE_DOMAIN$BASE_DOMAIN     A   $MASTER_IPEOF    increment_serial &quot;$ZONE_FILE&quot;  # 初始化时递增 serialfi# **追加 A 记录（使用短名）**if ! grep -q &quot;^$SHORT_NAME[[:space:]]*A[[:space:]]*$IP&quot; &quot;$ZONE_FILE&quot;; then    echo &quot;$SHORT_NAME    A       $IP&quot; &gt;&gt; &quot;$ZONE_FILE&quot;    increment_serial &quot;$ZONE_FILE&quot;  # 追加记录后递增 serialfi# **初始化 Zone 文件（反向解析）**if [ ! -s &quot;$REV_ZONE_FILE&quot; ]; then    cat &gt; &quot;$REV_ZONE_FILE&quot; &lt;&lt;EOF\$TTL 1D@       IN SOA  $BASE_DOMAIN rname.invalid. (                                        1       ; serial                                        1D      ; refresh                                        1H      ; retry                                        1W      ; expire                                        3H )    ; minimum        NS      $BASE_DOMAIN$BASE_DOMAIN     A   $MASTER_IP        EOF    increment_serial &quot;$REV_ZONE_FILE&quot;  # 初始化时递增 serialfi# **追加 PTR 记录**if ! grep -q &quot;^$REV_HOST[[:space:]]*PTR[[:space:]]*$SHORT_NAME.$BASE_DOMAIN.&quot; &quot;$REV_ZONE_FILE&quot;; then    echo &quot;$REV_HOST     PTR     $SHORT_NAME.$BASE_DOMAIN.&quot; &gt;&gt; &quot;$REV_ZONE_FILE&quot;    increment_serial &quot;$REV_ZONE_FILE&quot;  # 追加记录后递增 serialfi# 在从服务器上追加主从同步配置ssh -T root@$SLAVE_IP &lt;&lt;EOF# 检查从服务器配置文件是否已存在主从同步区域if ! grep -q &quot;zone \&quot;$BASE_DOMAIN\&quot; IN&quot; &quot;$ZONE_CONFIG&quot;; then    cat &gt;&gt; &quot;$ZONE_CONFIG&quot; &lt;&lt;EOTzone &quot;$BASE_DOMAIN&quot; IN &#123;     type slave;    file &quot;slaves/$BASE_DOMAIN.zone&quot;;     masters &#123; $MASTER_IP; &#125;; # 主服务器的IP&#125;;EOTfi# 检查反向解析的从服务器配置if ! grep -q &quot;zone \&quot;$REV_IP.in-addr.arpa\&quot; IN&quot; &quot;$ZONE_CONFIG&quot;; then    cat &gt;&gt; &quot;$ZONE_CONFIG&quot; &lt;&lt;EOTzone &quot;$REV_IP.in-addr.arpa&quot; IN &#123;     type slave;    file &quot;slaves/$REV_IP.loopback&quot;;    masters &#123; $MASTER_IP; &#125;; # 主服务器的IP&#125;;EOTfiEOF# 重新加载主服务器的 Bind 配置systemctl restart namedecho &quot;✅ DNS 记录已添加并重新加载 Bind 服务&quot;# 重新加载从服务器上的 Bind 配置ssh -T root@$SLAVE_IP &lt;&lt;EOFsystemctl restart namedecho &quot;✅ 从服务器 $SLAVE_IP 配置已更新并重新加载 Bind 服务&quot;EOFecho &quot;✅ 主从同步配置已成功添加到从服务器 $SLAVE_IP&quot;systemctl restart named
bash jx.sh 192.168.111.201 bs.qianyios12.topbash jx.sh 192.168.123.202 bs1.qianyios1245.topbash jx.sh 192.168.236.203 bs3.qianyios22224.top

接下来进行测试，三台机都可以进行测试
nslookup bs.qianyios12.topnslookup bs1.qianyios1245.topnslookup bs3.qianyios22224.tophost 192.168.111.201host 192.168.123.202host 192.168.236.203
master

slave

client


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>OpenEuler</tag>
        <tag>Linux</tag>
        <tag>Dns</tag>
      </tags>
  </entry>
  <entry>
    <title>Lsky Pro图床</title>
    <url>/posts/6726/</url>
    <content><![CDATA[
Lsky Pro图床
系统：centos 8 stream  ip：192.168.48.11
介绍

支持本地等多种第三方云储存 AWS S3、阿里云 OSS、腾讯云 COS、七牛云、又拍云、SFTP、FTP、WebDav、Minio
多种数据库驱动支持，MySQL 5.7+、PostgreSQL 9.6+、SQLite 3.8.8+、SQL Server 2017+
支持配置使用多种缓存驱动，Memcached、Redis、DynamoDB、等其他关系型数据库，默认以文件的方式缓存
多图上传、拖拽上传、粘贴上传、动态设置策略上传、复制、一键复制链接
强大的图片管理功能，瀑布流展示，支持鼠标右键、单选多选、重命名等操作
自由度极高的角色组配置，可以为每个组配置多个储存策略，同时储存策略可以配置多个角色组
可针对角色组设置上传文件、文件夹路径命名规则、上传频率限制、图片审核等功能
支持图片水印、文字水印、水印平铺、设置水印位置、X/y 轴偏移量设置、旋转角度等
支持通过接口上传、管理图片、管理相册
支持在线增量更新、跨版本更新
图片广场

要求

PHP &gt;= 8.0.2
BCMath PHP 扩展
Ctype PHP 扩展
DOM PHP 拓展
Fileinfo PHP 扩展
JSON PHP 扩展
Mbstring PHP 扩展
OpenSSL PHP 扩展
PDO PHP 扩展
Tokenizer PHP 扩展
XML PHP 扩展
Imagick 拓展
exec、shell_exec 函数
readlink、symlink 函数
putenv、getenv 函数
chmod、chown、fileperms 函数

安装Lsky Pro
下载Lsky Pro项目文件
项目地址:Releases · lsky-org/lsky-pro (github.com)
wget https://github.com/lsky-org/lsky-pro/releases/download/2.1/lsky-pro-2.1.zipunzip lsky-pro-2.1.zip -d /var/www/html/lskychmod 755 -R /var/www/html/lskychown -R nginx:nginx /var/www/html/lsky
安装nginx
sudo dnf upgrade --refresh -ysudo dnf install \   https://dl.fedoraproject.org/pub/epel/8/Everything/x86_64/Packages/e/epel-release-8-19.el8.noarch.rpm \   https://dl.fedoraproject.org/pub/epel/8/Everything/x86_64/Packages/e/epel-next-release-8-19.el8.noarch.rpm \   http://rpms.remirepo.net/enterprise/remi-release-8.8.rpm \   dnf-utils -ydnf module list | grep php# 删除 PHPsudo dnf -y remove php php-fpm   # 删除相关扩展包sudo dnf -y remove php*   # 重置 PHP 模块列表sudo dnf -y module reset php启用（安装）PHP 8.0sudo dnf -y module enable php:remi-8.0sudo dnf -y install php php-fpmsudo dnf install php-cli php-fpm php-curl php-mysqlnd php-gd php-opcache php-zip php-intl php-common php-bcmath php-imagick php-xmlrpc php-json php-readline php-memcached php-redis php-mbstring php-apcu php-xml php-dom php-redis php-memcached php-memcache php-devel php-ctype php-fileinfo  php-openssl php-pdo php-tokenizer -ysed -i &#x27;s/user = apache/user = nginx/; s/group = apache/group = nginx/&#x27; /etc/php-fpm.d/www.confdnf install -y http://nginx.org/packages/rhel/8/x86_64/RPMS/nginx-1.24.0-1.el8.ngx.x86_64.rpmsystemctl	enable --now nginxsudo systemctl restart php-fpm.service
安装mysql
sudo dnf install mysql-server mysql -ysystemctl enable --now mysqld#mysql初始化sudo mysql_secure_installation根据自身需要设置mysql -uroot -p
设置兰空页面
[root@localhost ~]# cat /etc/nginx/conf.d/default.confserver &#123;    listen       80;    server_name  localhost;    root          /var/www/html/lsky/public/;    index        index.php;    location / &#123;      try_files $uri $uri/ /index.php?$query_string;    &#125;    location ~ \.php$ &#123;      root           /var/www/html/lsky/public/;      fastcgi_pass   unix:/run/php-fpm/www.sock;      fastcgi_index  index.php;      fastcgi_param  SCRIPT_FILENAME $document_root$fastcgi_script_name;      include        fastcgi_params;    &#125;&#125;#重启nginxnginx -s reload
访问页面192.168.48.11


利用postman获取token
官网：Postman API Platform
自己注册一个账号，并且下载桌面版的postman（如果你的项目部署在服务器，可以不用下载，直接从第二步开始）
软件下载：Download Postman | Get Started for Free
下载软件并双击运行


打开页面


6|0kKPF29VXPv2Kv6lEqu1chyxHjTpjiiEdwwkRE0T
实现typro自动上传
Releases · ygxbnet/lsky-upload (github.com)
下载自动上传项目
https://github.com/ygxbnet/lsky-upload/releases/download/0.3.0/lsky-upload_0.3.0_windows_amd64.zip

打开配置文件可以看见一下信息，更换为前面的获取的token

验证能否自动上传
打开typroa


再去看看后台图库

至此项目成功

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>杂项</category>
      </categories>
      <tags>
        <tag>Centos 8 stream</tag>
        <tag>Lsky Pro</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx1.22实现内网自签证书</title>
    <url>/posts/10165/</url>
    <content><![CDATA[
Nginx1.22实现内网自签证书
HTTPS（全称：HyperText Transfer Protocol Secure）是HTTP（超文本传输协议）的安全版本。本质上，HTTPS在HTTP的基础上，通过SSL/TLS协议提供了数据加密、完整性保护和身份验证，以确保网络数据传输的安全性。HTTPS被广泛用于互联网上的安全通信，特别是在线交易和处理敏感信息时，本文以Nginx为例部署自签发https证书。
前情提要
本次实验仅仅用于实验测试有ssl需求的实验，生产环境建议不要用
本次实验环境是Rocky8.9（和centos 8 stream 大差不差）
安装nginx
dnf update -ydnf module list nginxdnf remove @nginxdnf module reset nginxdnf module install nginx:1.22 -ysystemctl enable --now nginxnginx -V
查看是否有--with-http_ssl_module
--with-http_ssl_module 是 Nginx 配置选项之一，用于启用 Nginx 的 SSL 功能模块。当 Nginx 编译时包含了 --with-http_ssl_module 选项时，表示 Nginx 将支持处理 HTTPS 请求，即通过 SSL/TLS 加密协议保护数据传输。

安装openssl
OpenSSL 是一个开放源代码的加密库，广泛用于安全通信、加密和解密数据。它提供了一组功能丰富的工具和库，用于处理安全通信所需的各种加密操作。
dnf install openssldnf install openssl-devel
生成证书
#没有就创建sslkey文件夹cd /etc/nginx/sslkey#创建本地私有密钥openssl genrsa -out ssl.key 2048 #按提示输入即可openssl req -new -key ssl.key -out ssl.csr ---------------------------------------------------------------- 国家名称(2字母代码)[XX]:CNCountry Name (2 letter code) [XX]:CN州或省名(全称)[]:GuangdongState or Province Name (full name) []:Guangdong地区名称(如城市)[默认城市]:广州Locality Name (eg, city) [Default City]:Guangzhou组织机构名称(如公司)【默认公司有限公司】:qianyiosOrganization Name (eg, company) [Default Company Ltd]:qianyios组织单位名称(如section) []:NONEOrganizational Unit Name (eg, section) []:NONE通用名称(例如，您的名字或服务器主机名)[]:qianyiosCommon Name (eg, your name or your server&#x27;s hostname) []:qianyios邮箱地址[]:abc@qq.comEmail Address []:abc@qq.com请输入以下“额外”属性Please enter the following &#x27;extra&#x27; attributes与您的证书请求一起发送to be sent with your certificate request挑战密码[]:123456A challenge password []:123456可选的公司名称[]:NONEAn optional company name []:NONE ----------------------------------------------------------------#创建证书crtopenssl x509 -req -days 1460 -in ssl.csr -signkey ssl.key -out ssl.crt #创建证书pemopenssl dhparam -out ssl.pem 2048
Nginx配置
vi /etc/nginx/nginx.conf...........include /etc/nginx/sslkey/*.conf;...........保存退出
在nginx默认配置下加上include /etc/nginx/sslkey/*.conf;

给ssl证书单独生成一个conf。
cat &gt;&gt; /etc/nginx/sslkey/ssl.conf &lt;&lt;&quot;EOF&quot;server &#123;    listen 443   ssl;    ssl_certificate                          /etc/nginx/sslkey/ssl.crt;    ssl_certificate_key                        /etc/nginx/sslkey/ssl.key;    ssl_session_timeout 5m;    ssl_protocols TLSv1.2;    ssl_ciphers EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!KRB5:!aECDH:!EDH+3DES;    add_header Strict-Transport-Security &quot;max-age=31536000; includeSubDomains&quot;;&#125;EOF
重点！！！
在你需要的网站配置加入重定向至https，因为我们默认访问nginx页面的时候是http的，所以要重定向。现在我们模拟访问80端口的默认网页
vi /etc/nginx/nginx.conf    include /etc/nginx/conf.d/*.conf;    include /etc/nginx/sslkey/*.conf;    server &#123;        listen       80;        listen       [::]:80;        return 301 https://$host$request_uri;   #就加这一句        server_name  _;        root         /usr/share/nginx/html;

重启nginx
nginx -tnginx -s reload
这时候访问80网页，自签证书已经好了，只是不受信任而已


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Rocky 8</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>Mysql主从复制案例</title>
    <url>/posts/8acdcc65/</url>
    <content><![CDATA[
Mysql主从复制案例
各种复制架构

主从复制原理

主从复制相关线程

主节点：

​	dump Thread： 为每个Slave的I/O Thread启动一个dump线程，用于向其发送binary log events

从节点：

​	I/O Thread： 向Master请求二进制日志事件，并保存于中继日志中
​	SQL Thread： 从中继日志中读取日志事件，在本地完成重放
跟复制功能相关的文件：

master.info ：用于保存slave连接至master时的相关信息，例如账号、密码、服务器地址等
relay-log.info ：保存在当前slave节点上已经复制的当前二进制日志和本地relay log日志的对应关系
mysql-relay-bin.00000#: 中继日志,保存从主节点复制过来的二进制日志,本质就是二进制日志

二进制日志功能
【安装好mysql再来查询】
这里讲一下二进制日志是干啥的，你只要开启了二进制日志的功能
sql_log_bin=ON：开启二进制日志记录的功能，你执行一个mysql语句他就会记录一下，他在mysql默认是开启的就不用管了
log-bin=路径/文件前缀：开启二进制日志功能,这个可以直接写在/etc/my.cnf里,一主一从的步骤里有介绍
一个是记录，一个是开启，如果你只有记录，没有开启是没有二进制日志的，所以二者缺一不可
先看看怎么查
[root@master ~]# mysql -uroot -p123456 -e &quot;select @@log_bin;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+-----------+| @@log_bin |+-----------+|         1 |+-----------+[root@master ~]# mysql -uroot -p123456 -e &quot;select @@sql_log_bin;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+| @@sql_log_bin |+---------------+|             1 |+---------------+[root@master ~]#
如果你看到两个都是1或者ON那就是全部开启了
一主一从
master：192.168.48.128
slave：192.168.48.11
安装mysql

自行关闭firewalld，selinux

操作节点：【所有节点】
二进制安装（适合新机）
#你可以选择用我的脚本进行安装二进制的mysql#二进制安装是基本所有linux都可以使用的，如果有哪个系统运行不了请留言wget https://blog.qianyios.top/file/mysql_install.shbash mysql_install.sh#密码我设置了123456（建议不要太简单）#建议选清理一遍（输入3），再进行安装mysql8.0（输入2）
其他安装
其他方式自行安装，我就不解释了，只要确保同个版本差不多就行了8.0.x这些。
主节点配置
操作节点：【master】
修改/etc/my.cnf
#其他安装mysql的方法，你只需要确保有如下选项就行了[mysqld]sever-id=128log-bin=/data/logbin/qylog
sever-id ：必须是整个集群里面唯一的，不能重复
log-bin:后续会在/data/logbin生成qylog.000001的二进制文件，你可以自定义路径和文件名不用加后缀
如果是用了我的脚本安装的mysql可以直接执行下面的语句，不是的话就看上面那个步骤手动修改
cat &gt; /etc/my.cnf &lt;&lt; &quot;EOF&quot;[mysqld]server-id=128datadir=/data/mysqlskip_name_resolve=1socket=/data/mysql/mysql.socklog-error=/data/mysql/mysql.logpid-file=/data/mysql/mysql.pidcharacter-set-server=utf8mb4#开启二进制选项log-bin=/data/logbin/qylog#二进制日志记录的格式，mariadb5.5默认STATEMENT#binlog_format=STATEMENT|ROW|MIXED[client]socket=/data/mysql/mysql.sock[mysql]prompt=(\\u@\\h) [\\d]&gt;\\_EOF
由于前面不是开启了二进制文件的功能吗
我的路径是：/data/logbin/qylog，其中/data/logbin并不存在，你要手动创建，不然开启mysql会报错
mkdir -p /data/logbinchown -R mysql:mysql /data/logbinsystemctl restart mysqld
此时二进制文件已经创建成功
[root@master ~]# ll /data/logbin/total 8-rw-r----- 1 mysql mysql 157 Jul 31 18:53 qylog.000001-rw-r----- 1 mysql mysql  26 Jul 31 18:53 qylog.index
创建用于主从复制的账号
[root@master ~]# mysql -uroot -p123456 -e &quot;show master status&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+--------------+----------+--------------+------------------+-------------------+| File         | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+--------------+----------+--------------+------------------+-------------------+| qylog.000001 |      157 |              |                  |                   |+--------------+----------+--------------+------------------+-------------------+#记住这个qylog.000001和起始位置157，后面会用到
#创建repluser用户，密码是123456，允许从192.168.48.0来连接master的mysqlmysql -uroot -p123456 -e &quot;create user repluser@&#x27;192.168.48.%&#x27; identified by &#x27;123456&#x27;;&quot;mysql -uroot -p123456 -e &quot;grant replication slave on *.* to repluser@&#x27;192.168.48.%&#x27;;&quot;
从节点配置
操作节点：【slave1】
修改/etc/my.cnf
#其他安装mysql的方法，你只需要确保有如下选项就行了[mysqld]server-id=11log-bin=/data/logbin/qylogread_only=ON #设置数据库只读，针对supper user无效#启动中继日志relay_log=/data/relaylog/relay-log #relay log的文件路径，默认值hostname-relay-binrelay_log_index=/data/relaylog/relay-log.index  #默认值hostname-relay-bin.index
sever-id ：必须是整个集群里面唯一的，不能重复
log-bin:后续会在/data/logbin生成qylog.000001的二进制文件，你可以自定义路径和文件名不用加后缀
如果是用了我的脚本安装的mysql可以直接执行下面的语句，不是的话就看上面那个步骤手动修改
cat &gt; /etc/my.cnf &lt;&lt; &quot;EOF&quot;[mysqld]server-id=11datadir=/data/mysqlsocket=/data/mysql/mysql.socklog-error=/data/mysql/mysql.logpid-file=/data/mysql/mysql.pidcharacter-set-server=utf8mb4#开启二进制选项log-bin=/data/logbin/qylog#二进制日志记录的格式，mariadb5.5默认STATEMENT#binlog_format=STATEMENT|ROW|MIXEDread_only=ON #设置数据库只读，针对supper user无效relay_log=/data/relaylog/relay-log #relay log的文件路径，默认值hostname-relay-binrelay_log_index=/data/relaylog/relay-log.index  #默认值hostname-relay-bin.index[client]socket=/data/mysql/mysql.sock[mysql]prompt=(\\u@\\h) [\\d]&gt;\\_EOF
由于前面不是开启了二进制文件的功能，还有开启中继日志的功能吗
我的路径是：
log-bin=/data/logbin/qylog，其中/data/logbin并不存在
relay_log=/data/relaylog/relay-log，其中/data/relaylog并不存在
你要手动创建，不然开启mysql会报错
mkdir -p /data/logbin /data/relaylogchown -R mysql:mysql /data/logbin /data/relaylogsystemctl restart mysqld
启动复制线程
操作节点：[slave]
使用有复制权限的用户账号连接至主服务器
mysql -u root -p123456CHANGE MASTER TO MASTER_HOST=&#x27;192.168.48.128&#x27;, MASTER_USER=&#x27;repluser&#x27;, MASTER_PASSWORD=&#x27;123456&#x27;, MASTER_LOG_FILE=&#x27;qylog.000001&#x27;, MASTER_LOG_POS=157,get_master_public_key=1,MASTER_DELAY=10; 
CHANGE MASTER TO
MASTER_HOST=‘192.168.48.128’,  master的ip
MASTER_USER=‘repluser’,              master创建的主从复制的账号
MASTER_PASSWORD=‘123456’,   master创建的主从复制的账号的密码
MASTER_LOG_FILE=‘qylog.000001’,   master查看的二进制文件
MASTER_LOG_POS=157,              master查看的二进制文件的起始位置
MASTER_DELAY=10;                延迟更新10s
启动复制线程
start slave;show slave status\G;
确保有两个yes就行了

如果你想重置线程可以用以下命令
stop slave;reset slave;
有一个重要的点，因为二进制日志是记录你的操作的嘛，我们在开启二进制日志之后，不是在主节点创建了一个repluser用户吗，那这个操作肯定也被记录，然后这不是主从复制了吗，这里肯定，也会同步，也会运行，二进制日志本身就是一个sql文件，普通cat是看不了，你得用这个命令
[root@master ~]# mysqlbinlog -uroot -p123456 /data/logbin/qylog.000001 -v

所有既然主从复制了，那在从节点，应该也运行了这个二进制文件，就说明从节点也有这个账号
select host,user from mysql.user;

测试
接下来在主节点导入测试数据
全选复制粘贴退出mysql运行
cat &gt; hellodb.sql &lt;&lt;&quot;EOF&quot;CREATE DATABASE /*!32312 IF NOT EXISTS*/ `hellodb` /*!40100 DEFAULT CHARACTER SET utf8 */;USE `hellodb`;DROP TABLE IF EXISTS `classes`;CREATE TABLE `classes` (  `ClassID` tinyint(3) unsigned NOT NULL AUTO_INCREMENT,  `Class` varchar(100) DEFAULT NULL,  `NumOfStu` smallint(5) unsigned DEFAULT NULL,  PRIMARY KEY (`ClassID`)) ENGINE=InnoDB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8;LOCK TABLES `classes` WRITE;INSERT INTO `classes` VALUES(1,&#x27;Shaolin Pai&#x27;,10),(2,&#x27;Emei Pai&#x27;,7),(3,&#x27;QingCheng Pai&#x27;,11),(4,&#x27;Wudang Pai&#x27;,12),(5,&#x27;Riyue Shenjiao&#x27;,31),(6,&#x27;Lianshan Pai&#x27;,27),(7,&#x27;Ming Jiao&#x27;,27),(8,&#x27;Xiaoyao Pai&#x27;,15);UNLOCK TABLES;DROP TABLE IF EXISTS `coc`;CREATE TABLE `coc` (  `ID` int(10) unsigned NOT NULL AUTO_INCREMENT,  `ClassID` tinyint(3) unsigned NOT NULL,  `CourseID` smallint(5) unsigned DEFAULT NULL,  PRIMARY KEY (`ID`)) ENGINE=InnoDB AUTO_INCREMENT=15 DEFAULT CHARSET=utf8;LOCK TABLES `coc` WRITE;INSERT INTO `coc` VALUES (1,1,2),(2,1,5),(3,2,2),(4,2,6),(5,3,1),(6,3,7),(7,4,5),(8,4,2),(9,5,1),(10,5,9),(11,6,3),(12,6,4),(13,7,4),(14,7,3);UNLOCK TABLES;DROP TABLE IF EXISTS `courses`;CREATE TABLE `courses` (  `CourseID` smallint(5) unsigned NOT NULL AUTO_INCREMENT,  `Course` varchar(100) NOT NULL,  PRIMARY KEY (`CourseID`)) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8;LOCK TABLES `courses` WRITE;/*!40000 ALTER TABLE `courses` DISABLE KEYS */;INSERT INTO `courses` VALUES (1,&#x27;Hamo Gong&#x27;),(2,&#x27;Kuihua Baodian&#x27;),(3,&#x27;Jinshe Jianfa&#x27;),(4,&#x27;Taiji Quan&#x27;),(5,&#x27;Daiyu Zanghua&#x27;),(6,&#x27;Weituo Zhang&#x27;),(7,&#x27;Dagou Bangfa&#x27;);/*!40000 ALTER TABLE `courses` ENABLE KEYS */;UNLOCK TABLES;DROP TABLE IF EXISTS `scores`;CREATE TABLE `scores` (  `ID` int(10) unsigned NOT NULL AUTO_INCREMENT,  `StuID` int(10) unsigned NOT NULL,  `CourseID` smallint(5) unsigned NOT NULL,  `Score` tinyint(3) unsigned DEFAULT NULL,  PRIMARY KEY (`ID`)) ENGINE=InnoDB AUTO_INCREMENT=16 DEFAULT CHARSET=utf8;LOCK TABLES `scores` WRITE;INSERT INTO `scores` VALUES (1,1,2,77),(2,1,6,93),(3,2,2,47),(4,2,5,97),(5,3,2,88),(6,3,6,75),(7,4,5,71),(8,4,2,89),(9,5,1,39),(10,5,7,63),(11,6,1,96),(12,7,1,86),(13,7,7,83),(14,8,4,57),(15,8,3,93);UNLOCK TABLES;DROP TABLE IF EXISTS `students`;CREATE TABLE `students` (  `StuID` int(10) unsigned NOT NULL AUTO_INCREMENT,  `Name` varchar(50) NOT NULL,  `Age` tinyint(3) unsigned NOT NULL,  `Gender` enum(&#x27;F&#x27;,&#x27;M&#x27;) NOT NULL,  `ClassID` tinyint(3) unsigned DEFAULT NULL,  `TeacherID` int(10) unsigned DEFAULT NULL,  PRIMARY KEY (`StuID`)) ENGINE=InnoDB AUTO_INCREMENT=26 DEFAULT CHARSET=utf8;LOCK TABLES `students` WRITE;INSERT INTO `students` VALUES (1,&#x27;Shi Zhongyu&#x27;,22,&#x27;M&#x27;,2,3),(2,&#x27;Shi Potian&#x27;,22,&#x27;M&#x27;,1,7),(3,&#x27;Xie Yanke&#x27;,53,&#x27;M&#x27;,2,16),(4,&#x27;Ding Dian&#x27;,32,&#x27;M&#x27;,4,4),(5,&#x27;Yu Yutong&#x27;,26,&#x27;M&#x27;,3,1),(6,&#x27;Shi Qing&#x27;,46,&#x27;M&#x27;,5,NULL),(7,&#x27;Xi Ren&#x27;,19,&#x27;F&#x27;,3,NULL),(8,&#x27;Lin Daiyu&#x27;,17,&#x27;F&#x27;,7,NULL),(9,&#x27;Ren Yingying&#x27;,20,&#x27;F&#x27;,6,NULL),(10,&#x27;Yue Lingshan&#x27;,19,&#x27;F&#x27;,3,NULL),(11,&#x27;Yuan Chengzhi&#x27;,23,&#x27;M&#x27;,6,NULL),(12,&#x27;Wen Qingqing&#x27;,19,&#x27;F&#x27;,1,NULL),(13,&#x27;Tian Boguang&#x27;,33,&#x27;M&#x27;,2,NULL),(14,&#x27;Lu Wushuang&#x27;,17,&#x27;F&#x27;,3,NULL),(15,&#x27;Duan Yu&#x27;,19,&#x27;M&#x27;,4,NULL),(16,&#x27;Xu Zhu&#x27;,21,&#x27;M&#x27;,1,NULL),(17,&#x27;Lin Chong&#x27;,25,&#x27;M&#x27;,4,NULL),(18,&#x27;Hua Rong&#x27;,23,&#x27;M&#x27;,7,NULL),(19,&#x27;Xue Baochai&#x27;,18,&#x27;F&#x27;,6,NULL),(20,&#x27;Diao Chan&#x27;,19,&#x27;F&#x27;,7,NULL),(21,&#x27;Huang Yueying&#x27;,22,&#x27;F&#x27;,6,NULL),(22,&#x27;Xiao Qiao&#x27;,20,&#x27;F&#x27;,1,NULL),(23,&#x27;Ma Chao&#x27;,23,&#x27;M&#x27;,4,NULL),(24,&#x27;Xu Xian&#x27;,27,&#x27;M&#x27;,NULL,NULL),(25,&#x27;Sun Dasheng&#x27;,100,&#x27;M&#x27;,NULL,NULL);UNLOCK TABLES;DROP TABLE IF EXISTS `teachers`;CREATE TABLE `teachers` (  `TID` smallint(5) unsigned NOT NULL AUTO_INCREMENT,  `Name` varchar(100) NOT NULL,  `Age` tinyint(3) unsigned NOT NULL,  `Gender` enum(&#x27;F&#x27;,&#x27;M&#x27;) DEFAULT NULL,  PRIMARY KEY (`TID`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;LOCK TABLES `teachers` WRITE;INSERT INTO `teachers` VALUES (1,&#x27;Song Jiang&#x27;,45,&#x27;M&#x27;),(2,&#x27;Zhang Sanfeng&#x27;,94,&#x27;M&#x27;),(3,&#x27;Miejue Shitai&#x27;,77,&#x27;F&#x27;),(4,&#x27;Lin Chaoying&#x27;,93,&#x27;F&#x27;);UNLOCK TABLES;DROP TABLE IF EXISTS `toc`;CREATE TABLE `toc` (  `ID` int(10) unsigned NOT NULL AUTO_INCREMENT,  `CourseID` smallint(5) unsigned DEFAULT NULL,  `TID` smallint(5) unsigned DEFAULT NULL,  PRIMARY KEY (`ID`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;LOCK TABLES `toc` WRITE;UNLOCK TABLES;EOF
在主节点导入
#导入[root@master ~]# mysql -uroot -p123456 &lt; hellodb.sqlmysql: [Warning] Using a password on the command line interface can be insecure.#这个警告没关系[root@master ~]# mysql -uroot -p123456 hellodb -e &quot;show tables;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+-------------------+| Tables_in_hellodb |+-------------------+| classes           || coc               || courses           || scores            || students          || teachers          || toc               |+-------------------+#成功导入#在主节点插入一条数据mysql -uroot -p123456use hellodb;insert into teachers (name,age,gender)values(&quot;XIAOHU&quot;,18,&#x27;M&#x27;);select * from teachers;#成功插入数据(root@localhost) [hellodb]&gt; select * from teachers;+-----+---------------+-----+--------+| TID | Name          | Age | Gender |+-----+---------------+-----+--------+|   1 | Song Jiang    |  45 | M      ||   2 | Zhang Sanfeng |  94 | M      ||   3 | Miejue Shitai |  77 | F      ||   4 | Lin Chaoying  |  93 | F      ||   5 | XIAOHU        |  18 | M      |+-----+---------------+-----+--------+
接下来看看从节点是否有成功实现主从复制
[root@slave ~]# mysql -uroot -p123456use hellodb;show tables;select * from teachers;#看吧，已经成功从主节点复制数据到从节点(root@localhost) [hellodb]&gt; show tables;+-------------------+| Tables_in_hellodb |+-------------------+| classes           || coc               || courses           || scores            || students          || teachers          || toc               |+-------------------+7 rows in set (0.00 sec)#新插入的数据也在(root@localhost) [hellodb]&gt; select * from teachers;+-----+---------------+-----+--------+| TID | Name          | Age | Gender |+-----+---------------+-----+--------+|   1 | Song Jiang    |  45 | M      ||   2 | Zhang Sanfeng |  94 | M      ||   3 | Miejue Shitai |  77 | F      ||   4 | Lin Chaoying  |  93 | F      ||   5 | XIAOHU        |  18 | M      |+-----+---------------+-----+--------+5 rows in set (0.00 sec)
至此，一主一从的主从复制实验成功，你必须理解，主要是通过二进制日志来依靠传输的
案例
实战案例：将已有的MySQL8.0单机架构变成主从复制架构
这台单机，可能运行很久了，有很多数据了，现在要让他变成主从架构
前提：两者开启二进制日志功能
1.mysqldump备份所有数据库
2.创建传输用户并且授权
3.将备份scp复制到从节点
4.从节点开启read-only等参数
5.从节点设置主从复制连接
6.从节点进行还原备份
7.开启主从复制，start slave；
#已有的master节点,备份所有的数据库mysqldump -uroot -p123456 -A -F -E -R --triggers --single-transaction \--source-data=1 --flush-privileges --hex-blob --default-character-set=utf8 &gt; all.sql#创建传输用户并且授权mysql -uroot -p123456 -e &quot;create user repluser@&#x27;192.168.48.%&#x27; identified by &#x27;123456&#x27;;&quot;mysql -uroot -p123456 -e &quot;grant replication slave on *.* to repluser@&#x27;192.168.48.%&#x27;;&quot;#将备份scp复制到从节点scp -p all.sql 192.168.48.11:/root/#从节点开启read-only等参数[root@slave ~]# cat /etc/my.cnfserver-id=8log-bin=/data/logbin/qylogread_only=ONrelay_log=/data/relaylog/relay-logrelay_log_index=/data/relaylog/relay-log.index#从节点设置主从复制连接#1.查看all.sql的二进制文件和起始位置head -n 30 all.sqlCHANGE MASTER TO MASTER_LOG_FILE=&#x27;qylog.000002&#x27;, MASTER_LOG_POS=157;#2.设置复制连接，填入前面的二进制文件和起始位置mysql -u root -p123456CHANGE MASTER TO MASTER_HOST=&#x27;192.168.48.128&#x27;, MASTER_USER=&#x27;repluser&#x27;, MASTER_PASSWORD=&#x27;123456&#x27;, MASTER_LOG_FILE=&#x27;qylog.000002&#x27;, MASTER_LOG_POS=157,MASTER_DELAY=10; #从节点进行还原备份#先停止记录二进制日志mysql -u root -p123456set sql_log_bin=0;source all.sqlset sql_log_bin=1;#开启主从复制start slave;show slave status\G;#可能等待60秒才能主从双yes#主节点插入新数据mysql -uroot -p123456use hellodb;insert into teachers (name,age,gender)values(&quot;qianyios&quot;,18,&#x27;M&#x27;);select * from teachers;#最后看看是否有数据库已经新插入的数据库是否同步成功(root@localhost) [hellodb]&gt; show tables;+-------------------+| Tables_in_hellodb |+-------------------+| classes           || coc               || courses           || scores            || students          || teachers          || toc               |+-------------------+7 rows in set (0.00 sec)(root@localhost) [hellodb]&gt; show databases;+--------------------+| Database           |+--------------------+| hellodb            || information_schema || mysql              || performance_schema || sys                |+--------------------+5 rows in set (0.00 sec)(root@localhost) [hellodb]&gt; select * from teachers;+-----+---------------+-----+--------+| TID | Name          | Age | Gender |+-----+---------------+-----+--------+|   1 | Song Jiang    |  45 | M      ||   2 | Zhang Sanfeng |  94 | M      ||   3 | Miejue Shitai |  77 | F      ||   4 | Lin Chaoying  |  93 | F      ||   5 | XIAOHU        |  18 | M      ||   6 | qianyios      |  18 | M      |+-----+---------------+-----+--------+
一主多从
master：192.168.48.128
slave1：192.168.48.11
slave2：192.168.48.10
接下来在前面的一主一从复制架构，变成一主多从，一样，如果这个mysql的主节点是已经运行很久了，你就要全部备份，拉到从节点，注意新的从节点要安装和另外一个从节点一样的mysql版本（这里我就略过安装教程了），然后只需要在新从节点设置好/etc/my.cnf和主从连接信息，关闭二进制记录，还原all.sql文件，开启二进制记录，开启主从复制即可
这里就接着那个案例来
#在案例的时间点之后，又运行了一段时间，此时你必须重新备份全部数据库，拉到从节点mysqldump -uroot -p123456 -A -F -E -R --triggers --single-transaction \--source-data=1 --flush-privileges --hex-blob --default-character-set=utf8 &gt; all.sqlscp -p all.sql 192.168.48.10:/root/#新从节点创建传输用户并且授权mysql -uroot -p123456 -e &quot;create user repluser@&#x27;192.168.48.%&#x27; identified by &#x27;123456&#x27;;&quot;mysql -uroot -p123456 -e &quot;grant replication slave on *.* to repluser@&#x27;192.168.48.%&#x27;;&quot;#vim进去all.sql会有一段CHANGE MASTER TO MASTER_LOG_FILE=&#x27;qylog.000003&#x27;, MASTER_LOG_POS=157;#是不是很像连接信息，那你就把它变成下面那样，就其实它二进制文件和起始位置都有了，只需要把其他补齐就行了CHANGE MASTER TO MASTER_HOST=&#x27;192.168.48.128&#x27;, MASTER_USER=&#x27;repluser&#x27;, MASTER_PASSWORD=&#x27;123456&#x27;, MASTER_LOG_FILE=&#x27;qylog.000003&#x27;, MASTER_LOG_POS=157; #然后保存退出，这和案例里的先设置主从复制连接效果是一样，我就直接在文件里设置也一样#没关系的，这样进文件，省得我还要看一遍二进制文件是啥了，对不对，我直接进去设置#接下来进行还原set sql_log_bin=0;source all.sql;set sql_log_bin=1;#开启slave主从复制start slave;show slave status\G;#接下来你就自行测试，看看是否有数据库已经还原成功和新插入数据是否成功并确认是否同步
至此一主多从成功！
级联复制
master：192.168.48.128
slave1：192.168.48.11（充当级联slave中间节点）
slave2：192.168.48.10
这个实验，我就只用了一个slave2，slave3和slave2的操作一样的

为啥会有级联呢！
就好比前面的一主多从，他要进行多个复制任务给从节点，那主节点的压力是不是大一点，那么就有了级联，把复制任务分担给从节点
这里还有一个问题
首先我们都知道主从复制的原理就是，主节点通过二进制日志bin传输数据到从节点的io线程接收写到中继日志relay,那后面的slave2和slave3应该也是这样，只需要接收slave1的bin的数据就行了是不是
错！大错特错！！！！！！
slave1的bin的数据从哪里来，他自己本身就是生成自己自身的数据，他怎么可能有主节点的数据呢？是不是得从自身的relay来，所以后面会讲到一个参数，就是可以让relay的数据传输过来的时候自动写入到bin，那就后面再说
这里延续前面的一主多从
#首先我得把slave2的一些信息删除，脱离原来一主多从#/etc/my.cnf的信息不用删，本身就是给从节点用的#要删就要删除主从复制的连接信息，顺便重置二进制信息stop slave;reset slave all;reset master;
在中间节点slave1启用以下配置，实现中间slave节点能将master的二进制日志在本机进行数据库更新，并且也同时更新本机的二进制，从而实现级联复制
[mysqld]sever-id=11log-bin=/data/logbin/qylogread_only=ONrelay_log=/data/relaylog/relay-logrelay_log_index=/data/relaylog/relay-log.indexlog_slave_updates #级联复制中间节点的必选，,MySQL8.0此为默认值,可以不用人为添加,其它版本默认不开启
重启mysql
systemctl restart mysqld
接着在新的从节点（slave2）进行操作，假设他是新的，你要进行的是，配置my.cnf，创建主从复制账号和授权，还有因为他是新的，没有数据，你还要在从节点备份全部数据库到从节点
#新的从节点[mysqld]server-id=10log-bin=/data/logbin/qylogread_only=ON relay_log=/data/relaylog/relay-logrelay_log_index=/data/relaylog/relay-log.index#从节点导出全部数据库的备份（注意是从节点）mysqldump -uroot -p123456 -A -F -E -R --triggers --single-transaction \--source-data=1 --flush-privileges --hex-blob --default-character-set=utf8 &gt; all.sql#将备份scp复制到从节点scp -p all.sql 192.168.48.10:/root/#新的从节点创建传输用户并且授权mysql -uroot -p123456 -e &quot;create user repluser@&#x27;192.168.48.%&#x27; identified by &#x27;123456&#x27;;&quot;mysql -uroot -p123456 -e &quot;grant replication slave on *.* to repluser@&#x27;192.168.48.%&#x27;;&quot;#新的从节点vim进入all.sql#编辑那条语句，这时候这里的master就不是主节点而是指向中间节点了CHANGE MASTER TO MASTER_HOST=&#x27;192.168.48.11&#x27;, MASTER_USER=&#x27;repluser&#x27;, MASTER_PASSWORD=&#x27;123456&#x27;, MASTER_LOG_FILE=&#x27;qylog.000003&#x27;, MASTER_LOG_POS=157; #然后保存退出#新的从节点接下来进行还原set sql_log_bin=0;source all.sql;set sql_log_bin=1;#新的从节点开启slave主从复制start slave;show slave status\G;#接着我在主节点插入数据use hellodb;insert into teachers (name,age,gender)values(&quot;qianyios2&quot;,18,&#x27;M&#x27;);#之后在中间从节点看到了新的数据#在最后的从节点也看到了数据
主主复制
master：192.168.48.128
slave1：192.168.48.11（让他变成master主节点）
slave2：192.168.48.10
容易产生的问题：数据不一致；因此慎用
配置步骤
(1) 各节点使用一个惟一server_id
(2) 都启动binary log和relay log
(3) 创建拥有复制权限的用户账号
(4) 均把对方指定为主节点，并启动复制线程

keepalived提供vip地址，实现数据库集群高可用，这个点后面再说
双主的意思就是，这两个节点互为主从，实现双向复制，我是主的时候你是从，你是主的时候我是从
#将slave1的/etc/my.cnf里的read only相关参数要删掉，既然都是主节点，那么肯定要可读可写，保留如下[mysqld]server-id=11log-bin=/data/logbin/qylogrelay_log=/data/relaylog/relay-logrelay_log_index=/data/relaylog/relay-log.index#主节点添加relay_log相关参数[mysqld]server-id=128log-bin=/data/logbin/qyloglog_slave_updatesrelay_log=/data/relaylog/relay-logrelay_log_index=/data/relaylog/relay-log.index#主节点还要创建/data/relaylogmkdir -p /data/relaylogchown -R mysql:mysql /data/relaylogsystemctl restart mysqld#由于slave1的主从复制连接已经指向了master节点，我就不改了，（这是之前的）CHANGE MASTER TO MASTER_HOST=&#x27;192.168.48.128&#x27;, MASTER_USER=&#x27;repluser&#x27;, MASTER_PASSWORD=&#x27;123456&#x27;, MASTER_LOG_FILE=&#x27;qylog.000001&#x27;, MASTER_LOG_POS=157,MASTER_DELAY=10; 
#现在接着配置主节点的主从复制连接指向第二个主节点，slave1#你首先要在第二个主节点(slave1)查看二进制文件的位置(root@localhost) [hellodb]&gt; show master logs;+--------------+-----------+-----------+| Log_name     | File_size | Encrypted |+--------------+-----------+-----------+| qylog.000001 |      1405 | No        || qylog.000002 |       200 | No        || qylog.000003 |       180 | No        || qylog.000004 |       488 | No        || qylog.000005 |       180 | No        || qylog.000006 |       465 | No        |+--------------+-----------+-----------+#记住qylog.000006 | 465 （看最新的就行了）#在第一个主节点设置复制连接，指向第二个主节点CHANGE MASTER TO MASTER_HOST=&#x27;192.168.48.11&#x27;, MASTER_USER=&#x27;repluser&#x27;, MASTER_PASSWORD=&#x27;123456&#x27;, MASTER_LOG_FILE=&#x27;qylog.000006&#x27;, MASTER_LOG_POS=465,MASTER_DELAY=10;start slave;show slave status\G;#这时候第一个主节点显示的是双yes
测试两个节点各自插入新数据，看看对方有没有实现双向复制
#第一个主节点use hellodb;insert into teachers (name,age,gender)values(&quot;qi1&quot;,18,&#x27;M&#x27;);#第二个节点查询(root@localhost) [hellodb]&gt;  select * from teachers;+-----+---------------+-----+--------+| TID | Name          | Age | Gender |+-----+---------------+-----+--------+|   1 | Song Jiang    |  45 | M      ||   2 | Zhang Sanfeng |  94 | M      ||   3 | Miejue Shitai |  77 | F      ||   4 | Lin Chaoying  |  93 | F      ||   5 | XIAOHU        |  18 | M      ||   6 | qianyios      |  18 | M      ||   7 | qianyios1     |  18 | M      ||   8 | qianyios2     |  18 | M      ||   9 | qianyios3     |  18 | M      ||  10 | qi1           |  18 | M      |+-----+---------------+-----+--------+#第二个主节点use hellodb;insert into teachers (name,age,gender)values(&quot;qi2&quot;,18,&#x27;M&#x27;);#好奇怪，我这里就出问题了，第二个节点复制不到主节点不知道什么问题#后来才知道是设置了时间延迟#但起始也不是这个原因，你要关闭延迟也没问题，主要原因（可以千万最后一个章节bug集锦中查看）STOP SLAVE SQL_THREAD;CHANGE MASTER TO MASTER_DELAY = 0;START SLAVE SQL_THREAD;SHOW SLAVE STATUS\G;#两个节点都执行一下#查看第一个节点是否有数据(root@localhost) [hellodb]&gt; select * from teachers;+-----+---------------+-----+--------+| TID | Name          | Age | Gender |+-----+---------------+-----+--------+|   1 | Song Jiang    |  45 | M      ||   2 | Zhang Sanfeng |  94 | M      ||   3 | Miejue Shitai |  77 | F      ||   4 | Lin Chaoying  |  93 | F      ||   9 | qi1           |  18 | M      ||  10 | qi2           |  18 | M      |+-----+---------------+-----+--------+6 rows in set (0.00 sec)
如果你两个节点都同时插入数据，会出现一个主键的冲突比如说教师id你第一个节点这边添加了一个新数据，id自动分配为5但是又有10秒的延迟，都还没同步过去你在第二个节点又添加一个新数据，Id自动分配为5，那这样就会报错，所以两个数据库把插入的数据记得删掉
所以要这样配置
添加auto_increment_offset和auto_increment_increment
#slave1[mysqld]server-id=11log-bin=/data/logbin/qylogrelay_log=/data/relaylog/relay-logrelay_log_index=/data/relaylog/relay-log.indexauto_increment_offset=2     #开始点auto_increment_increment=2   #增长幅度 #主节点[mysqld]server-id=128log-bin=/data/logbin/qyloglog_slave_updatesrelay_log=/data/relaylog/relay-logrelay_log_index=/data/relaylog/relay-log.indexauto_increment_offset=1     #开始点auto_increment_increment=2   #增长幅度
两个节点重启服务
systemctl restart mysqld
再次两个节点同时插入数据
#第一个节点use hellodb;insert into teachers (name,age,gender)values(&quot;qi111&quot;,18,&#x27;M&#x27;);select * from teachers;......，#id自动增长为19|  17 | qi13          |  18 | M      ||  19 | qi111         |  18 | M      |#第二个节点use hellodb;insert into teachers (name,age,gender)values(&quot;qi112&quot;,18,&#x27;M&#x27;);select * from teachers;......，#id自动增长为17|  17 | qi13          |  18 | M      ||  18 | qi112         |  18 | M      |#就这样就不会导致主键的冲突#最后等待10秒的延迟，数据就会一致，两个节点都同步成功了(root@localhost) [hellodb]&gt; select * from teachers;+-----+---------------+-----+--------+| TID | Name          | Age | Gender |+-----+---------------+-----+--------+|   1 | Song Jiang    |  45 | M      ||   2 | Zhang Sanfeng |  94 | M      ||   3 | Miejue Shitai |  77 | F      ||   4 | Lin Chaoying  |  93 | F      |......|  17 | qi13          |  18 | M      ||  18 | qi112         |  18 | M      ||  19 | qi111         |  18 | M      |+-----+---------------+-----+--------+13 rows in set (0.00 sec)

看前面的架构图，两主一从，其实现在的状况是，数据从master写入到slave再到slave2的，slave2是没有连接master，所以只能后期写脚本，监测keepalived的vip变化，一旦变化，就去修改那个主从复制连接中的master ip去变化。

半同步复制


官方文档
https://dev.mysql.com/doc/refman/8.0/en/replication-semisync.htmlhttps://dev.mysql.com/doc/refman/5.7/en/replication-semisync.htmlhttps://mariadb.com/kb/en/library/semisynchronous-replication/
经过前两个图的理解，现在就开始实操
#接着上面一个主主的案例下来，如果不是，你只需要保留/etc/my.cnf有如下选项即可#master[mysqld]server-id=128log-bin=/data/logbin/qylogrpl_semi_sync_master_enabled=ON   #添加此行,需要先安装semisync_master.so插件后,再重启配置文件,否则无法启动rpl_semi_sync_master_timeout=10000  #设置10s内无法同步，也将返回成功信息给客户端#slave1server-id=11log-bin=/data/logbin/qylogrelay_log=/data/relaylog/relay-logrelay_log_index=/data/relaylog/relay-log.indexrpl_semi_sync_slave_enabled=ON #修改此行,需要先安装semisync_slave.so插件后,再重启,否则无法启动#slave1server-id=10log-bin=/data/logbin/qylogrelay_log=/data/relaylog/relay-logrelay_log_index=/data/relaylog/relay-log.indexrpl_semi_sync_slave_enabled=ON #修改此行,需要先安装semisync_slave.so插件后,再重启,否则无法启动#注意先别那么快重启mysql！！要安装半同步复制插件#master(永久安装)，如果想要临时的直接设置全局变量即可mysql -uroot -p123456 -e &quot;INSTALL PLUGIN rpl_semi_sync_master SONAME &#x27;semisync_master.so&#x27;;&quot;systemctl restart mysql#slave1mysql -uroot -p123456 -e &quot;INSTALL PLUGIN rpl_semi_sync_slave SONAME &#x27;semisync_slave.so&#x27;;&quot;systemctl restart mysql#slave2mysql -uroot -p123456 -e &quot;INSTALL PLUGIN rpl_semi_sync_slave SONAME &#x27;semisync_slave.so&#x27;;&quot;systemctl restart mysql
#三个节点的复制关系都要删掉mysql -uroot -p123456stop slave;reset slave all;#三个节点重置二进制日志reset master;
master导出全部备份给slave1和slave2，创建复制账号，关闭二进制日志记录
还原数据库，开启二进制日志记录，自行验证是否同步，这里就略过了
（注意是一主二从）相当于重新开始了
半同步复制的一些参数
#master节点(root@localhost) [hellodb]&gt; SHOW GLOBAL VARIABLES LIKE &#x27;%semi%&#x27;;+-------------------------------------------+------------+| Variable_name                             | Value      |+-------------------------------------------+------------+| rpl_semi_sync_master_enabled             | ON         || rpl_semi_sync_master_timeout             | 10000      || rpl_semi_sync_master_trace_level          | 32         || rpl_semi_sync_master_wait_for_slave_count | 1          || rpl_semi_sync_master_wait_no_slave        | ON         || rpl_semi_sync_master_wait_point           | AFTER_SYNC |+-------------------------------------------+------------+

rpl_semi_sync_master_enabled：是否启用半同步复制功能。ON 表示启用，OFF 表示禁用。
rpl_semi_sync_master_timeout：主库等待从库响应的超时时间（单位：毫秒）。超时后主库会切换为异步复制。
rpl_semi_sync_master_trace_level：日志记录的详细程度，数值越大记录越详细。
rpl_semi_sync_master_wait_for_slave_count：主库提交事务时需要等待的从库数量。
rpl_semi_sync_master_wait_no_slave：当没有从库连接时，主库是否继续使用半同步复制。ON 表示继续等待，OFF 表示切换为异步复制。
rpl_semi_sync_master_wait_point：半同步复制的等待点。AFTER_SYNC 表示在同步完成后等待，AFTER_FLUSH 表示在数据刷新到磁盘后等待。

#master(root@localhost) [hellodb]&gt; SHOW GLOBAL STATUS LIKE &#x27;%semi%&#x27;;+--------------------------------------------+-------+| Variable_name                              | Value |+--------------------------------------------+-------+| Rpl_semi_sync_master_clients               | 2     | #成功的有两个| Rpl_semi_sync_master_net_avg_wait_time     | 0     || Rpl_semi_sync_master_net_wait_time         | 0     || Rpl_semi_sync_master_net_waits             | 2     || Rpl_semi_sync_master_no_times              | 0     || Rpl_semi_sync_master_no_tx                 | 0     || Rpl_semi_sync_master_status                | ON    || Rpl_semi_sync_master_timefunc_failures     | 0     || Rpl_semi_sync_master_tx_avg_wait_time      | 674   || Rpl_semi_sync_master_tx_wait_time          | 674   || Rpl_semi_sync_master_tx_waits              | 1     || Rpl_semi_sync_master_wait_pos_backtraverse | 0     || Rpl_semi_sync_master_wait_sessions         | 0     || Rpl_semi_sync_master_yes_tx                | 1     |+--------------------------------------------+-------+
Rpl_semi_sync_master_clients：当前连接到主库并支持半同步复制的从库数量。
Rpl_semi_sync_master_net_avg_wait_time：半同步复制中网络平均等待时间（单位：微秒）。
Rpl_semi_sync_master_net_wait_time：半同步复制中网络总等待时间（单位：微秒）。
Rpl_semi_sync_master_net_waits：半同步复制中网络等待的总次数。
Rpl_semi_sync_master_no_times：半同步复制未成功（切换为异步复制）的次数。
Rpl_semi_sync_master_no_tx：半同步复制未成功时的事务数量。
Rpl_semi_sync_master_status：当前半同步复制的状态，ON 表示启用，OFF 表示禁用。
Rpl_semi_sync_master_timefunc_failures：半同步复制中时间函数失败的次数。
Rpl_semi_sync_master_tx_avg_wait_time：事务平均等待时间（单位：微秒）。
Rpl_semi_sync_master_tx_wait_time：事务总等待时间（单位：微秒）。
Rpl_semi_sync_master_tx_waits：事务等待的总次数。
接下来进行测试
1.测试都启动的情况下，一主二从，是否正常的主从同步
#主节点use hellodb;insert into teachers (name,age,gender)values(&quot;oooo1&quot;,18,&#x27;M&#x27;);use hellodb;select * from teachers;#从节点use hellodb;select * from teachers;#经测试同步成功(root@localhost) [hellodb]&gt; use hellodb;select * from teachers;Database changed+-----+---------------+-----+--------+| TID | Name          | Age | Gender |+-----+---------------+-----+--------+|   1 | Song Jiang    |  45 | M      |......|  21 | qy2           |  56 | F      ||  22 | oooo1         |  18 | M      |+-----+---------------+-----+--------+15 rows in set (0.00 sec)
2.接下来测试，slave关闭mysql模拟宕机的操作，再次进行插入数据
#slave2systemctl stop mysqld#主节点use hellodb;insert into teachers (name,age,gender)values(&quot;oooo1&quot;,18,&#x27;M&#x27;);use hellodb;select * from teachers;#slave1use hellodb;select * from teachers;#slave1成功同步数据(root@localhost) [hellodb]&gt; use hellodb;select * from teachers;Database changed+-----+---------------+-----+--------+| TID | Name          | Age | Gender |+-----+---------------+-----+--------+|   1 | Song Jiang    |  45 | M      ||   2 | Zhang Sanfeng |  94 | M      ||   3 | Miejue Shitai |  77 | F      ||   4 | Lin Chaoying  |  93 | F      ||   9 | qi1           |  18 | M      ||  10 | qi2           |  18 | M      ||  11 | qi3           |  18 | M      ||  12 | qi4           |  18 | M      ||  13 | qi19          |  18 | M      ||  14 | qi20          |  18 | M      ||  17 | qi13          |  18 | M      ||  18 | qi112         |  18 | M      ||  19 | qi111         |  18 | M      ||  21 | qy2           |  56 | F      ||  22 | oooo1         |  18 | M      ||  23 | oooo1         |  18 | M      |+-----+---------------+-----+--------+16 rows in set (0.00 sec)
3.接下来，让最后一个slave1也宕机看看是什么情况
#slave1systemctl stop mysqld#主节点use hellodb;insert into teachers (name,age,gender)values(&quot;oooo2&quot;,18,&#x27;M&#x27;);use hellodb;select * from teachers;#情况如下，默认等待10s(root@localhost) [hellodb]&gt; insert into teachers (name,age,gender)values(&quot;oooo2&quot;,18,&#x27;M&#x27;);Query OK, 1 row affected (10.01 sec)#此时已经没有slave节点连接数了(root@localhost) [hellodb]&gt; SHOW GLOBAL STATUS LIKE &#x27;%semi%&#x27;;+--------------------------------------------+----------+| Variable_name                              | Value    |+--------------------------------------------+----------+| Rpl_semi_sync_master_clients               | 0        |#在插入一次数据，这里就没有等待了(root@localhost) [hellodb]&gt; insert into teachers (name,age,gender)values(&quot;oooo3&quot;,18,&#x27;M&#x27;);Query OK, 1 row affected (0.05 sec)

第一次插入操作等待 10 秒，是因为主节点在半同步复制模式下等待从节点确认，但超时了。
第二次插入操作没有等待，是因为主节点已经检测到没有从节点连接，直接以异步复制模式执行。

4.恢复所有的slave节点，看看有无同步
#slave1和slave2systemctl start mysqld#主节点的数据(root@localhost) [hellodb]&gt; use hellodb;select * from teachers;Database changed+-----+---------------+-----+--------+| TID | Name          | Age | Gender |+-----+---------------+-----+--------+|   1 | Song Jiang    |  45 | M      |......|  22 | oooo1         |  18 | M      ||  23 | oooo1         |  18 | M      ||  24 | oooo1         |  18 | M      ||  32 | oooo2         |  18 | M      ||  33 | oooo3         |  18 | M      |+-----+---------------+-----+--------+19 rows in set (0.00 sec)#slave1和slave2的数据成功同步(root@localhost) [hellodb]&gt; use hellodb;select * from teachers;Database changed+-----+---------------+-----+--------+| TID | Name          | Age | Gender |+-----+---------------+-----+--------+|   1 | Song Jiang    |  45 | M      |......|  22 | oooo1         |  18 | M      ||  23 | oooo1         |  18 | M      ||  24 | oooo1         |  18 | M      ||  32 | oooo2         |  18 | M      ||  33 | oooo3         |  18 | M      |+-----+---------------+-----+--------+19 rows in set (0.00 sec)
至此半同步复制成功
复制过滤器
让从节点仅复制指定的数据库，或指定数据库的指定表
复制过滤器两种实现方式：
(1) 服务器选项：主服务器仅向二进制日志中记录与特定数据库相关的事件
缺点：基于二进制还原将无法实现；不建议使用
优点: 只需要在主节点配置一次即可


只需要在主服务器配置，如果我不想同步论坛，只想同步其他的，只需要禁用论坛的二进制日志，就行了，但是这就有一个问题，你论坛的二进制日志都禁用了，那你这个论坛的这个数据库突然出现问题了，数据怎么办？这就是缺点

这个禁用二进制，不用禁用log_bin选项，看下面的来配置黑白名单就行
注意：此项和 binlog_format相关
参看：https://mariadb.com/kb/en/library/mysqld-options/#-binlog-ignore-db
#只同步db1和db2，也就是只有这两个会开启二进制日志，其他数据库不会#（白名单和黑名单必须二选一)#默认就是白名单)vim /etc/my.cnf#白名单binlog-do-db=db1 #数据库白名单列表，不支持同时指定多个值，如果想实现多个数据库需多行实现binlog-do-db=db2 #也可以设置黑名单binlog-ignore-db=db3 #数据库黑名单列表
注意：
This option will not work with cross-database updates with statement-based  logging. See the Statement-Based Logging section for more information.This option can not be set dynamically.When setting it on the command-line or in a server option group in an option  file, the option does not accept a comma-separated list. If you would like to  specify multiple filters, then you need to specify the option multiple times.#此选项不适用于使用基于语句的日志记录的跨数据库更新。有关更多信息，请参阅基于语句的日志记录部分。#此选项不能动态设置。#在命令行或选项文件中的服务器选项组中设置该选项时，该选项不接受逗号分隔的列表。如果希望指定多个过滤器，则需要多次指定该选项。
来测试吧
#master#配置文件只需要保留如下选项vim /etc/my.cnf[mysqld]server-id=128log-bin=/data/logbin/qylogbinlog-do-db=db1binlog-do-db=db2 binlog-ignore-db=db3#slave1和slave2记得删除半同步的选项，最后所有服务器重启mysqlsystemctl restart mysqld#主节点创建测试的数据库mysql -u root -p123456 -e&quot;create database db1;&quot;mysql -u root -p123456 -e&quot;create database db2;&quot;mysql -u root -p123456 -e&quot;create database db3;&quot;#db1和db2插入数据mysql -u root -p123456 -e&quot;USE db1;CREATE TABLE test ( id INT);INSERT INTO test (id) VALUES (1);SELECT * FROM test;&quot;mysql -u root -p123456 -e&quot;USE db2;CREATE TABLE test ( id INT);INSERT INTO test (id) VALUES (1);SELECT * FROM test;&quot;#查看数据库mysql -u root -p123456 -e&quot;show databases;&quot;+--------------------+| Database           |+--------------------+| db1                || db2                || db3                || hellodb            || information_schema || mysql              || performance_schema || sys                |+--------------------+#这时候其他从节点可以查看是否有这个db1和db2数据库mysql -u root -p123456 -e&quot;show databases;&quot;mysql -u root -p123456 -e&quot;USE db1;SELECT * FROM test;&quot;mysql -u root -p123456 -e&quot;USE db2;SELECT * FROM test;&quot;#同步成功，slave1和slave2都一样，都成功同步了白名单的数据库，黑名单的数据库就没同步[root@slave1 ~]# mysql -u root -p123456 -e&quot;show databases;&quot;+--------------------+| Database           |+--------------------+| db1                || db2                || hellodb            || information_schema || mysql              || performance_schema || sys                |+--------------------+[root@slave1 ~]# mysql -u root -p123456 -e&quot;USE db1;SELECT * FROM test;&quot;+------+| id   |+------+|    1 |+------+[root@slave1 ~]# mysql -u root -p123456 -e&quot;USE db2;SELECT * FROM test;&quot;+------+| id   |+------+|    1 |+------+[root@slave ~]#
(2) 从服务器SQL_THREAD在relay log中的事件时，仅读取与特定数据库(特定表)相关的事件并应用于本地
缺点：会造成网络及磁盘IO浪费,在所有从节点都要配置
优点: 不影响二进制备份还原
从服务器上的复制过滤器相关变量
白名单和黑名单二选一，复制库和复制表，以及通配符类型三选一（白名单和黑名单二选一），不然会出现数据不一致的情况
replicate_do_db=db1 #指定复制库的白名单，选项不支持多值,只能分别写多行实现replicate_do_db=db2 #指定复制库的白名单，选项不支持多值,只能分别写多行实现replicate_ignore_db=db4  #指定复制库黑名单replicate_do_table=db1.test #指定复制数据库中的表的白名单replicate_ignore_table=db2.test #指定复制数据库中表的黑名单replicate_wild_do_table= foo%.bar% #支持通配符 只复制数据库名以 foo 开头，表名以 bar 开头的表replicate_wild_ignore_table=db1.tbl_% #忽略 db1 数据库中所有以 tbl_ 开头的表的复制

#删除master的第一步的配置，保留如下[mysqld]server-id=128log-bin=/data/logbin/qylog#然后重启systemctl restart mysqld
在所有的从节点，注意是所有的从节点
[mysqld]replicate_do_db=db2 replicate_do_db=db1#这次我不加黑名单，我看会不会同步db3#然后重启systemctl restart mysqld
master创建测试数据
mysql -u root -p123456#db1创建tes1表，以及在原来的test表插入新数据USE db1;CREATE TABLE test1 ( id INT);INSERT INTO test1 (id) VALUES (1);SELECT * FROM test1;USE db1;INSERT INTO test (id) VALUES (2);SELECT * FROM test;#在db2原来的test插入新数据，且创建新表USE db2;CREATE TABLE test1 ( id INT);INSERT INTO test1 (id) VALUES (1);SELECT * FROM test1;USE db2;INSERT INTO test (id) VALUES (2);SELECT * FROM test;#在db3创建新表USE db3;CREATE TABLE test ( id INT);INSERT INTO test (id) VALUES (1);SELECT * FROM test;USE db3;INSERT INTO test (id) VALUES (2);SELECT * FROM test;
slave查看从节点
USE db1;SELECT * FROM test1;USE db1;SELECT * FROM test;USE db2;SELECT * FROM test1;USE db2;SELECT * FROM test;use db3;#都是可以成功同步数据，只有db3是没有同步过来的(root@localhost) [(none)]&gt; USE db1;SELECT * FROM test1;+------+| id   |+------+|    1 |+------+1 row in set (0.00 sec)(root@localhost) [db1]&gt; USE db1;SELECT * FROM test;+------+| id   |+------+|    1 ||    2 |+------+2 rows in set (0.00 sec)(root@localhost) [db1]&gt; USE db2;SELECT * FROM test1;+------+| id   |+------+|    1 |+------+1 row in set (0.00 sec)(root@localhost) [db2]&gt; USE db2;SELECT * FROM test;+------+| id   |+------+|    1 ||    2 |+------+2 rows in set (0.00 sec)(root@localhost) [db2]&gt; use db3;ERROR 1049 (42000): Unknown database &#x27;db3&#x27;
GTID复制
全局复制Id
背景

💥 传统复制机制的问题（基于 binlog 位置点）

多个客户端（如图中）分别发起事务 1001、1002、1003，主库依次执行，写入 binlog。
主库通过只有一个的 dump 线程把 binlog 发送给从库。
从库通过 IO 线程读取 binlog，通过 SQL 线程 一个一个串行执行事务。

❗问题来了：

从库自身也有客户端访问，可能也会产生事务，编号也可能是 1001、1002、1003（编号只是应用内部的业务编号，不是数据库唯一标识）。
主库传过来的 binlog，只记录了操作内容、表名、数据改动，但没有全局唯一事务 ID。
所以从库根本分不清：这个 1001 是主库的？还是我自己产生的？
为了避免数据冲突、保证顺序一致，从库只能串行地照着 binlog 执行事务，而不能乱序执行或并发执行。


正是因为分不清，怕双方的事务乱序执行，所以才会选择串行执行，确保安全
而且如果一个事务特别特别慢，他会拖累整个串行执行的进度，效率慢

GTID（全局事务标识符） 从 MySQL 5.6 开始引入，建议使用 5.7 及以上版本，更稳定可靠。
在 MySQL 5.7 中，即使未开启 GTID，也会生成匿名 GTID，具备一定追踪能力。
GTID 复制的优势包括：

支持 master_auto_position=1，无需指定 binlog 文件和 POS 点，主从复制更简化。


就是你那个主从复制连接里，不用记录了是哪个二进制文件和起点了，有了这个参数就可以自动识别，前提是主从的数据库要一致


实现了事务级的幂等性，重复执行事务不会出错。
配合 5.6 的库级并行复制和 5.7 的事务级并行复制（逻辑时钟机制），显著提升复制并发性能，降低延迟。
可安全支持多个 dump 线程 + 多 SQL 线程 并发复制。

GTID 优点:

保证事务全局统一
截取日志更加方便。跨多文件，判断起点终点更加方便
判断主从工作状态更加方便
传输日志，可以并发传输。SQL回放可以更高并发
主从复制构建更加方便


GTID = server_uuid:transaction_id，在一组复制中，全局唯一
server_uuid 来源于 /var/lib/mysql/auto.cnf

GTID服务器相关选项
gtid_mode=ON #gtid模式enforce_gtid_consistency=ON #保证GTID安全的参数
测试案例：前提是主从的数据库要一致，并且创建好复制传输的账号
#如果主服务器和从服务器数据不一致,需要先将主库数据备份还原至从库,再执行下面操作
mysqldump -uroot -p123456 -A -F -E -R --triggers --single-transaction \--source-data=1 --flush-privileges --hex-blob --default-character-set=utf8 &gt; all.sql#自行复制的从节点，还原的步骤我就不说了
1.主节点
vim /etc/my.cnf#保留如下[mysqld]server-id=128log-bin=/data/logbin/qyloggtid_mode=ONenforce_gtid_consistency=ON#然后重启mysqlsystemctl restart mysqld
2.从节点（slave1和slave2都一样）
vim /etc/my.cnfserver-id=（从节点的唯一id）log-bin=/data/logbin/qylogrelay_log=/data/relaylog/relay-logrelay_log_index=/data/relaylog/relay-log.indexgtid_mode=ONenforce_gtid_consistency=ON#然后重启mysqlsystemctl restart mysqld
3.从节点重置前面实验的slave复制连接
#slave1和slave2都执行stop slave;reset slave all;CHANGE MASTER TO MASTER_HOST=&#x27;192.168.48.128&#x27;, MASTER_USER=&#x27;repluser&#x27;, MASTER_PASSWORD=&#x27;123456&#x27;, get_master_public_key=1,MASTER_AUTO_POSITION=1; #使用GTIDstart slave;show slave status\G;#然后就是双yes
4.查看是否成功
主节点执行一个操作，我这里就执行创建一个db4数据库(root@localhost) [(none)]&gt; create database db5;Query OK, 1 row affected (0.05 sec)(root@localhost) [(none)]&gt; SHOW MASTER STATUS\G*************************** 1. row ***************************             File: qylog.000004         Position: 339     Binlog_Do_DB: Binlog_Ignore_DB:Executed_Gtid_Set: 363255e7-6ebc-11f0-80de-000c293bf782:1#这里就可以看到gtid了1 row in set (0.00 sec)#然后从节点(root@localhost) [(none)]&gt; show slave status\G;......           Retrieved_Gtid_Set: 363255e7-6ebc-11f0-80de-000c293bf782:1            Executed_Gtid_Set: 363255e7-6ebc-11f0-80de-000c293bf782:1  #是不是和前面主节点一样......
至此gtid成功
主从复制bug集锦
1.主从复制在从节点你可能会看见
show slave status\G;
Last_IO_Error: error connecting to master &#x27;repluser@192.168.48.11:3306&#x27; - retry-time: 60 retries: 1 message: Authentication plugin &#x27;caching_sha2_password&#x27; reported error: Authentication requires secure connection.
原因：MySQL 8 默认使用 caching_sha2_password 作为认证插件，这种认证方式需要安全连接（SSL/TLS）。需要生成证书，如果不想生成证书，你自己现在从节点进行一次登入
如果你是级联最后的slave，主节点的地址要指向中间节点
在这种情况下，服务器将RSA公钥发送给客户端，后者使用它来加密密码并将结果返回给服务器。插件使用服务器端的RSA私钥解密密码，并根据密码是否正确来接受或拒绝连接。
mysql -urepluser -p123456 -h192.168.48.128 --get-server-public-key
然后重启slave复制线程
stop slave;start slave;show slave status\G;
以上只是临时的解决方案，还有一个办法就是
加这个参数get_master_public_key=1
如果这个不行就换get_source_public_key=1
CHANGE MASTER TO MASTER_HOST=&#x27;192.168.48.128&#x27;, MASTER_USER=&#x27;repluser&#x27;, MASTER_PASSWORD=&#x27;123456&#x27;, MASTER_LOG_FILE=&#x27;qylog.000001&#x27;, MASTER_LOG_POS=157,get_master_public_key=1,MASTER_DELAY=10; 
2.在主主复制的时候，我知道不是设置了延迟复制为10秒吗，就是那个主从复制的连接最后一项
但是情况是：
第一个主节点插入数据成功复制到从节点
但是在第二个主节点，插入数据之后，就出现l下面的情况
SQL_Remaining_Delay: 3995
Slave_SQL_Running_State: Waiting until SOURCE_DELAY seconds after source executed event           Master_Retry_Count: 86400
问了ai
你设置了延迟复制 MASTER_DELAY = 10；从库显示 SQL_Remaining_Delay: 3995，说明它在“等事件执行时间到了再处理”；这个是正常现象，不是错误；延迟时间这么长，可能是主节点的系统时间比从节点慢；
结果你知道吗，真的服了，两个主机的时间不一致
[root@slave ~]# dateFri Aug  1 03:06:40 CST 2025[root@master ~]# dateFri Aug  1 02:00:20 AM CST 2025#但是是我的从节点时间有问题，我更新了一下时间
复制的监控和维护
清理日志
PURGE &#123; BINARY | MASTER &#125; LOGS  &#123; TO &#x27;log_name&#x27; | BEFORE datetime_expr &#125;RESET MASTER TO # #mysql 不支持RESET SLAVE [ALL]
复制监控
SHOW MASTER STATUSSHOW BINARY LOGSSHOW BINLOG EVENTSSHOW SLAVE STATUSSHOW PROCESSLIST
从服务器是否落后于主服务
Seconds_Behind_Master：0
如何确定主从节点数据是否一致
percona-toolkit
数据不一致如何修复
删除从数据库，重新复制
复制的问题和解决方案
数据损坏或丢失

Master：MHA + semisync replication
Slave： 重新复制

不惟一的 server id
重新复制
复制延迟

升级到MySQL5.7以上版本(5.7之前的版本，没有开GTID之前，主库可以并发事务，但是dump传输时是串行)利用GTID(MySQL5.6需要手动开启,MySQL5.7以上默认开启)支持并发传输binlog及并行多个SQL线程
减少大事务,将大事务拆分成小事务
减少锁
sync_binlog=1 加快binlog更新时间,从而加快日志复制
需要额外的监控工具的辅助
一从多主：Mariadb10 版后支持
多线程复制：对多个数据库复制

MySQL 主从数据不一致
造成主从不一致的原因

主库binlog格式为Statement，同步到从库执行后可能造成主从不一致。
主库执行更改前有执行set sql_log_bin=0，会使主库不记录binlog，从库也无法变更这部分数据。
从节点未设置只读，误操作写入数据
主库或从库意外宕机，宕机可能会造成binlog或者relaylog文件出现损坏，导致主从不一致
主从实例版本不一致，特别是高版本是主，低版本为从的情况下，主数据库上面支持的功能，从数据库上面可能不支持该功能
主从sql_mode 不一致
MySQL自身bug导致

主从不一致修复方法

将从库重新实现

虽然这也是一种解决方法，但是这个方案恢复时间比较慢，而且有时候从库也是承担一部分的查询操作的，不能贸然重建。

使用percona-toolkit工具辅助

PT工具包中包含pt-table-checksum和pt-table-sync两个工具，主要用于检测主从是否一致以及修复数据不一致情况。这种方案优点是修复速度快，不需要停止主从辅助，缺点是需要知识积累，需要时间去学习，去测试，特别是在生产环境，还是要小心使用
关于使用方法，可以参考下面链接：https://www.cnblogs.com/feiren/p/7777218.html

手动重建不一致的表

在从库发现某几张表与主库数据不一致，而这几张表数据量也比较大，手工比对数据不现实，并且重做整个库也比较慢，这个时候可以只重做这几张表来修复主从不一致这种方案缺点是在执行导入期间需要暂时停止从库复制，不过也是可以接受的
范例：A,B,C这三张表主从数据不一致
1、从库停止Slave复制mysql&gt;stop slave;2、在主库上dump这三张表，并记录下同步的binlog和POS点mysqldump -uroot -pmagedu -q --single-transaction --master-data=2 testdb A B C &gt;/backup/A_B_C.sql3、查看A_B_C.sql文件，找出记录的binlog和POS点head A_B_C.sql例如:MASTERLOGFILE=&#x27;mysql-bin.888888&#x27;, MASTERLOGPOS=666666;#以下指令是为了保障其他表的数据不丢失，一直同步直到那个点结束，A,B,C表的数据在之前的备份已经生成了一份快照，只需要导入进入，然后开启同步即可4、把A_B_C.sql拷贝到Slave机器上，并做指向新位置mysql&gt;start slave until MASTERLOGFILE=&#x27;mysql-bin.888888&#x27;, MASTERLOGPOS=666666;5、在Slave机器上导入A_B_C.sqlmysql -uroot -pmagedu testdb mysql&gt;set sql_log_bin=0;mysql&gt;source /backup/A_B_C.sqlmysql&gt;set sql_log_bin=1;6、导入完毕后，从库开启同步即可。mysql&gt;start slave;
如何避免主从不一致

主库binlog采用ROW格式
主从实例数据库版本保持一致
主库做好账号权限把控，不可以执行set sql_log_bin=0
从库开启只读，不允许人为写入
定期进行主从一致性检验


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx入门笔记</title>
    <url>/posts/c0d89ad/</url>
    <content><![CDATA[
Nginx入门笔记
Nginx介绍
Nginx是一个高性能的HTTP、反向代理服务器
主要功能：

反向代理
实现集群和负载均衡
静态资源虚拟化

Nginx的版本：


Nginx开源版 http://nginx.org/en/
官方原始的Nginx版本


Nginx plus商业版
开箱即用，集成了大量功能


Open Resty https://openresty.org/cn/
OpenResty是一个基于Nginx与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。更适用于需要大量二次开发的场景，有极强的扩展性


Tengine https://tengine.taobao.org/
由淘宝网发起的Web服务器项目。它在Nginx的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。Tengine的性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的检验。相比于Open Resty，扩展性不够强，但是能够满足绝多数使用场景


什么是代理
正向代理
正向代理可以理解为「客户端」的代理

反向代理
反向代理可以理解为「服务器」的代理

Nginx安装
以下有两种方式，按需选择，编译会慢一点，但是可以离线安装，不需要从网络下载所需的包，Rpm安装全部安装都需要网络。两个都差不多（我接下来测试全部用编译安装）
编译安装
不限制;Linux系统
这里我就用OpenEuler22.03LTS做演示，命令都和CentOS差不多，不做过多解释
1.安装必要工具
yum install -y tar gcc make pcre pcre-devel zlib zlib-devel openssl openssl-devel
2.下载源码包
官网：nginx: download

你可以右键复制链接在虚拟机下载
wget https://nginx.org/download/nginx-1.26.2.tar.gz
也可以直接点击红色框框的下载下来然后上传到虚拟机（root用户一般都上传到/root）去

3.创建nginx安装目录并解压安装包
mkdir -p /etc/nginx/#解压并移动到/app/nginxtar -zxf nginx-1.26.2.tar.gz#进入安装目录cd nginx-1.26.2# 配置nginx安装路径./configure --prefix=/etc/nginx/# 编译&amp;&amp;安装make &amp;&amp; make install
4.设置守护进程实现自启动
cat &gt; /etc/systemd/system/nginx.service &lt;&lt;&quot;EOF&quot;[Unit]Description=nginxAfter=network.target remote-fs.target nss-ookup.target[Service]Type=forkingPIDFile=/etc/nginx/logs/nginx.pidExecStartPre=/etc/nginx/sbin/nginx -t -c /etc/nginx/conf/nginx.confExecStart=/etc/nginx/sbin/nginx -c /etc/nginx/conf/nginx.confExecReload=/etc/nginx/sbin/nginx -s reloadExecStop=/etc/nginx/sbin/nginx -s stopExecQuit=/etc/nginx/sbin/nginx -s quitPrivateTmp=true[Install]WantedBy=multi-user.targetEOF# 重载配置systemctl daemon-reload# 加入自启systemctl enable nginx# 启动nginxsystemctl start nginx#将nginx可执行文件添加到$PATH，这样才能全局使用nginx命令cat &gt;&gt; /etc/profile &lt;&lt; &quot;EOF&quot;export PATH=$PATH:/etc/nginx/sbinEOFsource /etc/profile
5.访问测试页
访问你虚拟机的ip就行

到这编译版nginx安装成功
普及一下nginx命令：
systemctl start nginx 启动nginx
nginx -s reload 重载nginx
RPM安装
有两个系统的教程，我下面有标注
官网文档：nginx: Linux packages

Centos7或Centos8
yum install yum-utils -y#添加nginx源cat &gt;/etc/yum.repos.d/nginx.repo &lt;&lt; &quot;EOF&quot;[nginx-stable]name=nginx stable repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=1enabled=1gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=true[nginx-mainline]name=nginx mainline repobaseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/gpgcheck=1enabled=0gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=trueEOFyum install nginx -ysystemctl enable nginx --now
OpenEuler系统
因为没有适配openeuler的版本，这里直接用centos8的来代替，兼容的没关系，你可以根据你的系统去自行替换
原本是$releasever的，但是没有openeuler的版本直接用8来代替也就是centos8，openeuler兼容centos
cat &gt;/etc/yum.repos.d/nginx.repo &lt;&lt; &quot;EOF&quot;[nginx-stable]name=nginx stable repo#baseurl=http://nginx.org/packages/centos/$releasever/$basearch/baseurl=http://nginx.org/packages/centos/8/$basearch/gpgcheck=1enabled=1gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=true[nginx-mainline]name=nginx mainline repo#baseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/baseurl=http://nginx.org/packages/mainline/centos/8/$basearch/gpgcheck=1enabled=0gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=trueEOFyum install nginx -ysystemctl enable nginx --now
访问测试页
访问你虚拟机的ip就行

关闭防火墙
systemctl disable firewalld &amp;&gt; /dev/nullsystemctl stop firewalldsed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsetenforce 0#重启服务器reboot
Nginx目录

conf #配置文件	｜-nginx.conf # 主配置文件	｜-其他配置文件 # 可通过那个include关键字，引入到了nginx.conf生效	html #静态页面logs（默认不是放在这里，可以在配置文件中修改为这里）	｜-access.log #访问日志(每次访问都会记录)	｜-error.log #错误日志	｜-nginx.pid #进程号	sbin	｜-nginx #主进程文件	*_temp #运行时，生成临时文件
Nginx进程模型
一个Master：监听请求，并分配worker进程处理
默认一个worker进程（可以在配置文件中修改worker数量）：处理客户端请求

每个worker之间彼此独立，每一个worker处理多个请求

Nginx配置文件
Nginx配置生成工具
nginx.conf默认配置文件
这是nginx默认的配置文件
展示部分核心配置，后续更多配置会在后面进行慢慢升华
[root@localtion ~]# vim /etc/nginx/conf/nginx.conf# Nginx 主配置文件# master进程会启动worker进程，该选项设置在系统中显示启动该进程的用户名（一般不改动，默认nobody）# user nobody# 设置工作进程的数量，通常设置为自动或与CPU核心数相同worker_processes  1;     #默认为1，表示开启一个业务进程# 错误日志放置的路径 notice、info是错误日志的级别，比如：info就是日志级别大于info才生成日志# 默认地址为/var/log/nginx/error.log ，可通过nginx -V返回的--eror-log-path字段获取实际值#error_log  logs/error.log;#error_log  logs/error.log  notice;#error_log  logs/error.log  info;# pid文件存放路径,默认：/var/run/nginx/nginx.pid，可通过nginx -V返回的--pid-path字段获取实际值#pid        logs/nginx.pid;# 事件驱动模块配置events &#123;    # 每个工作进程的最大连接数    worker_connections  1024;   # 单个业务进程可接受连接数&#125;# HTTP 模块配置http &#123;    # 引入 mime.types 文件，用于定义文件扩展名与MIME类型的映射关系    include       mime.types;      # 默认MIME类型，当请求的文件类型不在mime.types中定义时使用    default_type  application/octet-stream;         # 访问日志格式    #log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;    #                  &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;    #                  &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;    # 访问日志地址，默认：/var/log/nginx/access.log，可通过nginx -V返回的--http-log-path字段获取实际值    #access_log  logs/access.log  main;        # 是否开启sendfile传输文件，开启可以提高效率    sendfile        on;       # 连接超时时间    keepalive_timeout  65;    # 服务器模块配置    server &#123;        # 监听80端口        listen       80;        # 服务器名称        server_name  localhost;  #可以设置主机名和域名        # 根目录配置        location / &#123;            root   html; # 设置网站根目录            index  index.html index.htm; # 默认首页文件        &#125;        # 错误页面配置        error_page   500 502 503 504  /50x.html;        # 定义50x.html错误页面的位置        location = /50x.html &#123;            root   html;        &#125;    &#125;&#125;
刚刚访问安装成功后的测试页
测试有的index文件在/etc/nginx/html/index.html
一般所有的网页的文件都放在nginx安装目录下的html下
这里nginx的根目录/，监听的端口是80

location / &#123;    root   html; # 设置网站根目录    index  index.html index.htm; # 默认首页文件&#125; # 根目录配置    location / &#123;        root   html; # 设置网站根目录        index  index.html index.htm; # 默认首页文件    &#125;

每次修改配置文件要重载配置
nginx -s reload
sendfile配置
在高负载情况下，启用Nginx的sendfile功能可以减少CPU和内存的使用，提高服务器性能，因为它允许数据直接从磁盘传输到网络，无需先加载到Nginx的内存中。这不仅降低了CPU使用率，还减少了内存占用，从而提高了I/O效率和服务器的吞吐量，并且降低了响应延迟。
http&#123;	sendfile:on # off&#125;# 或者指定某个server开启server &#123;    location / &#123;        sendfile on;        ...    &#125;&#125;

gzip配置
http&#123;   gzip on; # 开启压缩，压缩后发送给客户端	gzip_min_length 1;# 设置最小压缩下限。1就是小于1字节的文件不压缩	gzip_comp_level 3 # 压缩级别0-9，值越大文件就压缩的越小，相应的会损耗更多性能	gzip_type text/plain application/javascript image/* # 指定哪些 MIME 类型，开启压缩（不写默认全部），可以使用通配符 image/* 就是所有图片。具体哪些类型可以看conf/mime.types文件&#125;
nginx 中的 gzip 压缩分为动态压缩、静态压缩


动态压缩：服务器给客户端返回响应时，消耗自身的资源进行实时压缩，保证客户端拿到 gzip 格式的文件
gzip on开启的就是动态压缩，gzip_comp_level设置的级别高，可能会造成CPU占用过高（文章：简单一招竟把nginx服务器性能提升50倍）


静态压缩：直接将预先压缩过的 .gz 文件返回给客户端，不再实时压缩文件，如果找不到 .gz 文件，会使用对应的原始文件
该功能需要模块： ngx_http_gzip_static_module（默认不会被构建）
我们可以通过下面命令查看，当前安装的是否包含该模块


我们可以通过下面命令查看，当前安装的是否包含该模块
nginx -V

如若没有，则要重新编译：
cd nginx-1.26.2/./configure --prefix=/etc/nginx --with-http_gzip_static_module# 指定编译配置，这个参数安装模块`ngx_http_gzip_static_module` 一定要指定你的安装目录make # 编译make install # 安装
启用以下配置
http &#123;    gzip  on;&#125;
记得重载配置
nginx -s reload
Server配置
虚拟主机配置（可以启用多个），多个server字段，会根据请求的域名+端口从前向后匹配
以下是例子：
如果是公网：qianyios.top这个域名要解析到这个这个服务器的公网ip
如果是内网：这个域名要在本机进行hosts匹配映射ip qianyios.top
http &#123;......    # 虚拟主机(相当于一样站点)    server &#123;        # 监听80端口        listen       80;        # 服务器名称        #server_name  *.qianyios.top; 当你配置了通配符域名，就是不管你是www.qianyios.top还是xxx.qianyios.top,等各种前缀都能访问到这里                server_name  qianyios.top;  #可以设置主机名和域名，如果有多个，用空格隔开，支持通配符        #访问的时候就是域名+端口就可以访问到这个server下的网站                #我会在这里插入例子，下面我会展示例子！！！    &#125;&#125;
【例子1】当访问http://qianyios.top:80就会访问以下页面（也就是nginx欢迎页），这里只是例子，你可以根据你自己情况匹配
/  这个路径很特殊，只要用户访问的地址qianyios.top:80后面没有加什么被其他location定位到的话都会来到这个
http &#123;......    server &#123;        listen       80;        server_name  qianyios.top;         location / &#123;            root   html; # 设置网站根目录            index  index.html index.htm; # 默认首页文件            #当访问的时候会在nginx目录下的html/下寻找index.html index.htm;        &#125; &#125;
【例子2】当访问http://qianyios.top:80/abc/就会访问以下页面，一个server可以存在多个location，但是其定位的路径不要出现相同的位置。比如：有两个location /abc&#123;内容&#125;
注意有两个误区
①如果root是/home 这里实际访问路径是/home/abc/index.html
②如果root是/home/abc 这里实际访问路径是/home/abc/abc/index.html
其实root的功能就是将qianyios.top:80定义为root（网站根目录），然后访问地址后面的/abc就会加在实际访问路径里
http &#123;......    server &#123;        listen       80;        server_name  qianyios.top;          location / &#123;            root   html; # 设置网站根目录            index  index.html index.htm; # 默认首页文件            #当访问的时候会在nginx目录下的html/下寻找index.html index.htm;        &#125;                        location /abc &#123;        #这个/abc下的网站根目录可以不用在nginx目录下，你可以定义其他位置            root   /home; # 设置定位/abc站点以/home为网站根目录            index  index.html index.htm; # 默认首页文件            #当访问的时候会在/home目录下寻找index.html index.htm;        &#125; &#125;

mkdir /home/abcecho &quot;welccome to /home/abc&quot; &gt;&gt; /home/abc/index.htmlnginx -s reloadhttp://qianyios.top:80/abc
这里又会有两个报错

我们要去找index.html文件只有像以下输入命令才会能找得到
[root@localtion ~]# ls /home/abc/index.html
如果你只是访问http://qianyios.top:80/abc实际访问路径是/home/abc，但是我认为nginx会因为abc是个文件，我们本身的目的就是要去abc下去找index.html,所以最后要加个/
但是呢在宿主机的浏览器实际访问地址是http://192.168.48.101/abc（这里是我的宿主机，我没做hosts映射，我就用ip了，我只在虚拟机做了映射一样的。）注意，这里我最后没加斜杠它自动给我加上了。

总结：我搞不懂两者怎么回事，可能linux不会自动加上吧，浏览器会，就是说如果在虚拟机访问要加/。为了规范就是不管在哪都在配置文件加上末尾的斜杠
【例子3】
一般静态文件会用alias,比如说/var/www/images/下有个图片

当我设置了以下配置
location /images/ &#123;	# alias设置请求的别名，用于替换文件系统路径。       alias /var/www/images/;&#125;
重载配置之后
访问http://192.168.48.101/images/1.png

结论：
访问地址：192.168.48.101/images/1.png
实际访问地址：/var/www/images/1.png
同类型的案例再来一个
location /abc/ &#123;   alias   /home/abc/;    index  index.html index.htm; # 默认首页文件   #当访问的时候会在/home目录下寻找index.html index.htm;&#125;

【例子4】
精确匹配（顾名思义不做过多解释了）
访问：http://qianyios.top/50x.html
location = /50x.html &#123;            root   html;        &#125;
【例子5】

~：大小写敏感（正则表达式）
= : 精确匹配（必须全部相等）
~*：忽略大小写（正则表达式），这里要注意忽略大小写的意思是请求的字符大小写都可以， 但是不会进行大小转换，请求的大小写对应的文件必须存在。
^~ ：只需匹配uri部分
@ ：内部服务跳转

#1.精确匹配 location = /index.html &#123;  root /etc/nginx/html;&#125;# 则匹配到http://192.168.48.101/index.html这种请求。#2.大小写铭感匹配 location ~ /ABC/ &#123;    [ configuration ] &#125; #请求示例 #http://qianyios.top/ABC/ [成功] #http://qianyios.top/abc/ [失败]#3.大小写不敏感匹配location ~* /abc.html &#123;    [ configuration ]&#125;# 则会忽略 uri 部分的大小写#http://qianyios.top/ABC.html [成功] 可以成功匹配，但是目录中要ABC.html文件#http://qianyios.top/abc.html [成功] 可以成功匹配，但是目录中要abc.html文件#4.指定后缀匹配location ~* \.(gif|jpg|jpeg|png)$ &#123; root /var/www/images;&#125;#http://qianyios.top/1.png [成功]#5.忽略正则匹配location ^~ /images/ &#123;   alias /var/www/images;&#125;#以 /img/ 开头的请求，都会匹配上#http://qianyios.top/images/1.jpg  [成功]#http://qianyios.top/images/1.png [成功]
注意：如果配置了#5，那么所有url里请求 /images/ 下的图片会被上面#5的处理，因为 ^~ 指令匹配到了/image/，则不检查#4正则表达式。对比这两个location，可以设置不同目录，相同文件进行实验。
反向代理和正向代理
反向代理

根据以上的代码可以知道，我们设置三个server，分别表示3个网站，他们都有对应的域名，然后通过配置反向代理（右边的配置文件）实现反向代理。即：当用户访问user1.com，nginx就会将请求转到发到https://website1.com上。对应的后端服务器web1接收到请求并返回响应给 Nginx，Nginx 接收到来自后端服务器的响应，并将其返回给用户。

对于 user1.com 的请求，Nginx 将其转发到 http://website1.com/。
对于 user2.com 的请求，Nginx 将其转发到 http://website2.com/。
对于 user3.com 的请求，Nginx 将其转发到 http://website3.com/。

图片中说的无法跳过nginx去直接访问后端服务器，说法太死了，只能说是个例，如果项目本身就不想让你知道后端的地址，只想让你通过nginx来进行访问，方便管理，举个例子，就好像baidu.com    总不能说baidu.com只绑定到一个机子上吧，他肯定会有一个庞大的负载均衡以及反向代理集群，去均衡负载分散流量等操作，如果只绑定到一个机子，也承受不住每天几百万的访问下面会讲到负载均衡。就好像上面的图片的例子，我的后端三个都是百度的网站，有三个不同的域名，但是不可能让用户去记住三个域名吧，所以有个nginx去做反向代理，然后设置一个baidu.com去代理这三个web也可以做到反向代理
正向代理
用户需要通过代理服务器去访问外网。这个代理服务器可以是其他工具不一定是nginx

总结：简单来说局域网访问互联网就是正向代理,互联网访问局域网就是反向代理.这些都是隧道式代理，进出都要经过代理服务器
负载均衡
负载均衡策略
轮询
默认情况下使用轮询方式，逐一转发，这种方式适用于无状态请求。(在无参数情况下平均分配所有请求)

http &#123;#upstream和server是同一级别，都包含在http内    upstream qianyi &#123;        server website1.com   weight=10 down;  #每十次请求之后轮询到下一个        server website2.com   weight=1;        server website3.com   weight=1 backup;    &#125;    server &#123;        listen 80;	server_name qianyios.top        location / &#123;            proxy_pass http://qianyi;#名字随意，要和upstream后的名字一样        &#125;    &#125;&#125;

down：表示当前的server暂时不参与负载
weight：默认为1.weight越大，负载的权重就越大。
backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。

ip_hash
对用户的ip进行计算：Hash(IP)%upstream_node_count，返回要使用机器的索引
每一个用户会固定分配到一台机器，防止在A机器上创建Session的用户，后续被分配到其他机器，导致Session失效
开启ip_hash后，如果想要移除一台server，必须使用down配置。如果直接删除，会导致upstream_node_count变化，使得所有用户访问的访问的机器发生变化
缺点：

增加服务节点会导致upstream_node_count变化，进而导致所有用户访问的机器变化
某个用户短时间发起大量请求，会打到一台固定的机器，导致这台机器性能大幅下降，而其他机器可能还是空闲

upstream test_server&#123;    # 开启ip_hash，    ip_hash;    		server web1.com:80;		server web2.com:80;&#125;
least_conn
尽可能将请求转发到当前连接数最少的后端服务器
upstream test_server&#123;    least_conn;    		server web1.com:80;		server web2.com:80;&#125;
下面例子，开启least_conn，Nginx会优先转发到Tomcat3

url_hash
根据用户访问的url定向转发请求
fair
根据后端服务器响应时间转发请求
总结：当我们访问http://qianyios.top时在upstream无参数的情况下，这三个网页都会平均的访问，且地址栏里的qianyios.top不会变
其次有参数时，都会按照参数的性质，进行轮询访问
动静分离


静：前端项目（静态资源）


动：接口服务


域名A.com访问到A项目、B.com访问到B项目
Api.com访问接口服务
将3个域名都解析到Nginx所在机器


# 前端server &#123;        listen       80;        server_name  A.com;                location ~ &#123;              root /websit/xxx; # A项目目录            index  index.html index.htm;        &#125;        error_page   500 502 503 504  /50x.html;        location = /50x.html &#123;            root   html;        &#125;&#125;server &#123;        listen       80;        server_name  B.com;                location ~ &#123;              root /websit/xxx; # B项目目录            index  index.html index.htm;        &#125;        error_page   500 502 503 504  /50x.html;        location = /50x.html &#123;            root   html;        &#125;&#125;# 接口server &#123;        listen       80;        server_name  Api.com;					# 接口	location  &#123;        proxy_pass http://接口机器的IP:端口;        &#125;        error_page   500 502 503 504  /50x.html;        location = /50x.html &#123;            root   html;        &#125;&#125;
location ~* /(js|css|img)&#123;	root html;  index  index.html index.htm;&#125;#   ~* 表示这是一个不区分大小写的正则表达式匹配。#   /(css|img|js) 意味着这个location块将匹配所有以/css、/img或/js结尾的URL路径。#  Nginx会在服务器的文件系统中对应的html目录下查找文件。
URL重写
return
把Http重定向为Https
server &#123;        listen       80;        server_name  www.qianyios.com;		location / &#123; 			return 302 https://www.qianyios.com$request_uri 			# 302是状态码			# $request_uri是路径和参数 ，例如：/xxx/xx?xx=xx        &#125;&#125;
rewrite
rewrite是URL重写的关键指令，根据regex（正则表达式）部分内容，重定向到replacement，结尼是flag标记。rewrite    &lt;regex&gt;   &lt;replacement&gt;  [flag];关键字				正则				替代内容     flagt标记正则：per1森容正则表达式语句进行规则匹配替代内容：将正则匹配的内容替换成replacementflag标记说明：last  #本条规则匹配完成后，继续向下匹配新的1ocation URI规则break #本条规则匹配完成即终止，不再匹配后面的任何规则redirect #返回302临重定向，游览器地址会显示跳转后的URL地址permanent #返回301永久重定向，测览器地址栏会显示跳转后的URL地址
把Http重定向为Https
server &#123;        listen       80;        server_name  www.qianyios.com;		location / &#123; 		   rewrite ^/(.*) https://www.qianyios.top.com/$1 redirect;		   # 匹配到uri的/后的内容，并放到$1中，执行重定向        	  proxy_pass http://xxx;        &#125;&#125;
当你尝试访问http://www.qianyios.top/1.html时，由于上述rewrite指令的存在，你的浏览器实际上会被引导至https://www.qianyios.top.com/1.html
上述命令改成return会更高效
return 301 https://www.qianyios.top.com$request_uri;
实例：
rewrite ^/([0-9]+).html$  /index.jsp?pageNum=$1 break;
假设我们的真实地址是192.168.48.101/index.jsp?pageNum=12   但是我不想客户知道真实地址
配置了这个规则之后，他就会变成192.168.48.101/12.html
我们访问192.168.48.101/12.html（但是12.html是不存在的）之后，他会把流量转发到192.168.48.101/index.jsp?pageNum=12（真实地址）
Nginx网关服务器
企业中，无论是前端页面、静态资源、接口，都是通过Nginx进行访问（使用proxy_pass），这时候这台Nginx服务器就成为了网关服务器（承担入口的功能）

所以，我们启动web服务器的防火墙，设置其只能接受这台Nginx服务器的请求
systemctl start firewalld
添加rich规则
#这里的192.168.48.101是网关服务器(nginx)地址firewall-cmd --permanent --add-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;192.168.48.101&quot; port protocol=&quot;tcp&quot; port=&quot;8080&quot; accept&quot; 

firewall-cmd：这是用来与firewalld进行交互的命令行工具。
--permanent：表示这个更改会永久生效，即使系统重启后也会保留。不过请注意，为了使永久性规则立即生效，您需要重新加载防火墙配置。
--add-rich-rule=&quot;规则&quot;：这是用来添加一条富规则到firewalld的参数。富规则允许更复杂的条件和操作。

在引号内的部分定义了具体的规则：

rule family=&quot;ipv4&quot;：指定这条规则适用于IPv4协议。
source address=&quot;192.168.48.101&quot;：规则只适用于源地址为192.168.48.101的流量。
port protocol=&quot;tcp&quot; port=&quot;8080&quot;：这条规则针对的是TCP协议，并且仅限于端口号为8080的连接。
accept：这表示如果所有上述条件都满足，则接受（允许）该网络流量。

配置完之后重启firewalld
systemctl restart firewalld
这时候，原本web开启了防火墙之后，不管访问web地址还是nginx服务器都无法访问到我们的web
且nginx配置proxy_pass http://192.168.48.105:8080反向代理到web
但是，我们在web端配置了防火墙规则之后，让web同意接收来自nginx的192.168.48.101的请求，且自身开放8080端口，这样我们访问192.168.48.101（这是nginx的地址反向代理到了web）就可以访问web了
移除rich规则
#这里的192.168.48.101是网关服务器(nginx)地址firewall-cmd --permanent --remove-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;192.168.48.101&quot; port protocol=&quot;tcp&quot; port=&quot;8080&quot; accept&quot; 
查看所有规则
firewall-cmd --list-all #所有开启的规则
防盗链配置
当我们请求到一个页面后，这个页面一般会再去请求其中的静态资源，这时候请求头中，会有一个refer字段，表示当前这个请求的来源，我们可以限制指定来源的请求才返回，否则就不返回，这样可以节省资源


valid_referers none|server_name
设置有效的refer值

none：不校验refer
server_name：校验refer地址是否为server_name（server_name可以使用通配符）

注意： if ($invalid_referer)中if后有个空格，不写就会报错
nginx: [emerg] unknown directive &quot;if($invalid_referer)&quot; in /usr/local/nginx/conf/nginx.conf:27
1.例子：这里设置nginx服务器中的img目录下的图片必须refer为https://blog.qianyios.top才能访问
server &#123;        listen       80;        server_name  localhost;	  location /img&#123;                valid_referers https://blog.qianyios.top/;                if ($invalid_referer)&#123;#无效的refere                        return 403;#返回状态码403                &#125;                root html;                index  index.html index.htm;        &#125;&#125;
这样其他人，不管是引用图片到他自己web或者直接在地址栏输入直接访问都不行

如果可以开放直接在地址栏输入直接访问可以加none
valid_referers none https://blog.qianyios.top/;
2.直接跳转到自定义图片
假设123.png是写有禁止盗取字样的图片
假设我的图片是http://qianyios.top/images/1.png
他会直接跳转到/var/www/image/123.png
location ^~ /image/ &#123;   root /var/www/;&#125;location ^~ /images/ &#123;          valid_referers qianyios.top;          if ($invalid_referer)&#123;#无效的refere             rewrite ^/images/(.*)$ /image/123.png break;             #return 302 /abc/123.png;          &#125;          root /var/www/;&#125;
^/images/(.*)$ 匹配所有以 /images/ 开头的路径，并捕获后续部分（即 (.*)）。
用curl工具进行访问
http://qianyios.top/images/1.png

带引用的访问
curl -e &quot;http://qianyios.top&quot; -I http://qianyios.top/images/1.png

高可用配置
背景
如下图，如果只有一个Nginx作为网关，一旦出现故障会导致全部服务不可用

高可用方案：

注意：Nginx主备机器配置要基本一致，如果配置相差较大，在切换时大量流量进入备用机，容易造成宕机
VRRP协议
keepalived是基于VRRP（Virtual Router Redundancy Protocol）协议的
VRRP可以将多个Nginx网关机器分为 master、backup两种类型，并生成一个VIP（虚拟IP:Virtual IP Address）
每台机器上的keepalived会相互通信，根据其他机器上的keepalived进程是否存在，判断服务器状态，如果默认的Master停止了，就会在剩下的Backup机器中，竞选出一台Nginx服务器作为Master
由Master服务器使用这个VIP，用户访问时，访问的是VIP
yum install -y keepalived
#keepalived的配置文件vim /etc/keepalived/keepalived.conf
nginx1的配置
! Configuration File for keepalivedglobal_defs &#123;   # keepalived邮件通知（可配置多个）   notification_email &#123;     acassen@firewall.loc     failover@firewall.loc     sysadmin@firewall.loc   &#125;   # 邮件发件人地址   notification_email_from Alexandre.Cassen@firewall.loc   # 邮件服务器（SMTP）地址   smtp_server 192.168.200.1   # 连接SMTP服务器的超时时间   smtp_connect_timeout 30   # 后面会提到   router_id lb1 # 路由id，可以随意取，但是要保证每个配置了keepalive的机器不重复就行      # vrrp相关配置，用的比较少   vrrp_skip_check_adv_addr   vrrp_strict   vrrp_garp_interval 0   vrrp_gna_interval 0&#125;# 节点名可以随意取，但要保证主、备节点之间保持一致即可vrrp_instance VI_1 &#123;    state MASTER  #主服务器    interface eth160 #vip到时候会生成在这个网卡下    virtual_router_id 51    priority 100  #优先级    advert_int 1  #检测间隔时间    authentication &#123;   #认证机制，这样可以区分实现不同作用的keepalived集群，不会混乱        auth_type PASS # 指定了认证类型为密码（PASS）        auth_pass 1111  # 设置了认证密码，这里设置的密码是&quot;1111&quot;    &#125;    virtual_ipaddress &#123;        192.168.48.200 #总ip    &#125;&#125;······其余配置······&#125;

authentication、virtual_router_id、virtual_ipaddress这几个一样的机器，才算是同一个组里。这个组才会选出一个作为Master机器

nginx2的配置
! Configuration File for keepalivedglobal_defs &#123;   router_id lb2&#125;vrrp_instance VI_1 &#123;    state BACKUP  #主服务器    interface eth160 #vip到时候会生成在这个网卡下    virtual_router_id 51    priority 50  #降低优先级    advert_int 1  #检测间隔时间    authentication &#123;   #认证机制，这样可以区分实现不同作用的keepalived集群，不会混乱        auth_type PASS # 指定了认证类型为密码（PASS）        auth_pass 1111  # 设置了认证密码，这里设置的密码是&quot;1111&quot;    &#125;    virtual_ipaddress &#123;        192.168.48.200 #总ip    &#125;&#125;······其余配置······&#125;
配置完成后，重启keepalived就可以实现高可用

假设我访问192.168.48.200就会访问到竞争到master的服务器，然后master是个nginx网关，将流量负载到Tomcat，假设master节点挂了，vip就会跳转backup的ens160上，这样访问192.168.48.200就会访问到backup的服务器，当master恢复，那vip就会跳回到master。

这里有keepalived实验过程
可以看一下
K8S高可用集群（内部etcd） | 严千屹博客

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenEuler-K8S高可用集群（外部etcd）</title>
    <url>/posts/2459/</url>
    <content><![CDATA[
OpenEuler-部署K8S高可用集群（外部etcd）
主机拓扑



主机名
ip1（NAT）
系统
磁盘
内存




master1
192.168.48.101
OpenEuler-22.03-LTS
100G
4G


master2
192.168.48.102
OpenEuler-22.03-LTS
100G
4G


master3
192.168.48.103
OpenEuler-22.03-LTS
100G
4G


node01
192.168.48.104
OpenEuler-22.03-LTS
100G
8G



镜像下载地址：OpenEuler-22.03-LTS
下载名为openEuler-22.03-LTS-SP4-x86_64-dvd.iso
基础配置
Openeuler通过单独安装，非克隆。安装完后进行基本环境的配置，配置一下几个方面：

设置主机名
关闭firewalld、dnsmasq、selinux
设置ens33
备份并新增、docker-ce源、k8s源
更新yum源软件包缓存
添加hosts解析
关闭swap分区
安装chrony服务，并同步时间
配置limits.conf
安装必备工具
升级系统并重启

操作主机：[master1,master2,master3,node01]
#将以下脚本内容添加进去vi k8s_system_init.sh
#!/bin/bashif [ $# -eq 2 ];then  echo &quot;设置主机名为：$1&quot;  echo &quot;ens33设置IP地址为：192.168.48.$2&quot;else  echo  &quot;使用方法：sh $0 主机名 主机位&quot;  exit 2fiecho &quot;--------------------------------------&quot;echo &quot;1.正在设置主机名：$1&quot;hostnamectl set-hostname $1echo &quot;2.正在关闭firewalld、dnsmasq、selinux&quot;systemctl disable firewalld &amp;&gt; /dev/nullsystemctl disable dnsmasq &amp;&gt; /dev/nullsystemctl stop firewalldsystemctl stop dnsmasqsed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsetenforce 0echo &quot;3.正在设置ens33：192.168.48.$2&quot;cat &gt; /etc/sysconfig/network-scripts/ifcfg-ens33 &lt;&lt;EOFTYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noNAME=ens33UUID=53b402ff-5865-47dd-a853-7afcd6521738DEVICE=ens33ONBOOT=yesIPADDR=192.168.48.$2GATEWAY=192.168.48.2PREFIX=24DNS1=192.168.48.2DNS2=114.114.114.114EOFnmcli c reloadnmcli c up ens33echo &quot;4.新增docker-ce源、k8s源&quot;mkdir /etc/yum.repos.d/bak/cp /etc/yum.repos.d/* /etc/yum.repos.d/bak/sleep 3cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg EOF#切换为华为云，下载速度更快sed -i &#x27;s/\$basearch/x86_64/g&#x27; /etc/yum.repos.d/openEuler.reposed -i &#x27;s/http\:\/\/repo.openeuler.org/https\:\/\/mirrors.huaweicloud.com\/openeuler/g&#x27; /etc/yum.repos.d/openEuler.repocurl -o /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposed -i &#x27;s/\$releasever/7/g&#x27; /etc/yum.repos.d/docker-ce.repoecho &quot;5.更新yum源软件包缓存&quot; yum clean all &amp;&amp; yum makecacheecho &quot;6.添加hosts解析&quot;cat &gt; /etc/hosts &lt;&lt;EOF127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.48.101 master1192.168.48.102 master2192.168.48.103 master3192.168.48.104 node01192.168.48.105 node02EOFecho &quot;7.关闭swap分区&quot;swapoff -a &amp;&amp; sysctl -w vm.swappiness=0 &amp;&gt; /dev/nullsed -ri &#x27;/^[^#]*swap/s@^@#@&#x27; /etc/fstabecho &quot;8.安装chrony服务，并同步时间&quot;yum install chrony -ysystemctl start chronydsystemctl enable chronydchronyc sourceschronyc sourcesecho &quot;9.配置limits.conf&quot;ulimit -SHn 65535cat &gt;&gt; /etc/security/limits.conf &lt;&lt;EOF* soft nofile 65536* hard nofile 131072* soft nproc 65535* hard nproc 655350* soft memlock unlimited* hard memlock unlimitedEOFecho &quot;10.必备工具安装&quot;yum install wget psmisc vim net-tools telnet device-mapper-persistent-data lvm2 git -yecho &quot;11.重启&quot;reboot
sh k8s_system_init.sh 主机名  主机位[master1] sh k8s_system_init.sh master1 101[master2] sh k8s_system_init.sh master2 102[master3] sh k8s_system_init.sh master3 103[node01] sh k8s_system_init.sh node01 104
配置ssh免密
操作节点[master1]
yum install -y sshpass cat &gt; sshmianmi.sh &lt;&lt; &quot;EOF&quot;#!/bin/bash# 目标主机列表hosts=(&quot;master1&quot; &quot;master2&quot; &quot;master3&quot; &quot;node01&quot;)# 密码password=&quot;Lj201840.&quot;# 生成 SSH 密钥对ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa# 循环遍历目标主机for host in &quot;$&#123;hosts[@]&#125;&quot;do    # 复制公钥到目标主机    sshpass -p &quot;$password&quot; ssh-copy-id -o StrictHostKeyChecking=no &quot;$host&quot;        # 验证免密登录    sshpass -p &quot;$password&quot; ssh -o StrictHostKeyChecking=no &quot;$host&quot; &quot;echo &#x27;免密登录成功&#x27;&quot;doneEOFsh sshmianmi.sh
内核及ipvs模块配置
此步骤是配置ipvs模块，开启一些k8s集群中必须的内核参数。配置一下几个方面：

更改内核启动顺序
安装ipvsadm
配置ipvs模块
开启k8s集群必须的内核参数
配置完内核，重启服务器

操作主机：[master1,master2,master3,node01]
vi kernel_update.sh
#!/bin/bashecho &quot;1.更改内核启动顺序&quot;grub2-set-default  0 &amp;&amp; grub2-mkconfig -o /etc/grub2.cfggrubby --args=&quot;user_namespace.enable=1&quot; --update-kernel=&quot;$(grubby --default-kernel)&quot;echo &quot;2.安装ipvsadm&quot;yum install ipvsadm ipset sysstat conntrack libseccomp -y &amp;&gt; /dev/nullecho &quot;3.配置ipvs模块&quot;modprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrackcat &gt;&gt; /etc/modules-load.d/ipvs.conf &lt;&lt;EOFip_vsip_vs_lcip_vs_wlcip_vs_rrip_vs_wrrip_vs_lblcip_vs_lblcrip_vs_dhip_vs_ship_vs_foip_vs_nqip_vs_sedip_vs_ftpip_vs_shnf_conntrackip_tablesip_setxt_setipt_setipt_rpfilteript_REJECTipipEOFsystemctl enable --now systemd-modules-load.service &amp;&gt; /dev/nullecho &quot;4.开启k8s集群必须的内核参数&quot;cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.ip_nonlocal_bind = 1 net.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1fs.may_detach_mounts = 1net.ipv4.conf.all.route_localnet = 1vm.overcommit_memory=1vm.panic_on_oom=0fs.inotify.max_user_watches=89100fs.file-max=52706963fs.nr_open=52706963net.netfilter.nf_conntrack_max=2310720net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_probes = 3net.ipv4.tcp_keepalive_intvl =15net.ipv4.tcp_max_tw_buckets = 36000net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_max_orphans = 327680net.ipv4.tcp_orphan_retries = 3net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.ip_conntrack_max = 65536net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.tcp_timestamps = 0net.core.somaxconn = 16384EOFsysctl --systemecho &quot;5.配置完内核，重启服务器！&quot;reboot
sh kernel_update.sh
检查ipvs加载、内核版本验证
lsmod | grep --color=auto -e ip_vs -e nf_conntrackuname -a

部署ETCD集群
本次在master1、master2、master3上进行etcd集群部署
安装etcd
下载安装包
wget https://github.com/etcd-io/etcd/releases/download/v3.5.13/etcd-v3.5.13-linux-amd64.tar.gz

解压
tar xf etcd-v3.5.13-linux-amd64.tar.gzmv etcd-v3.5.13-linux-amd64 /tmp/etcdcp /tmp/etcd/etcd* /usr/local/bin/

添加环境变量
将文件夹中etcd和etcdctl两个文件添加到环境变量中
mkdir -p /var/lib/etcd/mkdir -p /etc/etcd/chmod 700 /var/lib/etcd

创建默认配置文件
cat &lt;&lt;EOF | sudo tee /etc/etcd/etcd.conf#节点名称ETCD_NAME=$(hostname -s)#数据存放位置ETCD_DATA_DIR=/var/lib/etcdEOF

创建etcd服务
cat &lt;&lt;EOF | sudo tee /etc/systemd/system/etcd.service [Unit]Description=Etcd ServerDocumentation=https://github.com/coreos/etcdAfter=network.target [Service]User=rootType=notifyEnvironmentFile=-/etc/etcd/etcd.confExecStart=/usr/local/bin/etcdRestart=on-failureRestartSec=10sLimitNOFILE=40000 [Install]WantedBy=multi-user.targetEOF

开启服务
systemctl daemon-reload &amp;&amp; systemctl enable etcd &amp;&amp; systemctl start etcd

查看版本信息
etcd -version

在master1节点上生成etcd配置文件
vim etcd_install.sh
etcd1=192.168.48.101etcd2=192.168.48.102etcd3=192.168.48.103TOKEN=smartgoETCDHOSTS=($etcd1 $etcd2 $etcd3)NAMES=(&quot;master1&quot; &quot;master2&quot; &quot;master3&quot;)for i in &quot;$&#123;!ETCDHOSTS[@]&#125;&quot;; doHOST=$&#123;ETCDHOSTS[$i]&#125;NAME=$&#123;NAMES[$i]&#125;cat &lt;&lt; EOF &gt; /tmp/$NAME.conf# [member]ETCD_NAME=$NAMEETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;http://$HOST:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;http://$HOST:2379,http://127.0.0.1:2379&quot;#[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;http://$HOST:2380&quot;ETCD_INITIAL_CLUSTER=&quot;$&#123;NAMES[0]&#125;=http://$&#123;ETCDHOSTS[0]&#125;:2380,$&#123;NAMES[1]&#125;=http://$&#123;ETCDHOSTS[1]&#125;:2380,$&#123;NAMES[2]&#125;=http://$&#123;ETCDHOSTS[2]&#125;:2380&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;$TOKEN&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;http://$HOST:2379&quot;EOFdonels /tmp/master*scp /tmp/master2.conf $etcd2:/etc/etcd/etcd.confscp /tmp/master3.conf $etcd3:/etc/etcd/etcd.confcp /tmp/master1.conf /etc/etcd/etcd.confrm -f /tmp/master*.conf

sh etcd_install.sh

在k8s集群master节点上启动etcd
systemctl restart etcdsystemctl enable --now etcd

检查etcd集群是否正常
etcdctl member listetcdctl endpoint health



高可用组件安装
haproxy配置
操作节点：[master1，master2,master3]
yum install keepalived haproxy -y
所有Master节点配置HAProxy，所有Master节点的HAProxy配置相同。
操作节点：[master1，master2, master3]cat &gt; /etc/haproxy/haproxy.cfg &lt;&lt;&quot;EOF&quot;global  maxconn  2000  ulimit-n  16384  log  127.0.0.1 local0 err  stats timeout 30sdefaults  log global  mode  http  option  httplog  timeout connect 5000  timeout client  50000  timeout server  50000  timeout http-request 15s  timeout http-keep-alive 15sfrontend monitor-in  bind *:33305  mode http  option httplog  monitor-uri /monitorfrontend k8s-master  bind 0.0.0.0:16443  bind 127.0.0.1:16443  mode tcp  option tcplog  tcp-request inspect-delay 5s  default_backend k8s-masterbackend k8s-master  mode tcp  option tcplog  option tcp-check  balance roundrobin  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100  server master1   192.168.48.101:6443  check  server master2   192.168.48.102:6443  check  server master3   192.168.48.103:6443  checkEOF
Keepalived配置
操作节点：[master1，master2,master3]
所有Master节点配置Keepalived，以下三个Master节点配置注意ip和网卡。
操作节点：[master1]cat &gt;/etc/keepalived/keepalived.conf  &lt;&lt;&quot;EOF&quot;! Configuration File for keepalivedglobal_defs &#123;    router_id LVS_DEVELscript_user root    enable_script_security&#125;vrrp_script chk_apiserver &#123;    script &quot;/etc/keepalived/check_apiserver.sh&quot;    interval 5    weight -5    fall 2  rise 1&#125;vrrp_instance VI_1 &#123;    state MASTER    interface ens33    mcast_src_ip 192.168.48.101    virtual_router_id 51    priority 101    advert_int 2    authentication &#123;        auth_type PASS        auth_pass K8SHA_KA_AUTH    &#125;    virtual_ipaddress &#123;        192.168.48.200    &#125;    track_script &#123;       chk_apiserver    &#125;&#125;EOF
操作节点：[master2]cat &gt;/etc/keepalived/keepalived.conf  &lt;&lt;&quot;EOF&quot;! Configuration File for keepalivedglobal_defs &#123;    router_id LVS_DEVELscript_user root    enable_script_security&#125;vrrp_script chk_apiserver &#123;    script &quot;/etc/keepalived/check_apiserver.sh&quot;    interval 5    weight -5    fall 2  rise 1&#125;vrrp_instance VI_1 &#123;    state BACKUP    interface ens33    mcast_src_ip 192.168.48.102    virtual_router_id 51    priority 100    advert_int 2    authentication &#123;        auth_type PASS        auth_pass K8SHA_KA_AUTH    &#125;    virtual_ipaddress &#123;        192.168.48.200    &#125;    track_script &#123;       chk_apiserver    &#125;&#125;EOF
操作节点：[master3]cat &gt;/etc/keepalived/keepalived.conf  &lt;&lt;&quot;EOF&quot;! Configuration File for keepalivedglobal_defs &#123;    router_id LVS_DEVELscript_user root    enable_script_security&#125;vrrp_script chk_apiserver &#123;    script &quot;/etc/keepalived/check_apiserver.sh&quot;    interval 5    weight -5    fall 2  rise 1&#125;vrrp_instance VI_1 &#123;    state BACKUP    interface ens33    mcast_src_ip 192.168.48.103    virtual_router_id 51    priority 99    advert_int 2    authentication &#123;        auth_type PASS        auth_pass K8SHA_KA_AUTH    &#125;    virtual_ipaddress &#123;        192.168.48.200    &#125;    track_script &#123;       chk_apiserver    &#125;&#125;EOF
配置Keepalived健康检查文件
操作节点：[master1，master2,master3]
cat &gt; /etc/keepalived/check_apiserver.sh &lt;&lt;&quot;EOF&quot; #!/bin/bash err=0 for k in $(seq 1 3) do    check_code=$(pgrep haproxy)    if [[ $check_code == &quot;&quot; ]]; then        err=$(expr $err + 1)        sleep 1        continue    else        err=0        break    fi done  if [[ $err != &quot;0&quot; ]]; then    echo &quot;systemctl stop keepalived&quot;    /usr/bin/systemctl stop keepalived    exit 1 else    exit 0 fiEOFchmod +x /etc/keepalived/check_apiserver.sh
启动haproxy和keepalived
操作节点：[master，master2,master3]systemctl daemon-reloadsystemctl enable --now haproxysystemctl enable --now keepalived
测试集群负载均衡高可用
查看master1的vip
ip a

模拟master1的宕机测试，看看vip会不会漂移到master2去
[master1] poweroff

这时候查看master2的ip列表
[master2] ip a

结论：这时可以知道，负载均衡集群成功，当master1出现宕机情况，vip会从master1漂移到master2
docker安装
安装docker
操作节点[master1，master2，master3,node01]
wget https://download.docker.com/linux/static/stable/x86_64/docker-24.0.7.tgztar xf docker-*.tgzcp -rf docker/* /usr/bin/#创建containerd的service文件,并且启动cat &gt;/etc/systemd/system/containerd.service &lt;&lt;EOF[Unit]Description=containerd container runtimeDocumentation=https://containerd.ioAfter=network.target local-fs.target[Service]ExecStartPre=-/sbin/modprobe overlayExecStart=/usr/bin/containerdType=notifyDelegate=yesKillMode=processRestart=alwaysRestartSec=5LimitNPROC=infinityLimitCORE=infinityLimitNOFILE=1048576TasksMax=infinityOOMScoreAdjust=-999[Install]WantedBy=multi-user.targetEOFsystemctl enable --now containerd.service#准备docker的service文件cat &gt; /etc/systemd/system/docker.service &lt;&lt;EOF[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.service containerd.serviceWants=network-online.targetRequires=docker.socket containerd.service[Service]Type=notifyExecStart=/usr/bin/dockerd --config-file=/etc/docker/daemon.json -H fd://containerd=/run/containerd/containerd.sockExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=alwaysStartLimitBurst=3StartLimitInterval=60sLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTasksMax=infinityDelegate=yesKillMode=processOOMScoreAdjust=-500[Install]WantedBy=multi-user.targetEOF#准备docker的socket文件cat &gt; /etc/systemd/system/docker.socket &lt;&lt;EOF[Unit]Description=Docker Socket for the API[Socket]ListenStream=/var/run/docker.sockSocketMode=0660SocketUser=rootSocketGroup=docker[Install]WantedBy=sockets.targetEOFgroupadd dockersystemctl enable --now docker.socket  &amp;&amp; systemctl enable --now docker.service#验证mkdir /etc/dockersudo tee /etc/docker/daemon.json &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;&#123;  &quot;registry-mirrors&quot;: [    &quot;https://docker.xuanyuan.me&quot;,    &quot;https://docker.m.daocloud.io&quot;,    &quot;https://docker.1ms.run&quot;,    &quot;https://docker.1panel.live&quot;,    &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,		&quot;https://docker.qianyios.top&quot;  ],  &quot;max-concurrent-downloads&quot;: 10,  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-level&quot;: &quot;warn&quot;,  &quot;log-opts&quot;: &#123;    &quot;max-size&quot;: &quot;10m&quot;,    &quot;max-file&quot;: &quot;3&quot;  &#125;,  &quot;data-root&quot;: &quot;/var/lib/docker&quot;&#125;EOFsystemctl daemon-reloadsystemctl restart docker
安装cri-docker
操作节点[master1，master2，master3,node01]
wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.12/cri-dockerd-0.3.12.amd64.tgztar -zxvf cri-dockerd-0.3.12.amd64.tgzcp cri-dockerd/cri-dockerd  /usr/bin/chmod +x /usr/bin/cri-dockerd#写入启动配置文件cat &gt;  /usr/lib/systemd/system/cri-docker.service &lt;&lt;EOF[Unit]Description=CRI Interface for Docker Application Container EngineDocumentation=https://docs.mirantis.comAfter=network-online.target firewalld.service docker.serviceWants=network-online.targetRequires=cri-docker.socket [Service]Type=notifyExecStart=/usr/bin/cri-dockerd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9ExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=always StartLimitBurst=3 StartLimitInterval=60s LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity TasksMax=infinityDelegate=yesKillMode=process [Install]WantedBy=multi-user.targetEOF#写入socket配置文件cat &gt; /usr/lib/systemd/system/cri-docker.socket &lt;&lt;EOF[Unit]Description=CRI Docker Socket for the APIPartOf=cri-docker.service [Socket]ListenStream=%t/cri-dockerd.sockSocketMode=0660SocketUser=rootSocketGroup=docker [Install]WantedBy=sockets.targetEOFsystemctl daemon-reload &amp;&amp; systemctl enable cri-docker --now
K8S集群安装
安装k8s所需的工具
操作节点[master1，master2，master3,node01]yum -y install  kubeadm kubelet kubectl#为了实现docker使用的cgroupdriver与kubelet使用的cgroup的一致性，配置如下：sed -i &#x27;s/^KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=&quot;--cgroup-driver=systemd&quot;/g&#x27; /etc/sysconfig/kubelet#设置kubelet为开机自启动即可，由于没有生成配置文件，集群初始化后自动启动systemctl enable kubeletsystemctl enable kubelet.service
初始化集群
cat &gt; kubeadm-config.yaml &lt;&lt; EOF---apiVersion: kubeadm.k8s.io/v1beta3bootstrapTokens:- groups:  - system:bootstrappers:kubeadm:default-node-token  token: abcdef.0123456789abcdef  ttl: 24h0m0s  usages:  - signing  - authenticationkind: InitConfigurationlocalAPIEndpoint:  advertiseAddress: 192.168.48.101  bindPort: 6443nodeRegistration:  criSocket: unix:///var/run/cri-dockerd.sock---apiVersion: kubeadm.k8s.io/v1beta3kind: ClusterConfigurationkubernetesVersion: 1.28.2imageRepository: registry.aliyuncs.com/google_containersnetworking:  dnsDomain: cluster.local  podSubnet: 10.244.0.0/16  serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125;apiServerCertSANs:- 192.168.48.200controlPlaneEndpoint: &quot;192.168.48.200:16443&quot;etcd:  external:    endpoints:      - http://192.168.48.101:2379      - http://192.168.48.102:2379      - http://192.168.48.103:2379---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationfeatureGates:  # SupportIPVSProxyMode: falsemode: ipvs---apiVersion: kubelet.config.k8s.io/v1beta1authentication:  anonymous:    enabled: false  webhook:    cacheTTL: 0s    enabled: true  x509:    clientCAFile: /etc/kubernetes/pki/ca.crtauthorization:  mode: Webhook  webhook:    cacheAuthorizedTTL: 0s    cacheUnauthorizedTTL: 0scgroupDriver: systemdclusterDNS:- 10.96.0.10clusterDomain: cluster.localcpuManagerReconcilePeriod: 0sevictionPressureTransitionPeriod: 0sfileCheckFrequency: 0shealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 0simageMinimumGCAge: 0skind: KubeletConfigurationlogging:  flushFrequency: 0  options:    json:      infoBufferSize: &quot;0&quot;  verbosity: 0memorySwap: &#123;&#125;nodeStatusReportFrequency: 0snodeStatusUpdateFrequency: 0srotateCertificates: trueruntimeRequestTimeout: 0sshutdownGracePeriod: 0sshutdownGracePeriodCriticalPods: 0sstaticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 0ssyncFrequency: 0svolumeStatsAggPeriod: 0sEOF
准备k8s所需的镜像
操作节点[master1]kubeadm config images pull --config kubeadm-config.yaml

master1节点初始化
操作节点[master1]
kubeadm init --config kubeadm-config.yaml --upload-certs --v=9
会生成信息

记录信息后面会用到
初始化成功以后，会产生Token值，用于其他节点加入时使用，因此要记录下初始化成功生成的token值（令牌值），有效期24小时，后续需要操作可以重新生成Token
操作节点[master1]
kubeadm join 192.168.48.200:16443 --token abcdef.0123456789abcdef \        --discovery-token-ca-cert-hash sha256:bc27d821932d27bb4b49c187ddd412569ead01b14009e03b6d03aa3ea54e4612 \        --control-plane --certificate-key 37b1370abd3584c354517603bf97e10c795e3367e05e5ff3f2f6ca14288d8054kubeadm join 192.168.48.200:16443 --token abcdef.0123456789abcdef \        --discovery-token-ca-cert-hash sha256:bc27d821932d27bb4b49c187ddd412569ead01b14009e03b6d03aa3ea54e4612
操作kubect报错：

此时通过kubectl操作，会出现失败，因为还没有将集群的&quot;钥匙&quot;交给root用户。/etc/kubernetes/admin.conf 文件是 Kubernetes（K8s）集群中的管理员配置文件，它包含了用于管理集群的身份验证和访问信息。所以下面进行配置环境变量，用于访问Kubernetes集群：
添加环境变量
操作节点[master1]
mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config
添加其他master节点至集群中
操作节点[master2,master3]
操作节点[master2,master3]kubeadm join 192.168.48.200:16443 --token abcdef.0123456789abcdef \        --discovery-token-ca-cert-hash sha256:bc27d821932d27bb4b49c187ddd412569ead01b14009e03b6d03aa3ea54e4612 \        --control-plane --certificate-key 37b1370abd3584c354517603bf97e10c795e3367e05e5ff3f2f6ca14288d8054 \        --cri-socket unix:///var/run/cri-dockerd.sock 
注意：这里末尾添加了--cri-socket unix:///var/run/cri-dockerd.sock 
接着给master2添加环境变量
操作节点[master2,master3]mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config

这里没有展示master3的图片，但是步骤一样的
模拟Token过期重新生成并加入Node节点
假设加入集群的token过期了。node01无法加入了，这里就模拟一下这种情况

Token过期后生成新的token：

kubeadm token create --print-join-command
[root@master1 ~]# kubeadm token create --print-join-commandkubeadm join 192.168.48.200:16443 --token ke773y.6hv9utk33to4vwfy --discovery-token-ca-cert-hash sha256:bc27d821932d27bb4b49c187ddd412569ead01b14009e03b6d03aa3ea54e4612[root@master1 ~]#
其中，192.168.48.200:16443 是你的 Kubernetes API 服务器的地址和端口，ke773y.6hv9utk33to4vwfy 是新的令牌，sha256:bc27d821932d27bb4b49c187ddd412569ead01b14009e03b6d03aa3ea54e4612 是令牌的 CA 证书哈希值。

Master需要生成–certificate-key：

kubeadm init phase upload-certs --upload-certs
[root@master1 ~]# kubeadm init phase upload-certs --upload-certsW1110 18:12:03.426245   10185 version.go:104] could not fetch a Kubernetes version from the internet: unable to get URL &quot;https://dl.k8s.io/release/stable-1.txt&quot;: Get &quot;https://dl.k8s.io/release/stable-1.txt&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)W1110 18:12:03.426320   10185 version.go:105] falling back to the local client version: v1.28.2[upload-certs] Storing the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace[upload-certs] Using certificate key:5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128
其中，5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128 是证书密钥。

生成新的Token用于集群添加新Node节点

操作节点[node01]
kubeadm join 192.168.48.200:16443 \        --token ke773y.6hv9utk33to4vwfy \        --discovery-token-ca-cert-hash sha256:bc27d821932d27bb4b49c187ddd412569ead01b14009e03b6d03aa3ea54e4612  \        --cri-socket unix:///var/run/cri-dockerd.sock 
注意：这里末尾添加了--cri-socket unix:///var/run/cri-dockerd.sock 

这时在master查看node状态（显示为notready不影响）

模拟新加master节点的加入K8S集群中
假设我们新加master节点的话，就拼接token，从刚刚生成的token拼接
[root@master1 ~]# kubeadm token create --print-join-commandkubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f
这里提取信息1
kubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f
接着
[root@master1 ~]# kubeadm init phase upload-certs --upload-certsW1110 18:12:03.426245   10185 version.go:104] could not fetch a Kubernetes version from the internet: unable to get URL &quot;https://dl.k8s.io/release/stable-1.txt&quot;: Get &quot;https://dl.k8s.io/release/stable-1.txt&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)W1110 18:12:03.426320   10185 version.go:105] falling back to the local client version: v1.28.2[upload-certs] Storing the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace[upload-certs] Using certificate key:5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128
这里提取信息2：这里前面要加上--control-plane --certificate-key
--control-plane --certificate-key 5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128
合成
kubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in \        --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f  \        --control-plane --certificate-key 5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128 \        --cri-socket unix:///var/run/cri-dockerd.sock                kubeadm join 192.168.48.200:16443 --token lnkno8.u4v1l8n9pahzf0kj \        --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f \        --control-plane --certificate-key 41e441fe56cb4bdfcc2fc0291958e6b1da54d01f4649b6651471c07583f85cdf \        --cri-socket unix:///var/run/cri-dockerd.sock
注意：这里末尾添加了--cri-socket unix:///var/run/cri-dockerd.sock 
图示

安装calico网络插件
操作节点[master1]
添加解析记录，否则无法访问
echo &#x27;185.199.108.133 raw.githubusercontent.com&#x27; &gt;&gt; /etc/hosts
应用operator资源清单文件
网络组件有很多种，只需要部署其中一个即可，推荐Calico。
Calico是一个纯三层的数据中心网络方案，Calico支持广泛的平台，包括Kubernetes、OpenStack等。
Calico 在每一个计算节点利用 Linux Kernel 实现了一个高效的虚拟路由器（ vRouter） 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息向整个 Calico 网络内传播。
此外，Calico 项目还实现了 Kubernetes 网络策略，提供ACL功能。
curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/calico.yaml -O
[root@master1 ~]# vim calico.yaml。。。。。- name: WAIT_FOR_DATASTORE  value: &quot;true&quot;  #添加以下两行- name: IP_AUTODETECTION_METHOD  value: interface=ens33#ens33是你的网卡

sed -i &#x27;s| docker.io/calico/| registry.cn-hangzhou.aliyuncs.com/qianyios/|&#x27; calico.yamlkubectl apply -f calico.yaml

监视kube-system命名空间中pod运行情况
等待估计20分钟左右吧(确保全部running)
kubectl get pods -n kube-system

拿掉master节点的污点
节点 master1 和 master2 都有一个名为 node-role.kubernetes.io/control-plane:NoSchedule 的污点。这个污点的作用是阻止普通的 Pod 被调度到这些节点上，只允许特定的控制平面组件（如 kube-apiserver、kube-controller-manager 和 kube-scheduler）在这些节点上运行。
这种设置有助于确保控制平面节点专门用于运行 Kubernetes 的核心组件，而不会被普通的工作负载占用。通过将污点添加到节点上，可以确保只有被授权的控制平面组件才能在这些节点上运行。
kubectl describe node master1 | grep -i taintkubectl describe node master2 | grep -i taintkubectl describe node master3 | grep -i taint

去除污点
kubectl taint node master1 node-role.kubernetes.io/control-plane:NoSchedule-kubectl taint node master2 node-role.kubernetes.io/control-plane:NoSchedule-kubectl taint node master3 node-role.kubernetes.io/control-plane:NoSchedule-

安装dashboard
操作节点[master1]
下载文件
https://github.com/kubernetes/dashboard/releases/tag/v2.7.0
目前最新版本v2.7.0
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yamlsed -i &#x27;s/kubernetesui\/dashboard:v2.7.0/registry.cn-hangzhou.aliyuncs.com\/qianyios\/dashboard:v2.7.0/g&#x27; recommended.yamlsed -i &#x27;s/kubernetesui\/metrics-scraper:v1.0.8/registry.cn-hangzhou.aliyuncs.com\/qianyios\/metrics-scraper:v1.0.8/g&#x27; recommended.yaml

修改配置文件
vim recommended.yaml---kind: ServiceapiVersion: v1metadata:  labels:    app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboardspec:  ports:    - port: 443      targetPort: 8443      nodePort: 30001  type: NodePort  selector:    app: kubernetes-dashboard---

运行dashboard
kubectl apply -f recommended.yaml

检查运行状态
kubectl get pods -n kubernetes-dashboardkubectl get pod,svc -o wide -n kubernetes-dashboard

创建cluster-admin用户
创建service account并绑定默认cluster-admin管理员群角色#创建用户kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard#用户授权kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin#获取用户Tokenkubectl create token dashboard-admin -n kubernetes-dashboard

记录token
eyJhbGciOiJSUzI1NiIsImtpZCI6IjJxb21sRTZSckhEZ09FMnlvWU5IY3dfTTRIWDEzRUpsQ000MThhSWxYNDgifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjk5NjQ2Mjg3LCJpYXQiOjE2OTk2NDI2ODcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJkYXNoYm9hcmQtYWRtaW4iLCJ1aWQiOiIyNTBhNTQ2MS03MGJlLTRhZTItOWY2Yi1hMDQwOWE1NWJhMTMifX0sIm5iZiI6MTY5OTY0MjY4Nywic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.U-lH9_sRU4TVrpAznqS60INNSev9NxHu61igGUTzTBoaWo7WjPG7vzTnpZYhvsoglTzMEKhfranJkkkn95pe-prkvSasaAL6kXHw0jQjlSMzcYiF7DoLdkOtDJSukuALEubidf9eIwZXFZ-sezZdZHm4hnk5nWme5YtdOmYOJPh5sv1dzRvM1XuOHknJPTA1BbdZuVAtSGHSjkhwx-wl-41uuQoROW5GjJs0bz4zLBFn1w_pWaSMCn7pjGJNcbr6IuDV41km_etpwwxacWyfAcxNykzCtIiE1abJj7m-e944GvAn_eqxz3wCZD6Bgt41FWRzyMHjrppJfDjk7FaHNw
登录浏览器访问
https://192.168.48.200:30001输入token：----eyJhbGciOiJSUzI1NiIsImtpZCI6IjJxb21sRTZSckhEZ09FMnlvWU5IY3dfTTRIWDEzRUpsQ000MThhSWxYNDgifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjk5NjQ2Mjg3LCJpYXQiOjE2OTk2NDI2ODcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJkYXNoYm9hcmQtYWRtaW4iLCJ1aWQiOiIyNTBhNTQ2MS03MGJlLTRhZTItOWY2Yi1hMDQwOWE1NWJhMTMifX0sIm5iZiI6MTY5OTY0MjY4Nywic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.U-lH9_sRU4TVrpAznqS60INNSev9NxHu61igGUTzTBoaWo7WjPG7vzTnpZYhvsoglTzMEKhfranJkkkn95pe-prkvSasaAL6kXHw0jQjlSMzcYiF7DoLdkOtDJSukuALEubidf9eIwZXFZ-sezZdZHm4hnk5nWme5YtdOmYOJPh5sv1dzRvM1XuOHknJPTA1BbdZuVAtSGHSjkhwx-wl-41uuQoROW5GjJs0bz4zLBFn1w_pWaSMCn7pjGJNcbr6IuDV41km_etpwwxacWyfAcxNykzCtIiE1abJj7m-e944GvAn_eqxz3wCZD6Bgt41FWRzyMHjrppJfDjk7FaHNw----

部署一个nginx测试
操作节点[master1]
vim web.yamlkind: Deployment#apiVersion: extensions/v1beta1apiVersion: apps/v1metadata:  labels:    app: web-deployment-label  name: web-deployment  namespace: defaultspec:  replicas: 3  selector:    matchLabels:      app: web-selector  template:    metadata:      labels:        app: web-selector    spec:      containers:      - name: web-container        image: nginx:latest        imagePullPolicy: Always        ports:        - containerPort: 80          protocol: TCP          name: http        - containerPort: 443          protocol: TCP          name: https---kind: ServiceapiVersion: v1metadata:  labels:    app: web-service-label  name: web-service  namespace: defaultspec:  type: NodePort  ports:  - name: http    port: 80    protocol: TCP    targetPort: 80    nodePort: 30080  - name: https    port: 443    protocol: TCP    targetPort: 443    nodePort: 30443  selector:    app: web-selector    kubectl apply -f web.yaml 

### 查看nginx的pod 的详细信息kubectl get deploy,svc,pod -o wide

访问nginx网站
http://192.168.48.200:30080


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>OpenEuler</tag>
        <tag>K8s</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenEuler22.03 LTS部署Zabbix</title>
    <url>/posts/1ca7aa3d/</url>
    <content><![CDATA[
OpenEuler22.03 LTS部署Zabbix
以下文档部分参考：Zabbix 6.2 安装：国产系统篇（OpenEuler）-zabbix 5.0安装
主机拓扑图



主机名
ip
硬盘
cpu
备注




zabbix-server
192.168.48.101
100g
2v
主控


zabbix-agent
192.168.48.102
100g
2v
测试机



基础配置
操作节点：[server]
不要一股脑的复制，注意修改网卡的名字，我这里是ens33，包括修改ip段，比如我的是192.168.48.你就要修改成你的172.8.3.最后那一个主机位就不用管，其他不变
vi system_init.sh
#!/bin/bashif [ $# -eq 2 ];then  echo &quot;设置主机名为：$1&quot;  echo &quot;ens33设置IP地址为：192.168.48.$2&quot;else  echo  &quot;使用方法：sh $0 主机名 主机位&quot;  exit 2fiecho &quot;--------------------------------------&quot;echo &quot;1.正在设置主机名：$1&quot;hostnamectl set-hostname $1echo &quot;2.正在关闭firewalld、selinux&quot;systemctl disable firewalld &amp;&gt; /dev/nullsystemctl stop firewalldsed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsetenforce 0echo &quot;3.正在设置ens33：192.168.48.$2&quot;cat &gt; /etc/sysconfig/network-scripts/ifcfg-ens33 &lt;&lt;EOFTYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noNAME=ens33UUID=53b402ff-5865-47dd-a853-7afcd6521738DEVICE=ens33ONBOOT=yesIPADDR=192.168.48.$2GATEWAY=192.168.48.2PREFIX=24DNS1=192.168.48.2DNS2=114.114.114.114EOFnmcli c reloadnmcli c up ens33echo &quot;4.新增华为云源&quot;mkdir /etc/yum.repos.d/bak/cp /etc/yum.repos.d/* /etc/yum.repos.d/bak/sleep 3#切换为华为云，下载速度更快sed -i &#x27;s/\$basearch/x86_64/g&#x27; /etc/yum.repos.d/openEuler.reposed -i &#x27;s/http\:\/\/repo.openeuler.org/https\:\/\/mirrors.huaweicloud.com\/openeuler/g&#x27; /etc/yum.repos.d/openEuler.repoecho &quot;5.更新yum源软件包缓存&quot; yum clean all &amp;&amp; yum makecacheecho &quot;6.添加hosts解析&quot;cat &gt; /etc/hosts &lt;&lt;EOF127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.48.101 zabbix-server192.168.48.102 zabbix-agentEOFecho &quot;7.安装chrony服务，并同步时间&quot;yum install chrony -ysystemctl enable chronyd --nowtimedatectl set-timezone Asia/Shanghaitimedatectl set-local-rtc 1timedatectl set-ntp yeschronyc -a makestepchronyc trackingchronyc sourcesecho &quot;8.必备工具安装&quot;yum install wget psmisc vim net-tools telnet socat device-mapper-persistent-data lvm2 git gcc -yecho &quot;9.重启&quot;reboot
运行
sh system_init.sh 主机名  主机位[zabbix-server] sh system_init.sh zabbix-server 101[zabbix-agent] sh system_init.sh zabbix-agent 102
zabbix-server安装
zabbix服务段安装
操作节点：[server]
#创建zabbix用户groupadd --system zabbixuseradd --system -g zabbix -d /usr/lib/zabbix -s /sbin/nologin -c &quot;Zabbix Monitoring System&quot; zabbix#创建zabbix安装目录mkdir -p /app/zabbix
下载 Zabbix 源码

#server端下载源码包,你也可以手动下载上传到虚拟机即可，我是放到了root目录下wget https://cdn.zabbix.com/zabbix/sources/stable/7.0/zabbix-7.0.5.tar.gztar -zxvf zabbix-7.*.tar.gz
开始编译安装

prefix 指定安装目录。
enable-server 启用 Zabbix Server。
enable-agent 启用 Zabbix agent。
with-mysql 后端指定数据库为mysql。
net-snmp 支持 snmp 协议。

其实还有很多参数,大家可以参考 ./configure --help 自行研究 ，官方文档里也有案例
#安装需要的组件dnf -y install mysql-devel libevent-devel pcre-devel#开始编译（/app/zabbix）是zabbix的安装目录，以下编译会安装在/app/zabbixmv zabbix-7.0.5 zabbix &amp;&amp; cd zabbix./configure --prefix=/app/zabbix --enable-server --enable-agent --with-mysql
出现这个页面是编译完成

#开始安装make &amp;&amp; make install#安装完成！

整体目录情况

PHP部分
openEuler 22.04 自带8.0版本，所以符合6.0以上版本的需求。

dnf -y install php php-fpm
Apache
由于安装PHP会自动安装apache（httpd）服务，所以安装过程并未提及apache的安装过程
现在移动zabbix前端网页文件到/var/http/html（这是apache网页的运行根目录文件）
[root@zabbix-sever zabbix]# pwd/root/zabbix#我现在/root/zabbix是刚刚解压源码包的地方，别走错了，看你的位置在哪cd /root/zabbix#这个命令会递归地复制 ui 目录下的所有内容，包括子目录和文件，到 /var/www/html/ 目录中并且保留文件的权限和属性cp -rp ui/* /var/www/html/#启动apachesystemctl start httpd &amp;&amp; systemctl enable httpdsystemctl start php-fpm &amp;&amp; systemctl enable php-fpm
访问192.168.48.101
选择中文

这里显示有些php扩展没下载等

dnf -y install php-gd php-mysqlnd php-bcmath php-xml php-mbstring
安装完依赖此时不需要重启任何服务，接下来调整PHP的配置文件。
根据报错来调整三个值分别是 post_max_size,max_execution_time,max_input_time。

所要求的最小PHP post大小是16M (配置项&quot;post_max_size&quot;)。
所要求的最小PHP脚本执行时间是300 (配置项 “max_execution_time”)。
所要求的PHP脚本最小解析时间是300 (配置项&quot;max_input_time&quot;)。

#一键修改命令sed -i &#x27;s/post_max_size = 8M/post_max_size = 16M/g&#x27; /etc/php.inised -i &#x27;s/max_execution_time = 30/max_execution_time = 300/g&#x27; /etc/php.inised -i &#x27;s/max_input_time = 60/max_input_time = 300/g&#x27; /etc/php.ini
改完之后需要重启php-fpm服务。
systemctl restart php-fpm
刷新页面就ok了

安装mysql
安装mysql
dnf -y install mysql-serversystemctl start mysqld &amp;&amp; systemctl enable mysqldmysql_secure_installation#提示信息：VALIDATE PASSWORD COMPONENT can be u ····· gh. Would you like to setup VALIDATE PASSWORD component?#验证密码组件可用于测试密码和提高安全性。它检查密码的强度，并允许用户只设置那些足够安全的密码。要设置VALIDATE PASSWORD组件吗？Press y|Y for Yes, any other key for No: NO (填NO)New password: Re-enter new password:#输入你的秘密，我这里是qianyios007#是否删除匿名用户Remove anonymous users?  (Press y|Y for Yes, any other key for No) : Y#是否禁止root用户远程登录Disallow root login remotely? (Press y|Y for Yes, any other key for No) : No#是否删除测试数据库？Remove test database and access to it? (Press y|Y for Yes, any other key for No) : Y#是否重新加载特权表吗？Reload privilege tables now?  (Press y|Y for Yes, any other key for No) : Y# 验证登入数据库mysql -u root -pqianyios007

创建Zabbix所需要的数据库和用户
mysql -u root -pqianyios007#创建一个名为zabbix的新数据库，并指定字符集为utf8mb4，排序规则为utf8mb4_bin。create database zabbix character set utf8mb4 collate utf8mb4_bin;#创建一个新用户zabbix，该用户只能从localhost连接，密码设置为123456。create user zabbix@localhost identified by &#x27;123456&#x27;;#授予zabbix用户对zabbix数据库的所有表的所有权限。grant all privileges on zabbix.* to zabbix@localhost;#设置全局变量log_bin_trust_function_creators为1，这允许创建存储函数时不受二进制日志的限制set global log_bin_trust_function_creators = 1;quit;
导入Zbbix 的数据文件
cd /root/zabbixcat database/mysql/schema.sql | mysql -uzabbix -p123456 zabbixcat database/mysql/images.sql | mysql -uzabbix -p123456 zabbixcat database/mysql/data.sql | mysql -uzabbix -p123456 zabbixmysql -u root -pqianyios007#相关数据导入完成后关掉log_bin_trust_function_creators功能。set global log_bin_trust_function_creators = 0;quit;
完成这部分，就可以在前端进入数据库配配置界面。



报错了，此时会出现无法创建，这是由于目标目录没有权限导致的，可以点击蓝色字体(网页中下载配置文件)将配置得好的文件下载下来，然后传到前端提示的目录里，该文这里路径为：
/var/www/html/conf/

自行下载用工具传过去即可

放过去之后刷新即可

管理员用户名密码为Admin/zabbix


服务端安装成功!

zabbix-server基本配置
这里可以看姐server端是没有启动的，说明要对于他进行配置

首先来说说zabbix server 目录的一些情况

程序文件路径为/app/zabbix/sbin/
配置文件路径为/app/zabbix/etc/


编辑配置文件

sed -i &#x27;s|^LogFile=.*|LogFile=/var/log/zabbix/zabbix_server.log|&#x27; /app/zabbix/etc/zabbix_server.confsed -i &#x27;/^# DBPassword=/a \  DBPassword=123456&#x27; /app/zabbix/etc/zabbix_server.confsed -i &#x27;/^# PidFile=\/tmp\/zabbix_server.pid$/a \  PidFile=/var/log/zabbix/zabbix_server.pid&#x27; /app/zabbix/etc/zabbix_server.conf

制作zabbix server守护文件

cat &gt; /usr/lib/systemd/system/zabbix-server.service &lt;&lt;&quot;EOF&quot;[Unit]Description=Zabbix ServerAfter=syslog.targetAfter=network.targetAfter=postgresql.serviceAfter=pgbouncer.serviceAfter=postgresql-13.service[Service]Environment=&quot;CONFFILE=/app/zabbix/etc/zabbix_server.conf&quot;EnvironmentFile=-/etc/sysconfig/zabbix-serverType=forkingRestart=on-failure#pid文件要改哦，看你在哪个位置PIDFile=/var/log/zabbix/zabbix_server.pidKillMode=control-groupExecStart=/app/zabbix/sbin/zabbix_server -c $CONFFILEExecStop=/bin/kill -SIGTERM $MAINPIDRestartSec=10sTimeoutSec=0[Install]WantedBy=multi-user.targetEOF
由于是源码编译安装，所以存放日志文件的目录不存在，所以需要自行创建和给予权限
mkdir /var/log/zabbixchown zabbix:zabbix /var/log/zabbix
启动zabbix-server和查看服务的状态。
systemctl start zabbix-server &amp;&amp; systemctl enable zabbix-serversystemctl status zabbix-server

查看日志也正常
tail -f /var/log/zabbix/zabbix_server.log

查看网页已经在运行了

zabbix-agent安装
server端安装agent
操作节点[server]
#制作 Zabbix agent 守护文件cat &gt; /usr/lib/systemd/system/zabbix-agent.service&lt;&lt;&quot;EOF&quot;[Unit]Description=Zabbix AgentAfter=syslog.targetAfter=network.target[Service]Environment=&quot;CONFFILE=/app/zabbix/etc/zabbix_agentd.conf&quot;Type=simpleRestart=on-failure#PID文件要改PIDFile=/var/log/zabbix/zabbix_agentd.pidKillMode=control-groupExecStart=/app/zabbix/sbin/zabbix_agentd -c $CONFFILEExecStop=/bin/kill -SIGTERM $MAINPIDRestartSec=10sUser=zabbixGroup=zabbix[Install]WantedBy=multi-user.targetEOF#修改agent配置文件也是和server一样修改日志文件和pid文件位置sed -i &#x27;s|^LogFile=.*|LogFile=/var/log/zabbix/zabbix_agentd.log|&#x27; /app/zabbix/etc/zabbix_agentd.confsed -i &#x27;/^# PidFile=\/tmp\/zabbix_agentd.pid$/a \  PidFile=/var/log/zabbix/zabbix_agentd.pid&#x27; /app/zabbix/etc/zabbix_agentd.conf#启动zabbix agentsystemctl daemon-reloadsystemctl start zabbix-agent &amp;&amp; systemctl enable zabbix-agent

这样在server主机的zabbix的server端和agent端已经安装好了，但是现在就是说只有一个agent端也就是本机，只能监控自己，现在就教你们去监控其他机子


其他主机安装agent
操作节点：[zabbix-agent]
#agent端下载源码包,你也可以手动下载上传到虚拟机即可，我是放到了root目录下wget https://cdn.zabbix.com/zabbix/sources/stable/7.0/zabbix-7.0.5.tar.gztar -zxvf zabbix-7.*.tar.gz#创建zabbix用户groupadd --system zabbixuseradd --system -g zabbix -d /usr/lib/zabbix -s /sbin/nologin -c &quot;Zabbix Monitoring System&quot; zabbix#创建zabbix安装目录mkdir -p /app/zabbix#安装需要的组件dnf -y install mysql-devel libevent-devel pcre-devel#开始编译（/app/zabbix）是zabbix的安装目录，以下编译会安装在/app/zabbix 这里只需要启用agent即可server不用mv zabbix-7.0.5 zabbix &amp;&amp; cd zabbix./configure --prefix=/app/zabbix --enable-agent #开始安装make install#修改配置文件sed -i &#x27;s|^LogFile=.*|LogFile=/var/log/zabbix/zabbix_agentd.log|&#x27; /app/zabbix/etc/zabbix_agentd.confsed -i &#x27;/^# PidFile=\/tmp\/zabbix_agentd.pid$/a \  PidFile=/var/log/zabbix/zabbix_agentd.pid&#x27; /app/zabbix/etc/zabbix_agentd.conf#修改测试机agent配置文件里的serve-ip，这里要填server机的ip（修改192.168.48.101即可）sed -i &#x27;s/Server=127.0.0.1/Server=192.168.48.101/g&#x27; /app/zabbix/etc/zabbix_agentd.confsed -i &#x27;s/ServerActive=127.0.0.1/ServerActive=192.168.48.101/g&#x27; /app/zabbix/etc/zabbix_agentd.conf#zabbix-agent修改成你测试机的主机名sed -i &#x27;s/Hostname=Zabbix server/Hostname=zabbix-agent/g&#x27; /app/zabbix/etc/zabbix_agentd.conf#制作 Zabbix agent 守护文件cat &gt; /usr/lib/systemd/system/zabbix-agent.service&lt;&lt;&quot;EOF&quot;[Unit]Description=Zabbix AgentAfter=syslog.targetAfter=network.target[Service]Environment=&quot;CONFFILE=/app/zabbix/etc/zabbix_agentd.conf&quot;Type=simpleRestart=on-failure#PID文件要改PIDFile=/var/log/zabbix/zabbix_agentd.pidKillMode=control-groupExecStart=/app/zabbix/sbin/zabbix_agentd -c $CONFFILEExecStop=/bin/kill -SIGTERM $MAINPIDRestartSec=10sUser=zabbixGroup=zabbix[Install]WantedBy=multi-user.targetEOF#由于是源码编译安装，所以存放日志文件的目录不存在，所以需要自行创建和给予权限mkdir /var/log/zabbixchown zabbix:zabbix /var/log/zabbix#启动zabbix agentsystemctl daemon-reloadsystemctl start zabbix-agent &amp;&amp; systemctl enable zabbix-agent
zabbix页面配置测试机监控端



最终效果，点击添加即可


测试端的机子已经添加好了
测试
Zabbix Web 配置模板(监听 Port 80)
操作节点：[zabbix-agent]
安装nginx并配置网页
因为没有适配openeuler的版本，这里直接用centos8的来代替，兼容的没关系，你可以根据你的系统去自行替换
cat &gt;/etc/yum.repos.d/nginx.repo &lt;&lt; &quot;EOF&quot;[nginx-stable]name=nginx stable repo#baseurl=http://nginx.org/packages/centos/$releasever/$basearch/#原本是$releasever的，但是没有openeuler的版本直接用8来代替也就是centos8，openeuler兼容centosbaseurl=http://nginx.org/packages/centos/8/$basearch/gpgcheck=1enabled=1gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=true[nginx-mainline]name=nginx mainline repo#baseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/baseurl=http://nginx.org/packages/mainline/centos/8/$basearch/gpgcheck=1enabled=0gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=trueEOFyum install nginx -ysystemctl enable nginx --now
这样就可以随时安装最新版本的nginx
添加测试网页
/etc/nginx/conf.d/default.conf
[root@zabbix-agent ~]# cat /etc/nginx/conf.d/default.confserver &#123;#80端口    listen       80;    server_name  localhost;    #access_log  /var/log/nginx/host.access.log  main;    location / &#123;    #这里就是网站的根目录了        root   /usr/share/nginx/html;        index  index.html index.htm;    &#125;    error_page   500 502 503 504  /50x.html;    location = /50x.html &#123;        root   /usr/share/nginx/html;    &#125;    #。。。。。&#125;
直接修改80端口下的网站里面的内容
vim /usr/share/nginx/html/index.html
里面的内容全部删了,写入以下内容，然后保存退出即可
&lt;h1&gt;welcome to qianyios.blog&lt;/h1&gt;
重启服务
nginx -s reload

配置80端口模板
创建监控80端口状态的模板

创建监控项


创建触发器
点击触发器，创建触发器


创建图形


配置测试机应用80端口模板

更新一下

80端口模板测试

当我们模拟80端口down
systemctl stop nginx


然后我们再开起来
systemctl start nginx

到此监测80端口成功

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenEuler部署K8s-1.31.1</title>
    <url>/posts/50cdf140/</url>
    <content><![CDATA[
OpenEuler部署K8s-1.31.1
主机拓扑图



主机名
ip
内存
硬盘
cpu
OS




master
192.168.48.101
5G
100G
2
openEuler-22.03-LTS-SP4


node01
192.168.48.102
5G
100G
2
openEuler-22.03-LTS-SP4


node02
192.168.48.103
5G
100G
2
openEuler-22.03-LTS-SP4



镜像下载地址：OpenEuler-22.03-LTS
下载名为openEuler-22.03-LTS-SP4-x86_64-dvd.iso
基本配置
注意一下你的网卡叫什么，我的是ens33，如果你是其他的记得替换，不要无脑的复制粘贴，看看脚本那些需要改的，目测需要改的是ip这些，还有第三步和第六步
基本配置
操作节点：[所有节点]
vi k8s_system_init.sh
#!/bin/bashif [ $# -eq 2 ];then  echo &quot;设置主机名为：$1&quot;  echo &quot;ens33设置IP地址为：192.168.48.$2&quot;else  echo  &quot;使用方法：sh $0 主机名 主机位&quot;  exit 2fiecho &quot;--------------------------------------&quot;echo &quot;1.正在设置主机名：$1&quot;hostnamectl set-hostname $1echo &quot;2.正在关闭firewalld、selinux&quot;systemctl disable firewalld &amp;&gt; /dev/nullsystemctl stop firewalldsed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsetenforce 0echo &quot;3.正在设置ens33：192.168.48.$2&quot;cat &gt; /etc/sysconfig/network-scripts/ifcfg-ens33 &lt;&lt;EOFTYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noNAME=ens33UUID=53b402ff-5865-47dd-a853-7afcd6521738DEVICE=ens33ONBOOT=yesIPADDR=192.168.48.$2GATEWAY=192.168.48.2PREFIX=24DNS1=192.168.48.2DNS2=114.114.114.114EOFnmcli c reloadnmcli c up ens33echo &quot;4.新增华为云源、k8s源&quot;mkdir /etc/yum.repos.d/bak/cp /etc/yum.repos.d/* /etc/yum.repos.d/bak/sleep 3cat &lt;&lt;EOF | tee /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.31/rpm/enabled=1gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.31/rpm/repodata/repomd.xml.keyEOF#切换为华为云，下载速度更快sed -i &#x27;s/\$basearch/x86_64/g&#x27; /etc/yum.repos.d/openEuler.reposed -i &#x27;s/http\:\/\/repo.openeuler.org/https\:\/\/mirrors.huaweicloud.com\/openeuler/g&#x27; /etc/yum.repos.d/openEuler.repoecho &quot;5.更新yum源软件包缓存&quot; yum clean all &amp;&amp; yum makecacheecho &quot;6.添加hosts解析&quot;cat &gt; /etc/hosts &lt;&lt;EOF127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.48.101 master192.168.48.102 node01192.168.48.103 node02EOFecho &quot;7.关闭swap分区&quot;swapoff -a &amp;&amp; sysctl -w vm.swappiness=0 &amp;&gt; /dev/nullsed -ri &#x27;/^[^#]*swap/s@^@#@&#x27; /etc/fstabecho &quot;8.安装chrony服务，并同步时间&quot;yum install chrony -ysystemctl enable chronyd --nowtimedatectl set-timezone Asia/Shanghaitimedatectl set-local-rtc 1timedatectl set-ntp yeschronyc -a makestepchronyc trackingchronyc sourcesecho &quot;9.必备工具安装&quot;yum install wget psmisc vim net-tools telnet socat device-mapper-persistent-data lvm2 git -yecho &quot;10.重启&quot;reboot
运行脚本
sh k8s_system_init.sh 主机名  主机位[master] sh k8s_system_init.sh master 101[node01] sh k8s_system_init.sh node01 102[node02] sh k8s_system_init.sh node02 103
配置ssh免密
操作节点:[所有节点]
注意修改你的主机密码和主机列表的主机名
yum install -y sshpass cat &gt; sshmianmi.sh &lt;&lt; &quot;EOF&quot;#!/bin/bash# 目标主机列表hosts=(&quot;master&quot; &quot;node01&quot; &quot;node02&quot;)# 密码password=&quot;Lj201840.&quot;# 生成 SSH 密钥对ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa# 循环遍历目标主机for host in &quot;$&#123;hosts[@]&#125;&quot;do    # 复制公钥到目标主机    sshpass -p &quot;$password&quot; ssh-copy-id -o StrictHostKeyChecking=no &quot;$host&quot;        # 验证免密登录    sshpass -p &quot;$password&quot; ssh -o StrictHostKeyChecking=no &quot;$host&quot; &quot;echo &#x27;免密登录成功&#x27;&quot;doneEOFsh sshmianmi.sh
配置内核路由转发及网桥过滤以及安装ipset及ipvsadm
操作节点:[所有节点]
sed -i &#x27;s/net.ipv4.ip_forward=0/net.ipv4.ip_forward=1/g&#x27; /etc/sysctl.confcat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1vm.swappiness = 0EOF# 配置加载br_netfilter模块cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.confoverlaybr_netfilterEOF#加载br_netfilter overlay模块modprobe br_netfiltermodprobe overlaysysctl --systemsysctl -p # 使用新添加配置文件生效sysctl -p /etc/sysctl.d/k8s.conf  yum -y install ipset ipvsadmcat &gt; /etc/sysconfig/modules/ipvs.module &lt;&lt;EOF#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrackEOF#授权、运行、检查是否加载chmod 755 /etc/sysconfig/modules/ipvs.module &amp;&amp;  /etc/sysconfig/modules/ipvs.module查看对应的模块是否加载成功lsmod | grep -e ip_vs -e nf_conntrack_ipv4
containerd容器环境安装
操作节点：[所有节点]
#下载所需软件包wget https://github.com/containerd/containerd/releases/download/v1.7.22/containerd-1.7.22-linux-amd64.tar.gzwget https://github.com/opencontainers/runc/releases/download/v1.1.15/runc.amd64wget https://github.com/containernetworking/plugins/releases/download/v1.5.1/cni-plugins-linux-amd64-v1.5.1.tgz#安装containerdtar Cxzvf /usr/local containerd-1.7.22-linux-amd64.tar.gz# 创建服务，所有主机都要操作cat &lt;&lt; EOF &gt; /usr/lib/systemd/system/containerd.service[Unit]Description=containerd container runtimeDocumentation=https://containerd.ioAfter=network.target local-fs.target[Service]ExecStartPre=-/sbin/modprobe overlayExecStart=/usr/local/bin/containerdType=notifyDelegate=yesKillMode=processRestart=alwaysRestartSec=5# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNPROC=infinityLimitCORE=infinity# Comment TasksMax if your systemd version does not supports it.# Only systemd 226 and above support this version.TasksMax=infinityOOMScoreAdjust=-999[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reload &amp;&amp; systemctl enable --now containerd#安装runcinstall -m 755 runc.amd64 /usr/local/sbin/runc#安装cnimkdir -p /opt/cni/bin &amp;&amp; tar -xzf cni-plugins-linux-amd64-v1.5.1.tgz -C /opt/cni/bin/#生成容器配置文件mkdir -p /etc/containerd &amp;&amp; containerd config default &gt; /etc/containerd/config.tomlsed -i &#x27;s#sandbox_image = &quot;registry.k8s.io/pause:.*&quot;#sandbox_image = &quot;registry.aliyuncs.com/google_containers/pause:3.10&quot;#&#x27; /etc/containerd/config.tomlsed -i &#x27;s/SystemdCgroup = false/SystemdCgroup = true/g&#x27; /etc/containerd/config.tomlcat &gt;/etc/crictl.yaml &lt;&lt;&quot;EOF&quot;runtime-endpoint: unix:///run/containerd/containerd.sockimage-endpoint: unix:///run/containerd/containerd.socktimeout: 10debug: falseEOF#配置containerd镜像加速# 修改 /etc/containerd/config.toml 中的 config_pathsed -i &#x27;s|^  config_path =.*$|  config_path = &quot;/etc/containerd/certs.d&quot;|&#x27; /etc/containerd/config.toml# 创建必要的目录mkdir -p /etc/containerd/certs.d/docker.iomkdir -p /etc/containerd/certs.d/registry.k8s.io# 配置 docker.io 的 hosts.tomlcat &lt;&lt;EOF &gt; /etc/containerd/certs.d/docker.io/hosts.tomlserver = &quot;https://docker.io&quot;[host.&quot;https://docker.m.daocloud.io&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://reg-mirror.giniu.com&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOF# 配置 registry.k8s.io 的 hosts.tomlcat &lt;&lt;EOF &gt; /etc/containerd/certs.d/registry.k8s.io/hosts.tomlserver = &quot;https://registry.k8s.io&quot;[host.&quot;https://k8s.m.daocloud.io&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;, &quot;push&quot;]EOF# 重启 containerd 服务systemctl daemon-reloadsystemctl restart containerd.service
安装K8s1.31.1
操作节点:[所有节点]
不出意外第一步安装的就是1.31.1的版本
yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes#  配置kubelet为了实现docker使用的cgroupdriver与kubelet使用的cgroup的一致性，建议修改如下文件内容。所有节点均要安装sed -i &#x27;s/^KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=&quot;--cgroup-driver=systemd&quot;/g&#x27; /etc/sysconfig/kubelet#设置kubelet为开机自启动即可，由于没有生成配置文件，集群初始化后自动启动systemctl enable kubelet --now
开始安装K8s
操作节点:[master]
kubeadm config print init-defaults &gt; /etc/kubernetes/init-default.yaml# 修改为国内阿里源sed -i &#x27;s/registry.k8s.io/registry.aliyuncs.com\/google_containers/&#x27; /etc/kubernetes/init-default.yaml # 设置 apiServerIP 地址. 请自行替换192.168.48.101为自己master的IPsed -i &#x27;s/1.2.3.4/192.168.48.101/&#x27; /etc/kubernetes/init-default.yamlsed -i &#x27;/serviceSubnet: 10.96.0.0\/12/a \  podSubnet: 192.168.0.0/16&#x27; /etc/kubernetes/init-default.yamlsed -i &#x27;s/1.31.0/1.31.1/g&#x27; /etc/kubernetes/init-default.yaml#拉取所需镜像kubeadm config images pull --config /etc/kubernetes/init-default.yamlkubeadm init --image-repository registry.aliyuncs.com/google_containers --upload-certs
如果要重置集群，或者报错则运行以下命令，报错了就找原因看看哪里出错了
kubeadm reset[reset] Are you sure you want to proceed? [y/N]: y#输入Ysudo rm -rf /etc/kubernetes/manifests/*sudo iptables -F &amp;&amp; sudo ipvsadm --clear
初始化后运行以下命令
mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config
加入node节点
kubeadm token create --print-join-command
生成以下信息
kubeadm join 192.168.48.101:6443 --token mxybd6.j56dbce9cy698ejr --discovery-token-ca-cert-hash sha256:c1b1c7248f6aeea4d01244a226489958bfaaaa76926077b5c09b143c760b68e9
将这个命令复制给node01和node02运行就可以加入集群了
安装网络插件
echo &#x27;185.199.108.133 raw.githubusercontent.com&#x27; &gt;&gt; /etc/hostscurl https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/calico.yaml -Osed -i &#x27;/- name: WAIT_FOR_DATASTORE/i \ \ \ \ \ \ \ \ \ \ \ \ - name: IP_AUTODETECTION_METHOD\n              value: interface=ens33&#x27; calico.yamlsed -i &#x27;s| docker.io/calico/| registry.cn-hangzhou.aliyuncs.com/qianyios/|&#x27; calico.yamlkubectl apply -f calico.yaml
等待十几分钟这样子，就出现以下全部ready和running就说明K8s集群部署成功
kubectl get pods -A

K8S-dashboard
配置yaml
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yamlsed -i &#x27;s/kubernetesui\/dashboard:v2.7.0/registry.cn-hangzhou.aliyuncs.com\/qianyios\/dashboard:v2.7.0/g&#x27; recommended.yamlsed -i &#x27;s/kubernetesui\/metrics-scraper:v1.0.8/registry.cn-hangzhou.aliyuncs.com\/qianyios\/metrics-scraper:v1.0.8/g&#x27; recommended.yaml
修改配置文件
vim recommended.yaml---kind: ServiceapiVersion: v1metadata:  labels:    app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboardspec:  ports:    - port: 443      targetPort: 8443      nodePort: 30001  type: NodePort  selector:    app: kubernetes-dashboard---
运行
kubectl apply -f recommended.yaml
创建cluster-admin用户
#创建service account并绑定默认cluster-admin管理员群角色#创建用户kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard#用户授权kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin#获取用户Tokenkubectl create token dashboard-admin -n kubernetes-dashboard
记录token
eyJhbGciOiJSUzI1NiIsImtpZCI6IjhsSUtJbk93YU5xR1V2ZndOS0lFMnpVLVR1cEl1YUF5U0JBd2NRUXFHVE0ifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzI4Mzk1MzcxLCJpYXQiOjE3MjgzOTE3NzEsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiYjA5NjA2OTEtNTVkNy00YzhmLTliZGItZDRkNzljYTU0YTJiIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJkYXNoYm9hcmQtYWRtaW4iLCJ1aWQiOiIwYzU0YWE0OC0zYTFkLTQyNmYtODI5ZS01ODVjZWNjMzEyYjAifX0sIm5iZiI6MTcyODM5MTc3MSwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.dYb1MRmrKqNSTs2SKGsXZT_Nq4xtt3JeQEXOaHIjvFNtn_iCk1qTuY6oUBE8xdRv4S1oyqb52udGj0Zb5gYpMEBIfpQxTL_KJkeR-1S-tyl2U1FsH6UCnPE_j7KWh5suU3YJncIhQ26Ei7hC12WuZ9l-_UD3mL2tEPzwjbhnT0qir2Qe4rqrSJNNSQHrtwNVD2O-zv13VaUx6azXArec2GPDYR5ZYbSqMXuklaelwtZoKPLzP0DFnZy4jJ4n1JM7PRzqS5sWT_2nMgpSFZ_a5E0b7knvcNvyQHgHzeIYTrY88wjaCQi3x3cIn2hUvtVsroZySjx3Mz-ZECco5WN-eQ
浏览器访问即可并输入以上token
https://192.168.48.101:30001/

至此K8s-1.31.1部署完成

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>OpenEuler</tag>
        <tag>K8s</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenStack-Train-三节点部署</title>
    <url>/posts/26892/</url>
    <content><![CDATA[
  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
  
    
      
      
        点击输入密码
      
    
  

]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>Centos 7</tag>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenEuler-K8S高可用集群（内部etcd）</title>
    <url>/posts/62904/</url>
    <content><![CDATA[
OpenEuler-部署K8S高可用集群（内部etcd）
主机拓扑



主机名
ip1（NAT）
系统
磁盘
内存




master1
192.168.48.101
OpenEuler-22.03-LTS
100G
4G


master2
192.168.48.102
OpenEuler-22.03-LTS
100G
4G


master3
192.168.48.103
OpenEuler-22.03-LTS
100G
4G


node01
192.168.48.104
OpenEuler-22.03-LTS
100G
8G



镜像下载地址：OpenEuler-22.03-LTS
下载名为openEuler-22.03-LTS-SP4-x86_64-dvd.iso
基础配置
Openeuler通过单独安装，非克隆。安装完后进行基本环境的配置，配置一下几个方面：

设置主机名
关闭firewalld、dnsmasq、selinux
设置ens33
备份并新增、docker-ce源、k8s源
更新yum源软件包缓存
添加hosts解析
关闭swap分区
安装chrony服务，并同步时间
配置limits.conf
安装必备工具
升级系统并重启

操作主机：[master1,master2,master3,node01]
#将以下脚本内容添加进去vi k8s_system_init.sh
#!/bin/bashif [ $# -eq 2 ];then  echo &quot;设置主机名为：$1&quot;  echo &quot;ens33设置IP地址为：192.168.48.$2&quot;else  echo  &quot;使用方法：sh $0 主机名 主机位&quot;  exit 2fiecho &quot;--------------------------------------&quot;echo &quot;1.正在设置主机名：$1&quot;hostnamectl set-hostname $1echo &quot;2.正在关闭firewalld、dnsmasq、selinux&quot;systemctl disable firewalld &amp;&gt; /dev/nullsystemctl disable dnsmasq &amp;&gt; /dev/nullsystemctl stop firewalldsystemctl stop dnsmasqsed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsetenforce 0echo &quot;3.正在设置ens33：192.168.48.$2&quot;cat &gt; /etc/sysconfig/network-scripts/ifcfg-ens33 &lt;&lt;EOFTYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noNAME=ens33UUID=53b402ff-5865-47dd-a853-7afcd6521738DEVICE=ens33ONBOOT=yesIPADDR=192.168.48.$2GATEWAY=192.168.48.2PREFIX=24DNS1=192.168.48.2DNS2=114.114.114.114EOFnmcli c reloadnmcli c up ens33echo &quot;4.新增docker-ce源、k8s源&quot;mkdir /etc/yum.repos.d/bak/cp /etc/yum.repos.d/* /etc/yum.repos.d/bak/sleep 3cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg EOF#切换为华为云，下载速度更快sed -i &#x27;s/\$basearch/x86_64/g&#x27; /etc/yum.repos.d/openEuler.reposed -i &#x27;s/http\:\/\/repo.openeuler.org/https\:\/\/mirrors.huaweicloud.com\/openeuler/g&#x27; /etc/yum.repos.d/openEuler.repocurl -o /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposed -i &#x27;s/\$releasever/7/g&#x27; /etc/yum.repos.d/docker-ce.repoecho &quot;5.更新yum源软件包缓存&quot; yum clean all &amp;&amp; yum makecacheecho &quot;6.添加hosts解析&quot;cat &gt; /etc/hosts &lt;&lt;EOF127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.48.101 master1192.168.48.102 master2192.168.48.103 master3192.168.48.104 node01192.168.48.105 node02EOFecho &quot;7.关闭swap分区&quot;swapoff -a &amp;&amp; sysctl -w vm.swappiness=0 &amp;&gt; /dev/nullsed -ri &#x27;/^[^#]*swap/s@^@#@&#x27; /etc/fstabecho &quot;8.安装chrony服务，并同步时间&quot;yum install chrony -ysystemctl start chronydsystemctl enable chronydchronyc sourceschronyc sourcesecho &quot;9.配置limits.conf&quot;ulimit -SHn 65535cat &gt;&gt; /etc/security/limits.conf &lt;&lt;EOF* soft nofile 65536* hard nofile 131072* soft nproc 65535* hard nproc 655350* soft memlock unlimited* hard memlock unlimitedEOFecho &quot;10.必备工具安装&quot;yum install wget psmisc vim net-tools telnet device-mapper-persistent-data lvm2 git -yecho &quot;11.重启&quot;reboot
sh k8s_system_init.sh 主机名  主机位[master1] sh k8s_system_init.sh master1 101[master2] sh k8s_system_init.sh master2 102[master3] sh k8s_system_init.sh master3 103[node01] sh k8s_system_init.sh node01 104
配置ssh免密
操作节点[master1]
yum install -y sshpass cat &gt; sshmianmi.sh &lt;&lt; &quot;EOF&quot;#!/bin/bash# 目标主机列表hosts=(&quot;master1&quot; &quot;master2&quot; &quot;master3&quot; &quot;node01&quot;)# 密码password=&quot;Lj201840.&quot;# 生成 SSH 密钥对ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa# 循环遍历目标主机for host in &quot;$&#123;hosts[@]&#125;&quot;do    # 复制公钥到目标主机    sshpass -p &quot;$password&quot; ssh-copy-id -o StrictHostKeyChecking=no &quot;$host&quot;        # 验证免密登录    sshpass -p &quot;$password&quot; ssh -o StrictHostKeyChecking=no &quot;$host&quot; &quot;echo &#x27;免密登录成功&#x27;&quot;doneEOFsh sshmianmi.sh
内核及ipvs模块配置
此步骤是配置ipvs模块，开启一些k8s集群中必须的内核参数。配置一下几个方面：

更改内核启动顺序
安装ipvsadm
配置ipvs模块
开启k8s集群必须的内核参数
配置完内核，重启服务器

操作主机：[master1,master2,master3,node01]
vi kernel_update.sh
#!/bin/bashecho &quot;1.更改内核启动顺序&quot;grub2-set-default  0 &amp;&amp; grub2-mkconfig -o /etc/grub2.cfggrubby --args=&quot;user_namespace.enable=1&quot; --update-kernel=&quot;$(grubby --default-kernel)&quot;echo &quot;2.安装ipvsadm&quot;yum install ipvsadm ipset sysstat conntrack libseccomp -y &amp;&gt; /dev/nullecho &quot;3.配置ipvs模块&quot;modprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrackcat &gt;&gt; /etc/modules-load.d/ipvs.conf &lt;&lt;EOFip_vsip_vs_lcip_vs_wlcip_vs_rrip_vs_wrrip_vs_lblcip_vs_lblcrip_vs_dhip_vs_ship_vs_foip_vs_nqip_vs_sedip_vs_ftpip_vs_shnf_conntrackip_tablesip_setxt_setipt_setipt_rpfilteript_REJECTipipEOFsystemctl enable --now systemd-modules-load.service &amp;&gt; /dev/nullecho &quot;4.开启k8s集群必须的内核参数&quot;cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.ip_nonlocal_bind = 1 net.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1fs.may_detach_mounts = 1net.ipv4.conf.all.route_localnet = 1vm.overcommit_memory=1vm.panic_on_oom=0fs.inotify.max_user_watches=89100fs.file-max=52706963fs.nr_open=52706963net.netfilter.nf_conntrack_max=2310720net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_probes = 3net.ipv4.tcp_keepalive_intvl =15net.ipv4.tcp_max_tw_buckets = 36000net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_max_orphans = 327680net.ipv4.tcp_orphan_retries = 3net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.ip_conntrack_max = 65536net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.tcp_timestamps = 0net.core.somaxconn = 16384EOFsysctl --systemecho &quot;5.配置完内核，重启服务器！&quot;reboot
sh kernel_update.sh
检查ipvs加载、内核版本验证
lsmod | grep --color=auto -e ip_vs -e nf_conntrackuname -a

高可用组件安装
haproxy配置
操作节点：[master1，master2,master3]
yum install keepalived haproxy -y
所有Master节点配置HAProxy，所有Master节点的HAProxy配置相同。
操作节点：[master1，master2，master3]cat &gt; /etc/haproxy/haproxy.cfg &lt;&lt;&quot;EOF&quot;global  maxconn  2000  ulimit-n  16384  log  127.0.0.1 local0 err  stats timeout 30sdefaults  log global  mode  http  option  httplog  timeout connect 5000  timeout client  50000  timeout server  50000  timeout http-request 15s  timeout http-keep-alive 15sfrontend monitor-in  bind *:33305  mode http  option httplog  monitor-uri /monitorfrontend k8s-master  bind 0.0.0.0:16443  bind 127.0.0.1:16443  mode tcp  option tcplog  tcp-request inspect-delay 5s  default_backend k8s-masterbackend k8s-master  mode tcp  option tcplog  option tcp-check  balance roundrobin  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100  server master1   192.168.48.101:6443  check  server master2   192.168.48.102:6443  check  server master3   192.168.48.103:6443  checkEOF
Keepalived配置
操作节点：[master1，master2,master3]
所有Master节点配置Keepalived，以下三个Master节点配置注意ip和网卡。
cat &gt; /etc/keepalived/keepalived.conf &lt;&lt; &quot;EOF&quot;! Configuration File for keepalivedglobal_defs &#123;    router_id LVS_DEVEL    script_user root    enable_script_security&#125;vrrp_script chk_apiserver &#123;    script &quot;/etc/keepalived/check_apiserver.sh&quot;    interval 5    weight -5    fall 2    rise 1&#125;vrrp_instance VI_1 &#123;    state MASTER    interface ens33    mcast_src_ip 192.168.48.101    virtual_router_id 51    priority 201    advert_int 2    authentication &#123;        auth_type PASS        auth_pass 1111 # 限制在8个字符以内    &#125;    virtual_ipaddress &#123;        192.168.48.200    &#125;    track_script &#123;        chk_apiserver    &#125;&#125;EOF
cat &gt; /etc/keepalived/keepalived.conf &lt;&lt; &quot;EOF&quot;! Configuration File for keepalivedglobal_defs &#123;    router_id LVS_DEVEL    script_user root    enable_script_security&#125;vrrp_script chk_apiserver &#123;    script &quot;/etc/keepalived/check_apiserver.sh&quot;    interval 5    weight -5    fall 2    rise 1&#125;vrrp_instance VI_1 &#123;    state BACKUP    interface ens33    mcast_src_ip 192.168.48.102    virtual_router_id 51    priority 150    advert_int 2    authentication &#123;        auth_type PASS        auth_pass 1111 # 限制在8个字符以内    &#125;    virtual_ipaddress &#123;        192.168.48.200    &#125;    track_script &#123;        chk_apiserver    &#125;&#125;EOF
cat &gt; /etc/keepalived/keepalived.conf &lt;&lt; &quot;EOF&quot;! Configuration File for keepalivedglobal_defs &#123;    router_id LVS_DEVEL    script_user root    enable_script_security&#125;vrrp_script chk_apiserver &#123;    script &quot;/etc/keepalived/check_apiserver.sh&quot;    interval 5    weight -5    fall 2    rise 1&#125;vrrp_instance VI_1 &#123;    state BACKUP    interface ens33    mcast_src_ip 192.168.48.103    virtual_router_id 51    priority 99    advert_int 2    authentication &#123;        auth_type PASS        auth_pass 1111 # 限制在8个字符以内    &#125;    virtual_ipaddress &#123;        192.168.48.200    &#125;    track_script &#123;        chk_apiserver    &#125;&#125;EOF
配置Keepalived健康检查文件
操作节点：[master1，master2,master3]
cat &gt; /etc/keepalived/check_apiserver.sh &lt;&lt; &quot;EOF&quot;#!/bin/basherr=0for k in \$(seq 1 3)do    check_code=\$(pgrep haproxy)    if [[ \$check_code == &quot;&quot; ]]; then        err=\$(expr \$err + 1)        sleep 1        continue    else        err=0        break    fidoneif [[ \$err != &quot;0&quot; ]]; then    echo &quot;Stopping keepalived due to haproxy failure.&quot;    /usr/bin/systemctl stop keepalived    exit 1else    exit 0fiEOFchmod +x /etc/keepalived/check_apiserver.sh
启动haproxy和keepalived
操作节点：[master，master2,master3]systemctl daemon-reloadsystemctl enable --now haproxysystemctl enable --now keepalivedsystemctl restart haproxy keepalived
测试集群负载均衡高可用
查看master1的vip
ip a

模拟master1的宕机测试，看看vip会不会漂移到master2去
[master1] poweroff

这时候查看master2的ip列表
[master2] ip a

结论：这时可以知道，负载均衡集群成功，当master1出现宕机情况，vip会从master1漂移到master2
docker安装
安装docker
操作节点[master1，master2，master3,node01]
wget https://download.docker.com/linux/static/stable/x86_64/docker-24.0.7.tgztar xf docker-*.tgzcp -rf docker/* /usr/bin/#创建containerd的service文件,并且启动cat &gt;/etc/systemd/system/containerd.service &lt;&lt;EOF[Unit]Description=containerd container runtimeDocumentation=https://containerd.ioAfter=network.target local-fs.target[Service]ExecStartPre=-/sbin/modprobe overlayExecStart=/usr/bin/containerdType=notifyDelegate=yesKillMode=processRestart=alwaysRestartSec=5LimitNPROC=infinityLimitCORE=infinityLimitNOFILE=1048576TasksMax=infinityOOMScoreAdjust=-999[Install]WantedBy=multi-user.targetEOFsystemctl enable --now containerd.service#准备docker的service文件cat &gt; /etc/systemd/system/docker.service &lt;&lt;EOF[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.service containerd.serviceWants=network-online.targetRequires=docker.socket containerd.service[Service]Type=notifyExecStart=/usr/bin/dockerd --config-file=/etc/docker/daemon.json -H fd://containerd=/run/containerd/containerd.sockExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=alwaysStartLimitBurst=3StartLimitInterval=60sLimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityTasksMax=infinityDelegate=yesKillMode=processOOMScoreAdjust=-500[Install]WantedBy=multi-user.targetEOF#准备docker的socket文件cat &gt; /etc/systemd/system/docker.socket &lt;&lt;EOF[Unit]Description=Docker Socket for the API[Socket]ListenStream=/var/run/docker.sockSocketMode=0660SocketUser=rootSocketGroup=docker[Install]WantedBy=sockets.targetEOFgroupadd dockersystemctl enable --now docker.socket  &amp;&amp; systemctl enable --now docker.service#验证mkdir /etc/dockersudo tee /etc/docker/daemon.json &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;&#123;  &quot;registry-mirrors&quot;: [    &quot;https://docker.xuanyuan.me&quot;,    &quot;https://docker.m.daocloud.io&quot;,    &quot;https://docker.1ms.run&quot;,    &quot;https://docker.1panel.live&quot;,    &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,		&quot;https://docker.qianyios.top&quot;  ],  &quot;max-concurrent-downloads&quot;: 10,  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-level&quot;: &quot;warn&quot;,  &quot;log-opts&quot;: &#123;    &quot;max-size&quot;: &quot;10m&quot;,    &quot;max-file&quot;: &quot;3&quot;  &#125;,  &quot;data-root&quot;: &quot;/var/lib/docker&quot;&#125;EOFsystemctl daemon-reloadsystemctl restart docker
安装cri-docker
操作节点[master1，master2，master3,node01]
wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.12/cri-dockerd-0.3.12.amd64.tgztar -zxvf cri-dockerd-0.3.*.amd64.tgzcp cri-dockerd/cri-dockerd  /usr/bin/chmod +x /usr/bin/cri-dockerd#写入启动配置文件cat &gt;  /usr/lib/systemd/system/cri-docker.service &lt;&lt;EOF[Unit]Description=CRI Interface for Docker Application Container EngineDocumentation=https://docs.mirantis.comAfter=network-online.target firewalld.service docker.serviceWants=network-online.targetRequires=cri-docker.socket [Service]Type=notifyExecStart=/usr/bin/cri-dockerd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9ExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=always StartLimitBurst=3 StartLimitInterval=60s LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity TasksMax=infinityDelegate=yesKillMode=process [Install]WantedBy=multi-user.targetEOF#写入socket配置文件cat &gt; /usr/lib/systemd/system/cri-docker.socket &lt;&lt;EOF[Unit]Description=CRI Docker Socket for the APIPartOf=cri-docker.service [Socket]ListenStream=%t/cri-dockerd.sockSocketMode=0660SocketUser=rootSocketGroup=docker [Install]WantedBy=sockets.targetEOFsystemctl daemon-reload &amp;&amp; systemctl enable cri-docker --now
K8S集群安装
安装k8s所需的工具
操作节点[master1，master2，master3,node01]yum -y install  kubeadm kubelet kubectl#为了实现docker使用的cgroupdriver与kubelet使用的cgroup的一致性，配置如下：sed -i &#x27;s/^KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=&quot;--cgroup-driver=systemd&quot;/g&#x27; /etc/sysconfig/kubelet#设置kubelet为开机自启动即可，由于没有生成配置文件，集群初始化后自动启动systemctl enable kubeletsystemctl enable kubelet.service
集群初始化
操作节点[master1,master2,master3]cat &gt; kubeadm-config.yaml &lt;&lt; EOFapiVersion: kubeadm.k8s.io/v1beta3bootstrapTokens:- groups:  - system:bootstrappers:kubeadm:default-node-token  token: abcdef.0123456789abcdef  ttl: 24h0m0s  usages:  - signing  - authenticationkind: InitConfigurationlocalAPIEndpoint:  advertiseAddress: 192.168.48.101  bindPort: 6443nodeRegistration:  criSocket: unix:///var/run/cri-dockerd.sock  imagePullPolicy: IfNotPresent  taints: null---apiServer:  timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta3certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: &#123;&#125;dns: &#123;&#125;etcd:  local:    dataDir: /var/lib/etcdimageRepository: registry.aliyuncs.com/google_containerskind: ClusterConfigurationkubernetesVersion: 1.28.2networking:  dnsDomain: cluster.local  podSubnet: 10.244.0.0/16  serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125;controlPlaneEndpoint: &quot;192.168.48.200:16443&quot;---apiVersion: kubeproxy.config.k8s.io/v1alpha1bindAddress: 0.0.0.0bindAddressHardFail: falseclientConnection:  acceptContentTypes: &quot;&quot;  burst: 0  contentType: &quot;&quot;  kubeconfig: /var/lib/kube-proxy/kubeconfig.conf  qps: 0clusterCIDR: &quot;&quot;configSyncPeriod: 0sconntrack:  maxPerCore: null  min: null  tcpCloseWaitTimeout: null  tcpEstablishedTimeout: nulldetectLocal:  bridgeInterface: &quot;&quot;  interfaceNamePrefix: &quot;&quot;detectLocalMode: &quot;&quot;enableProfiling: falsehealthzBindAddress: &quot;&quot;hostnameOverride: &quot;&quot;iptables:  localhostNodePorts: null  masqueradeAll: false  masqueradeBit: null  minSyncPeriod: 0s  syncPeriod: 0sipvs:  excludeCIDRs: null  minSyncPeriod: 0s  scheduler: &quot;&quot;  strictARP: false  syncPeriod: 0s  tcpFinTimeout: 0s  tcpTimeout: 0s  udpTimeout: 0skind: KubeProxyConfigurationlogging:  flushFrequency: 0  options:    json:      infoBufferSize: &quot;0&quot;  verbosity: 0metricsBindAddress: &quot;&quot;mode: &quot;&quot;nodePortAddresses: nulloomScoreAdj: nullportRange: &quot;&quot;showHiddenMetricsForVersion: &quot;&quot;winkernel:  enableDSR: false  forwardHealthCheckVip: false  networkName: &quot;&quot;  rootHnsEndpointName: &quot;&quot;  sourceVip: &quot;&quot;---apiVersion: kubelet.config.k8s.io/v1beta1authentication:  anonymous:    enabled: false  webhook:    cacheTTL: 0s    enabled: true  x509:    clientCAFile: /etc/kubernetes/pki/ca.crtauthorization:  mode: Webhook  webhook:    cacheAuthorizedTTL: 0s    cacheUnauthorizedTTL: 0scgroupDriver: systemdclusterDNS:- 10.96.0.10clusterDomain: cluster.localcontainerRuntimeEndpoint: &quot;&quot;cpuManagerReconcilePeriod: 0sevictionPressureTransitionPeriod: 0sfileCheckFrequency: 0shealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 0simageMinimumGCAge: 0skind: KubeletConfigurationlogging:  flushFrequency: 0  options:    json:      infoBufferSize: &quot;0&quot;  verbosity: 0memorySwap: &#123;&#125;nodeStatusReportFrequency: 0snodeStatusUpdateFrequency: 0srotateCertificates: trueruntimeRequestTimeout: 0sshutdownGracePeriod: 0sshutdownGracePeriodCriticalPods: 0sstaticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 0ssyncFrequency: 0svolumeStatsAggPeriod: 0sEOFkubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml
准备k8s所需的镜像
操作节点[master1,master2,master3]kubeadm config images pull --config /root/new.yaml 

master1节点初始化
操作节点[master1]
kubeadm init --config /root/new.yaml  --upload-certs
会生成信息

记录信息后面会用到
初始化成功以后，会产生Token值，用于其他节点加入时使用，因此要记录下初始化成功生成的token值（令牌值），有效期24小时，后续需要操作可以重新生成Token
操作节点[master1]
kubeadm join 192.168.48.200:16443 --token abcdef.0123456789abcdef \        --discovery-token-ca-cert-hash sha256:fd91b75286ce9d50c840f380117e25961ad743a8962f3a9f2c80d74c928e1497 \        --control-plane --certificate-key 0dfa57d943f4a2b8b55840d17f67de731b137e11ca4720f7bf9bfd75d8247e22kubeadm join 192.168.48.200:16443 --token abcdef.0123456789abcdef \        --discovery-token-ca-cert-hash sha256:fd91b75286ce9d50c840f380117e25961ad743a8962f3a9f2c80d74c928e1497
操作kubect报错：

此时通过kubectl操作，会出现失败，因为还没有将集群的&quot;钥匙&quot;交给root用户。/etc/kubernetes/admin.conf 文件是 Kubernetes（K8s）集群中的管理员配置文件，它包含了用于管理集群的身份验证和访问信息。所以下面进行配置环境变量，用于访问Kubernetes集群：
添加环境变量
操作节点[master1]
mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config
添加其他master节点至集群中
操作节点[master2,master3]
操作节点[master2,master3]kubeadm join 192.168.48.200:16443 --token abcdef.0123456789abcdef \        --discovery-token-ca-cert-hash sha256:fd91b75286ce9d50c840f380117e25961ad743a8962f3a9f2c80d74c928e1497 \        --control-plane --certificate-key 0dfa57d943f4a2b8b55840d17f67de731b137e11ca4720f7bf9bfd75d8247e22 \        --cri-socket unix:///var/run/cri-dockerd.sock 
注意：这里末尾添加了--cri-socket unix:///var/run/cri-dockerd.sock 
接着给master2添加环境变量
操作节点[master2,master3]mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config

这里没有展示master3的图片，但是步骤一样的
模拟Token过期重新生成并加入Node节点
假设加入集群的token过期了。node01无法加入了，这里就模拟一下这种情况

Token过期后生成新的token：

kubeadm token create --print-join-command
[root@master1 ~]# kubeadm token create --print-join-commandkubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f
其中，192.168.48.200:16443 是你的 Kubernetes API 服务器的地址和端口，tn5q1b.7w1jj77ewup7k2in 是新的令牌，sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f 是令牌的 CA 证书哈希值。

Master需要生成–certificate-key：

kubeadm init phase upload-certs --upload-certs
[root@master1 ~]# kubeadm init phase upload-certs --upload-certsW1110 18:12:03.426245   10185 version.go:104] could not fetch a Kubernetes version from the internet: unable to get URL &quot;https://dl.k8s.io/release/stable-1.txt&quot;: Get &quot;https://dl.k8s.io/release/stable-1.txt&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)W1110 18:12:03.426320   10185 version.go:105] falling back to the local client version: v1.28.2[upload-certs] Storing the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace[upload-certs] Using certificate key:5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128
其中，5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128 是证书密钥。

生成新的Token用于集群添加新Node节点

操作节点[node01]
kubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f  \        --cri-socket unix:///var/run/cri-dockerd.sock 
注意：这里末尾添加了--cri-socket unix:///var/run/cri-dockerd.sock 

这时在master查看node状态（显示为notready不影响）

模拟新加master节点的加入K8S集群中
假设我们新加master节点的话，就拼接token，从刚刚生成的token拼接
[root@master1 ~]# kubeadm token create --print-join-commandkubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f
这里提取信息1
kubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f
接着
[root@master1 ~]# kubeadm init phase upload-certs --upload-certsW1110 18:12:03.426245   10185 version.go:104] could not fetch a Kubernetes version from the internet: unable to get URL &quot;https://dl.k8s.io/release/stable-1.txt&quot;: Get &quot;https://dl.k8s.io/release/stable-1.txt&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)W1110 18:12:03.426320   10185 version.go:105] falling back to the local client version: v1.28.2[upload-certs] Storing the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace[upload-certs] Using certificate key:5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128
这里提取信息2：这里前面要加上--control-plane --certificate-key
--control-plane --certificate-key 5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128
合成
kubeadm join 192.168.48.200:16443 --token tn5q1b.7w1jj77ewup7k2in \        --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f  \        --control-plane --certificate-key 5d3706028d5e569324a4c456c81ae0f5551ece88b3132f03917668c6b0605128 \        --cri-socket unix:///var/run/cri-dockerd.sock                kubeadm join 192.168.48.200:16443 --token lnkno8.u4v1l8n9pahzf0kj \        --discovery-token-ca-cert-hash sha256:cc5427e0e133d95250ab0aa90976fea2c383278b6345fd7e8c28702ac8dfc61f \        --control-plane --certificate-key 41e441fe56cb4bdfcc2fc0291958e6b1da54d01f4649b6651471c07583f85cdf \        --cri-socket unix:///var/run/cri-dockerd.sock
注意：这里末尾添加了--cri-socket unix:///var/run/cri-dockerd.sock 
图示

安装calico网络插件
操作节点[master1]
添加解析记录，否则无法访问
echo &#x27;185.199.108.133 raw.githubusercontent.com&#x27; &gt;&gt; /etc/hosts
应用operator资源清单文件
网络组件有很多种，只需要部署其中一个即可，推荐Calico。
Calico是一个纯三层的数据中心网络方案，Calico支持广泛的平台，包括Kubernetes、OpenStack等。
Calico 在每一个计算节点利用 Linux Kernel 实现了一个高效的虚拟路由器（ vRouter） 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息向整个 Calico 网络内传播。
此外，Calico 项目还实现了 Kubernetes 网络策略，提供ACL功能。
curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/calico.yaml -O
[root@master1 ~]# vim calico.yaml#参考信息 - name: WAIT_FOR_DATASTORE  value: &quot;true&quot;#添加以下两行- name: IP_AUTODETECTION_METHOD  value: interface=ens33#ens33是你的网卡

sed -i &#x27;s| docker.io/calico/| registry.cn-hangzhou.aliyuncs.com/qianyios/|&#x27; calico.yamlkubectl apply -f calico.yaml

监视kube-system命名空间中pod运行情况
等待估计20分钟左右吧(确保全部running)
kubectl get pods -n kube-system

拿掉master节点的污点
节点 master1 和 master2 都有一个名为 node-role.kubernetes.io/control-plane:NoSchedule 的污点。这个污点的作用是阻止普通的 Pod 被调度到这些节点上，只允许特定的控制平面组件（如 kube-apiserver、kube-controller-manager 和 kube-scheduler）在这些节点上运行。
这种设置有助于确保控制平面节点专门用于运行 Kubernetes 的核心组件，而不会被普通的工作负载占用。通过将污点添加到节点上，可以确保只有被授权的控制平面组件才能在这些节点上运行。
kubectl describe node master1 | grep -i taintkubectl describe node master2 | grep -i taintkubectl describe node master3 | grep -i taint

去除污点
kubectl taint node master1 node-role.kubernetes.io/control-plane:NoSchedule-kubectl taint node master2 node-role.kubernetes.io/control-plane:NoSchedule-kubectl taint node master3 node-role.kubernetes.io/control-plane:NoSchedule-

安装dashboard
操作节点[master1]
下载文件
https://github.com/kubernetes/dashboard/releases/tag/v2.7.0
目前最新版本v2.7.0
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yamlsed -i &#x27;s/kubernetesui\/dashboard:v2.7.0/registry.cn-hangzhou.aliyuncs.com\/qianyios\/dashboard:v2.7.0/g&#x27; recommended.yamlsed -i &#x27;s/kubernetesui\/metrics-scraper:v1.0.8/registry.cn-hangzhou.aliyuncs.com\/qianyios\/metrics-scraper:v1.0.8/g&#x27; recommended.yaml

修改配置文件
vim recommended.yaml---kind: ServiceapiVersion: v1metadata:  labels:    app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboardspec:  ports:    - port: 443      targetPort: 8443      nodePort: 30001  type: NodePort  selector:    app: kubernetes-dashboard---

运行dashboard
kubectl apply -f recommended.yaml

检查运行状态
kubectl get pods -n kubernetes-dashboardkubectl get pod,svc -o wide -n kubernetes-dashboard

创建cluster-admin用户
创建service account并绑定默认cluster-admin管理员群角色#创建用户kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard#用户授权kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin#获取用户Tokenkubectl create token dashboard-admin -n kubernetes-dashboard

记录token
eyJhbGciOiJSUzI1NiIsImtpZCI6IjJxb21sRTZSckhEZ09FMnlvWU5IY3dfTTRIWDEzRUpsQ000MThhSWxYNDgifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjk5NjQ2Mjg3LCJpYXQiOjE2OTk2NDI2ODcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJkYXNoYm9hcmQtYWRtaW4iLCJ1aWQiOiIyNTBhNTQ2MS03MGJlLTRhZTItOWY2Yi1hMDQwOWE1NWJhMTMifX0sIm5iZiI6MTY5OTY0MjY4Nywic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.U-lH9_sRU4TVrpAznqS60INNSev9NxHu61igGUTzTBoaWo7WjPG7vzTnpZYhvsoglTzMEKhfranJkkkn95pe-prkvSasaAL6kXHw0jQjlSMzcYiF7DoLdkOtDJSukuALEubidf9eIwZXFZ-sezZdZHm4hnk5nWme5YtdOmYOJPh5sv1dzRvM1XuOHknJPTA1BbdZuVAtSGHSjkhwx-wl-41uuQoROW5GjJs0bz4zLBFn1w_pWaSMCn7pjGJNcbr6IuDV41km_etpwwxacWyfAcxNykzCtIiE1abJj7m-e944GvAn_eqxz3wCZD6Bgt41FWRzyMHjrppJfDjk7FaHNw
登录浏览器访问
https://192.168.48.200:30001输入token：----eyJhbGciOiJSUzI1NiIsImtpZCI6IjJxb21sRTZSckhEZ09FMnlvWU5IY3dfTTRIWDEzRUpsQ000MThhSWxYNDgifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjk5NjQ2Mjg3LCJpYXQiOjE2OTk2NDI2ODcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJkYXNoYm9hcmQtYWRtaW4iLCJ1aWQiOiIyNTBhNTQ2MS03MGJlLTRhZTItOWY2Yi1hMDQwOWE1NWJhMTMifX0sIm5iZiI6MTY5OTY0MjY4Nywic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.U-lH9_sRU4TVrpAznqS60INNSev9NxHu61igGUTzTBoaWo7WjPG7vzTnpZYhvsoglTzMEKhfranJkkkn95pe-prkvSasaAL6kXHw0jQjlSMzcYiF7DoLdkOtDJSukuALEubidf9eIwZXFZ-sezZdZHm4hnk5nWme5YtdOmYOJPh5sv1dzRvM1XuOHknJPTA1BbdZuVAtSGHSjkhwx-wl-41uuQoROW5GjJs0bz4zLBFn1w_pWaSMCn7pjGJNcbr6IuDV41km_etpwwxacWyfAcxNykzCtIiE1abJj7m-e944GvAn_eqxz3wCZD6Bgt41FWRzyMHjrppJfDjk7FaHNw----

部署一个nginx测试
操作节点[master1]
vim web.yamlkind: Deployment#apiVersion: extensions/v1beta1apiVersion: apps/v1metadata:  labels:    app: web-deployment-label  name: web-deployment  namespace: defaultspec:  replicas: 3  selector:    matchLabels:      app: web-selector  template:    metadata:      labels:        app: web-selector    spec:      containers:      - name: web-container        image: nginx:latest        imagePullPolicy: Always        ports:        - containerPort: 80          protocol: TCP          name: http        - containerPort: 443          protocol: TCP          name: https---kind: ServiceapiVersion: v1metadata:  labels:    app: web-service-label  name: web-service  namespace: defaultspec:  type: NodePort  ports:  - name: http    port: 80    protocol: TCP    targetPort: 80    nodePort: 30080  - name: https    port: 443    protocol: TCP    targetPort: 443    nodePort: 30443  selector:    app: web-selector    kubectl apply -f web.yaml 

### 查看nginx的pod 的详细信息kubectl get deploy,svc,pod -o wide

访问nginx网站
http://192.168.48.200:30080


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>OpenEuler</tag>
        <tag>K8s</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenStack Antelope</title>
    <url>/posts/8c5be7b9/</url>
    <content><![CDATA[
OpenStack Antelope
本文档是基于 openEuler 24.03 LTS 的 OpenStack 部署指南
镜像下载： openEuler 24.03 LTS
主机拓扑



主机名
ip
硬盘1
硬盘2
cpu
内存




controller
192.168.48.101
100G

4
7G


compute
192.168.48.102
100G

4
4G


storage
192.168.48.103
100G
100G
4
2G



具体安装的服务如下：



节点名称
OpenStack 服务




控制节点
MariaDB RabbitMQ Memcache Keystone Placement Glance Nova Neutron Cinder Horizon Heat


计算节点
Nova Neutron


块存储节点
Cinder Swift



前情提要
compute启动虚拟机硬件加速（x86_64）

所有节点的内存和硬盘芯片都要按照我的来

安全性
物理节点关闭顺序
给每台机都加上两个脚本
cat &gt;&gt; stop.sh &lt;&lt; EOF#!/bin/bash# 关闭所有 OpenStack 节点# 依次关闭计算节点、网络节点、控制节点for server in \$(openstack server list -f value -c ID); do    openstack server stop \$serverdone# 关闭计算节点echo &quot;Stopping compute services...&quot;systemctl stop openstack-nova-compute.servicesystemctl stop libvirtd.service# 关闭网络节点echo &quot;Stopping network services...&quot;systemctl stop openvswitch.servicesystemctl stop neutron-server.servicesystemctl stop neutron-linuxbridge-agent.servicesystemctl stop neutron-dhcp-agent.servicesystemctl stop neutron-metadata-agent.servicesystemctl stop neutron-l3-agent.service# 关闭控制节点echo &quot;Stopping control services...&quot;systemctl stop mariadb.servicesystemctl stop rabbitmq-server.servicesystemctl stop memcached.servicesystemctl stop httpd.servicesystemctl stop openstack-glance-api.servicesystemctl stop openstack-glance-registry.servicesystemctl stop openstack-cinder-api.servicesystemctl stop openstack-cinder-scheduler.servicesystemctl stop openstack-cinder-volume.servicesystemctl stop openstack-nova-api.servicesystemctl stop openstack-nova-scheduler.servicesystemctl stop openstack-nova-conductor.servicesystemctl stop openstack-nova-novncproxy.servicesystemctl stop openstack-nova-consoleauth.servicesystemctl stop openstack-keystone.servicesystemctl stop openstack-heat-api.servicesystemctl stop openstack-heat-api-cfn.servicesystemctl stop openstack-heat-engine.servicesystemctl stop openstack-swift-proxy.servicesystemctl stop openstack-swift-account.servicesystemctl stop openstack-swift-container.servicesystemctl stop openstack-swift-object.serviceecho &quot;Stopping all services...&quot;systemctl stop --all# 关闭电源echo &quot;Shutting down the system...&quot;poweroffEOFcat &gt;&gt; start.sh &lt;&lt; EOF#!/bin/bash# 重新启动 OpenStack 服务# 依次启动控制节点、网络节点、计算节点# 启动控制节点echo &quot;Starting control services...&quot;systemctl start mariadb.servicesystemctl start rabbitmq-server.servicesystemctl start memcached.servicesystemctl start httpd.servicesystemctl start openstack-glance-api.servicesystemctl start openstack-glance-registry.servicesystemctl start openstack-cinder-api.servicesystemctl start openstack-cinder-scheduler.servicesystemctl start openstack-cinder-volume.servicesystemctl start openstack-nova-api.servicesystemctl start openstack-nova-scheduler.servicesystemctl start openstack-nova-conductor.servicesystemctl start openstack-nova-novncproxy.servicesystemctl start openstack-nova-consoleauth.servicesystemctl start openstack-keystone.servicesystemctl start openstack-heat-api.servicesystemctl start openstack-heat-api-cfn.servicesystemctl start openstack-heat-engine.servicesystemctl start openstack-swift-proxy.servicesystemctl start openstack-swift-account.servicesystemctl start openstack-swift-container.servicesystemctl start openstack-swift-object.service# 启动网络节点echo &quot;Starting network services...&quot;systemctl start openvswitch.servicesystemctl start neutron-server.servicesystemctl start neutron-linuxbridge-agent.servicesystemctl start neutron-dhcp-agent.servicesystemctl start neutron-metadata-agent.servicesystemctl start neutron-l3-agent.service# 启动计算节点echo &quot;Starting compute services...&quot;systemctl start libvirtd.servicesystemctl start openstack-nova-compute.serviceEOF
（先给coompute和storage节点执行-最后等这两个节点完全关闭，再给控制节点执行）
关闭物理机的时候运行sh stop.sh(运行的时候可能会提示你有些服务找不到，报错，这个没关系，但是要是告诉你有些服务起不来，要你自己去找报错了)一般情况下是没问题的
物理节点开启顺序

先开controller再开剩下的计算节点

基本用户信息
OpenStack 各组件都需要在控制节点数据库中注册专属账户以存放数据信息，故需要设置密码，强烈建议各组件的密码以及宿主机密码各不相同。



OpenStack 组件
密码




控制节点 root
Lj201840.


计算节点 root
Lj201840.


Metadata 元数据密钥
METADATA_SECRET


Mariadb root 账户
MARIADB_PASS


RabbitMQ 服务
RABBIT_PASS


OpenStack admin
ADMIN_PASS


Placement 服务
PLACEMENT_PASS


Keystone 数据库
KEYSTONE_DBPASS


Glance 服务
GLANCE_PASS


Glance 数据库
GLANCE_DBPASS


Nova 服务
NOVA_PASS


Nova 数据库
NOVA_DBPASS


Neutron 服务
NEUTRON_PASS


Neutron 数据库
NEUTRON_DBPASS


Cinder 服务
CINDER_PASS


Cinder 数据库
CINDER_DBPASS


Horizon 数据库
DASH_DBPASS


Swift服务
SWIFT_PASS


Heat服务
HEAT_PASS


Heat数据库服务
HEAT_DBPASS


heat_domain_admin用户
HEAT_DOMAIN_USER_PASS



测试用户



用户
密码




admin
123456


use_dog
123456



基础配置
操作节点：[所有节点]
不要一股脑的复制，注意修改网卡的名字，我这里是ens33，修改成你的网卡名字，包括修改ip段，比如我的是192.168.48.你就要修改成你的172.8.3.最后那一个主机位就不用管，其他不变
vi init.sh
#!/bin/bashif [ $# -eq 2 ];then  echo &quot;设置主机名为：$1&quot;  echo &quot;ens33设置IP地址为：192.168.48.$2&quot;else  echo  &quot;使用方法：sh $0 主机名 主机位&quot;  exit 2fiecho &quot;--------------------------------------&quot;echo &quot;1.正在设置主机名：$1&quot;hostnamectl set-hostname $1echo &quot;2.正在关闭firewalld、selinux&quot;systemctl disable firewalld &amp;&gt; /dev/nullsystemctl stop firewalldsed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsetenforce 0echo &quot;3.正在设置ens33：192.168.48.$2&quot;cat &gt; /etc/sysconfig/network-scripts/ifcfg-ens33 &lt;&lt;EOFTYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noNAME=ens33UUID=53b402ff-5865-47dd-a853-7afcd6521738DEVICE=ens33ONBOOT=yesIPADDR=192.168.48.$2GATEWAY=192.168.48.2PREFIX=24DNS1=192.168.48.2DNS2=114.114.114.114EOFnmcli c reloadnmcli c up ens33echo &quot;4.新增华为云源&quot;mkdir /etc/yum.repos.d/bak/cp /etc/yum.repos.d/* /etc/yum.repos.d/bak/#切换为华为云，下载速度更快sed -i &#x27;s/\$basearch/x86_64/g&#x27; /etc/yum.repos.d/openEuler.reposed -i &#x27;s/http\:\/\/repo.openeuler.org/https\:\/\/mirrors.huaweicloud.com\/openeuler/g&#x27; /etc/yum.repos.d/openEuler.repoecho &quot;5.更新yum源软件包缓存&quot;yum clean all &amp;&amp; yum makecacheecho &quot;6.添加hosts解析&quot;cat &gt; /etc/hosts &lt;&lt;EOF127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.48.101   controller192.168.48.102   compute192.168.48.103   storageEOFecho &quot;7.安装chrony服务，并同步时间&quot;yum install chrony -ysystemctl enable chronyd --nowtimedatectl set-timezone Asia/Shanghaitimedatectl set-local-rtc 1timedatectl set-ntp yeschronyc -a makestepchronyc trackingchronyc sourcesecho &quot;8.必备工具安装&quot;yum install wget psmisc vim net-tools telnet socat device-mapper-persistent-data lvm2 git -yecho &quot;9.安装openstack&quot;#新装的系统，默认启用 EPOL ，若没有启动，自行启用 EPOL 软件仓以支持 OpenStackyum update -yyum install openstack-release-antelope -yecho &quot;10.重启&quot;reboot
运行
sh system_init.sh 主机名  主机位[controller] sh init.sh controller 101[compute] sh init.sh compute 102[storage] sh init.sh storage 103
SSH免密
#各节点yum install -y sshpass cat &gt; sshmianmi.sh &lt;&lt; &quot;EOF&quot;#!/bin/bash# 目标主机列表hosts=(&quot;controller&quot; &quot;compute&quot; &quot;storage&quot;)# 密码（注意修改）!!!!!!!!!!!!!!!!!!!!!!!password=&quot;Lj201840.&quot;# SSH 配置文件ssh_config=&quot;$HOME/.ssh/config&quot;# 生成 SSH 密钥对ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa# 确保 SSH 配置文件存在mkdir -p &quot;$HOME/.ssh&quot;touch &quot;$ssh_config&quot;chmod 600 &quot;$ssh_config&quot;# 添加 SSH 配置&#123;    echo &quot;Host *&quot;    echo &quot;    StrictHostKeyChecking no&quot;    echo &quot;    UserKnownHostsFile /dev/null&quot;&#125; &gt;&gt; &quot;$ssh_config&quot;# 循环遍历目标主机for host in &quot;$&#123;hosts[@]&#125;&quot;do    # 复制公钥到目标主机    sshpass -p &quot;$password&quot; ssh-copy-id -o StrictHostKeyChecking=no &quot;$host&quot;        # 验证免密登录    sshpass -p &quot;$password&quot; ssh -o StrictHostKeyChecking=no &quot;$host&quot; &quot;echo &#x27;免密登录成功&#x27;&quot;doneEOFsh sshmianmi.sh
依赖组件
时间同步
集群环境时刻要求每个节点的时间一致，一般由时钟同步软件保证。本文使用chrony软件
由于前面基础配置已经安装chrony，这里就不再安装了
操作节点：[controller]
echo &quot;allow 192.168.48.0/24&quot; | sudo tee -a /etc/chrony.conf &gt; /dev/nullsystemctl restart chronyd
操作节点：[compute,storage]
# 192.168.48.101是controller IP，表示从这个机器获取时间，这里我们填192.168.48.101，或者在`/etc/hosts`里配置好的controller名字即可。echo &quot;server 192.168.48.101 iburst&quot; | sudo tee -a /etc/chrony.conf &gt; /dev/nullsystemctl restart chronyd
配置完成后，检查一下结果，在其他非controller节点执行chronyc sources，返回结果类似如下内容，表示成功从controller同步时钟


安装数据库
操作节点:[controller]
dnf install mysql-config mariadb mariadb-server python3-PyMySQL -ycat &gt; /etc/my.cnf.d/openstack.cnf &lt;&lt; &quot;EOF&quot;[mysqld]bind-address = 192.168.48.101default-storage-engine = innodbinnodb_file_per_table = onmax_connections = 4096collation-server = utf8_general_cicharacter-set-server = utf8EOFsystemctl enable --now mariadb#初始化数据库mysql_secure_installationEnter current password for root (enter for none): 回车Switch to unix_socket authentication [Y/n] nChange the root password? [Y/n] y# 将要求输入数据库 root 账户密码 MARIADB_PASSRemove anonymous users? [Y/n] yDisallow root login remotely? [Y/n] YRemove test database and access to it? [Y/n] yReload privilege tables now? [Y/n] y# 验证mysql -u root -pMARIADB_PASS
安装消息队列
操作节点:[controller]
dnf install rabbitmq-server -ysystemctl enable --now rabbitmq-serverrabbitmqctl add_user openstack RABBIT_PASSrabbitmqctl set_permissions openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;
安装缓存服务
操作节点:[controller]
controller是我的主机名，你要改成你的主机名
dnf install memcached python3-memcached -ysed -i &quot;s/OPTIONS=\&quot;-l 127.0.0.1,::1\&quot;/OPTIONS=\&quot;-l 127.0.0.1,::1,controller\&quot;/g&quot; /etc/sysconfig/memcachedsystemctl enable memcached --now
Keystone
Keystone是OpenStack提供的鉴权服务，是整个OpenStack的入口，提供了租户隔离、用户认证、服务发现等功能，必须安装。
建立一个脚本  (后续我都不做解释了，自行添加脚本，然后运行，要注意脚本里的密码啥的，改成自己需要的)
mkdir openstack-installcd openstack-installvim keystone.sh
添加以下内容
#!/bin/bash# 创建数据库并授权#替换 KEYSTONE_DBPASS，为 keystone 数据库设置密码echo &quot;CREATE DATABASE keystone;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON keystone.* TO &#x27;keystone&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;KEYSTONE_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON keystone.* TO &#x27;keystone&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;KEYSTONE_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSmysql -u root -pMARIADB_PASS -e &quot;FLUSH PRIVILEGES;&quot;sleep 3# 安装OpenStack Keystone和相关依赖dnf install openstack-keystone httpd mod_wsgi -y# 备份并配置keystone配置文件mv /etc/keystone/keystone.conf&#123;,.bak&#125;cat &gt; /etc/keystone/keystone.conf &lt;&lt;EOF[database]connection = mysql+pymysql://keystone:KEYSTONE_DBPASS@controller/keystone[token]provider = fernetEOF# 同步数据库su -s /bin/sh -c &quot;keystone-manage db_sync&quot; keystoneecho &quot;use keystone; show tables;&quot; | mysql -u root -pMARIADB_PASS# 初始化Fernet密钥仓库和凭证keystone-manage fernet_setup --keystone-user keystone --keystone-group keystonekeystone-manage credential_setup --keystone-user keystone --keystone-group keystone# 启动服务keystone-manage bootstrap --bootstrap-password 123456 \--bootstrap-admin-url http://controller:5000/v3/ \--bootstrap-internal-url http://controller:5000/v3/ \--bootstrap-public-url http://controller:5000/v3/ \--bootstrap-region-id RegionOne# 配置Apache HTTP servercp /etc/httpd/conf/httpd.conf&#123;,.bak&#125;sed -i &quot;s/#ServerName www.example.com:80/ServerName controller/g&quot; /etc/httpd/conf/httpd.confln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/systemctl enable httpd --now# 创建管理员账户配置文件cat &lt;&lt; EOF &gt; /root/admin-openrcexport OS_PROJECT_DOMAIN_NAME=Defaultexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=123456export OS_AUTH_URL=http://controller:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2EOF# 创建测试用户配置文件cat &lt;&lt; EOF &gt; /root/user_dog-openrcexport OS_USERNAME=user_dogexport OS_PASSWORD=123456export OS_PROJECT_NAME=Demoexport OS_USER_DOMAIN_NAME=RegionOneexport OS_PROJECT_DOMAIN_NAME=RegionOneexport OS_AUTH_URL=http://controller:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2EOF# 安装python3-openstackclientdnf install python3-openstackclient -y# 依次创建domain, projects, users, rolessource /root/admin-openrcopenstack domain create --description &quot;An RegionOne Domain&quot; RegionOneopenstack project create --domain default --description &quot;Service Project&quot; service# 在 RegionOne 域中创建一个 Demo 项目openstack project create --domain RegionOne --description &quot;Demo Project&quot; Demoopenstack user create --domain RegionOne --password 123456 user_dogopenstack role create user_dog_roleopenstack role add --project Demo --user user_dog user_dog_role# 列举当前所有域名openstack domain list
运行keystone脚本
sh keystone.sh

Glance
cd openstack-installvim glance.sh
#!/bin/bash#替换 GLANCE_DBPASS，为 glance 数据库设置密码echo &quot;CREATE DATABASE glance;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON glance.* TO &#x27;glance&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;GLANCE_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON glance.* TO &#x27;glance&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;GLANCE_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSmysql -u root -pMARIADB_PASS -e &quot;FLUSH PRIVILEGES;&quot;source /root/admin-openrc#创建glance用户并设置密码openstack user create --domain default --password GLANCE_PASS glance#添加glance用户到service project并指定admin角色openstack role add --project service --user glance admin#创建glance服务实体openstack service create --name glance --description &quot;OpenStack Image&quot; image#创建glance API服务openstack endpoint create --region RegionOne image public http://controller:9292openstack endpoint create --region RegionOne image internal http://controller:9292openstack endpoint create --region RegionOne image admin http://controller:9292#安装软件包dnf install openstack-glance -y#修改配置文件mv /etc/glance/glance-api.conf&#123;,.bak&#125;cat &gt;/etc/glance/glance-api.conf &lt;&lt; &quot;EOF&quot;[database]connection = mysql+pymysql://glance:GLANCE_DBPASS@controller/glance[keystone_authtoken]www_authenticate_uri  = http://controller:5000auth_url = http://controller:5000memcached_servers = controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = glancepassword = GLANCE_PASS[paste_deploy]flavor = keystone[glance_store]stores = file,httpdefault_store = filefilesystem_store_datadir = /var/lib/glance/images/EOF#同步数据库su -s /bin/sh -c &quot;glance-manage db_sync&quot; glance#启动服务systemctl enable openstack-glance-api.service --nowsource /root/admin-openrcmkdir /root/openstack-install/iso/#下载镜像wget -P /root/openstack-install/iso/ https://resource.qianyios.top/cloud/iso/cirros-0.4.0-x86_64-disk.img#向Image服务上传镜像source /root/admin-openrcopenstack image create --disk-format qcow2 --container-format bare \   --file /root/openstack-install/iso/cirros-0.4.0-x86_64-disk.img --public cirros#验证是否上传openstack image list
sh glance.sh

Placement
Placement是OpenStack提供的资源调度组件，一般不面向用户，由Nova等组件调用，安装在控制节点。
安装、配置Placement服务前，需要先创建相应的数据库、服务凭证和API endpoints。
操作节点[controller]
cd openstack-installvim placement.sh
#!/bin/bash#创建数据库并授权#替换PLACEMENT_DBPASS为placement数据库访问密码。echo &quot;CREATE DATABASE placement;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON placement.* TO &#x27;placement&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;PLACEMENT_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON placement.* TO &#x27;placement&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;PLACEMENT_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSmysql -u root -pMARIADB_PASS -e &quot;FLUSH PRIVILEGES;&quot;source /root/admin-openrc#创建placement用户并设置密码openstack user create --domain default --password PLACEMENT_PASS placement#添加placement用户到service project并指定admin角色openstack role add --project service --user placement admin#创建placement服务实体openstack service create --name placement \  --description &quot;Placement API&quot; placement#创建Placement API服务endpointsopenstack endpoint create --region RegionOne \  placement public http://controller:8778openstack endpoint create --region RegionOne \  placement internal http://controller:8778openstack endpoint create --region RegionOne \  placement admin http://controller:8778#安装placementdnf install openstack-placement-api -y#配置placementmv /etc/placement/placement.conf&#123;,.bak&#125;#PLACEMENT_DBPASS为placement服务的数据库账户密码#PLACEMENT_PASS为placement服务的密码#以下的placement.conf文件不要有任何的注释，不然后面会报错cat &gt; /etc/placement/placement.conf &lt;&lt; &quot;EOF&quot;[placement_database]connection = mysql+pymysql://placement:PLACEMENT_DBPASS@controller/placement[api]auth_strategy = keystone[keystone_authtoken]auth_url = http://controller:5000/v3memcached_servers = controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = placementpassword = PLACEMENT_PASSEOFsu -s /bin/sh -c &quot;placement-manage db sync&quot; placementoslopolicy-convert-json-to-yaml  --namespace placement \  --policy-file /etc/placement/policy.json \  --output-file /etc/placement/policy.yamlmv /etc/placement/policy.json&#123;,.bak&#125;systemctl restart httpddnf install python3-osc-placement -yplacement-status upgrade check
sh placement.sh

验证服务，列出可用的资源类别及特性
source /root/admin-openrcopenstack --os-placement-api-version 1.2 resource class list --sort-column nameopenstack --os-placement-api-version 1.6 trait list --sort-column name


Nova
Nova是OpenStack的计算服务，负责虚拟机的创建、发放等功能。
controller
操作节点[controller]
cd openstack-installvim nova-controller.sh
#创建数据库并授权#创建nova_api、nova和nova_cell0数据库：echo &quot;CREATE DATABASE nova_api;&quot; | mysql -u root -pMARIADB_PASSecho &quot;CREATE DATABASE nova;&quot; | mysql -u root -pMARIADB_PASSecho &quot;CREATE DATABASE nova_cell0;&quot; | mysql -u root -pMARIADB_PASS#授权echo &quot;GRANT ALL PRIVILEGES ON nova_api.* TO &#x27;nova&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;NOVA_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON nova_api.* TO &#x27;nova&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;NOVA_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON nova.* TO &#x27;nova&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;NOVA_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON nova.* TO &#x27;nova&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;NOVA_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON nova_cell0.* TO &#x27;nova&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;NOVA_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON nova_cell0.* TO &#x27;nova&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;NOVA_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSmysql -u root -pMARIADB_PASS -e &quot;FLUSH PRIVILEGES;&quot;source /root/admin-openrc#创建nova用户并设置用户密码openstack user create --domain default --password NOVA_PASS nova#添加nova用户到service project并指定admin角色openstack role add --project service --user nova admin#创建nova服务实体openstack service create --name nova \  --description &quot;OpenStack Compute&quot; compute#创建Nova API服务endpoints：openstack endpoint create --region RegionOne \  compute public http://controller:8774/v2.1openstack endpoint create --region RegionOne \  compute internal http://controller:8774/v2.1openstack endpoint create --region RegionOne \  compute admin http://controller:8774/v2.1dnf install openstack-nova-api openstack-nova-conductor \  openstack-nova-novncproxy openstack-nova-scheduler -ymv /etc/nova/nova.conf&#123;,.bak&#125;cat &gt; /etc/nova/nova.conf &lt;&lt;EOF[DEFAULT]enabled_apis = osapi_compute,metadatatransport_url = rabbit://openstack:RABBIT_PASS@controller:5672/# RABBIT_PASS为 rabbitmq 密码my_ip = 192.168.48.101# 控制节点的 IPlog_dir = /var/log/novastate_path = /var/lib/nova[api_database]connection = mysql+pymysql://nova:NOVA_DBPASS@controller/nova_api# NOVA_DBPASS 为数据库 Nova 账户密码[database]connection = mysql+pymysql://nova:NOVA_DBPASS@controller/nova# NOVA_DBPASS 为数据库 Nova 账户密码[api]auth_strategy = keystone[keystone_authtoken]auth_url = http://controller:5000/v3memcached_servers = controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = novapassword = NOVA_PASS# NOVA_PASS 为 Nova 服务的密码[vnc]enabled = trueserver_listen = \$my_ipserver_proxyclient_address = \$my_ip[glance]api_servers = http://controller:9292[oslo_concurrency]lock_path = /var/lib/nova/tmp[placement]region_name = RegionOneproject_domain_name = Defaultproject_name = serviceauth_type = passworduser_domain_name = Defaultauth_url = http://controller:5000/v3username = placementpassword = PLACEMENT_PASS# PLACEMENT_PASS 为 placement 服务的密码EOF#同步数据库su -s /bin/sh -c &quot;nova-manage api_db sync&quot; novasu -s /bin/sh -c &quot;nova-manage cell_v2 map_cell0&quot; novasu -s /bin/sh -c &quot;nova-manage cell_v2 create_cell --name=cell1 --verbose&quot; novasu -s /bin/sh -c &quot;nova-manage db sync&quot; nova#验证cell0和cell1注册正确su -s /bin/sh -c &quot;nova-manage cell_v2 list_cells&quot; nova#启动服务systemctl enable \  openstack-nova-api.service \  openstack-nova-scheduler.service \  openstack-nova-conductor.service \  openstack-nova-novncproxy.service --now
sh nova-controller.sh

compute
操作节点[compute]
确认计算节点是否支持虚拟机硬件加速（x86_64）
egrep -c &#x27;(vmx|svm)&#x27; /proc/cpuinfo
如果返回值为0则不支持硬件加速。
这时候请回到文章开头前情提要部分，认真看看如何开启硬件加速
不好好看文章，你就要回到开头呗，累吧？
mkdir openstack-install &amp;&amp; cd openstack-installvim nova-compute.sh
#!/bin/bashdnf install openstack-nova-compute -ymv /etc/nova/nova.conf&#123;,.bak&#125;cat &gt; /etc/nova/nova.conf &lt;&lt;EOF[DEFAULT]enabled_apis = osapi_compute,metadatatransport_url = rabbit://openstack:RABBIT_PASS@controller:5672/#替换RABBIT_PASS为RabbitMQ中openstack账户的密码。my_ip = 192.168.48.102compute_driver = libvirt.LibvirtDriverinstances_path = /var/lib/nova/instanceslog_dir = /var/log/novalock_path = /var/lock/novastate_path = /var/lib/nova[api]auth_strategy = keystone[keystone_authtoken]auth_url = http://controller:5000/v3memcached_servers = controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = novapassword = NOVA_PASS#替换NOVA_PASS为nova用户的密码。[vnc]enabled = trueserver_listen = \$my_ipserver_proxyclient_address = \$my_ipnovncproxy_base_url = http://192.168.48.101:6080/vnc_auto.html#建议设置成192.168.48.101，也就是controller，这个决定你在页面访问实例控制台的地址，如果你是自己的电脑访问，然后你又没有设置hosts映射到controller，你会访问不到的，改成ip可以访问[glance]api_servers = http://controller:9292[oslo_concurrency]lock_path = /var/lib/nova/tmp[placement]region_name = RegionOneproject_domain_name = Defaultproject_name = serviceauth_type = passworduser_domain_name = Defaultauth_url = http://controller:5000/v3username = placementpassword = PLACEMENT_PASS#PLACEMENT_PASS 为 Placement 服务密码EOFsystemctl enable libvirtd.service openstack-nova-compute.service --now
sh nova-compute.sh
验证服务
操作节点[controller]
source /root/admin-openrcopenstack compute service list --service nova-compute#当你向环境中添加新的计算节点时，确保这些新节点被加入到正确的Cell中su -s /bin/sh -c &quot;nova-manage cell_v2 discover_hosts --verbose&quot; nova#列出服务组件，验证每个流程都成功启动和注册：openstack compute service list#列出身份服务中的API端点，验证与身份服务的连接：openstack catalog list#列出镜像服务中的镜像，验证与镜像服务的连接：openstack image list#检查cells是否运作成功，以及其他必要条件是否已具备。nova-status upgrade check



Neutron
Neutron是OpenStack的网络服务，提供虚拟交换机、IP路由、DHCP等功能。
controller
操作节点[controller]
cd openstack-installvim neutron-controller.sh
#!/bin/bash#创建数据库并授权echo &quot;CREATE DATABASE neutron;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON neutron.* TO &#x27;neutron&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;NEUTRON_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON neutron.* TO &#x27;neutron&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;NEUTRON_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSmysql -u root -pMARIADB_PASS -e &quot;FLUSH PRIVILEGES;&quot;#创建neutron用户和服务source /root/admin-openrcopenstack user create --domain default --password NEUTRON_PASS neutronopenstack role add --project service --user neutron adminopenstack service create --name neutron \  --description &quot;OpenStack Networking&quot; network#部署 Neutron API 服务openstack endpoint create --region RegionOne \  network public http://controller:9696openstack endpoint create --region RegionOne \  network internal http://controller:9696openstack endpoint create --region RegionOne \  network admin http://controller:9696#安装软件包dnf install -y openstack-neutron openstack-neutron-linuxbridge ebtables ipset openstack-neutron-ml2#配置Neutronmv /etc/neutron/neutron.conf&#123;,.bak&#125;cat &gt;/etc/neutron/neutron.conf&lt;&lt;&quot;EOF&quot;[database]connection = mysql+pymysql://neutron:NEUTRON_DBPASS@controller/neutron[DEFAULT]core_plugin = ml2service_plugins = routerallow_overlapping_ips = truetransport_url = rabbit://openstack:RABBIT_PASS@controllerauth_strategy = keystonenotify_nova_on_port_status_changes = truenotify_nova_on_port_data_changes = true[keystone_authtoken]www_authenticate_uri = http://controller:5000auth_url = http://controller:5000memcached_servers = controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = neutronpassword = NEUTRON_PASS[nova]auth_url = http://controller:5000auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultregion_name = RegionOneproject_name = serviceusername = novapassword = NOVA_PASS[oslo_concurrency]lock_path = /var/lib/neutron/tmp[experimental]linuxbridge = trueEOF#配置ML2，ML2具体配置可以根据用户需求自行修改，本文使用的是provider network + linuxbridge**mv /etc/neutron/plugins/ml2/ml2_conf.ini&#123;,.bak&#125;#修改/etc/neutron/plugins/ml2/ml2_conf.inicat &gt; /etc/neutron/plugins/ml2/ml2_conf.ini &lt;&lt; EOF[ml2]type_drivers = flat,vlan,vxlantenant_network_types = vxlanmechanism_drivers = linuxbridge,l2populationextension_drivers = port_security[ml2_type_flat]flat_networks = provider[ml2_type_vxlan]vni_ranges = 1:1000[securitygroup]enable_ipset = trueEOF#修改/etc/neutron/plugins/ml2/linuxbridge_agent.inicat &gt; /etc/neutron/plugins/ml2/linuxbridge_agent.ini &lt;&lt;EOF[linux_bridge]physical_interface_mappings = provider:ens33# ens33 为第一块网卡名称[vxlan]enable_vxlan = truelocal_ip = 192.168.48.101l2_population = true# 192.168.48.101 为控制节点的 IP[securitygroup]enable_security_group = truefirewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriverEOF#配置Layer-3代理mv /etc/neutron/l3_agent.ini&#123;,.bak&#125;cat &gt; /etc/neutron/l3_agent.ini &lt;&lt; EOF[DEFAULT]interface_driver = linuxbridgeEOF#配置DHCP代理 mv /etc/neutron/dhcp_agent.ini&#123;,.bak&#125;cat &gt; /etc/neutron/dhcp_agent.ini &lt;&lt; EOF[DEFAULT]interface_driver = linuxbridgedhcp_driver = neutron.agent.linux.dhcp.Dnsmasqenable_isolated_metadata = trueEOF#配置metadata代理mv /etc/neutron/metadata_agent.ini&#123;,.bak&#125;cat &gt;&gt; /etc/neutron/metadata_agent.ini &lt;&lt; EOF[DEFAULT]nova_metadata_host = controllermetadata_proxy_shared_secret = METADATA_SECRET# METADATA_SECRET 为 元数据 的密钥EOFcat &gt;&gt; /etc/nova/nova.conf &lt;&lt; EOF#追加在末尾[neutron]auth_url = http://controller:5000auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = NEUTRON_PASSservice_metadata_proxy = truemetadata_proxy_shared_secret = METADATA_SECRET#NEUTRON_PASS  为 neutron 服务的密码#METADATA_SECRET 为上边设置的元数据密钥EOF#创建/etc/neutron/plugin.ini的符号链接ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.inisu -s /bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head&quot; neutronsystemctl restart openstack-nova-apisystemctl enable neutron-server.service neutron-linuxbridge-agent.service \neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.service --now
sh neutron-controller.sh
compute
操作节点[compute]
cd openstack-installvim neutron-compute.sh
#!/bin/bashdnf install openstack-neutron-linuxbridge ebtables ipset -y#配置neutroncat &gt;/etc/neutron/neutron.conf &lt;&lt; &quot;EOF&quot;[DEFAULT]transport_url = rabbit://openstack:RABBIT_PASS@controllerauth_strategy = keystone[keystone_authtoken]www_authenticate_uri = http://controller:5000auth_url = http://controller:5000memcached_servers = controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = neutronpassword = NEUTRON_PASS[oslo_concurrency]lock_path = /var/lib/neutron/tmpEOF#修改/etc/neutron/plugins/ml2/linuxbridge_agent.inicat &gt; /etc/neutron/plugins/ml2/linuxbridge_agent.ini &lt;&lt;EOF[linux_bridge]physical_interface_mappings = provider:ens33# ens33 为第一块网卡名称[vxlan]enable_vxlan = truelocal_ip = 192.168.48.102l2_population = true# 192.168.48.102 为计算节点的 IP[securitygroup]enable_security_group = truefirewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriverEOF#配置nova compute服务使用neutroncat &gt;&gt; /etc/nova/nova.conf &lt;&lt; EOF#追加在末尾[neutron]auth_url = http://controller:5000auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = NEUTRON_PASSservice_metadata_proxy = truemetadata_proxy_shared_secret = METADATA_SECRET#NEUTRON_PASS  为 neutron 服务的密码#METADATA_SECRET 为上边设置的元数据密钥EOFsystemctl restart openstack-nova-compute.servicesystemctl enable neutron-linuxbridge-agent --now
sh neutron-compute.sh
验证服务
操作节点[controller]
source admin-openrcopenstack network agent list
确保五个都是up你再做下一步

Cinder
Cinder是OpenStack的存储服务，提供块设备的创建、发放、备份等功能。
controller
操作节点[controller]
cd openstack-installvim cinder-controller.sh
my_ip = 192.168.48.101为控制节点ip
#!/bin/bashecho &quot;CREATE DATABASE cinder;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON cinder.* TO &#x27;cinder&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;CINDER_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON cinder.* TO &#x27;cinder&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;CINDER_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSmysql -u root -pMARIADB_PASS -e &quot;FLUSH PRIVILEGES;&quot;source /root/admin-openrcopenstack user create --domain default --password CINDER_PASS cinderopenstack role add --project service --user cinder adminopenstack service create --name cinderv3 \  --description &quot;OpenStack Block Storage&quot; volumev3openstack endpoint create --region RegionOne \  volumev3 public http://controller:8776/v3/%\(project_id\)sopenstack endpoint create --region RegionOne \  volumev3 internal http://controller:8776/v3/%\(project_id\)sopenstack endpoint create --region RegionOne \  volumev3 admin http://controller:8776/v3/%\(project_id\)sdnf install openstack-cinder-api openstack-cinder-scheduler -ycat &gt;/etc/cinder/cinder.conf&lt;&lt;&quot;EOF&quot;[DEFAULT]transport_url = rabbit://openstack:RABBIT_PASS@controllerauth_strategy = keystonemy_ip = 192.168.48.101[database]connection = mysql+pymysql://cinder:CINDER_DBPASS@controller/cinder[keystone_authtoken]www_authenticate_uri = http://controller:5000auth_url = http://controller:5000memcached_servers = controller:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = cinderpassword = CINDER_PASS[oslo_concurrency]lock_path = /var/lib/cinder/tmpEOFsu -s /bin/sh -c &quot;cinder-manage db sync&quot; cindercat &gt;&gt; /etc/nova/nova.conf &lt;&lt; EOF[cinder]os_region_name = RegionOneEOFsystemctl restart openstack-nova-apisystemctl enable openstack-cinder-api openstack-cinder-scheduler  --now
sh cinder-controller.sh
Storage
关闭所有节点(运行不了，请回开头，复制关闭顺序脚本)（注意关闭顺序，这里不在说明，回头看）
操作节点[所有节点]
sh stop.sh
给compute添加一块100g的磁盘

操作节点[Storage]
按照前情提要里的Storage节点，添加一块新硬盘，我这里是/dev/sdb
Cinder支持很多类型的后端存储，本指导使用最简单的lvm为参考，如果您想使用如ceph等其他后端，请自行配置。
mkdir openstack-install &amp;&amp; cd openstack-installvim cinder-storage.sh
#!/bin/bashdnf install lvm2 device-mapper-persistent-data scsi-target-utils rpcbind nfs-utils openstack-cinder-volume openstack-cinder-backup -y#配置lvm卷组pvcreate /dev/sdbvgcreate cinder-volumes /dev/sdb# sdb 为划分给块存储使用的磁盘# 如有多个磁盘，则需重复以上两个命令#配置cindermv /etc/cinder/cinder.conf&#123;,.bak&#125;cat &gt;/etc/cinder/cinder.conf&lt;&lt;&quot;EOF&quot;[DEFAULT]transport_url = rabbit://openstack:RABBIT_PASS@controllerauth_strategy = keystonemy_ip = 192.168.0.4enabled_backends = lvmglance_api_servers = http://controller:9292[keystone_authtoken]www_authenticate_uri = http://controller:5000auth_url = http://controller:5000memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = CINDER_PASS[database]connection = mysql+pymysql://cinder:CINDER_DBPASS@controller/cinder[lvm]volume_driver = cinder.volume.drivers.lvm.LVMVolumeDrivervolume_group = cinder-volumestarget_protocol = iscsitarget_helper = lioadm[oslo_concurrency]lock_path = /var/lib/cinder/tmpEOFsystemctl enable openstack-cinder-volume target --now
sh cinder-storage.sh
至此，Cinder服务的部署已全部完成，可以在controller通过以下命令进行简单的验证

配置cinder backup （可选）
cinder-backup是可选的备份服务，cinder同样支持很多种备份后端，本文使用swift存储，如果您想使用如NFS等后端，请自行配置，例如可以参考OpenStack官方文档对NFS的配置说明。
修改/etc/cinder/cinder.conf，在[DEFAULT]中新增
[DEFAULT]backup_driver = cinder.backup.drivers.swift.SwiftBackupDriverbackup_swift_url = SWIFT_URL
这里的SWIFT_URL是指环境中swift服务的URL，在部署完swift服务后，执行openstack catalog show object-store命令获取。
systemctl start openstack-cinder-backup 

验证服务
操作节点[controller]
source admin-openrcsystemctl restart httpd memcachedopenstack volume service list

Horizon
Horizon是OpenStack提供的前端页面，可以让用户通过网页鼠标的操作来控制OpenStack集群，而不用繁琐的CLI命令行。Horizon一般部署在控制节点。
操作节点[controller]
cd openstack-installvim horizon-controller.sh
dnf install openstack-dashboard -ycp /etc/openstack-dashboard/local_settings&#123;,.bak&#125;sed -i &#x27;s/^ALLOWED_HOSTS/#&amp;/&#x27; /etc/openstack-dashboard/local_settingssed -i &#x27;s/^OPENSTACK_HOST/#&amp;/&#x27; /etc/openstack-dashboard/local_settingssed -i &#x27;s/^OPENSTACK_KEYSTONE_URL/#&amp;/&#x27; /etc/openstack-dashboard/local_settingssed -i &#x27;s/^TIME_ZONE/#&amp;/&#x27; /etc/openstack-dashboard/local_settingscat &gt;&gt;/etc/openstack-dashboard/local_settings &lt;&lt; &quot;EOF&quot;OPENSTACK_HOST = &quot;controller&quot;ALLOWED_HOSTS = [&#x27;*&#x27;]OPENSTACK_KEYSTONE_URL =  &quot;http://controller:5000/v3&quot;SESSION_ENGINE = &#x27;django.contrib.sessions.backends.cache&#x27;CACHES = &#123;&#x27;default&#x27;: &#123;    &#x27;BACKEND&#x27;: &#x27;django.core.cache.backends.memcached.MemcachedCache&#x27;,    &#x27;LOCATION&#x27;: &#x27;controller:11211&#x27;,    &#125;&#125;OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = TrueOPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &quot;Default&quot;OPENSTACK_KEYSTONE_DEFAULT_ROLE = &quot;member&quot;WEBROOT = &#x27;/dashboard&#x27;TIME_ZONE = &quot;Asia/Shanghai&quot;POLICY_FILES_PATH = &quot;/etc/openstack-dashboard&quot;OPENSTACK_API_VERSIONS = &#123;    &quot;identity&quot;: 3,    &quot;image&quot;: 2,    &quot;volume&quot;: 3,&#125;EOFsystemctl restart httpd memcached
sh horizon-controller.sh
至此，horizon服务的部署已全部完成，打开浏览器，输入http://192.168.48.101/dashboard，打开horizon登录页面。


Swift
Swift 提供了弹性可伸缩、高可用的分布式对象存储服务，适合存储大规模非结构化数据。
controller
操作节点[controller]
cd openstack-installvim swift-controller-1.sh
#!/bin/bash#创建服务凭证以及API端点。# 创建swift用户source /root/admin-openrcopenstack user create --domain default --password SWIFT_PASS swift# 添加admin角色openstack role add --project service --user swift admin# 创建对象存储服务openstack service create --name swift --description &quot;OpenStack Object Storage&quot; object-store#创建API端点。openstack endpoint create --region RegionOne object-store public http://controller:8080/v1/AUTH_%\(project_id\)sopenstack endpoint create --region RegionOne object-store internal http://controller:8080/v1/AUTH_%\(project_id\)sopenstack endpoint create --region RegionOne object-store admin http://controller:8080/v1 #安装Swift。dnf install openstack-swift-proxy python3-swiftclient \python3-keystoneclient python3-keystonemiddleware memcached -y#配置proxy-server。#Swift RPM包里已经包含了一个基本可用的proxy-server.conf，只需要手动修改其中的ip（controller可以不改，看你自己需要）和SWIFT_PASS即可。sed -i &quot;s/password = swift/password = SWIFT_PASS/g&quot; /etc/swift/proxy-server.conf
sh swift-controller-1.sh
Storage
本节点，添加了四块硬盘作为swift使用的硬盘
关闭所有节点，回头看物理关闭顺序
给storage添加4张硬盘，添加完之后就可以开机了

操作节点[Storage]
cd openstack-installvim swift-storage-1.sh
#!/bin/bashdnf install openstack-swift-account openstack-swift-container openstack-swift-object -ydnf install xfsprogs rsync -y #将sdc,sdd,sde,sdf设备格式化为XFS：mkfs.xfs -f /dev/sdcmkfs.xfs -f /dev/sddmkfs.xfs -f /dev/sdemkfs.xfs -f /dev/sdf#创建安装点目录结构：mkdir -p /srv/node/sdcmkdir -p /srv/node/sddmkdir -p /srv/node/sdemkdir -p /srv/node/sdf#写入自动挂载cat &gt;&gt; /etc/fstab &lt;&lt; EOF/dev/sdc /srv/node/sdc xfs noatime 0 2/dev/sdd /srv/node/sdd xfs noatime 0 2/dev/sde /srv/node/sde xfs noatime 0 2/dev/sdf /srv/node/sdf xfs noatime 0 2EOF#挂载设备systemctl daemon-reloadmount -t xfs /dev/sdc /srv/node/sdcmount -t xfs /dev/sdd /srv/node/sddmount -t xfs /dev/sde /srv/node/sdemount -t xfs /dev/sdf /srv/node/sdf#如果用户不需要容灾功能，以上步骤只需要创建一个设备即可，同时可以跳过下面的rsync配置。mv /etc/rsyncd.conf&#123;,.bak&#125;cat&gt;/etc/rsyncd.conf&lt;&lt;EOF[DEFAULT]uid = swiftgid = swiftlog file = /var/log/rsyncd.logpid file = /var/run/rsyncd.pidaddress = 192.168.48.103#192.168.48.103为storage的ip[account]max connections = 2path = /srv/node/read only = Falselock file = /var/lock/account.lock[container]max connections = 2path = /srv/node/read only = Falselock file = /var/lock/container.lock[object]max connections = 2path = /srv/node/read only = Falselock file = /var/lock/object.lockEOFsystemctl enable rsyncd.service --now#编辑/etc/swift目录的account-server.conf、container-server.conf和object-server.conf文件，替换bind_ip为存储节点的IP地址。sudo sed -i &#x27;s/^bind_ip = 127\.0\.0\.1$/bind_ip = 192.168.48.103/&#x27; /etc/swift/account-server.conf /etc/swift/container-server.conf /etc/swift/object-server.conf#确保挂载点目录结构的正确所有权。chown -R swift:swift /srv/node#创建recon目录并确保其拥有正确的所有权。mkdir -p /var/cache/swiftchown -R root:swift /var/cache/swiftchmod -R 775 /var/cache/swift
sh swift-storage-1.sh
controller
操作节点[controller]
cd openstack-installvim swift-controller-2.sh
#!/bin/bash#创建和分发初始环#转到/etc/swift目录。(所以操作在此目录，执行)cd /etc/swift##第一部分（6202）创建用户环swift-ring-builder account.builder create 10 3 1#将每个存储节点添加到环中。swift-ring-builder account.builder add \   --region 1 --zone 1 --ip 192.168.48.103 \   --port 6202 --device sdc --weight 100swift-ring-builder account.builder add \   --region 1 --zone 1 --ip 192.168.48.103 \   --port 6202 --device sdd --weight 100swift-ring-builder account.builder add \   --region 1 --zone 2 --ip 192.168.48.103 \   --port 6202 --device sde --weight 100swift-ring-builder account.builder add \   --region 1 --zone 2 --ip 192.168.48.103 \   --port 6202 --device sdf --weight 100swift-ring-builder account.builder rebalanceswift-ring-builder account.builder##第二部分（6201）创建容器环swift-ring-builder container.builder create 10 3 1swift-ring-builder container.builder add \   --region 1 --zone 1 --ip 192.168.48.103 \   --port 6201 --device sdc --weight 100swift-ring-builder container.builder add \   --region 1 --zone 1 --ip 192.168.48.103 \   --port 6201 --device sdd --weight 100swift-ring-builder container.builder add \   --region 1 --zone 2 --ip 192.168.48.103 \   --port 6201 --device sde --weight 100swift-ring-builder container.builder add \   --region 1 --zone 2 --ip 192.168.48.103 \   --port 6201 --device sdf --weight 100swift-ring-builder container.builderswift-ring-builder container.builder rebalance##第三部分（6200）创建对象环swift-ring-builder object.builder create 10 3 1swift-ring-builder object.builder add \   --region 1 --zone 1 --ip 192.168.48.103 \   --port 6200 --device sdc --weight 100swift-ring-builder object.builder add \   --region 1 --zone 1 --ip 192.168.48.103 \   --port 6200 --device sdd --weight 100swift-ring-builder object.builder add \   --region 1 --zone 2 --ip 192.168.48.103 \   --port 6200 --device sde --weight 100swift-ring-builder object.builder add \   --region 1 --zone 2 --ip 192.168.48.103 \   --port 6200 --device sdf --weight 100swift-ring-builder object.builderswift-ring-builder object.builder rebalance#将account.ring.gz，container.ring.gz以及 object.ring.gz文件复制到每个存储节点和运行代理服务的任何其他节点上的/etc/swift目录。scp /etc/swift/account.ring.gz \     /etc/swift/container.ring.gz \     /etc/swift/object.ring.gz \     192.168.48.103:/etc/swiftmv  /etc/swift/swift.conf&#123;,.bak&#125;cat&gt; /etc/swift/swift.conf&lt;&lt;EOF[swift-hash]swift_hash_path_suffix = swiftswift_hash_path_prefix = swift[storage-policy:0]name = Policy-0default = yesEOF-------------------------------------------------------------------------------------#将控制节点的swift配置文件复制到存储节点（storage）sshpass -p &#x27;Lj201840.&#x27; scp /etc/swift/swift.conf 192.168.48.103:/etc/swift#swift_hash_path_suffix和swift_hash_path_prefix作为哈希算法的一部分用于确定数据在集群中的位置。#这些值应该保持机密，并且在部署集群之后不能更改丢失。可自定义-------------------------------------------------------------------------------------#在所有节点确保对配置目录拥有适当的所有权：####存储节点与控制节点同时执行（注意！！！！两个节点同时执行）#操作节点[controller，storage]chown -R root:swift /etc/swift yum install ansible -ycat &lt;&lt; EOF &gt;&gt; /etc/ansible/hosts[storage]192.168.48.103 ansible_user=rootEOFansible storage -m command -a &quot;chown -R root:swift /etc/swift&quot; -b --become-user root#在控制器节点和任何其他运行代理服务的节点上，启动对象存储代理服务及其相关性，并将它们配置为在系统启动时启动(存储节点无代理服务)#-------------------------------------------------------------------------------------#重启服务(操作节点[controller]systemctl enable openstack-swift-proxy.service memcached.service --nowsystemctl restart openstack-swift-proxy.service memcached.service
sh swift-controller-2.sh
Storage
cd openstack-installvim swift-storage-2.sh
#!/bin/bash#在存储节点启动所有服务 systemctl enable openstack-swift-account.service openstack-swift-account-auditor.service \  openstack-swift-account-reaper.service openstack-swift-account-replicator.service systemctl start openstack-swift-account.service openstack-swift-account-auditor.service \  openstack-swift-account-reaper.service openstack-swift-account-replicator.service systemctl enable openstack-swift-container.service \  openstack-swift-container-auditor.service openstack-swift-container-replicator.service \  openstack-swift-container-updater.service systemctl start openstack-swift-container.service \  openstack-swift-container-auditor.service openstack-swift-container-replicator.service \  openstack-swift-container-updater.service systemctl enable openstack-swift-object.service openstack-swift-object-auditor.service \  openstack-swift-object-replicator.service openstack-swift-object-updater.service systemctl start openstack-swift-object.service openstack-swift-object-auditor.service \  openstack-swift-object-replicator.service openstack-swift-object-updater.serviceyum install ansible -ycat &lt;&lt; EOF &gt;&gt; /etc/ansible/hosts[controller]192.168.48.101 ansible_user=rootEOFansible controller -m command -a &quot;systemctl restart httpd memcached&quot; -b --become-user rootansible controller -m command -a &quot;systemctl restart openstack-nova*&quot; -b --become-user root
sh swift-storage-2.sh
验证
操作节点[controller]
source /root/admin-openrccd /etc/swiftswift stat[root@controller swift]# swift stat               Account: AUTH_07a1ce96dca54f1bb0d3b968f1343617            Containers: 0               Objects: 0                 Bytes: 0       X-Put-Timestamp: 1684919814.32783           X-Timestamp: 1684919814.32783            X-Trans-Id: txd6f3affa0140455b935ff-00646dd605          Content-Type: text/plain; charset=utf-8X-Openstack-Request-Id: txd6f3affa0140455b935ff-00646dd605#测试上传镜像[root@controller swift]# cd[root@controller ~]# swift upload demo /root/openstack-install/iso/cirros-0.4.0-x86_64-disk.img --object-name imageimage
Heat
Heat是 OpenStack 自动编排服务，基于描述性的模板来编排复合云应用，也称为Orchestration Service。Heat 的各服务一般安装在Controller节点上。
操作节点[controller]
cd openstack-installvim heat-controller.sh
#！/bin/bashecho &quot;CREATE DATABASE heat;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON heat.* TO &#x27;heat&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;HEAT_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSecho &quot;GRANT ALL PRIVILEGES ON heat.* TO &#x27;heat&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;HEAT_DBPASS&#x27;;&quot; | mysql -u root -pMARIADB_PASSmysql -u root -pMARIADB_PASS -e &quot;FLUSH PRIVILEGES;&quot;source /root/admin-openrc#创建heat用户openstack user create --domain default --password HEAT_PASS heat#添加 admin 角色到 heat 用户上openstack role add --project service --user heat admin##创建heat和 heat-cfn 服务实体openstack service create --name heat \  --description &quot;Orchestration&quot; orchestrationopenstack service create --name heat-cfn \  --description &quot;Orchestration&quot;  cloudformation  ##创建 Orchestration 服务的 API 端点openstack endpoint create --region RegionOne \  orchestration public http://controller:8004/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \  orchestration internal http://controller:8004/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \  orchestration admin http://controller:8004/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \  cloudformation public http://controller:8000/v1openstack endpoint create --region RegionOne \  cloudformation internal http://controller:8000/v1openstack endpoint create --region RegionOne \  cloudformation admin http://controller:8000/v1  #创建stack管理的额外信息  #创建 heat domain  #控制节点#为栈创建 heat 包含项目和用户的域openstack domain create --description &quot;Stack projects and users&quot; heat#在 heat 域中创建管理项目和用户的heat_domain_admin用户：openstack user create --domain heat --password=HEAT_DOMAIN_USER_PASS heat_domain_admin#)添加admin角色到 heat 域 中的heat_domain_admin用户，启用heat_domain_admin用户#管理栈的管理权限openstack role add --domain heat --user-domain heat --user heat_domain_admin admin#为栈创建 heat 包含项目和用户的域openstack role create heat_stack_owner#添加heat_stack_owner 角色到demo 项目和用户，启用demo 用户管理栈。openstack role add --project demo --user demo heat_stack_owner#必须添加 heat_stack_owner 角色到每个管理栈的用户。#heat_stack_user 角色openstack role create heat_stack_userdnf install openstack-heat-api openstack-heat-api-cfn openstack-heat-engine -ymv /etc/heat/heat.conf&#123;,.bak&#125;cat &gt; /etc/heat/heat.conf &lt;&lt; EOF[database]connection = mysql+pymysql://heat:HEAT_DBPASS@controller/heat  #HEAT_DBPASS是HEAT数据库密码[DEFAULT]transport_url = rabbit://openstack:RABBIT_PASS@controllerheat_metadata_server_url = http://controller:8000heat_waitcondition_server_url = http://controller:8000/v1/waitconditionstack_domain_admin = heat_domain_adminstack_domain_admin_password = HEAT_DOMAIN_PASSstack_user_domain_name = heat#RABBIT_PASS为Rabbitmq服务密码 用户名是openstack[keystone_authtoken]www_authenticate_uri = http://controller:5000auth_url = http://controller:5000memcached_servers = controller:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = heatpassword = HEAT_PASS#HEAT_PASS是heat用户密码[trustee]auth_type = passwordauth_url = http://controller:5000username = heatpassword = HEAT_PASS#HEAT_PASS是heat用户密码user_domain_name = default[clients_keystone]auth_uri = http://controller:5000EOFsu -s /bin/sh -c &quot;heat-manage db_sync&quot; heat##启动 Orchestration 编排服务heat组件并将其设置为随系统启动systemctl enable openstack-heat-api.service \  openstack-heat-api-cfn.service openstack-heat-engine.service --nowsystemctl restart openstack-heat-api.service \  openstack-heat-api-cfn.service openstack-heat-engine.servicesystemctl restart httpd memcached
sh heat-controller.sh
验证
source /root/admin-openrcsystemctl list-unit-files |grep openstack-heat*openstack service listopenstack orchestration service list#该输出显示表明在控制节点上有应该四个heat-engine组件。该输出显示表明在控制节点上有应该四个heat-engine组件。

创建实例
创建实例类型
左侧选择管理员，点击计算，点击实例类型，右侧点击创建实例类型。

根据以上图片步骤依次填入：实例名称、VCPU数量、内存大小、根磁盘大小，确认无误后点击创建实例类型。
创建镜像
测试镜像：https://download.cirros-cloud.net/0.6.2/cirros-0.6.2-x86_64-disk.img
有两种上传方式（二选一）！！！
1.Windows上传镜像方式
左侧选择管理员，点击计算，点击镜像，右侧点击创建镜像。
Windows下载到本地即可

根据以上图片步骤依次填入：镜像名称、选择文件、镜像格式，确认无误后点击创建镜像。
注：演示上传的 img 镜像格式需选用 QCOW2 - QEMU模拟器 才可正常加载。
2.Linux上传镜像方式
source admin-openrcwget https://download.cirros-cloud.net/0.6.2/cirros-0.6.2-x86_64-disk.img#可能会下载不到，可以复制链接到浏览器下载，然后移到/root/目录下glance image-create --name &quot;cirros&quot; \  --file cirros-0.6.2-x86_64-disk.img \  --disk-format qcow2 --container-format bare \  --visibility=publicopenstack image list[root@controller-1 ~]# openstack image list+--------------------------------------+--------+--------+| ID                                   | Name   | Status |+--------------------------------------+--------+--------+| 627761da-7f8c-4780-842a-e50e62f5c464 | cirros | active |+--------------------------------------+--------+--------+
创建内部网络
左侧选择管理员，点击网络，点击网络，右侧点击创建网络。



创建外部网络
左侧选择管理员，点击网络，点击网络，右侧点击创建网络。
如果你是按照本文档搭建的，就填provider



创建路由
左侧选择管理员，点击网络，点击路由，右侧点击创建路由。



添加安全组规则


最后效果长这样

创建实例




然后点击创建实例
分配浮动ip


结论：创建实例成功

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>OpenEuler</tag>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title>Python-总手册</title>
    <url>/posts/28626/</url>
    <content><![CDATA[
Python-总手册

Python环境准备
Python程序手册

Python环境准备
Thonny（首推，自用！！）
Thonny 由爱沙尼亚的 Tartu 大学开发，它采用了不同的方法，因为它的调试器是专为学习和教学编程而设计的。
该软件基于python内置图形库tkinter开发，体积小巧，界面直观，支持语法着色、代码自动补全、debug等强劲功能，并具备了一个友好的IDE，为您提供了几个有用的学习工具，所有这些都打包成一个直观的GUI，能够让你更快的熟悉Python编程语言。同时，thonny内置了Python3.7，因此只需要一个简单的安装程序，您就可以学习编程了，可谓称的上是最好的自助服务教学工具。
官网地址：Thonny，Python IDE初学者
进入官网下载


双击运行


接受安装协议，安装
请选择一个不带中文的路径存放他
这里的c盘仅做测试

创建桌面图标

选择中文

安装成功

Centos7部署Python
在 CentOS 7.9 上安装 Python，你可以使用 yum 包管理器和 EPEL 存储库来完成。以下是安装的步骤：

首先，确保你的系统已经安装了 EPEL 存储库。EPEL 是一个额外的软件源，提供了许多额外的软件包。如果你的系统尚未安装 EPEL 存储库，可以使用以下命令安装它：

sudo yum update -ysudo yum install epel-release
自选版本

安装 Python 3：CentOS 7 默认使用 Python 2.7，但是你可以安装 Python 3。使用以下命令安装 Python 3：

sudo yum install python3

安装 Python 2：如果你还需要安装 Python 2，可以使用以下命令安装：

sudo yum install python2

验证安装：安装完成后，你可以使用以下命令验证 Python 的安装：

python3 --versionpython2 --version
这些命令将显示安装的 Python 版本号，确认安装成功。
使用python
python
Pycham
安装教程：PyCharm2024.1安装教程 - 严千屹博客 (qianyios.top)
官网地址PyCharm：JetBrains为专业开发者提供的Python IDE
Python程序手册
Python程序基础
初识Python程序
新建一个hello.py
→
打开输入代码→运行

#格式print(输出项1,输出项2,······)
每个输出项可以是数值，字符串等
Python程序风格
一个程序往往有输入输出语句，还带有逻辑判断，现在介绍一个程序
例2.1 输入一个数，计算平方根
import maths=input(&quot;请输入一个数：&quot;)s=float(s)if s&gt;=0:    s=math.sqrt(s)    print(&quot;平方根是：&quot;,s)else:    print(&quot;负数不是平方根&quot;)print(&quot;The End&quot;)
在这可以很清晰看到python程序的风格，不会像c语言，需要先声明变量类型，才可以使用
如：
c语言：(可以看见没行语句，需要一个分号)
int sprintf(&quot;请输入一个数:&quot;);scanf(&quot;%d&quot;,&amp;s);

在这显得python更加简单快捷


开始分解

输入语句

s=input(&quot;请输入一个数：&quot;)
此时输入4之后，会存入到s，但是此时的s是个字符串，还不是数值，不能进行计算，需要转换类型

转化类型

s=float(s)
在这，需要将s（字符串）转化为实数，才可以计算

缩进

要注意缩进量，否则代码会串行

如：
错误代码：

正确显示结果：

通过之前修改代码变成else的语句之后，就没显示了，

必须输入负数之后才显示出来

所以要注意缩进量
Python注释
在 Python 中，注释用于在代码中添加解释、说明或提供文档。Python 支持两种类型的注释：单行注释和多行注释。

单行注释：以 # 开头，用于在一行中添加注释。

# 这是一个单行注释

多行注释：用三个单引号 ''' 或三个双引号 &quot;&quot;&quot; 包围的多行文本，用于添加多行注释。

&#x27;&#x27;&#x27;这是一个多行注释。可以在这里添加多行的说明。&#x27;&#x27;&#x27;
或者
&quot;&quot;&quot;这是一个多行注释。可以在这里添加多行的说明。&quot;&quot;&quot;
注释是非常有用的工具，可以提高代码的可读性和可维护性。它们可以用于解释代码的目的、算法、参数、返回值等信息，使其他人更容易理解和使用你的代码。
请注意，注释不会被 Python 解释器执行，它们只是用于代码的解释和文档目的。在运行程序时，注释部分会被忽略。
Python数据类型
数据类型
常量：
Python 中有许多内置的数据类型，用于存储和操作不同类型的数据。下面是 Python 的一些常见数据类型：

整数（int）：用于表示整数值，例如 42、-10、0 等。
浮点数（float）：用于表示带有小数部分的数值，例如 3.14、-0.5 等。
字符串（str）：用于表示文本数据，由一系列字符组成，例如 &quot;Hello, World!&quot;、'Python' 等。
列表（list）：用于存储多个有序的元素，可以包含不同类型的数据，例如 [1, 2, 3]、['apple', 'banana', 'cherry'] 等。

变量：

变量命名规则：


变量名由字母、数字和下划线组成。
变量名必须以字母或下划线开头，不能以数字开头。
变量名区分大小写，例如 count 和 Count 是不同的变量名。
避免使用 Python 的关键字（如 if、for、while 等）作为变量名。
建议使用有意义的变量名，以提高代码的可读性。


变量赋值：


使用赋值运算符 = 将一个值赋给一个变量。
变量在首次赋值时被创建，并根据赋值的值确定其数据类型。
可以将不同类型的数据赋给同一个变量，变量的类型会随着赋值而改变。

x = 42  # 整数y = 3.14  # 浮点数name = &quot;Alice&quot;  # 字符串

变量使用：


可以使用变量名来访问存储在变量中的值。
可以在表达式中使用变量，并对其进行操作。
变量的值可以随时改变，可以重新赋值给同一个变量。

x = 10y = x + 5print(y)  # 输出：15x = 20print(x)  # 输出：20
变量在 Python 中是一种非常重要的概念，它们允许我们存储和操作数据。通过合理使用变量，可以使代码更灵活、可读性更好，并且可以减少重复代码。
数据类型的转换
1.数值转换字符串
a=1b=1.2x=str(a)y=str(b)#此时，要是输出，他们在系统中是带引号的字符串#  &quot;1&quot;,&quot;1.2&quot;
2.字符串转换数值
a=&quot;1&quot;b=&quot;1.2&quot;x=int(a)y=float(b)  #实数（带小数）#此时，要是输出，他们在系统中是数值#  1,1.2
错误形态：
s=&quot;1a&quot;a=int(s)
在这种情况下，字符串 &quot;1a&quot; 包含了字母 &quot;a&quot;，因此无法将其转换为整数。如果你想要将一个合法的整数字符串转换为整数，确保字符串只包含数字字符。
整数格式化输出

使用 format() 方法格式化输出整数的示例：

x = 10y = 20print(&quot;x: &#123;&#125;, y: &#123;&#125;&quot;.format(x, y))
输出结果为：
x: 10, y: 20
在这个示例中，我们使用了 &quot;x: &#123;&#125;, y: &#123;&#125;&quot; 这个字符串作为格式化模板。在模板中的 &#123;&#125; 部分会被 format() 方法中的参数依次替换。

使用 f-string。下面是使用 f-string 格式化输出整数的示例：

x = 10y = 20print(f&quot;x: &#123;x&#125;, y: &#123;y&#125;&quot;)
输出结果也是：
x: 10, y: 20
在这个示例中，我们在字符串前面加上了 f，然后在字符串中使用花括号 &#123;&#125; 来引用变量。


使用使用 % 符号进行格式化输出整数的示例：

x = 10y = 20print(&quot;x: %d, y: %d&quot; % (x, y))
输出结果为：
x: 10, y: 20
在这个示例中，我们使用了 &quot;x: %d, y: %d&quot; 这个字符串作为格式化模板。%d 是整数的占位符。然后，我们将要格式化的整数值 (x, y) 作为元组传递给 % 运算符。
例：1.4.3.1输出年月日

浮点数格式化输出
使用 % 符号配合格式化字符串来实现。下面是一个示例代码：
num = 3.14159print(&quot;浮点数：%.2f&quot; % num)
在这个示例中，%.2f 是浮点数的格式化字符串。% 符号后的 .2f 表示将浮点数保留两位小数进行输出。如果要保留更多或更少的小数位数，只需相应地调整数字即可。
输出结果为：
浮点数：3.14
这样就实现了将浮点数格式化输出并保留指定小数位数。
Python表达式



类别
运算符
描述
实例




算术运算符
+
加法
2 + 3 = 5



-
减法
5 - 2 = 3



*
乘法
2 * 3 = 6



/
除法
6 / 2 = 3



%
取模（取余数）
7 % 3 = 1



**
幂运算
2 ** 3 = 8



//
整除（取商的整数部分）
7 // 3 = 2


赋值运算符
=
赋值
x = 5



+=
加法赋值
x += 3 （等价于 x = x + 3）



-=
减法赋值
x -= 2 （等价于 x = x - 2）



*=
乘法赋值
x *= 2 （等价于 x = x * 2）



/=
除法赋值
x /= 4 （等价于 x = x / 4）



%=
取模赋值
x %= 3 （等价于 x = x % 3）


比较运算符
==
等于
3 == 3 （返回 True）



!=
不等于
2 != 3 （返回 True）



&gt;
大于
5 &gt; 2 （返回 True）



&lt;
小于
2 &lt; 5 （返回 True）



&gt;=
大于等于
5 &gt;= 5 （返回 True）



&lt;=
小于等于
2 &lt;= 5 （返回 True）


逻辑运算符
and
与
True and False （返回 False）两者为真才为真



or
或
True or False （返回 True）一个为真就为真



not
非
not True （返回 False）


成员运算符
in
存在于
2 in [1, 2, 3] （返回 True）



not in
不存在于
4 not in [1, 2, 3] （返回 True）


身份运算符
is
是
x is None （返回 True，如果 x 为 None）



is not
不是
x is not None （返回 True，如果 x 不为 None）




 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenStack-Train双节点部署</title>
    <url>/posts/56647/</url>
    <content><![CDATA[
OpenStack-Train双节点部署
节点架构图（最小配置）



主机名
ip1（NAT）
ip2（仅主机）
硬盘
内存




controller-48
192.168.48.10
192.168.148.10
100G
8G


computer-48
192.168.48.20
192.168.148.20
100G
3G



双节点均采用CentOS-7-x86_64-DVD-2009.iso英文最小化安装，安装过程不做解释
Computer-48需要特别注意加硬盘
computer-48需要勾选处理器虚拟化

网络设置
控制节点网络设置
ens33

ens36

计算节点
原理和控制节点一样，第二张网卡不用设置网关
ens33

ens36

温馨提示（小贴士）

这里就说一下为什么第二张仅主机网卡不用设置网关，因为我们本意是通过NAT进行与外网做数据交换和获取外网资源可以连接互联网的，仅主机仅仅只是用于进行虚拟机内部资源的数据交换，不具备与外网连接的作用，是无法访问互联网的，如果两张网卡同时设置了网关，可能会造成无法访问openstack里创建的虚拟机无法访问互联网，或者本机无法访问互联网的情况，原因是默认路由可能会以仅主机网卡的网络进行与外网链接，但是没办法联网。所以请不要在第二块网卡设置网关

版本对照表



OpenStack 版本
CentOS 版本




Train 以及更早
7


Ussuri and Victoria
8


Wallaby 到 Yoga
Stream 8



安全性
基本用户信息
OpenStack 各组件都需要在控制节点数据库中注册专属账户以存放数据信息，故需要设置密码，强烈建议各组件的密码以及宿主机密码各不相同。



OpenStack 组件
密码




控制节点 root
123456


计算节点 root
123456


Metadata 元数据密钥
METADATA_SECRET


Mariadb root 账户
MARIADB_PASS


RabbitMQ 服务
RABBIT_PASS


OpenStack admin
ADMIN_PASS


Placement 服务
PLACEMENT_PASS


Keystone 数据库
KEYSTONE_DBPASS


Glance 服务
GLANCE_PASS


Glance 数据库
GLANCE_DBPASS


Nova 服务
NOVA_PASS


Nova 数据库
NOVA_DBPASS


Neutron 服务
NEUTRON_PASS


Neutron 数据库
NEUTRON_DBPASS


Cinder 服务
CINDER_PASS


Cinder 数据库
CINDER_DBPASS


Horizon 数据库
DASH_DBPASS


Swift服务
SWIFT_PASS


Heat服务
HEAT_PASS


Heat数据库服务
HEAT_DBPASS


heat_domain_admin用户
HEAT_DOMAIN_USER_PASS



身份验证
控制节点管理 OpenStack 服务时需要进行身份认证，可将认证信息导入到控制节点环境变量中，方便后续安装配置使用。
admin-openrc.sh 文件需提前编写并放入控制节点中，后续安装将不再说明由来
cat &gt;&gt; admin-openrc.sh &lt;&lt; EOFexport OS_PROJECT_DOMAIN_NAME=Defaultexport OS_USER_DOMAIN_NAME=Defaultexport OS_PROJECT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=123456export OS_AUTH_URL=http://controller-48:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2EOFcat &gt;&gt; user_dog-openrc.sh &lt;&lt; EOFexport OS_USERNAME=user_dogexport OS_PASSWORD=123456export OS_PROJECT_NAME=Trainexport OS_USER_DOMAIN_NAME=RegionOneexport OS_PROJECT_DOMAIN_NAME=RegionOneexport OS_AUTH_URL=http://controller-48:5000/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2EOF# OS_USERNAME  登录 OpenStack 服务的用户名# OS_PASSWORD  登录 OpenStack 服务的用户密码# OS_PROJECT_NAME 登录时进入的项目名# OS_USER_DOMAIN_NAME  登录时进入的域名# OS_PROJECT_DOMAIN_NAME  登录时进入的项目域名# OS_AUTH_URL 指定 Keystone（身份认证服务）的 URL  # 如未部署 DNS 服务器，则需要在 hosts中指定 controller-48 映射，或将 controller-48 用控制节点 IP 替代# OS_IDENTITY_API_VERSION 身份认证服务的 API 版本号 # OS_IMAGE_API_VERSION 镜像服务的 API 版本号
测试用户



用户
密码




admin
123456


use_dog
123456



物理节点关闭顺序
给每台机都加上两个脚本
cat &gt;&gt; stop.sh &lt;&lt; EOF#!/bin/bash# 关闭所有 OpenStack 节点# 依次关闭计算节点、网络节点、控制节点for server in \$(openstack server list -f value -c ID); do    openstack server stop \$serverdone# 关闭计算节点echo &quot;Stopping compute services...&quot;systemctl stop openstack-nova-compute.servicesystemctl stop libvirtd.service# 关闭网络节点echo &quot;Stopping network services...&quot;systemctl stop openvswitch.servicesystemctl stop neutron-server.servicesystemctl stop neutron-linuxbridge-agent.servicesystemctl stop neutron-dhcp-agent.servicesystemctl stop neutron-metadata-agent.servicesystemctl stop neutron-l3-agent.service# 关闭控制节点echo &quot;Stopping control services...&quot;systemctl stop mariadb.servicesystemctl stop rabbitmq-server.servicesystemctl stop memcached.servicesystemctl stop httpd.servicesystemctl stop openstack-glance-api.servicesystemctl stop openstack-glance-registry.servicesystemctl stop openstack-cinder-api.servicesystemctl stop openstack-cinder-scheduler.servicesystemctl stop openstack-cinder-volume.servicesystemctl stop openstack-nova-api.servicesystemctl stop openstack-nova-scheduler.servicesystemctl stop openstack-nova-conductor.servicesystemctl stop openstack-nova-novncproxy.servicesystemctl stop openstack-nova-consoleauth.servicesystemctl stop openstack-keystone.servicesystemctl stop openstack-heat-api.servicesystemctl stop openstack-heat-api-cfn.servicesystemctl stop openstack-heat-engine.servicesystemctl stop openstack-swift-proxy.servicesystemctl stop openstack-swift-account.servicesystemctl stop openstack-swift-container.servicesystemctl stop openstack-swift-object.serviceecho &quot;Stopping all services...&quot;systemctl stop --all# 关闭电源echo &quot;Shutting down the system...&quot;poweroffEOFcat &gt;&gt; start.sh &lt;&lt; EOF#!/bin/bash# 重新启动 OpenStack 服务# 依次启动控制节点、网络节点、计算节点# 启动控制节点echo &quot;Starting control services...&quot;systemctl start mariadb.servicesystemctl start rabbitmq-server.servicesystemctl start memcached.servicesystemctl start httpd.servicesystemctl start openstack-glance-api.servicesystemctl start openstack-glance-registry.servicesystemctl start openstack-cinder-api.servicesystemctl start openstack-cinder-scheduler.servicesystemctl start openstack-cinder-volume.servicesystemctl start openstack-nova-api.servicesystemctl start openstack-nova-scheduler.servicesystemctl start openstack-nova-conductor.servicesystemctl start openstack-nova-novncproxy.servicesystemctl start openstack-nova-consoleauth.servicesystemctl start openstack-keystone.servicesystemctl start openstack-heat-api.servicesystemctl start openstack-heat-api-cfn.servicesystemctl start openstack-heat-engine.servicesystemctl start openstack-swift-proxy.servicesystemctl start openstack-swift-account.servicesystemctl start openstack-swift-container.servicesystemctl start openstack-swift-object.service# 启动网络节点echo &quot;Starting network services...&quot;systemctl start openvswitch.servicesystemctl start neutron-server.servicesystemctl start neutron-linuxbridge-agent.servicesystemctl start neutron-dhcp-agent.servicesystemctl start neutron-metadata-agent.servicesystemctl start neutron-l3-agent.service# 启动计算节点echo &quot;Starting compute services...&quot;systemctl start libvirtd.servicesystemctl start openstack-nova-compute.serviceEOF
（先给两个计算节点执行-最后等计算节点完全关闭，再给控制节点执行）
关闭物理机的时候运行sh stop.sh(运行的时候可能会提示你有些服务找不到，报错，这个没关系，一般情况下是没问题的
物理节点开启顺序

先给controller-48运行start.sh再给计算节点运行start.sh
sh start.sh

基础环境
修改主机名和防火墙
controller-48节点
hostnamectl set-hostname controller-48 &amp;&amp; bashsystemctl disable firewalld --nowsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/sysconfig/selinuxsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/configecho &quot;192.168.48.10 controller-48&quot; &gt;&gt; /etc/hostsecho &quot;192.168.48.20 computer-48&quot; &gt;&gt; /etc/hostssetenforce 0reboot
computer-48节点
hostnamectl set-hostname computer-48 &amp;&amp; bashsystemctl disable firewalld --nowsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/sysconfig/selinuxsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/configecho &quot;192.168.48.10 controller-48&quot; &gt;&gt; /etc/hostsecho &quot;192.168.48.20 computer-48&quot; &gt;&gt; /etc/hostssetenforce 0reboot
修改yum
controller-48和computer-48节点
rm -rf /etc/yum.repos.d/*curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repocurl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repocat &gt;&gt;/etc/yum.repos.d/openstack.repo&lt;&lt;EOF[openstack]name=openstackbaseurl=https://mirrors.aliyun.com/centos/7/cloud/x86_64/openstack-train/gpgcheck=0 enabled=1EOFyum clean all &amp;&amp; yum makecacheyum update -y
SSH免密
#各节点yum install -y sshpass cat &gt; sshmianmi.sh &lt;&lt; &quot;EOF&quot;#!/bin/bash# 目标主机列表hosts=(&quot;controller-48&quot; &quot;computer-48&quot;)# 密码（注意修改）password=&quot;123456&quot;# 生成 SSH 密钥对ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa# 循环遍历目标主机for host in &quot;$&#123;hosts[@]&#125;&quot;do    # 复制公钥到目标主机    sshpass -p &quot;$password&quot; ssh-copy-id -o StrictHostKeyChecking=no &quot;$host&quot;        # 验证免密登录    sshpass -p &quot;$password&quot; ssh -o StrictHostKeyChecking=no &quot;$host&quot; &quot;echo &#x27;免密登录成功&#x27;&quot;doneEOFsh sshmianmi.sh
安装OpenStack包
#各节点yum -y install openstack-utils openstack-selinux python-openstackclientyum upgrade
依赖组件
四个组件安装在controller-48节点上
Mariadb数据库
yum install mariadb mariadb-server python2-PyMySQL -ycat &gt;&gt;/etc/my.cnf.d/openstack.cnf&lt;&lt;EOF[mysqld]bind-address =192.168.48.10default-storage-engine = innodbinnodb_file_per_table = onmax_connections =4096collation-server = utf8_general_cicharacter-set-server = utf8EOFsystemctl enable mariadb --nowmysql_secure_installationEnter current password for root (enter for none): 回车Set root password? [Y/n] y# 将要求输入数据库 root 账户密码 MARIADB_PASSRemove anonymous users? [Y/n] yDisallow root login remotely? [Y/n] nRemove test database and access to it? [Y/n] yReload privilege tables now? [Y/n] y# 验证mysql -u root -pMARIADB_PASS
Rabbitmq
yum install rabbitmq-server -ysystemctl enable rabbitmq-server --nowrabbitmqctl add_user openstack RABBIT_PASS# 注意将 RABBIT_PASS  修改为 Rabbitmq消息队列密码rabbitmqctl set_permissions openstack &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;
Memcached
yum install memcached python-memcached -ysed -i &quot;s/OPTIONS=\&quot;-l 127.0.0.1,::1\&quot;/OPTIONS=\&quot;-l 127.0.0.1,::1,controller-48\&quot;/g&quot; /etc/sysconfig/memcachedsystemctl enable memcached --now
注意这里的-l 127.0.0.1,::1,controller-48中controller-48是你的主机名，后续不做解释
etcd
yum install -y etcdmv /etc/etcd/etcd.conf&#123;,.bak&#125; cat &gt;&gt; /etc/etcd/etcd.conf &lt;&lt; EOF#[Member]ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;http://192.168.48.10:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;http://192.168.48.10:2379&quot;ETCD_NAME=&quot;controller-48&quot;#controller-48是你的主机名#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;http://192.168.48.10:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;http://192.168.48.10:2379&quot;ETCD_INITIAL_CLUSTER=&quot;controller-48=http://192.168.48.10:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster-01&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;EOFsystemctl enable etcd --now
Keystone(身份验证服务)
#controller-48节点mysql -u root -pMARIADB_PASSCREATE DATABASE keystone;GRANT ALL PRIVILEGES ON keystone.* TO &#x27;keystone&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;KEYSTONE_DBPASS&#x27;;GRANT ALL PRIVILEGES ON keystone.* TO &#x27;keystone&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;KEYSTONE_DBPASS&#x27;;flush privileges;exit# KEYSTONE_DBPASS  为 Keystone 数据库密码yum -y install yum-utilsyum -y install qpid-proton-c-0.26.0-2.el7.x86_64yum install -y openstack-keystone httpd mod_wsgi mv /etc/keystone/keystone.conf&#123;,.bak&#125;cat&gt;&gt; /etc/keystone/keystone.conf &lt;&lt; EOF[database]connection = mysql+pymysql://keystone:KEYSTONE_DBPASS@controller-48/keystone[token]provider = fernetEOF#同步服务器su -s /bin/sh -c &quot;keystone-manage db_sync&quot; keystone#查看是否成功mysql -u keystone -p&quot;KEYSTONE_DBPASS&quot;use keystone;show tables;exit#有表就行#配置keystone-manage fernet_setup --keystone-user keystone --keystone-group keystonekeystone-manage credential_setup --keystone-user keystone --keystone-group keystonekeystone-manage bootstrap --bootstrap-password 123456 \  --bootstrap-admin-url http://controller-48:5000/v3/ \  --bootstrap-internal-url http://controller-48:5000/v3/ \  --bootstrap-public-url http://controller-48:5000/v3/ \  --bootstrap-region-id RegionOne# 123456 为 admin 账户密码cp /etc/httpd/conf/httpd.conf&#123;,.bak&#125;sed -i &quot;s/#ServerName www.example.com:80/ServerName controller-48/g&quot; /etc/httpd/conf/httpd.confln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/systemctl enable httpd  --now#创建域、项目、用户、角色source admin-openrc.sh# service 项目 创建在 default 用于 OpenStack 服务openstack project create --domain default \  --description &quot;Service Project&quot; service# 创建一个 RegionOne 域名作为后续云实例创建域名openstack domain create --description &quot;RegionOne Domain&quot; RegionOne# 在 RegionOne 域中创建一个 Train 项目openstack project create --domain RegionOne \  --description &quot;Train Project&quot; Train# 在 RegionOne 域中创建普通用户 user_dog openstack user create --domain RegionOne \  --password 123456 user_dog# 创建普通用户 user_dog  的规则 user_dog_roleopenstack role create user_dog_role# 将规则与用户绑定openstack role add --project Train --user user_dog user_dog_role# 注：可以重复上边步骤以创建更多项目、用户及规则# 验证服务可用性# 卸载 admin 用户的环境unset OS_AUTH_URL OS_PASSWORD# 验证 admin 用户可用性openstack --os-auth-url http://controller-48:5000/v3 \  --os-project-domain-name Default --os-user-domain-name Default --os-project-name admin --os-username admin token issue# 输入后将要求输入 管理员 admin 的密码# 返回  token 信息则服务正常# 验证 user_dog 用户可用性openstack --os-auth-url http://controller-48:5000/v3 \  --os-project-domain-name RegionOne --os-user-domain-name RegionOne --os-project-name Train --os-username user_dog token issuesource admin-openrc.sh# 列举当前所有域名openstack domain list+----------------------------------+-----------+---------+--------------------+| ID                               | Name      | Enabled | Description        |+----------------------------------+-----------+---------+--------------------+| 7fcb64a8c47f40a48265a9db94f0c963 | RegionOne | True    | RegionOne Domain   || default                          | Default   | True    | The default domain |+----------------------------------+-----------+---------+--------------------+
Glance(镜像服务)
#控制节点mysql -u root -pMARIADB_PASSCREATE DATABASE glance;GRANT ALL PRIVILEGES ON glance.* TO &#x27;glance&#x27;@&#x27;localhost&#x27; \  IDENTIFIED BY &#x27;GLANCE_DBPASS&#x27;;GRANT ALL PRIVILEGES ON glance.* TO &#x27;glance&#x27;@&#x27;%&#x27; \  IDENTIFIED BY &#x27;GLANCE_DBPASS&#x27;;flush privileges;exit#将 GLANCE_DBPASS 替换为 glance数据库服务的密码source admin-openrc.sh#创建用户服务和api端点openstack user create --domain default --password GLANCE_PASS glance#GLANCE_PASS 为 glance 服务的密码# 为 Glance 用户添加 admin 规则到系统项目 serviceopenstack role add --project service --user glance admin# 没有输出内容# 为 Glance 添加管理镜像的服务openstack service create --name glance \  --description &quot;OpenStack Image&quot; image# 为 RegionOne 域名添加服务接口openstack endpoint create --region RegionOne \  image public http://controller-48:9292openstack endpoint create --region RegionOne \  image internal http://controller-48:9292openstack endpoint create --region RegionOne \  image admin http://controller-48:9292#安装glance服务yum install openstack-glance -ymv /etc/glance/glance-api.conf&#123;,.bak&#125;cat &gt;&gt;/etc/glance/glance-api.conf &lt;&lt; EOF[DEFAULT]use_keystone_quotas = Truelog_file = /var/log/glance/glance.log[database]connection = mysql+pymysql://glance:GLANCE_DBPASS@controller-48/glance# GLANCE_DBPASS 为 Glance 服务的数据库账户密码[keystone_authtoken]www_authenticate_uri  = http://controller-48:5000auth_url = http://controller-48:5000memcached_servers = controller-48:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = glancepassword = GLANCE_PASSservice_token_roles_required = true# GLANCE_PASS 为 Glance 服务的数据库账户密码[paste_deploy]flavor = keystone[glance_store]stores = file,httpdefault_store = filedefault_backend = &#123;&#x27;store_one&#x27;: &#x27;http&#x27;, &#x27;store_two&#x27;: &#x27;file&#x27;&#125;filesystem_store_datadir = /var/lib/glance/images/EOF# 同步 Glance 数据到数据库su -s /bin/sh -c &quot;glance-manage db_sync&quot; glancesystemctl enable openstack-glance-api  --now# 验证服务可用性source admin-openrc.shwget https://download.cirros-cloud.net/0.3.3/cirros-0.3.3-x86_64-disk.img#可能会拉取失败，可以自行复制网址，去浏览器下载，然后上传到/root/目录下glance image-create --name &quot;cirros&quot; \  --file cirros-0.3.3-x86_64-disk.img \  --disk-format qcow2 --container-format bare \  --visibility=publicopenstack image list# +--------------------------------------+--------+--------+# | ID                                   | Name   | Status |# +--------------------------------------+--------+--------+# | 4e022193-03c2-40c4-872f-0adb606f31e4 | cirros | active |# +--------------------------------------+--------+--------+
Placement（资源调度）
mysql -u root -pMARIADB_PASSCREATE DATABASE placement;GRANT ALL PRIVILEGES ON placement.* TO &#x27;placement&#x27;@&#x27;localhost&#x27;  IDENTIFIED BY &#x27;PLACEMENT_DBPASS&#x27;;GRANT ALL PRIVILEGES ON placement.* TO &#x27;placement&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;PLACEMENT_DBPASS&#x27;;flush privileges;exit#PLACEMENT_DBPASS 为 placement 服务的密码source admin-openrc.shopenstack user create --domain default --password PLACEMENT_PASS placementopenstack role add --project service --user placement adminopenstack service create --name placement \  --description &quot;Placement API&quot; placementopenstack endpoint create --region RegionOne \  placement public http://controller-48:8778openstack endpoint create --region RegionOne \  placement internal http://controller-48:8778openstack endpoint create --region RegionOne \  placement admin http://controller-48:8778yum install openstack-placement-api -ymv /etc/placement/placement.conf&#123;,.bak&#125;cat &gt;&gt; /etc/placement/placement.conf &lt;&lt; EOF[placement_database]connection = mysql+pymysql://placement:PLACEMENT_DBPASS@controller-48/placement# PLACEMENT_DBPASS 为 placement 服务的数据库账户密码[api]auth_strategy = keystone[keystone_authtoken]auth_url = http://controller-48:5000/v3memcached_servers = controller-48:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = placementpassword = PLACEMENT_PASS# PLACEMENT_PASS 为 placement 服务的密码EOF#同步数据库su -s /bin/sh -c &quot;placement-manage db sync&quot; placementcp /etc/httpd/conf.d/00-placement-api.conf&#123;,.bak&#125;cat &gt;&gt; /etc/httpd/conf.d/00-placement-api.conf &lt;&lt; EOF#在#SSLCertificateKeyFile ...下添加&lt;Directory /usr/bin&gt;&lt;IfVersion &gt;= 2.4&gt;	Require all granted&lt;/IfVersion&gt;&lt;IfVersion &lt; 2.4&gt;	Order allow,deny		Allow from all&lt;/IfVersion&gt;&lt;/Directory&gt;EOFsystemctl restart httpd# 验证服务source admin-openrc.shplacement-status upgrade check#安装pip osc组件验证资源yum install python-pip -ypip install osc-placement==2.2.0systemctl restart httpd# 验证openstack --os-placement-api-version 1.2 resource class list --sort-column name# +----------------------------------------+# | name                                   |# +----------------------------------------+# | DISK_GB                                |......openstack --os-placement-api-version 1.6 trait list --sort-column name# +---------------------------------------+# | name                                  |# +---------------------------------------+# | computer-48_ACCELERATORS                  |# | computer-48_ARCH_AARCH64                  |# ...
Nova(计算服务)
控制节点
#控制节点controller-48mysql -u root -pMARIADB_PASSCREATE DATABASE nova_api;CREATE DATABASE nova;CREATE DATABASE nova_cell0;GRANT ALL PRIVILEGES ON nova_api.* TO &#x27;nova&#x27;@&#x27;localhost&#x27; \  IDENTIFIED BY &#x27;NOVA_DBPASS&#x27;;GRANT ALL PRIVILEGES ON nova_api.* TO &#x27;nova&#x27;@&#x27;%&#x27; \  IDENTIFIED BY &#x27;NOVA_DBPASS&#x27;;GRANT ALL PRIVILEGES ON nova.* TO &#x27;nova&#x27;@&#x27;localhost&#x27; \  IDENTIFIED BY &#x27;NOVA_DBPASS&#x27;;GRANT ALL PRIVILEGES ON nova.* TO &#x27;nova&#x27;@&#x27;%&#x27; \  IDENTIFIED BY &#x27;NOVA_DBPASS&#x27;;GRANT ALL PRIVILEGES ON nova_cell0.* TO &#x27;nova&#x27;@&#x27;localhost&#x27; \  IDENTIFIED BY &#x27;NOVA_DBPASS&#x27;;GRANT ALL PRIVILEGES ON nova_cell0.* TO &#x27;nova&#x27;@&#x27;%&#x27; \  IDENTIFIED BY &#x27;NOVA_DBPASS&#x27;;flush privileges;exit# NOVA_DBPASS 为 nova 服务的密码source admin-openrc.shopenstack user create --domain default --password NOVA_PASS novaopenstack role add --project service --user nova adminopenstack service create --name nova \  --description &quot;OpenStack Compute&quot; computeopenstack endpoint create --region RegionOne \  compute public http://controller-48:8774/v2.1openstack endpoint create --region RegionOne \  compute internal http://controller-48:8774/v2.1openstack endpoint create --region RegionOne \  compute admin http://controller-48:8774/v2.1mv /etc/yum.repos.d/epel.repo&#123;,.bak&#125; yum install -y \    openstack-nova-api \    openstack-nova-scheduler \    openstack-nova-conductor \    openstack-nova-novncproxymv /etc/yum.repos.d/epel.repo&#123;.bak,&#125; mv /etc/nova/nova.conf&#123;,.bak&#125;cat &gt;&gt; /etc/nova/nova.conf &lt;&lt;EOF[DEFAULT]enabled_apis = osapi_compute,metadatatransport_url = rabbit://openstack:RABBIT_PASS@controller-48:5672/# RABBIT_PASS为 rabbitmq 密码my_ip = 192.168.48.10# 控制节点控制网络的 IPlog_file = /var/log/nova/nova-controller.logrootwrap_config = /etc/nova/rootwrap.conf[api_database]connection = mysql+pymysql://nova:NOVA_DBPASS@controller-48/nova_api# NOVA_DBPASS 为数据库 Nova 账户密码[database]connection = mysql+pymysql://nova:NOVA_DBPASS@controller-48/nova# NOVA_DBPASS 为数据库 Nova 账户密码[api]auth_strategy = keystone[keystone_authtoken]www_authenticate_uri = http://controller-48:5000/auth_url = http://controller-48:5000/memcached_servers = controller-48:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = novapassword =NOVA_PASS# NOVA_PASS 为 Nova 服务的密码[vnc]enabled = trueserver_listen = \$my_ipserver_proxyclient_address = \$my_ip[glance]api_servers = http://controller-48:9292[oslo_concurrency]lock_path = /var/run/nova[placement]region_name = RegionOneproject_domain_name = Defaultproject_name = serviceauth_type = passworduser_domain_name = Defaultauth_url = http://controller-48:5000/v3username = placementpassword = PLACEMENT_PASS# PLACEMENT_PASS 为 placement 服务的密码EOFsu -s /bin/sh -c &quot;nova-manage api_db sync&quot; novasu -s /bin/sh -c &quot;nova-manage cell_v2 map_cell0&quot; novasu -s /bin/sh -c &quot;nova-manage cell_v2 create_cell --name=cell1 --verbose&quot; novasu -s /bin/sh -c &quot;nova-manage db sync&quot; nova# 验证su -s /bin/sh -c &quot;nova-manage cell_v2 list_cells&quot; novasystemctl enable --now \    openstack-nova-api.service \    openstack-nova-scheduler.service \    openstack-nova-conductor.service \    openstack-nova-novncproxy.service    systemctl status \    openstack-nova-api.service \    openstack-nova-scheduler.service \    openstack-nova-conductor.service \    openstack-nova-novncproxy.service    
计算节点
##computer-48计算节点cat &gt;&gt;/etc/yum.repos.d/CentOS-Base.repo&lt;&lt;EOF[Virt]name=CentOS-\$releasever - Basebaseurl=http://mirrors.aliyun.com/centos/7.9.2009/virt/x86_64/kvm-common/gpgcheck=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7EOFyum install qpid-proton-c-0.26.0-2.el7.x86_64 -yyum install openstack-nova-compute -ymv /etc/nova/nova.conf&#123;,.bak&#125;cat &gt;&gt; /etc/nova/nova.conf &lt;&lt;EOF[DEFAULT]enabled_apis = osapi_compute,metadatatransport_url = rabbit://openstack:RABBIT_PASS@controller-48my_ip = 192.168.48.20compute_driver=libvirt.LibvirtDriverlog_file = /var/log/nova/nova-compute.log# 192.168.48.20替换为 计算节点管理网络 IP 地址[api]auth_strategy = keystone[keystone_authtoken]www_authenticate_uri = http://controller-48:5000/auth_url = http://controller-48:5000/memcached_servers = controller-48:11211auth_type = passwordproject_domain_name = Defaultuser_domain_name = Defaultproject_name = serviceusername = novapassword = NOVA_PASS#NOVA_PASS为nova服务密码[vnc]enabled = trueserver_listen = 0.0.0.0server_proxyclient_address = \$my_ipnovncproxy_base_url = http://192.168.48.10:6080/vnc_auto.html# 将 192.168.48.10修改为控制节点管理网络 IP [glance]api_servers = http://controller-48:9292[oslo_concurrency]lock_path = /var/lib/nova/tmp[placement]region_name = RegionOneproject_domain_name = Defaultproject_name = serviceauth_type = passworduser_domain_name = Defaultauth_url = http://controller-48:5000/v3username = placementpassword = PLACEMENT_PASS#PLACEMENT_PASS 为 Placement 服务密码[neutron]auth_url = http://controller-48:5000auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = NEUTRON_PASS# NEUTRON_PASS 为 Neutron 服务密码EOFegrep -c &#x27;(vmx|svm)&#x27; /proc/cpuinfo---------------------------------------------------------------------------------# 如果返回值大于 1 则说明已经开启硬件虚拟化，无需配置 qemu# 如等于 0 ，则需要配置 qemu 以代替默认的 kvmvi /etc/nova/nova.conf[libvirt]# ...virt_type = qemu# 以上配置仅当 egrep -c &#x27;(vmx|svm)&#x27; /proc/cpuinfo 结果为 0 时才进行配置---------------------------------------------------------------------------------systemctl enable libvirtd.service openstack-nova-compute.service --nowsystemctl status libvirtd.service openstack-nova-compute.service 
控制节点验证
# 在控制节点执行验证source admin-openrc.shopenstack compute service list --service nova-compute+----+--------------+----------+------+---------+-------+----------------------------+| ID | Binary       | Host     | Zone | Status  | State | Updated At                 |+----+--------------+----------+------+---------+-------+----------------------------+| 10 | nova-computer | computer-48 | nova | enabled | up    | 2023-04-02T17:17:08.000000 |+----+--------------+----------+------+---------+-------+----------------------------+# 在控制节点执行验证su -s /bin/sh -c &quot;nova-manage cell_v2 discover_hosts --verbose&quot; nova####Found 2 cell mappings.Skipping cell0 since it does not contain hosts.Getting computer-48s from cell &#x27;cell1&#x27;: 89e02b18-2a3c-437a-8dd5-15deb98676a4Checking host mapping for computer-48 host &#x27;computer-48r-48&#x27;: e862bd61-8f56-4d3a-a2b2-21ab7db90edeCreating host mapping for computer-48 host &#x27;computer-48r-48&#x27;: e862bd61-8f56-4d3a-a2b2-21ab7db90edeFound 1 unmapped computer-48s in cell: 89e02b18-2a3c-437a-8dd5-15deb98676a4openstack compute service list[root@controller-48 ~]# openstack compute service list+----+----------------+---------------+----------+---------+-------+----------------------------+| ID | Binary         | Host          | Zone     | Status  | State | Updated At                 |+----+----------------+---------------+----------+---------+-------+----------------------------+|  1 | nova-conductor | controller-48 | internal | enabled | up    | 2023-05-27T17:44:38.000000 ||  4 | nova-scheduler | controller-48 | internal | enabled | up    | 2023-05-27T17:44:40.000000 ||  5 | nova-compute   | computer-48   | nova     | enabled | up    | 2023-05-27T17:44:43.000000 |+----+----------------+---------------+----------+---------+-------+----------------------------+openstack catalog list+-----------+-----------+----------------------------------------------------------------------+| Name      | Type      | Endpoints                                                            |+-----------+-----------+----------------------------------------------------------------------+| placement | placement | RegionOne                                                            ||           |           |   internal: http://controller-48:8778                                   ||           |           | RegionOne                                                            ||           |           |   admin: http://controller-48:8778                                      ||           |           | RegionOne                                                            ||           |           |   public: http://controller-48:8778                                     ||           |           |                                                                      || keystone  | identity  | RegionOne                                                            ||           |           |   admin: http://controller-48:5000/v3/                                  ||           |           | RegionOne                                                            ||           |           |   internal: http://controller-48:5000/v3/                               ||           |           | RegionOne                                                            ||           |           |   public: http://controller-48:5000/v3/                                 ||           |           |                                                                      || neutron   | network   | RegionOne                                                            ||           |           |   public: http://controller-48:9696                                     ||           |           | RegionOne                                                            ||           |           |   internal: http://controller-48:9696                                   ||           |           | RegionOne                                                            ||           |           |   admin: http://controller-48:9696                                      ||           |           |                                                                      || glance    | image     | RegionOne                                                            ||           |           |   admin: http://controller-48:9292                                      ||           |           | RegionOne                                                            ||           |           |   internal: http://controller-48:9292                                   ||           |           | RegionOne                                                            ||           |           |   public: http://controller-48:9292                                     ||           |           |                                                                      || nova      | computer-48   | RegionOne                                                            ||           |           |   admin: http://controller-48:8774/v2.1                                 ||           |           | RegionOne                                                            ||           |           |   internal: http://controller-48:8774/v2.1                              ||           |           | RegionOne                                                            ||           |           |   public: http://controller-48:8774/v2.1                                ||           |           |                                                                      ||           |           |                                                                      |+-----------+-----------+----------------------------------------------------------------------+openstack image list+--------------------------------------+--------+--------+| ID                                   | Name   | Status |+--------------------------------------+--------+--------+| 4e022193-03c2-40c4-872f-0adb606f31e4 | cirros | active |+--------------------------------------+--------+--------+nova-status upgrade check[root@controller-48 ~]# nova-status upgrade check+--------------------------------+| Upgrade Check Results          |+--------------------------------+| Check: Cells v2                || Result: Success                || Details: None                  |+--------------------------------+| Check: Placement API           || Result: Success                || Details: None                  |+--------------------------------+| Check: Ironic Flavor Migration || Result: Success                || Details: None                  |+--------------------------------+| Check: Cinder API              || Result: Success                || Details: None                  |+--------------------------------+#在控制节点修改自动注册nova-computer-48节点cat &gt;&gt; /etc/nova/nova.conf &lt;&lt; EOF[scheduler]discover_hosts_in_cells_interval = 300EOF
Neutron（网络服务）
控制节点
##控制节点controller-48mysql -u root -pMARIADB_PASSCREATE DATABASE neutron;GRANT ALL PRIVILEGES ON neutron.* TO &#x27;neutron&#x27;@&#x27;localhost&#x27; \  IDENTIFIED BY &#x27;NEUTRON_DBPASS&#x27;;GRANT ALL PRIVILEGES ON neutron.* TO &#x27;neutron&#x27;@&#x27;%&#x27; \  IDENTIFIED BY &#x27;NEUTRON_DBPASS&#x27;;flush privileges;exitsource admin-openrc.shopenstack user create --domain default --password NEUTRON_PASS neutronopenstack role add --project service --user neutron adminopenstack service create --name neutron \  --description &quot;OpenStack Networking&quot; networkopenstack endpoint create --region RegionOne \  network public http://controller-48:9696openstack endpoint create --region RegionOne \  network internal http://controller-48:9696openstack endpoint create --region RegionOne \  network admin http://controller-48:9696# 选择安装 大二层 网络yum install openstack-neutron openstack-neutron-ml2 \  openstack-neutron-linuxbridge ebtables -ymv /etc/neutron/neutron.conf&#123;,.bak&#125;cat &gt;&gt; /etc/neutron/neutron.conf &lt;&lt;EOF[database]connection = mysql+pymysql://neutron:NEUTRON_DBPASS@controller-48/neutron#NEUTRON_DBPASS为 数据库 neutron 账户密码[DEFAULT]core_plugin = ml2service_plugins = routerallow_overlapping_ips = truetransport_url = rabbit://openstack:RABBIT_PASS@controller-48auth_strategy = keystonenotify_nova_on_port_status_changes = truenotify_nova_on_port_data_changes = true# RABBIT_PASS 为 消息队列密码[keystone_authtoken]www_authenticate_uri = http://controller-48:5000auth_url = http://controller-48:5000memcached_servers = controller-48:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = NEUTRON_PASS# NEUTRON_PASS为 neutron 服务密码[nova]auth_url = http://controller-48:5000auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = novapassword = NOVA_PASS# [nova]  没有则添加# NOVA_PASS为 Nova 服务密码[oslo_concurrency]EOFmv /etc/neutron/plugins/ml2/ml2_conf.ini&#123;,.bak&#125;cat &gt;&gt; /etc/neutron/plugins/ml2/ml2_conf.ini &lt;&lt; EOF[ml2]type_drivers = flat,vlan,vxlantenant_network_types = vxlanmechanism_drivers = linuxbridge,l2populationextension_drivers = port_security[ml2_type_flat]flat_networks = provider[ml2_type_vxlan]vni_ranges = 1:1000[securitygroup]enable_ipset = trueEOFmv /etc/neutron/plugins/ml2/linuxbridge_agent.ini&#123;,.bak&#125;cat &gt;&gt; /etc/neutron/plugins/ml2/linuxbridge_agent.ini &lt;&lt;EOF[linux_bridge]physical_interface_mappings = provider:ens33# ens33 为第一块网卡名称[vxlan]enable_vxlan = truelocal_ip = 192.168.48.10l2_population = true# 192.168.48.10 为管理网络 控制节点的 IP  即 controller-48 IP[securitygroup]enable_security_group = truefirewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriverEOFmv /etc/neutron/l3_agent.ini&#123;,.bak&#125;cat &gt;&gt; /etc/neutron/l3_agent.ini &lt;&lt; EOF[DEFAULT]interface_driver = linuxbridgeEOFmv /etc/neutron/dhcp_agent.ini&#123;,.bak&#125;cat &gt;&gt; /etc/neutron/dhcp_agent.ini &lt;&lt; EOF[DEFAULT]interface_driver = linuxbridgedhcp_driver = neutron.agent.linux.dhcp.Dnsmasqenable_isolated_metadata = trueEOF----------------------modprobe br_netfiltercat &gt;&gt;/etc/rc.sysinit&lt;&lt;EOF#!/bin/bashfor file in /etc/sysconfig/modules/*.modules ; do[ -x $file ] &amp;&amp; $filedoneEOFecho &quot;modprobe br_netfilter&quot; &gt;/etc/sysconfig/modules/br_netfilter.moduleschmod 755 /etc/sysconfig/modules/br_netfilter.modulessysctl -a | grep net.bridge.bridge-nf-call----------------------mv /etc/neutron/metadata_agent.ini&#123;,.bak&#125;cat &gt;&gt; /etc/neutron/metadata_agent.ini &lt;&lt; EOF[DEFAULT]nova_metadata_host = controller-48metadata_proxy_shared_secret = METADATA_SECRET# METADATA_SECRET 为 元数据 的密钥EOF-------------------cat &gt;&gt; /etc/nova/nova.conf &lt;&lt; EOF#追加在末尾[neutron]auth_url = http://controller-48:5000auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultregion_name = RegionOneproject_name = serviceusername = neutronpassword = NEUTRON_PASSservice_metadata_proxy = truemetadata_proxy_shared_secret = METADATA_SECRET# NEUTRON_PASS  为 neutron 服务的密码# METADATA_SECRET 为上边设置的元数据密钥EOF-------------------ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.inisu -s /bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head&quot; neutronsudo ip route del defaultsudo ip route add default via 192.168.48.2 dev ens33#192.168.48.2为ens33网关systemctl enable --now neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-l3-agent.servicesystemctl status neutron-server.service  neutron-linuxbridge-agent.service neutron-dhcp-agent.service   neutron-metadata-agent.service  neutron-l3-agent.service
计算节点
###compute计算节点yum install openstack-neutron-linuxbridge ebtables ipset -y mv /etc/neutron/neutron.conf&#123;,.bak&#125;cat &gt;&gt; /etc/neutron/neutron.conf &lt;&lt; EOF[DEFAULT]transport_url = rabbit://openstack:RABBIT_PASS@controller-48#RABBIT_PASS为 控制节点 消息队列 密码auth_strategy = keystone[keystone_authtoken]www_authenticate_uri = http://controller-48:5000auth_url = http://controller-48:5000memcached_servers = controller-48:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = neutronpassword = NEUTRON_PASS# NEUTRON_PASS  为控制节点 neutron 服务密码[oslo_concurrency]lock_path = /var/lib/neutron/tmpEOFmv /etc/neutron/plugins/ml2/linuxbridge_agent.ini&#123;,.bak&#125;cat &gt;&gt; /etc/neutron/plugins/ml2/linuxbridge_agent.ini &lt;&lt;EOF[linux_bridge]physical_interface_mappings = provider:ens36# ens36 为 第二块网卡名字[vxlan]enable_vxlan = truelocal_ip = 192.168.48.20l2_population = true# 192.168.48.20  为 计算节点 管理网络的 IP 地址[securitygroup]enable_security_group = truefirewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriverEOF-------------------modprobe br_netfiltercat &gt;&gt;/etc/rc.sysinit&lt;&lt;EOF#!/bin/bashfor file in /etc/sysconfig/modules/*.modules ; do[ -x $file ] &amp;&amp; $filedoneEOFecho &quot;modprobe br_netfilter&quot; &gt;/etc/sysconfig/modules/br_netfilter.moduleschmod 755 /etc/sysconfig/modules/br_netfilter.modulessysctl -a | grep net.bridge.bridrge-nf-call-------------------systemctl enable neutron-linuxbridge-agent.service --nowsystemctl restart openstack-nova-compute.service neutron-linuxbridge-agent.servicesystemctl status neutron-linuxbridge-agent.service
控制节点验证
# 验证# 控制节点执行source admin-openrc.shopenstack network agent list###等几分钟[root@controller-48 ~]# openstack network agent list+--------------------------------------+--------------------+---------------+-------------------+-------+-------+---------------------------+| ID                                   | Agent Type         | Host          | Availability Zone | Alive | State | Binary                    |+--------------------------------------+--------------------+---------------+-------------------+-------+-------+---------------------------+| 201870b9-aac0-4830-9788-03da13b125c7 | Metadata agent     | controller-48 | None              | :-)   | UP    | neutron-metadata-agent    || 55ae2391-4cd6-4cd1-bf4f-4465f1b561a1 | L3 agent           | controller-48 | nova              | :-)   | UP    | neutron-l3-agent          || bae3fe77-a005-4cdf-aee6-8cfe3cf918ba | Linux bridge agent | controller-48 | None              | :-)   | UP    | neutron-linuxbridge-agent || f0bd6fbc-2889-4558-80fa-8f2a08989b74 | Linux bridge agent | computer-48   | None              | :-)   | UP    | neutron-linuxbridge-agent || f5546196-9950-4c5a-b709-060a1bba5944 | DHCP agent         | controller-48 | nova              | :-)   | UP    | neutron-dhcp-agent        |+--------------------------------------+--------------------+---------------+-------------------+-------+-------+---------------------------+# 确保以上五个 Agent 都为 :-) 及 UP
Horizon(Web管理页面)
systemctl restart neutron* openstack-nova*###控制节点yum install openstack-dashboard -ycp /etc/openstack-dashboard/local_settings&#123;,.bak&#125;#注释以下信息sed -i &#x27;s/^ALLOWED_HOSTS/#&amp;/&#x27; /etc/openstack-dashboard/local_settingssed -i &#x27;s/^OPENSTACK_HOST/#&amp;/&#x27; /etc/openstack-dashboard/local_settingssed -i &#x27;s/^OPENSTACK_KEYSTONE_URL/#&amp;/&#x27; /etc/openstack-dashboard/local_settingssed -i &#x27;s/^TIME_ZONE/#&amp;/&#x27; /etc/openstack-dashboard/local_settings追加内容cat &gt;&gt; /etc/openstack-dashboard/local_settings &lt;&lt;EOFALLOWED_HOSTS = [&#x27;*&#x27;]SESSION_ENGINE = &#x27;django.contrib.sessions.backends.cache&#x27;CACHES = &#123;    &#x27;default&#x27;: &#123;         &#x27;BACKEND&#x27;: &#x27;django.core.cache.backends.memcached.MemcachedCache&#x27;,         &#x27;LOCATION&#x27;: &#x27;controller-48:11211&#x27;,    &#125;&#125;OPENSTACK_HOST = &quot;controller-48&quot;OPENSTACK_KEYSTONE_URL = &quot;http://%s:5000/identity/v3&quot; % OPENSTACK_HOSTOPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = TrueOPENSTACK_API_VERSIONS = &#123;    &quot;identity&quot;: 3,    &quot;image&quot;: 2,    &quot;volume&quot;: 3,&#125;OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &quot;Default&quot;OPENSTACK_KEYSTONE_DEFAULT_ROLE = &quot;user&quot;TIME_ZONE = &quot;Asia/Shanghai&quot;# 有则修改没有则添加EOFcp /etc/httpd/conf.d/openstack-dashboard.conf&#123;,.bak&#125;cat &gt;&gt; /etc/httpd/conf.d/openstack-dashboard.conf &lt;&lt; EOFWSGIApplicationGroup %&#123;GLOBAL&#125;EOF------------------------------------------------------------------------------------#把下面所有文件里面有WEBROOT = &#x27;/&#x27; 中的 / 改成 /dashboardsed -i.bak &quot;s#WEBROOT\s*=.*#WEBROOT = &#x27;/dashboard&#x27;#&quot; /usr/share/openstack-dashboard/openstack_dashboard/defaults.pysed -i.bak &quot;s#WEBROOT\s*=.*#WEBROOT = &#x27;/dashboard&#x27;#&quot;  /usr/share/openstack-dashboard/openstack_dashboard/test/settings.pysed -i.bak &#x27;s|WEBROOT\s*=.*|WEBROOT = &quot;/dashboard&quot;|&#x27; /usr/share/openstack-dashboard/static/dashboard/js/1453ede06e9f.js#如果第三条不行，注意一下1453ede06e9f.js是否存在，若不存在，则看下面三个文件中有WEBROOT = &#x27;/&#x27;替换文件名即可[root@controller-48 ~]# cd /usr/share/openstack-dashboard/static/dashboard/js/[root@controller-48 js]# lltotal 2472   #以下几个文件也要改 ，我这里就一个文件有-rw-r--r-- 1 root root  606959 May 17  2021 1453ede06e9f.js-rw-r--r-- 1 root root 1659039 May 17  2021 b5e88d434bd1.js-rw-r--r-- 1 root root  254022 May 17  2021 eb687af7228a.js------------------------------------------------------------------------------------systemctl restart httpd memcachedsystemctl status httpd memcached# 验证# 访问 http://192.168.48.10/dashboard  （控制节点ip）# 登录用户密码 可使用 admin 或 user_dog# 域名 使用 Default

cinder（块存储服务）
控制节点
###控制节点mysql -u root -pMARIADB_PASSCREATE DATABASE cinder;GRANT ALL PRIVILEGES ON cinder.* TO &#x27;cinder&#x27;@&#x27;localhost&#x27; \  IDENTIFIED BY &#x27;CINDER_DBPASS&#x27;;GRANT ALL PRIVILEGES ON cinder.* TO &#x27;cinder&#x27;@&#x27;%&#x27; \  IDENTIFIED BY &#x27;CINDER_DBPASS&#x27;;exit# CINDER_DBPASS 为 cinder 数据库账户密码source admin-openrc.shopenstack user create --domain default --password CINDER_PASS cinderopenstack role add --project service --user cinder admin  openstack service create --name cinderv3 \  --description &quot;OpenStack Block Storage&quot; volumev3openstack endpoint create --region RegionOne \  volumev3 public http://controller-48:8776/v3/%\(project_id\)sopenstack endpoint create --region RegionOne \  volumev3 internal http://controller-48:8776/v3/%\(project_id\)sopenstack endpoint create --region RegionOne \  volumev3 admin http://controller-48:8776/v3/%\(project_id\)syum install openstack-cinder -ymv /etc/cinder/cinder.conf&#123;,.bak&#125;cat &gt;&gt; /etc/cinder/cinder.conf &lt;&lt; EOF[DEFAULT]transport_url = rabbit://openstack:RABBIT_PASS@controller-48auth_strategy = keystonemy_ip = 192.168.48.10# 控制节点管理网络 IP[database]connection = mysql+pymysql://cinder:CINDER_DBPASS@controller-48/cinder# CINDER_DBPASS 为数据库 Cinder 账户密码[keystone_authtoken]www_authenticate_uri = http://controller-48:5000auth_url = http://controller-48:5000memcached_servers = controller-48:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = CINDER_PASS# CINDER_PASS 为 Cinder 服务密码[oslo_concurrency]lock_path = /var/lib/cinder/tmpEOFsu -s /bin/sh -c &quot;cinder-manage db sync&quot; cindercat &gt;&gt; /etc/nova/nova.conf &lt;&lt; EOF[cinder]os_region_name = RegionOneEOFsystemctl restart openstack-nova-api.servicesystemctl status openstack-nova-api.servicesystemctl enable --now openstack-cinder-api.service openstack-cinder-scheduler.servicesystemctl status openstack-cinder-api.service openstack-cinder-scheduler.service
计算节点
###computer-48节点添加一块物理磁盘[root@computer-48 ~]# lsblkNAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda               8:0    0  100G  0 disk├─sda1            8:1    0    1G  0 part /boot└─sda2            8:2    0   99G  0 part  ├─centos-root 253:0    0   50G  0 lvm  /  ├─centos-swap 253:1    0  3.6G  0 lvm  [SWAP]  └─centos-home 253:2    0 45.4G  0 lvm  /homesdb               8:16   0  100G  0 disksr0              11:0    1 1024M  0 romyum install lvm2 device-mapper-persistent-data -ysystemctl enable lvm2-lvmetad.service --now# 如显示不存在则说明系统默认安装了 lvm  以上步骤可忽略#创建/dev/sdb卷组pvcreate /dev/sdb# Physical volume &quot;/dev/sdb&quot; successfully created.vgcreate cinder-volumes /dev/sdb# Volume group &quot;cinder-volumes&quot; successfully created# sdb 为划分给块存储使用的磁盘# 如有多个磁盘，则需重复以上两个命令cp /etc/lvm/lvm.conf&#123;,.bak&#125;sed -i &#x27;130 a\filter = [ &quot;a/sdb/&quot;,&quot;r/.*/&quot;]&#x27; /etc/lvm/lvm.conf#sdb是上面添加的新的物理磁盘# 如有多个磁盘，则将磁盘编号以固定格式添加到过滤设备中，例如有两个磁盘 sdb sdc ，则为 filter = [ &quot;a/sdb/&quot;, &quot;a/sdc/&quot;,&quot;r/.*/&quot;]yum install openstack-cinder targetcli python-keystone -ymv /etc/cinder/cinder.conf&#123;,.bak&#125;cat &gt;&gt; /etc/cinder/cinder.conf &lt;&lt; EOF[DEFAULT]transport_url = rabbit://openstack:RABBIT_PASS@controller-48auth_strategy = keystonemy_ip = 192.168.48.20enabled_backends = lvmglance_api_servers = http://controller-48:9292# 192.168.48.20  为块存储节点 computer-48管理网络 的接口IP[database]connection = mysql+pymysql://cinder:CINDER_DBPASS@controller-48/cinder# CINDER_DBPASS 为数据库 Cinder 账户密码[keystone_authtoken]www_authenticate_uri = http://controller-48:5000auth_url = http://controller-48:5000memcached_servers = controller-48:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = cinderpassword = CINDER_PASS# CINDER_PASS 为 cinder 数据库账户密码[lvm]volume_driver = cinder.volume.drivers.lvm.LVMVolumeDrivervolume_group = cinder-volumestarget_protocol = iscsitarget_helper = lioadm# [lvm]  没有则新建[oslo_concurrency]lock_path = /var/lib/cinder/tmpEOFsystemctl enable openstack-cinder-volume.service target.service --nowsystemctl status openstack-cinder-volume.service target.service
控制节点验证
# 验证# 控制节点执行source admin-openrc.shopenstack volume service listsystemctl restart httpd memcached[root@controller-48 ~]# openstack volume service list+------------------+-------------+------+---------+-------+----------------------------+| Binary           | Host        | Zone | Status  | State | Updated At                 |+------------------+-------------+------+---------+-------+----------------------------+| cinder-scheduler | controller-48  | nova | enabled | up    | 2023-05-24T08:24:18.000000 || cinder-volume    | computer-48@lvm | nova | enabled | up    | 2023-05-24T08:24:17.000000 |+------------------+-------------+------+---------+-------+----------------------------+
Swift（对象存储）
控制节点
###控制节点source admin-openrc.shopenstack user create --domain default --password SWIFT_PASS swiftopenstack role add --project service --user swift admin#创建swift服务实体：openstack service create --name swift \  --description &quot;OpenStack Object Storage&quot; object-store#创建swift服务实体：openstack endpoint create --region RegionOne \object-store public http://controller-48:8080/v1/AUTH_%\(project_id\)sopenstack endpoint create --region RegionOne \object-store internal http://controller-48:8080/v1/AUTH_%\(project_id\)sopenstack endpoint create --region RegionOne \object-store admin http://controller-48:8080/v1#安装swift组件yum install -y openstack-swift-proxy python-swiftclient \  python-keystoneclient python-keystonemiddleware \  Memcachedmv /etc/swift/proxy-server.conf&#123;,.bak&#125;cat&gt; /etc/swift/proxy-server.conf&lt;&lt;EOF[DEFAULT]bind_ip = 0.0.0.0bind_port = 8080user = swift[pipeline:main]pipeline = catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk ratelimit authtoken keystoneauth container-quotas account-quotas slo dlo versioned_writes proxy-logging proxy-server[app:proxy-server]use = egg:swift#proxyallow_account_management = trueaccount_autocreate = true#Keystone auth info[filter:authtoken]paste.filter_factory = keystonemiddleware.auth_token:filter_factorywww_authenticate_uri = http://controller-48:5000auth_url = http://controller-48:5000/v3memcached_servers = controller-48:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = swiftpassword = SWIFT_PASSdelay_auth_decision = trueservice_token_roles_required = True[filter:keystoneauth]use = egg:swift#keystoneauthoperator_roles = admin,user[filter:healthcheck]use = egg:swift#healthcheck[filter:cache]use = egg:swift#memcachememcache_servers = controller-48:11211[filter:ratelimit]use = egg:swift#ratelimit[filter:domain_remap]use = egg:swift#domain_remap[filter:catch_errors]use = egg:swift#catch_errors[filter:cname_lookup]use = egg:swift#cname_lookup[filter:staticweb]use = egg:swift#staticweb[filter:tempurl]use = egg:swift#tempurl[filter:formpost]use = egg:swift#formpost[filter:name_check]use = egg:swift#name_check[filter:list-endpoints]use = egg:swift#list_endpoints[filter:proxy-logging]use = egg:swift#proxy_logging[filter:bulk]use = egg:swift#bulk[filter:slo]use = egg:swift#slo[filter:dlo]use = egg:swift#dlo[filter:container-quotas]use = egg:swift#container_quotas[filter:account-quotas]use = egg:swift#account_quotas[filter:gatekeeper]use = egg:swift#gatekeeper[filter:container_sync]use = egg:swift#container_sync[filter:xprofile]use = egg:swift#xprofile[filter:versioned_writes]use = egg:swift#versioned_writesEOF
computer-48
添加4张硬盘


#conpute节点yum install xfsprogs rsync -ylsblk#将/dev/sdb和/dev/sdc设备格式化为XFS：mkfs.xfs /dev/sdcmkfs.xfs /dev/sddmkfs.xfs /dev/sdemkfs.xfs /dev/sdf#创建安装点目录结构：mkdir -p /srv/node/sdcmkdir -p /srv/node/sddmkdir -p /srv/node/sdemkdir -p /srv/node/sdfcat &gt;&gt; /etc/fstab &lt;&lt; EOF/dev/sdc /srv/node/sdc xfs noatime,nodiratime,nobarrier,logbufs=8 0 2/dev/sdd /srv/node/sdd xfs noatime,nodiratime,nobarrier,logbufs=8 0 2/dev/sde /srv/node/sde xfs noatime,nodiratime,nobarrier,logbufs=8 0 2/dev/sdf /srv/node/sdf xfs noatime,nodiratime,nobarrier,logbufs=8 0 2EOF#安装设备mount /srv/node/sdcmount /srv/node/sddmount /srv/node/sdemount /srv/node/sdfcat&gt;/etc/rsyncd.conf&lt;&lt;EOFuid = swiftgid = swiftlog file = /var/log/rsyncd.logpid file = /var/run/rsyncd.pidaddress = 192.168.148.20[account]max connections = 2path = /srv/node/read only = Falselock file = /var/lock/account.lock[container]max connections = 2path = /srv/node/read only = Falselock file = /var/lock/container.lock[object]max connections = 2path = /srv/node/read only = Falselock file = /var/lock/object.lockEOF#重启服务systemctl enable rsyncd.servicesystemctl start rsyncd.service#安装swift组件yum install -y openstack-swift-account openstack-swift-container \  openstack-swift-object  mv /etc/swift/account-server.conf&#123;,.bak&#125;cat&gt; /etc/swift/account-server.conf&lt;&lt;EOF[DEFAULT]bind_ip = 192.168.148.20bind_port = 6202user = swiftswift_dir = /etc/swiftdevices = /srv/nodemount_check = true[pipeline:main]pipeline = healthcheck recon account-server[app:account-server]use = egg:swift#account[filter:healthcheck]use = egg:swift#healthcheck[filter:recon]use = egg:swift#reconrecon_cache_path = /var/cache/swift[account-replicator][account-auditor][account-reaper][filter:xprofile]use = egg:swift#xprofileEOFmv /etc/swift/container-server.conf&#123;,.bak&#125;cat&gt; /etc/swift/container-server.conf&lt;&lt;EOF[DEFAULT]bind_ip = 192.168.148.20bind_port = 6201user = swiftswift_dir = /etc/swiftdevices = /srv/nodemount_check = true[pipeline:main]pipeline = healthcheck recon container-server[app:container-server]use = egg:swift#container[filter:healthcheck]use = egg:swift#healthcheck[filter:recon]use = egg:swift#recon[container-replicator][container-updater][container-auditor][container-sync][filter:xprofile]use = egg:swift#xprofile[container-sharder]EOFmv /etc/swift/object-server.conf&#123;,.bak&#125;cat&gt; /etc/swift/object-server.conf&lt;&lt;EOF[DEFAULT]bind_ip = 0.0.0.0bind_port = 6200user = swiftswift_dir = /etc/swiftdevices = /srv/nodemount_check = true[pipeline:main]pipeline = healthcheck recon object-server[app:object-server]use = egg:swift#objectrecon_cache_path = /var/cache/swiftrecon_lock_path = /var/lock[filter:healthcheck]use = egg:swift#healthcheck[filter:recon]use = egg:swift#recon[object-replicator][object-reconstructor][object-updater][object-expirer][filter:xprofile]use = egg:swift#xprofile[object-relinker][object-auditor]log_name = object-auditorlog_facility = LOG_LOCAL0log_level = INFOlog_address=/dev/log   EOF#确保对安装点目录结构拥有适当的所有权：chown -R swift:swift /srv/node#创建recon目录并确保对其拥有适当的所有权： mkdir -p /var/cache/swift chown -R root:swift /var/cache/swift chmod -R 775 /var/cache/swift # 在防火墙中启用必要的访问(实验忽略)firewall-cmd --permanent --add-port=6200/tcpfirewall-cmd --permanent --add-port=6201/tcpfirewall-cmd --permanent --add-port=6202/tcp
创建和分发初始环 controller-48
#控制节点转到/etc/swift目录。(所以操作在此目录，执行)创建用户环account.builder文件：cd /etc/swift##第一部分（6202）创建用户环swift-ring-builder account.builder create 10 3 1swift-ring-builder account.builder add \   --region 1 --zone 1 --ip 192.168.148.20 --port 6202 --device sdc --weight 100swift-ring-builder account.builder add \   --region 1 --zone 1 --ip 192.168.148.20 --port 6202 --device sdd --weight 100swift-ring-builder account.builder add \   --region 1 --zone 2 --ip 192.168.148.20 --port 6202 --device sde --weight 100swift-ring-builder account.builder add \   --region 1 --zone 2 --ip 192.168.148.20 --port 6202 --device sdf --weight 100swift-ring-builder account.builder##重新平衡环且验证swift-ring-builder account.builder rebalanceswift-ring-builder account.builder##第二部分（6201）创建容器环swift-ring-builder container.builder create 10 3 1swift-ring-builder container.builder add \   --region 1 --zone 1 --ip 192.168.148.20 --port 6201 --device sdc --weight 100swift-ring-builder container.builder add \   --region 1 --zone 1 --ip 192.168.148.20 --port 6201 --device sdd --weight 100swift-ring-builder container.builder add \   --region 1 --zone 2 --ip 192.168.148.20 --port 6201 --device sde --weight 100swift-ring-builder container.builder add \   --region 1 --zone 2 --ip 192.168.148.20 --port 6201 --device sdf --weight 100swift-ring-builder container.builderswift-ring-builder container.builder rebalance##第三部分（6200）创建对象环swift-ring-builder object.builder create 10 3 1swift-ring-builder object.builder add \   --region 1 --zone 1 --ip 192.168.148.20 --port 6200 --device sdc --weight 100swift-ring-builder object.builder add \   --region 1 --zone 1 --ip 192.168.148.20 --port 6200 --device sdd --weight 100swift-ring-builder object.builder add \   --region 1 --zone 2 --ip 192.168.148.20 --port 6200 --device sde --weight 100swift-ring-builder object.builder add \   --region 1 --zone 2 --ip 192.168.148.20 --port 6200 --device sdf --weight 100swift-ring-builder object.builderswift-ring-builder object.builder rebalance将swift目录下生成三个.gz文件复制到存储节点的swift目录下scp account.ring.gz container.ring.gz object.ring.gz 192.168.148.20:/etc/swift##完成安装 controller-48mv  /etc/swift/swift.conf&#123;,.bak&#125;cat&gt; /etc/swift/swift.conf&lt;&lt;EOF[swift-hash]swift_hash_path_suffix = swiftswift_hash_path_prefix = swift[storage-policy:0]name = Policy-0default = yesEOF#复制到存储节点scp swift.conf 192.168.148.20:/etc/swiftswift_hash_path_suffix和swift_hash_path_prefix作为哈希算法的一部分用于确定数据在集群中的位置。这些值应该保持机密，并且在部署集群之后不能更改丢失。可自定义在所有节点确保对配置目录拥有适当的所有权：####存储节点与控制节点同时执行（注意！！！！两个节点同时执行）chown -R root:swift /etc/swift 在控制器节点和任何其他运行代理服务的节点上，启动对象存储代理服务及其相关性，并将它们配置为在系统启动时启动(存储节点无代理服务)#重启服务systemctl enable openstack-swift-proxy.service memcached.service --nowsystemctl restart openstack-swift-proxy.service memcached.service
计算节点
在存储节点启动所有服务 systemctl enable openstack-swift-account.service openstack-swift-account-auditor.service \  openstack-swift-account-reaper.service openstack-swift-account-replicator.service systemctl start openstack-swift-account.service openstack-swift-account-auditor.service \  openstack-swift-account-reaper.service openstack-swift-account-replicator.service systemctl enable openstack-swift-container.service \  openstack-swift-container-auditor.service openstack-swift-container-replicator.service \  openstack-swift-container-updater.service systemctl start openstack-swift-container.service \  openstack-swift-container-auditor.service openstack-swift-container-replicator.service \  openstack-swift-container-updater.service systemctl enable openstack-swift-object.service openstack-swift-object-auditor.service \  openstack-swift-object-replicator.service openstack-swift-object-updater.service systemctl start openstack-swift-object.service openstack-swift-object-auditor.service \  openstack-swift-object-replicator.service openstack-swift-object-updater.service
验证
[root@controller-48 swift]# swift stat               Account: AUTH_07a1ce96dca54f1bb0d3b968f1343617            Containers: 0               Objects: 0                 Bytes: 0       X-Put-Timestamp: 1684919814.32783           X-Timestamp: 1684919814.32783            X-Trans-Id: txd6f3affa0140455b935ff-00646dd605          Content-Type: text/plain; charset=utf-8X-Openstack-Request-Id: txd6f3affa0140455b935ff-00646dd605[root@controller-48 swift]# cd[root@controller-48 ~]# swift upload demo cirros-0.5.2-x86_64-disk.img --object-name imageimage##重启nova服务sudo systemctl restart openstack-nova*
Heat（编排）
控制节点
#创建heat数据库和用户mysql -u root -pMARIADB_PASSCREATE DATABASE heat;GRANT ALL PRIVILEGES ON heat.* TO &#x27;heat&#x27;@&#x27;localhost&#x27; \  IDENTIFIED BY &#x27;HEAT_DBPASS&#x27;;GRANT ALL PRIVILEGES ON heat.* TO &#x27;heat&#x27;@&#x27;%&#x27; \  IDENTIFIED BY &#x27;HEAT_DBPASS&#x27;;flush privileges;exitsource admin-openrc.shopenstack user create --domain default --password HEAT_PASS heat#添加 admin 角色到 heat 用户上openstack role add --project service --user heat admin##创建heat和 heat-cfn 服务实体openstack service create --name heat \  --description &quot;Orchestration&quot; orchestrationopenstack service create --name heat-cfn \  --description &quot;Orchestration&quot;  cloudformation  ##创建 Orchestration 服务的 API 端点openstack endpoint create --region RegionOne \  orchestration public http://controller-48:8004/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \  orchestration internal http://controller-48:8004/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \  orchestration admin http://controller-48:8004/v1/%\(tenant_id\)sopenstack endpoint create --region RegionOne \  cloudformation public http://controller-48:8000/v1openstack endpoint create --region RegionOne \  cloudformation internal http://controller-48:8000/v1openstack endpoint create --region RegionOne \  cloudformation admin http://controller-48:8000/v1
为了管理栈，在认证服务中Orchestration需要更多信息
#控制节点#为栈创建 heat 包含项目和用户的域openstack domain create --description &quot;Stack projects and users&quot; heat#在 heat 域中创建管理项目和用户的heat_domain_admin用户：openstack user create --domain heat --password=HEAT_DOMAIN_USER_PASS heat_domain_admin#)添加admin角色到 heat 域 中的heat_domain_admin用户，启用heat_domain_admin用户#管理栈的管理权限openstack role add --domain heat --user-domain heat --user heat_domain_admin admin#为栈创建 heat 包含项目和用户的域openstack role create heat_stack_owner#添加heat_stack_owner 角色到demo 项目和用户，启用demo 用户管理栈。openstack role add --project demo --user demo heat_stack_owner#必须添加 heat_stack_owner 角色到每个管理栈的用户。#heat_stack_user 角色openstack role create heat_stack_user
安装并配置Heat组件相关软件
#控制节点yum install openstack-heat-api openstack-heat-api-cfn \  openstack-heat-engine -ymv /etc/heat/heat.conf&#123;,.bak&#125;cat &gt;&gt; /etc/heat/heat.conf &lt;&lt; EOF[database]connection = mysql+pymysql://heat:HEAT_DBPASS@controller-48/heat  #HEAT_DBPASS是HEAT数据库密码[DEFAULT]transport_url = rabbit://openstack:RABBIT_PASS@controller-48#RABBIT_PASS为Rabbitmq服务密码 用户名是openstack[keystone_authtoken]www_authenticate_uri = http://controller-48:5000auth_url = http://controller-48:5000memcached_servers = controller-48:11211auth_type = passwordproject_domain_name = defaultuser_domain_name = defaultproject_name = serviceusername = heatpassword = HEAT_PASS#HEAT_PASS是heat用户密码[trustee]auth_type = passwordauth_url = http://controller-48:5000username = heatpassword = HEAT_PASS#HEAT_PASS是heat用户密码user_domain_name = default[clients_keystone]auth_uri = http://controller-48:5000[DEFAULT]heat_metadata_server_url = http://controller-48:8000heat_waitcondition_server_url = http://controller-48:8000/v1/waitcondition[DEFAULT]stack_domain_admin = heat_domain_adminstack_domain_admin_password = HEAT_DOMAIN_USER_PASSstack_user_domain_name = heatEOFsu -s /bin/sh -c &quot;heat-manage db_sync&quot; heat##启动 Orchestration 编排服务heat组件并将其设置为随系统启动systemctl enable openstack-heat-api.service \  openstack-heat-api-cfn.service openstack-heat-engine.servicesystemctl restart openstack-heat-api.service \  openstack-heat-api-cfn.service openstack-heat-engine.service  [root@controller-48 ~]# systemctl list-unit-files |grep openstack-heat*openstack-heat-api-cfn.service                enabledopenstack-heat-api.service                    enabledopenstack-heat-engine.service                 enabled
验证
cdsource admin-openrc.shopenstack service listopenstack orchestration service list该输出显示表明在控制节点上有应该四个heat-engine组件。[root@controller-48 ~]# openstack orchestration service list+------------+-------------+--------------------------------------+------------+--------+----------------------------+--------+| Hostname   | Binary      | Engine ID                            | Host       | Topic  | Updated At                 | Status |+------------+-------------+--------------------------------------+------------+--------+----------------------------+--------+| controller-48 | heat-engine | 230ae8e8-3c9f-4b82-b0ca-caef3d5497f1 | controller-48 | engine | 2023-05-27T14:42:52.000000 | up     || controller-48 | heat-engine | 626e74a4-918b-46b8-8993-d6db92eb861e | controller-48 | engine | 2023-05-27T14:42:52.000000 | up     || controller-48 | heat-engine | f648e766-cdb9-4e06-b190-a713baf33df8 | controller-48 | engine | 2023-05-27T14:42:52.000000 | up     || controller-48 | heat-engine | 2cb3dfd3-0636-432c-8d59-f22d850510d5 | controller-48 | engine | 2023-05-27T14:42:52.000000 | up     |+------------+-------------+--------------------------------------+------------+--------+----------------------------+--------+
创建实例
创建实例类型
左侧选择管理员，点击计算，点击实例类型，右侧点击创建实例类型。

根据以上图片步骤依次填入：实例名称、VCPU数量、内存大小、根磁盘大小，确认无误后点击创建实例类型。
创建镜像
测试镜像：https://download.cirros-cloud.net/0.6.2/cirros-0.6.2-x86_64-disk.img
有两种上传方式（二选一）！！！
1.Windows上传镜像方式
左侧选择管理员，点击计算，点击镜像，右侧点击创建镜像。
Windows下载到本地即可

根据以上图片步骤依次填入：镜像名称、选择文件、镜像格式，确认无误后点击创建镜像。
注：演示上传的 img 镜像格式需选用 QCOW2 - QEMU模拟器 才可正常加载。
2.Linux上传镜像方式
source admin-openrcwget https://download.cirros-cloud.net/0.6.2/cirros-0.6.2-x86_64-disk.img#可能会下载不到，可以复制链接到浏览器下载，然后移到/root/目录下glance image-create --name &quot;cirros&quot; \  --file cirros-0.6.2-x86_64-disk.img \  --disk-format qcow2 --container-format bare \  --visibility=publicopenstack image list[root@controller-1 ~]# openstack image list+--------------------------------------+--------+--------+| ID                                   | Name   | Status |+--------------------------------------+--------+--------+| 627761da-7f8c-4780-842a-e50e62f5c464 | cirros | active |+--------------------------------------+--------+--------+
创建内部网络
左侧选择管理员，点击网络，点击网络，右侧点击创建网络。



创建外部网络
左侧选择管理员，点击网络，点击网络，右侧点击创建网络。
如果你是按照本文档搭建的，就填provider



创建路由
左侧选择管理员，点击网络，点击路由，右侧点击创建路由。



添加安全组规则


最后效果长这样

创建实例




然后点击创建实例
分配浮动ip


结论：创建实例成功

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>Centos 7</tag>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title>Rancher容器管理平台</title>
    <url>/posts/e650e0d0/</url>
    <content><![CDATA[
Rancher容器管理平台

Rancher 中文文档: https://docs.rancher.cn/
什么是 Rancher？
Rancher 是一个 Kubernetes 管理工具，让你能在任何地方和任何提供商上部署和运行集群。
Rancher 可以创建来自 Kubernetes 托管服务提供商的集群，创建节点并安装 Kubernetes，或者导入在任何地方运行的现有 Kubernetes 集群。
Rancher 基于 Kubernetes 添加了新的功能，包括统一所有集群的身份验证和 RBAC，让系统管理员从一个位置控制全部集群的访问。
此外，Rancher 可以为集群和资源提供更精细的监控和告警，将日志发送到外部提供商，并通过应用商店（Application Catalog）直接集成  Helm。如果你拥有外部 CI/CD 系统，你可以将其与 Rancher 对接。没有的话，你也可以使用 Rancher 提供的 Fleet  自动部署和升级工作负载。
Rancher 是一个 全栈式 的 Kubernetes 容器管理平台，为你提供在任何地方都能成功运行 Kubernetes 的工具。
Docker部署Rancher
仅需三步！开启Rancher之旅
准备一台Linux主机 要求4GB内存，并且已经安装Docker
安装教程：使用 Docker 将 Rancher 安装到单个节点中

**Docker 安装在生产环境中不支持。**这些说明仅用于测试和开发目的。请不要使用此方法在生产环境中安装 Rancher。Rancher 的 Docker 安装仅推荐用于开发和测试环境中

1.基础配置
Rancher 容器内部启动了 k3s，它依赖于底层宿主机具备 iptables 支持（用于网络转发、容器间通信等），否则会因为 缺少 iptable_nat / iptable_filter 模块 而导致：
yum install -y iptablesmodprobe iptable_natmodprobe iptable_filtersudo tee /etc/modules-load.d/modules.conf &gt;/dev/null &lt;&lt;EOF iptable_natiptable_filterEOFreboot
1.创建rancher挂在目录
docker rm -f qianyios_rancherrm -rf /data/rancher_home/rancher/*rm -rf /data/rancher_home/auditlog/*mkdir -p /data/rancher_home/ranchermkdir -p /data/rancher_home/auditlog
2.安装rancher
docker run -d --privileged --restart=unless-stopped \  -p 8080:80 -p 4431:443 \  -v /data/rancher_home/rancher:/var/lib/rancher \  -v /data/rancher_home/auditlog:/var/log/auditlog \  --name qianyios_rancher registry.cn-guangzhou.aliyuncs.com/qianyios/rancher:stable
-p 8080:80   外部端口：8080（宿主机端口）：内部端口：80（容器端口）
-p 4431:443  外部端口：4431（宿主机端口）：内部端口：443（容器端口）
registry.cn-guangzhou.aliyuncs.com/qianyios/rancher:stable是我自己构建的镜像（不定期更新，可能会落后版本），如果有条件可以下载原版镜像 rancher/rancher:stable
或者用国内构建的官方镜像registry.cn-hangzhou.aliyuncs.com/qianyios/rancher:版本号这个你需要携带版本号
--privileged 标志会给容器内的进程几乎相同于宿主机的特权。这意味着容器内的进程可以访问宿主机的所有设备，并且可以执行一些通常需要特权的操作，比如加载内核模块等

访问地址：https://192.168.48.10:4431


查看密码
docker logs qianyios_rancher  2&gt;&amp;1 | grep &quot;Bootstrap Password:&quot;
qianyios_rancher是我前面创建容器的一个名字

Bootstrap Password: 5qnwrp6gm9q28zsqnkbd2ktjxpkqkbt9rdbpb46c5xzf5qqkr8snx7

设置自定义密码


首页就这样了，安装成功
设置中文

添加已
在Kubernetes上部署Rancher
在此之前添加一个环境变量
操作节点：[所有的server]
在root用户下
sudo echo &quot;export KUBECONFIG=/etc/rancher/k3s/k3s.yaml&quot; &gt;&gt; /etc/profilesudo echo &quot;export KUBECONFIG=/etc/rancher/k3s/k3s.yaml&quot; &gt;&gt; ~/.bashrcsource /etc/profilesource ~/.bashrcsudo kubectl config view --raw &gt;&gt;~/.kube/configecho $KUBECONFIG
先决条件

helm3
[Kubernetes 集群](#Kubernetes 集群)
[Ingress Controller](#Ingress Controller)
CLI 工具

Helm3
建议所有的server节点都安装
官网教程：Helm3 安装
本教材默认你使用的是helm3，如果没安装请按照以下教程安装，安装了就略过
sudo curl https://qygit.qianyisky.cn/https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

Kubernetes 集群
自行部署一个Kubernetes集群
已经测试过的Kubernetes
基于OpenEuler部署K3S
基于Centos7部署k3s
OpenEuler-K8S高可用集群（内部etcd）
Ingress Controller
Rancher UI 和 API 通过 Ingress 公开。换言之，安装 Rancher 的 Kubernetes 集群必须包含一个 Ingress Controller。
对于 RKE、RKE2 和 K3s，你不需要手动安装 Ingress Controller，因为它是默认安装的。
对于默认不包含 Ingress Controller 的发行版（例如 EKS、GKE 或 AKS 等托管 Kubernetes 集群），你必须先部署  Ingress Controller。请注意，Rancher Helm Chart 默认情况下不会在 Ingress 上设置 ingressClassName。因此，你必须将 Ingress Controller 配置为在没有 ingressClassName 的情况下也可以监视 Ingress。
上面的 Amazon EKS、AKS 和 GKE 教程中包含了示例。
如果不是RKE、RKE2 和 K3s的Kubernetes，你需要按照一下教程安装Ingress Controller
sudo helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginxsudo helm repo updatesudo helm search repo ingress-nginx -l
安装 Rancher Helm Chart
Rancher 是使用 Kubernetes 的 Helm 包管理器安装的。Helm Chart 为 Kubernetes YAML 清单文件提供了模板语法。通过 Helm，用户可以创建可配置的 deployment，而不仅仅只能使用静态文件。
如果系统无法直接访问互联网，请参见离线环境：Kubernetes 安装。
如果要指定安装的 Rancher 版本，请参见选择 Rancher 版本。
如果要指定用于安装 Rancher 的 Helm 版本，请参见Helm 版本要求。
本安装指南假定你使用的是 Helm 3。
添加 Helm Chart 仓库
操作节点：[server1]
Latest：建议用于试用最新功能
sudo helm repo add rancher-latest https://releases.rancher.com/server-charts/latest
Stable：建议用于生产环境(本实验实验这个)
sudo helm repo add rancher-stable https://releases.rancher.com/server-charts/stable
Alpha：即将发布的实验性预览。
sudo helm repo add rancher-alpha https://releases.rancher.com/server-charts/alpha
注意：不支持升级到 Alpha 版、从 Alpha 版升级或在 Alpha 版之间升级。
为 Rancher 创建命名空间
kubectl create namespace cattle-system
选择 SSL 配置
Rancher Management Server 默认需要 SSL/TLS 配置来保证访问的安全性。
你可以从以下三种证书来源中选择一种，用于在 Rancher Server 中设置 TLS：

Rancher 生成的 TLS 证书：要求你在集群中安装 cert-manager。Rancher 使用 cert-manager 签发并维护证书。Rancher 会生成自己的 CA 证书，并使用该 CA 签署证书。然后 cert-manager负责管理该证书。
Let’s Encrypt：Let’s Encrypt 选项也需要使用 cert-manager。但是，在这种情况下，cert-manager 与 Let’s Encrypt 的特殊颁发者相结合，该颁发者执行获取 Let’s Encrypt 颁发的证书所需的所有操作（包括请求和验证）。此配置使用 HTTP 验证（HTTP-01），因此负载均衡器必须具有可以从互联网访问的公共 DNS 记录。
你已有的证书：使用已有的 CA 颁发的公有或私有证书。Rancher 将使用该证书来保护 WebSocket 和 HTTPS 流量。在这种情况下，你必须上传名称分别为 tls.crt 和 tls.key的 PEM 格式的证书以及相关的密钥。如果你使用私有 CA，则还必须上传该 CA 证书。这是由于你的节点可能不信任此私有 CA。Rancher  将获取该 CA 证书，并从中生成一个校验和，各种 Rancher 组件将使用该校验和来验证其与 Rancher 的连接。




配置
Helm Chart 选项
是否需要 cert-manager




Rancher 生成的证书（默认）
ingress.tls.source=rancher
是


Let’s Encrypt
ingress.tls.source=letsEncrypt
是


你已有的证书
ingress.tls.source=secret
否



安装 cert-manager
建议提前下载这个镜像rancher/mirrored-library-traefik:3.3.6
如果你使用自己的证书文件（ingress.tls.source=secret）或使用外部负载均衡器的 TLS 设置，你可以跳过此步骤。
仅在使用 Rancher 生成的证书（ingress.tls.source=rancher）或 Let’s Encrypt 颁发的证书（ingress.tls.source=letsEncrypt）时，才需要安装 cert-manager。
# 如果你手动安装了CRD，而不是在 Helm 安装命令（下面最后一条安装命令）中添加了 `--set installCRDs=true` 选项，你应该在升级 Helm Chart 之前升级 CRD 资源。sudo kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/&lt;版本号&gt;/cert-manager.crds.yaml# 添加 Jetstack Helm 仓库sudo helm repo add jetstack https://charts.jetstack.io# 更新本地 Helm Chart 仓库缓存sudo helm repo update# 安装 cert-manager Helm Chartsudo helm install cert-manager jetstack/cert-manager \  --namespace cert-manager \  --create-namespace \  --set installCRDs=true \  --kubeconfig /etc/rancher/k3s/k3s.yaml

这里可能要等个一两分钟
sudo kubectl get pods --namespace cert-manager
[abc@server1 ~]$ sudo kubectl get pods --namespace cert-managerNAME                                       READY   STATUS    RESTARTS   AGEcert-manager-7ccbc46c67-w6bpt              1/1     Running   0          118scert-manager-cainjector-655bb66698-gsqm6   1/1     Running   0          118scert-manager-webhook-586666dcd7-mhcmn      1/1     Running   0          118s
如果要卸载就用下面的命令
helm uninstall cert-manager --namespace cert-managerkubectl delete namespace cert-manager
根据你选择的证书选项，通过 Helm 安装 Rancher
日期：2025年6月30日
目前他会用到的进行过是rancher/rancher:v2.11.3但是拉去很慢。那你就用下面的命令，后面如果更新了，我就不知道了，下面是我自己构建的镜像，如果版本变量，你就自己构建镜像，然后自己打tag就行了
sudo docker pull registry.cn-guangzhou.aliyuncs.com/qianyios/rancher:v2.11.3sudo docker rmi rancher/rancher:v2.11.3sudo docker tag registry.cn-guangzhou.aliyuncs.com/qianyios/rancher:v2.11.3 rancher/rancher:v2.11.3
不同的证书配置需要使用不同的 Rancher 安装命令。
但是，无论证书如何配置，Rancher 在 cattle-system 命名空间中的安装名称应该总是 rancher。

测试和开发：
这个安装 Rancher 的最终命令需要一个将流量转发到 Rancher 的域名。如果你使用 Helm CLI 设置概念证明，则可以在传入 hostname 选项时使用伪域名。伪域名的一个例子是 &lt;IP_OF_LINUX_NODE&gt;.sslip.io，这会把 Rancher 暴露在它运行的 IP 上。生产安装中要求填写真实的域名。

这里就用Rancher 生成的证书的方式，其他的方式需要自行访问官网根据你选择的证书选项，通过 Helm 安装 Rancher
#添加 Rancher 的 Helm 仓库sudo helm uninstall rancher --namespace cattle-systemsudo helm repo add rancher-stable https://releases.rancher.com/server-charts/stablesudo helm repo update
域名安装方式
默认情况是使用 Rancher 生成 CA，并使用 cert-manager 颁发用于访问 Rancher Server 接口的证书。
由于 rancher 是 ingress.tls.source 的默认选项，因此在执行 helm install 命令时，我们不需要指定 ingress.tls.source。

将 hostname 设置为解析到你的负载均衡器的 DNS 名称。
将 bootstrapPassword 设置为 admin 用户独有的值。
如果你需要安装指定的 Rancher 版本，使用 --version 标志，例如 --version 2.7.0。
对于 Kubernetes v1.25 或更高版本，使用 Rancher v2.7.2-v2.7.4 时，将 global.cattle.psp.enabled 设置为 false。对于 Rancher v2.7.5 及更高版本来说，这不是必需的，但你仍然可以手动设置该选项。

你要自行更改域名
sudo helm install rancher rancher-stable/rancher \  --namespace cattle-system \  --create-namespace \  --set hostname=qianyios.top \  --set bootstrapPassword=admin \  --set global.cattle.psp.enabled=false
如果你安装的是 alpha 版本，Helm 会要求你在安装命令中添加 --devel 选项：

如果你希望使用域名访问 Rancher，可以在宿主机上配置 hosts 文件，并将 Helm 安装命令中的 hostname 参数改为域名。
NodePort（本实验用这个）
如果你不想配置域名，也可以通过配置 Kubernetes 的 NodePort 或 LoadBalancer 来访问 Rancher。
sudo helm install rancher rancher-stable/rancher \  --namespace cattle-system \  --create-namespace \  --set hostname=192.168.48.200 \  --set bootstrapPassword=admin \  --set global.cattle.psp.enabled=false \  --set ingress.enabled=false \  --set service.type=NodePort
sudo kubectl get pods,svc -n cattle-system

Rancher的访问地址就是https://192.168.48.200:32684
LoadBalancer

如果你的 Kubernetes 环境支持 LoadBalancer，可以将 service.type 设置为 LoadBalancer：

sudo helm install rancher rancher-stable/rancher \  --namespace cattle-system \  --create-namespace \  --set hostname=192.168.48.200 \  --set bootstrapPassword=admin \  --set global.cattle.psp.enabled=false \  --set ingress.enabled=false \  --set service.type=LoadBalancer
等待 Rancher 运行：
kubectl -n cattle-system rollout status 
[root@server1 ~]# kubectl -n cattle-system rollout status deploy/rancherdeployment &quot;rancher&quot; successfully rolled out[root@server1 ~]#
Rancher Chart 有许多选项，用于为你的具体环境自定义安装。以下是一些常见的高级方案：

HTTP 代理
私有容器镜像仓库
外部负载均衡器上的 TLS 终止

如需获取完整的选项列表，请参见 Chart 选项。
验证 Rancher Server 是否部署成功
kubectl -n cattle-system get deploy rancher
[root@server1 ~]# kubectl -n cattle-system get deploy rancherNAME      READY   UP-TO-DATE   AVAILABLE   AGErancher   3/3     3            3           36m[root@server1 ~]#
DESIRED 和 AVAILABLE的个数应该相同。
页面访问


设置中文

添加已有集群



未完待续

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>K8s</tag>
        <tag>K3s</tag>
      </tags>
  </entry>
  <entry>
    <title>Samba服务器实战</title>
    <url>/posts/a8a56b19/</url>
    <content><![CDATA[
Samba服务器实战
Samba是在Linux和UNIX系统上实现SMB协议的一个免费软件，由服务器及客户端程序构成。SMB（Server Messages Block，信息服务块）是一种在局域网上共享文件和打印机的一种通信协议，它为局域网内的不同计算机之间提供文件及打印机等资源的共享服务。SMB协议是客户机/服务器型协议，客户机通过该协议可以访问服务器上的共享文件系统、打印机及其他资源。通过设置“NetBIOS over TCP/IP”使得Samba不但能与局域网络主机分享资源，还能与全世界的电脑分享资源。
主机拓扑



主机名
os
ip
内存
硬盘




qianyios
Openeuler22.03LTS
192.168.48.101
2G
100G



安装Samba
dnf  install -y samba#关闭防火墙systemctl disable firewalld &amp;&gt; /dev/nullsystemctl stop firewalldsed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsetenforce 0
Samba共享服务的匿名访问
配置文件
samba服务器最主要的配置文件其实只有一个，就是/etc/samba/samb.conf，这个配置文件可以分为两个部分，一个部分是全局参数，一部分是共享资源相关参数。
vim /etc/samba/smb.conf
先取消[homes]、[printers]的项目，添加[share]项目如下
[global]        #与主机名相关的设置        #工作组名称        workgroup = SAMBA        security = user        passdb backend = tdbsam        printing = cups        printcap name = cups        load printers = yes        cups options = raw        ##添加此项，开启匿名用户访问        map to guest = Bad User        include = /etc/samba/usershares.conf[share]	#设置共享路径        path = /abc         ##公共访问	 public=yes	 ##能够访问        browseable=yes        ##写权限        writable=yes        ##设置权限        create mask=0644        directory mask=0755
创建共享路径，并给权限
mkdir /abcchmod 777 /abcsystemctl enable --now smb.service#linux创建测试文件,客户端查看echo &quot;text&quot; &gt;&gt; /abc/test.txt
文件夹地址栏输入\192.168.48.101\share,他会提示你输入用户和密码，用户名是nobody，密码不用输，然后就客户端就可以看见linux创建的测试文件

客户端创建文件夹，linux查看


Samba共享服务的用户身份验证
配置文件
vim /etc/samba/smb.conf
[global]        #与主机名相关的设置        #工作组名称        workgroup = SAMBA        security = user        passdb backend = tdbsam        printing = cups        printcap name = cups        load printers = yes        cups options = raw        ##添加此项，开启匿名用户访问       ## map to guest = Bad User   #（删除匿名访问）        include = /etc/samba/usershares.conf[share]	#设置共享路径        path = /abc        ##公共访问        # public=yes   #（删除公共访问）         ##能够访问        browseable=yes        ##写权限        #writable=yes  #（删除写入权限）        ##设置权限        create mask=0644        directory mask=0755        ##允许访问的用户        valid users=qianyios, qianyios1        ##允许写入的用户        write list=qianyios
创建smb用户
useradd qianyiosuseradd qianyios1smbpasswd -a qianyios#有以下提示New SMB password:    ##设置密码Retype new SMB password:   ##确认密码Added user qianyios.#qianyios1也是这样的操作smbpasswd -a qianyios1

列出smb用户列表
pdbedit -L

#重启systemctl restart smb.service 
用linux的smb客户端进行测试
dnf install -y samba-client


刚好根据前面的配置文件
##允许访问的用户valid users=qianyios, qianyios1##允许写入的用户write list=qianyios
qianyios可创建文件qianyios1不可以创建，但是可以读取文件
windows客户的测试
由于之前做匿名测试连接
要在cmd清理网络驱动器缓存
net use * /del


在qianyios用户下可以创建文件夹

用qianyios1试一下

可查看文件但不能创建文件

Samba共享服务的账户名映射（账户别名登录）
配置映射文件和Samba配置文件
cat &gt; /etc/samba/smbusers &lt;&lt;&quot;EOF&quot;qianyios = qyosqianyios1 = qyos1EOF
vim /etc/samba/smb.conf#添加以下内容在对应位置username map = /etc/samba/smbusers

重启服务
systemctl restart smb.service
windows客户端测试
cmd清理缓存
net use * /del

qyos可以登入

qyos1也可以登入

Samba共享服务的访问控制列表
禁止某个网段访问
hosts deny=192.168.48.  ##添加拒绝192.168.48段访问share
或者如果你想允许除了192.168.48.0/24之外的所有主机访问：
hosts allow = ALL EXCEPT 192.168.48.
vim /etc/samba/smb.conf
[share]        #设置共享路径        path = /abc        ##公共访问        # public=yes   #（删除公共访问）         ##能够访问        browseable=yes        ##写权限        #writable=yes  #（删除写入权限）        ##设置权限        create mask=0644        directory mask=0755        ##允许访问的用户        valid users=qianyios,qianyios1        ##允许写入的用户        write list=qianyios        ##添加拒绝192.168.48段访问share	 hosts deny=192.168.48.  
##重启Samba服务systemctl restart smb.service  
windows就不测试了用linux的smb客户端来测试一下
smbclient //192.168.48.101/share -U qyos

我们本机是192.168.48.101的，因为禁用了192.168.48.0/24的网段访问，所以都访问不了
将samba挂载到linux使用
记得取消的禁止访问哦
#创建挂载点mkdir -p /opt/share#安装附属yum install cifs-utils  -y#将共享文件夹挂载到挂载点mount.cifs //192.168.48.101/share /opt/share -o username=qianyiosdf -hls /opt/share

已挂载成功

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Samba</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu 24.04.02 LTS 初始化安装</title>
    <url>/posts/14363/</url>
    <content><![CDATA[
Ubuntu 24.04.02 LTS 初始化安装
镜像下载
桌面版：ubuntu-24.04.2-desktop-amd64.iso
本次实验选用的是桌面版
使用的虚拟化软件 VMware Workstation
创建虚拟机



加装镜像

先别启动，接下来设置网络
配置NAT网卡

开启虚拟机安装系统
启动之后，会有一个黑色界面有一个选项是Try or install ubuntu默认是第一个，直接回车就行了







这是一个终端工具，接下来我就不在这里运行了，我会给出代码，你照着在上面那张图运行就行了
设置root用户密码

#允许root用户远程登入sed -i &#x27;/^#PermitRootLogin prohibit-password/s/^#//&#x27; /etc/ssh/sshd_configsed -i &#x27;/^PermitRootLogin/s/prohibit-password/yes/&#x27; /etc/ssh/sshd_config
配置静态ip
直接去那个终端运行就行了，我不截图了
安装必要工具
sudo apt-get install network-manager openssh-server vim inetutils-ping net-tools -y
查看需要配置的网卡，输入
ip a

vim /etc/netplan/01-network-manager-all.yaml
# Let NetworkManager manage all devices on this systemnetwork:  ethernets:    ens33:      addresses: [192.168.48.128/24]      dhcp4: false      nameservers:          addresses: [192.168.48.2, 114.114.114.114]      routes:        - to: default          via: 192.168.48.2  version: 2  renderer: NetworkManager

这个ip192.168.48.128最后一位128你随意设置，192.168.48.这是网段对应前面配置NAT网卡你的网段，自行修改，网关（routes）也是
最后按esc，然后按shift+冒号键，输入wq保存退出
netplan apply

静态ip已经配置好了
关闭防火墙
1. 查看防火墙状态：
要查看 Ubuntu 中 ufw 防火墙的状态，可以执行以下命令：
sudo ufw status
这将显示当前防火墙规则的状态，包括是否启用和允许的规则。
2. 开启防火墙：
如果防火墙没有启用，可以使用以下命令来启用 ufw 防火墙：
sudo ufw enable
启用防火墙后，它将按照默认规则开始工作，通常会拒绝所有传入连接，但允许所有传出连接。
3. 关闭防火墙：
要关闭 ufw 防火墙，可以执行以下命令：
sudo ufw disable
关闭防火墙后，所有传入和传出的连接将被允许，不再受到防火墙的限制。
4. 永久关闭防火墙：
如果想永久关闭 ufw 防火墙，可以执行以下步骤：


停止 ufw 服务：
sudo systemctl stop ufw


禁用 ufw 服务的自动启动：
sudo systemctl disable ufw --now


重启系统，以确保防火墙不会在系统启动时重新启用。


设置阿里源
全部复制粘贴就行了
cat &gt;/etc/apt/sources.list&lt;&lt;&quot;EOF&quot;deb https://mirrors.aliyun.com/ubuntu/ noble main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ noble main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ noble-security main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ noble-security main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ noble-updates main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ noble-updates main restricted universe multiverse# deb https://mirrors.aliyun.com/ubuntu/ noble-proposed main restricted universe multiverse# deb-src https://mirrors.aliyun.com/ubuntu/ noble-proposed main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ noble-backports main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ noble-backports main restricted universe multiverseEOFsudo apt-get update
设置模板
进行关机
sudo poweroff


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell</title>
    <url>/posts/24229/</url>
    <content><![CDATA[
Shell
概述
Shell是一个命令行解释器，他接收应用程序/用户命令，然后调用操作系统内核

Shell还是一个功能相当强大的编辑语言，易编写，易调试，灵活性强。
Shell解析器
Linux提供的Shell解析器有6种：
[root@Shell ~]# cat /etc/Shells/bin/sh/bin/bash/usr/bin/sh/usr/bin/bash/bin/tcsh/bin/csh
bash和sh的关系是
[root@Shell ~]# ll /bin/ | grep bash-rwxr-xr-x. 1 root root     964536 4月   1 2020 bashlrwxrwxrwx. 1 root root          4 9月  28 07:56 sh -&gt; bash软连接
Centos默认的解析器是bash
[root@Shell ~]# echo $Shell/bin/bash
Shell脚本入门
脚本格式
脚本以#!/bin/bash开头（指向解析器）
第一个Shell脚本：helloworld
需求：创建一个Shell脚本，输出helloworld
实例操作：
[root@Shell ~]# mkdir datas          ---创建一个脚本文件夹[root@Shell ~]# cd datas/            ---以后所有脚本放在这[root@Shell datas]# touch helloworld.sh[root@Shell datas]# vi helloworld.sh#!/bin/bashecho &quot;helloworld  严千屹&quot;~[root@Shell datas]# sh helloworld.sh        ---相对路径helloworld  严千屹[root@Shell ~]# bash datas/helloworld.sh    ---绝对路径helloworld  严千屹#上面都是bash和sh帮你执行脚本，脚本本身不需要执行权限。#下面本质是脚本需要自己执行，所以需要执行权限[root@Shell datas]# ./helloworld.sh-bash: ./helloworld.sh: 权限不够[root@shell datas]# chmod 777 helloworld.sh   ---给予权限[root@shell datas]# ./helloworld.shhelloworld  严千屹
第二个Shell脚本：多命令处理
需求：
在/root/datas/目录下创建一个qianyi.txt,在 qianyi.txt 文件中增加“qianyios”
[root@shell datas]# touch qy.sh[root@shell datas]# vi qy.sh#!/bin/bashcd /root/datas/touch qianyi.txtecho &quot;qianyios&quot; &gt;&gt; qianyi.txt[root@shell datas]# bash qy.sh#执行完成后会出现qianyi.txt[root@shell datas]# ll总用量 12-rwxrwxrwx 1 root root 43 1月  25 17:06 helloworld.sh-rw-r--r-- 1 root root  9 1月  25 17:22 qianyi.txt-rw-r--r-- 1 root root 76 1月  25 17:22 qy.sh[root@shell datas]# cat qianyi.txtqianyios
Shell中的变量
常用的系统变量
​	$HOME   $PWD   $SHELL  $USER等
实例操作
查看系统变量的值
[root@shell datas]# echo $HOME/root显示家目录
显示当前Shell中所有的变量：set
[root@shell datas]# setABRT_DEBUG_LOG=/dev/nullBASH=/usr/bin/bashBASH_ALIASES=()BASH_ARGC=()BASH_ARGV=()BASH_CMDS=()......
自定义变量
基本语法

定义变量：变量=值
撤销变量：unset 变量
声明静态变量：readonly 变量   注意不能unset

变量定义规则

变量名称可以有字母，数字和下划线组成，但是不能以数字开头，环境变量名建议全部大写
等号两侧不能有空格
在bash中，变量默认类型都是字符串类型，无法直接进行数值运算
变量的值如果有空格，需要使用双引号或单引号括起来

实例操作
#自定义变量[root@shell ~]# QY=qianyi[root@shell ~]# echo $QYqianyi[root@shell ~]# QY=&quot;qian yi&quot;[root@shell ~]# echo $QYqian yi变量为空和未定义变量是两个不同的概念[root@shell ~]# echo $age      ---未定义[root@shell ~]# age=&quot;&quot;[root@shell ~]# echo $age      ---已定义#两个没办法辨认[root@shell ~]# set -u        ---调用为声明变量会报错[root@shell ~]# echo $add-bash: add: unbound variable[root@shell ~]# add=123[root@shell ~]# echo $add123#删除变量  不需要加$add[root@shell ~]# echo $add123[root@shell ~]# unset add[root@shell ~]# echo $add-bash: add: unbound variable
环境变量

环境变量设置

[root@shell ~]# export qyage=&quot;18&quot;#使用export声明的变量即是环境变量

环境变量查询  set可以查看所有变量，env只能查看环境变量

[root@shell ~]# env | grep qyageqyage=18

系统默认环境变量

[root@shell ~]# envXDG_SESSION_ID=10HOSTNAME=shellTERM=xtermSHELL=/bin/bashHISTSIZE=1000qyage=18

PATH变量：系统查找命令的路径

先查询下PATH环境变量的值：
[root@shell ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin
PATH 变量的值是用“:”分割的路径，这些路径就是系统查找命令的路径。也就是说当我们输入了一个程序名，如果没有写入路径，系统就会到 PATH 变量定义的路径中去寻找，是否有可以执行的程序。如果找到则执行，否则会报“命令没有发现”的错误。
那么是不是我们把自己的脚本拷贝到 PATH 变量定义的路径中，我们自己写的脚本也可以不输入路径而直接运行呢?
[root@shell ~]# cp datas/helloworld.sh /usr/bin/[root@shell ~]# helloworld.shhelloworld  严千屹[root@shell ~]# rm -rf /usr/bin/helloworld.sh[root@shell ~]# helloworld.sh-bash: /usr/bin/helloworld.sh: 没有那个文件或目录
那么我们是不是可以修改 PATH变量的值，而不是把程序脚本复制到/bin/目录中。当然是可以的,我们通过变量的叠加就可以实现了:
[root@shell ~]# PATH=&quot;$PATH&quot;:/root/datas[root@shell ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/root/datas[root@shell ~]# helloworld.shhelloworld  严千屹
PS1变量：命令提示符设置
PS1是一个很有意思的变量，是用来定义命令行的提示符，可以安装我们自己的需求来定义自己喜欢的提示符。PS1可以支持以下这些选项：
\d ：#代表日期，格式为weekday month date，例如：&quot;Mon Aug 1&quot;   \H ：#完整的主机名称。   \h ：#仅取主机的第一个名字。 \t ：#显示时间为24小时格式，如：HH：MM：SS   \T ：#显示时间为12小时格式   \A ：#显示时间为24小时格式：HH：MM   \u ：#当前用户的账号名称   \v ：#BASH的版本信息   \w ：#完整的工作目录名称。家目录会以 ~代替   \W ：#利用basename取得工作目录名称，所以只会列出最后一个目录 \# ：#下达的第几个命令   \$ ：#提示字符，如果是root时，提示符为：# ，普通用户则为：$  
格式 PS1=&#x27;[ ]\$ &#x27;     \$空格&#x27;  有个空格  一定要单引号
[root@shell ~]# echo $PS1[\u@\h \W]\$[root@shell ~]# PS1=&#x27;[\u@\t \w]\$ &#x27;[root@20:14:51 ~]# cd datas/[root@20:14:54 ~/datas]#
位置参数变量



位置参数变量
作用




$n
表示第n个参数，$1则表示第一个参数，$2表示第二个参数……如果有10以上的参数用${10}


$0
当前程序的名称，也就是命令本身。


$*
传递给程序的所有参数组成的字符串（参数）。


$@
以“参数1”、“参数2”……保存所有的参数。


$#
代表命令行中所有参数个数


$?
上一个代码或者Shell程序在Shell中退出的情况，如果正常退出则返回0，否则返回非0值。


$$
本程序的（进程ID）PID。


$!
上一个命令的PID。



实例操作
[root@shell datas]# vi count.sh#!/bin/basha=$1b=$2sum=$(($a+$b))#$((  ))双小括号才能进行数字运算，$( )运行命令echo $sumecho $0[root@shell datas]# chmod 755 count.sh[root@shell datas]# ./count.sh 22 3355./count.sh       #$0输出命令本身
[root@shell datas]# vi para.sh#!/bin/bashecho &quot;\$* is $*&quot;echo &quot;\$@ is $@&quot;echo &quot;\$# is $#&quot;[root@shell datas]# chmod 777 para.sh[root@shell datas]# ./para.sh 11 22 33 44$* is 11 22 33 44   #&#123;11 22 33 44&#125;看成一个整体 $@ is 11 22 33 44   #&#123;11&#125;,&#123;22&#125;,&#123;33&#125;,&#123;44&#125;$# is 4             #个数验证过程[root@shell datas]# vi para2.sh#!/bin/bash#for循环，有多少次出现多少次for i in  &quot;$*&quot;      do        echo $i      doneecho &quot;------------------&quot;for y in  &quot;$@&quot;        do         echo $y      done[root@shell datas]# ./para2.sh 11 22 33 4411 22 33 44             #整体，循环1次------------------11                      #4个，循环4次223344
预定义变量



$?
最后一次执行的命令的返回状态。如果这个变量的值为 0，证明上一个命令正确执行:如果这个变量的值为非 0(具体是哪个数，由命令自己来决定)，则证明上一个命令执行不正确了。




$$
当前进程的进程号(PID)


$!
后台运行的最后一个进程的进程号(PID)



[root@shell ~]# lsanaconda-ks.cfg  bin  datas  repo.bak[root@shell ~]# echo $?0[root@shell ~]# sjkdhasjkd-bash: sjkdhasjkd: 未找到命令[root@shell ~]# echo $?127
read键盘接收
定义
[root@shell datas]# read --help
-bash: read: --: 无效选项
read: 用法:   read [选项] [变量名]
read [-ers]
[-a 数组]
[-d 分隔符]
[-i 缓冲区文字]
[-n 读取字符数]
[-N 读取字符数]
[-p 提示符]
[-t 超时]
[-u 文件描述符]
[名称 …]名称指的是变量名
[root@shell datas]# vi count2.sh#!/bin/bashread -t 30 -p &quot;请输入第一个数字：&quot; num1read -t 30 -p &quot;请输入第二个数字：&quot; num2sum=$(($num1+$num2))echo $sum[root@shell datas]# chmod 777 count2.sh[root@shell datas]# ./count2.sh请输入第一个数字：1请输入第二个数字：12#########前者需要回车，下面的不需要回车[root@shell datas]# vi count2.sh#!/bin/bashread -n 1 -t 30 -p &quot;请输入第一个数字：&quot; num1   #-n 1 限制输入1个字符数echo -e &quot;\n&quot;read -s -t 30 -p &quot;请输入第二个数字：&quot; num2     #-s 是隐藏echo -e &quot;\n&quot;sum=$(($num1+$num2))echo $sum[root@shell datas]# ./count2.sh请输入第一个数字：1请输入第二个数字：         #我这里输入的是1213
declare声明变量类型
定义
既然所有变量的默认类型是字符串型，那么只要我们把变量声明为整数型不就可以运算了吗？

-:给变量设定类型属性
+:取消变量的类型属性
-a:将变量声明为数组型
-i:将变量声明为整数型(integer)
-x:将变量声明为环境变量
-r:将变量声明为只读变量，注意，一旦设置为只读变量，既不能修改变量的值，也不能删除变量，甚至不能通过+r取消只读属性
-p:显示指定变量的被声明的类型、

声明数值进行运算
[root@shell datas]# a=1[root@shell datas]# b=2[root@shell datas]# c=$a+$b[root@shell datas]# echo $c1+2[root@shell datas]# declare -i c=$a+$b      #声明整数类型才可以进行数值运行[root@shell datas]# echo $c3
数组
[root@shell datas]# declare -a name[0]=&quot;qy&quot;  #声明数组[root@shell datas]# name[1]=&quot;qy1&quot;   #也可以不用declare，就知道你在声明数组[root@shell datas]# name[2]=&quot;qy2&quot;[root@shell datas]# echo $&#123;name[*]&#125;qy qy1 qy2
环境变量
两个声明变量作用一样[root@shell datas]# export test[root@shell datas]# declare -x test=&quot;123&quot;
只读属性
注意，一旦设置为只读变量，既不能修改变量的值，也不能删除变量，甚至不能通过+r取消只读属性
[root@shell datas]# declare -r test        #添加只读属性[root@shell datas]# declare -p......declare -rx test=&quot;123&quot;[root@shell datas]# echo $test123[root@shell datas]# test=1           #无法改值-bash: test: 只读变量[root@shell datas]# unset test        #无法删除变量-bash: unset: test: 无法反设定: 只读 variable[root@shell datas]# declare +r test      #无法删除属性-bash: declare: test: 只读变量
不过好在这个变量是命令行声明的，所以重启或重新登入，这个变量就会消失
数值运算
使用expr或let数值运算工具
[root@shell ~]# aa=11[root@shell ~]# bb=22[root@shell ~]# dd=$(expr $aa + $bb)[root@shell ~]# echo $dd33#dd的值是aa和bb的和。注意“+”号两侧比有空格#如果是let呢[root@shell ~]# let e=$aa+$bb[root@shell ~]# echo $e33
使用双小括号 $((运算式子))  或$[运算式子] 的方式运算 （看个人习惯，看你喜欢用哪种）

区分单小括号$(  )调用的是系统命令  双小括号是计算数学运算的

[root@shell ~]# aa=11[root@shell ~]# bb=22[root@shell ~]# ff=$(($aa+$bb))[root@shell ~]# echo $ff33[root@shell ~]# gg=$[$aa+$bb][root@shell ~]# echo $gg33
Shell常用运算符



优先级
运算符
说明




13
-，+
单目负，单目正


12
!,~
逻辑非，按位取反或补码


11
*,/,%
乘、除、取模


10
+,-
加、减


9
&lt;&lt;,&gt;&gt;
按位左移，按位右移


8
&lt; = ,&gt; = ,&lt; , &gt;
小于或等于、大于或等于、小于、大于


7
==, !=
等于


6
&amp;
按位与


5
^
按位异或


4
|
按位或


3
&amp;&amp;
逻辑与


2
||
逻辑或


1
=,=,-=,=,/=,%=,&amp;=,^=,|=,&lt;&lt;=,&gt;&gt;=
赋值，运算且赋值



取模运算
[root@shell ~]# bb=$((14%5))[root@shell ~]# echo $bb4#14不能被5整除，余数是4
逻辑与
[root@shell ~]# cc=$((1&amp;&amp;0))[root@shell ~]# echo $cc0逻辑与运算只有想与的两边都是1，与的结果才是1，否则与的结果是0
四则运算练习
[root@shell ~]# vi count3.sh#!/bin/bashvaule=$(($1 $2 $3))echo $vaule[root@shell ~]# ./count3.sh 11 + 1122[root@shell ~]# ./count3.sh 11 / 111
但是上面会有bug不能用
[root@shell datas]# vi count4.sh#!/bin/bashread -t 30 -p &quot;please input num1:&quot; num1read -t 30 -p &quot;please input num2:&quot; num2read -n 1 -t 30 -p &quot;please inpute operato[+-*/]:&quot; operecho -e “\n”[ &quot;$oper&quot; == &quot;+&quot; ] &amp;&amp; echo &quot;$(($num1 + $num2))&quot; &amp;&amp; exit[ &quot;$oper&quot; == &quot;-&quot; ] &amp;&amp; echo &quot;$(($num1 - $num2))&quot; &amp;&amp; exit[ &quot;$oper&quot; == &quot;*&quot; ] &amp;&amp; echo &quot;$(($num1 * $num2))&quot; &amp;&amp; exit[ &quot;$oper&quot; == &quot;/&quot; ] &amp;&amp; echo &quot;$(($num1 / $num2))&quot; &amp;&amp; exitecho &quot;please input right oper&quot;[空格&quot;$oper&quot;空格==空格&quot;+&quot;空格]
变量的测试与内容置换



变量置换方式
变量y没有设置
变量y为空值
变量y设置值




x=${y-新值}
x= 新值
x 为空
x=$y


x=${y:-新值}
x= 新值
x= 新值
x=$y


x=${y+新值}
x 为空
x= 新值
x=新值


x=${y:+新值}
x 为空
x 为空
x=新值


x=${y=新值}
x= 新值
x 为空
x=$y


同上
y= 新值
y 值不变
y值不变


x=${y:=新值}
x= 新值
X= 新值
x=$y


同上
y= 新值
y= 新值
y值不变


x=${y?新值}
新值输出到标准错误输出（屏幕）
x 为空
x=$y


x=${y:?新值}
新值输出到标准错误输出
新值输出到标准错误输出
x=$y



假设我们要测b变量，现在b变量我们从来没有设置过
以下可以判断变b不存在[root@shell ~]# x=$&#123;b-new&#125;[root@shell ~]# echo $xnew以下可以判断是否为空[root@shell ~]# b=&quot;&quot;[root@shell ~]# x=$&#123;b-new&#125;[root@shell ~]# echo $x以下可以判断是否有值[root@shell ~]# b=123[root@shell ~]# x=$&#123;b-new&#125;[root@shell ~]# echo $x123
环境变量配置文件
1、让环境变量生效的命令  source 配置文件  或  . 配置文件
2、环境变量配置文件
登录时生效的环境变量配置文件
在 Linux 系统登录时主要生效的环境变量配置文件有以下五个:
/etc/profile
/etc/profile.d/*.sh
/etc/bashrc
~/.bash_profile
~/.bashrc
写在前三个的配置文件对所有用户生效，写在最后两个对当前用户生效
基础正则
基础正则表达式



元字符
作	用




*
前一个字符匹配 0 次或任意多次。


.
匹配除了换行符外任意一个字符。


^
匹配行首。例如：^hello 会匹配以 hello 开头的行。


$
匹配行尾。例如：hello&amp;会匹配以 hello 结尾的行。


[]
匹配中括号中指定的任意一个字符，只匹配一个字符。例如：[aoeiu] 匹配任意一个元音字母，[0-9] 匹配任意一位数字，[a-z][0-9]匹配小写字和一位数字构成的两位字符。


[^]
匹配除中括号的字符以外的任意一个字符。例如：[^0-9] 匹配任意一位非数字字符，[^a-z] 表示任意一位非小写字母。


\
转义符。用于取消讲特殊符号的含义取消。


{n}
表示其前面的字符恰好出现 n 次。例如：[0-9]{4} 匹配 4 位数字，[1][3-8][0-9]{9} 匹配手机号码。


{n,}
表示其前面的字符出现不小于 n 次。例如： [0-9]{2,} 表示两位及以上的数字。


{n,m}
表示其前面的字符至少出现n 次，最多出现m 次。例如：[a-z]{6,8}匹配 6 到 8 位的小写字母。



~/.bashrc 文件中建立这个别名：
实现grep能显示颜色
echo &quot;alias grep=\&#x27;grep --color=auto\&#x27;&quot; &gt;&gt; /root/.bashrc 
建立练习文档
[root@shell datas]# vi test.txtMr. Li Ming said:he was the most honest man.123despise him.googlegoooooglegglegoglesoooooidBut since Mr. shen Chao came, he never saaaid those words. 5555nice!because,actuaaaally,Mr. Shen Chao is the most honest manLater,Mr. Li ming soid his hot body.hello ishello was
练习*号
# a*意思是最少包含0个a或无数个a[root@shell datas]# grep &quot;a*&quot; test.txtMr. Li Ming said:he was the most honest man.123despise him.But since Mr. shen Chao came, he never saaaid those words. 5555nice!because,actuaaaally,Mr. Shen Chao is the most honest manLater,Mr. Li ming soid his hot body.# aa*意思是最少包含1个a或无数个a[root@shell datas]# grep &quot;aa*&quot; test.txtMr. Li Ming said:he was the most honest man.But since Mr. shen Chao came, he never saaaid those words. 5555nice!because,actuaaaally,Mr. Shen Chao is the most honest manLater,Mr. Li ming soid his hot body.# aaa*意思是最少包含2个a或无数个a[root@shell datas]# grep &quot;aaa*&quot; test.txtBut since Mr. shen Chao came, he never saaaid those words. 5555nice!because,actuaaaally,grep &quot;a&quot; count4.sh   也行
练习 . 号   正则表达式“.”只能匹配一个字符，这个字符可以是任意字符
[root@shell datas]# grep &quot;s..d&quot; test.txtMr. Li Ming said:Later,Mr. Li ming soid his hot body.
[root@shell datas]# grep “s.*d” test.txt
Mr. Li Ming said:
But since Mr. shen Chao came, he never saaaid those words. 5555nice!
Later,Mr. Li ming soid his hot body.
“[]”会匹配中括号中指定任意一个字符，注意只能匹配一个字符。比如[ao]要不会匹配一个 a
字符，要不会匹配一个 o 字符：
[root@shell datas]# grep &quot;s[ao]id&quot; test.txtMr. Li Ming said:Later,Mr. Li ming soid his hot body.[root@shell datas]# grep &quot;[0-9]&quot; test.txt123despise him.But since Mr. shen Chao came, he never saaaid those words. 5555nice!
扩展正则



扩展元字符
作	用




+
前一个字符匹配 1 次或任意多次。如“go+gle”会匹配“gogle”、“google”或“gooogle”，当然如果“o”有更多个，也能匹配。


？
前一个字符匹配 0 次或 1 次。如“colou?r”可以匹配“colour”或“color”。


|
匹配两个或多个分支选择。如“was|his”会匹配既包含“was”的行，也匹配包含“his”的行。


（）
匹配其整体为一个字符，即模式单元。可以理解为由多个单个字符组成的大字符。如“(dog)+”会匹配“dog”、“dogdog”、“dogdogdog”等，因为被（）包含的字符会当成一个整体。但“hello （world|earth）”会匹配“hello world”及“hello earth”。



grep 参数列表

-i : 忽略大小写
-v : 查找不包含指定字符串的所有行（取反）
-r : 递归查找文件夹下的文件
-n : 显示匹配行所在位置（行号）
-l : 只显示包含搜索字符串的文件名，而非每个匹配行
-c : 统计符合条件的行数
-e pattern : 指定要查找的正则表达式模式
-w : 匹配整个单词，即只匹配独立的单词而非单词内的字符
-A num : 输出匹配行后 N 行内容
-B num : 输出匹配行前 N 行内容
-C[num] 或者 --context[=num]: 输出匹配行前后总共 N 行内容。
--exclude : 排除指定文件类型，多个文件类型用 “,” 隔开

[root@localhost sh]# grep -E &quot;go*gle&quot; test.txtgooglegoooooglegglegogle[root@localhost sh]# grep -E &quot;go+gle&quot; test.txtgooglegoooooglegogle[root@localhost sh]# grep -E &quot;go?gle&quot; test.txtgglegogle[root@localhost sh]# grep -E &quot;go+&quot; test.txtgooglegoooooglegogle[root@localhost sh]# grep -E &quot;g(oo)+&quot; test.txtgooglegooooogle[root@localhost sh]# grep -E &quot;g(ooo)+&quot; test.txtgooooogle[root@localhost sh]# grep -E &quot;hello (was|is)&quot; test.txthello ishello was
匹配邮箱
grep -E &quot;[0-9a-zA-Z_]+@[0-9a-zA-Z_]+(\.[0-9a-zA-Z_]+)&#123;1,3&#125;&quot; test.txt
字符截取和替换命令
[root@localhost ~]# cut [选项] 文件名选项：-f 列号：          提取第几列-d 分隔符：	     按照指定分隔符分割列-c 字符范围：    不依赖分隔符来区分列，而是通过字符范围（行首为 0）来进行字段提取。“n-”表示从第 n 个字符到行尾；“n-m”从第 n 个字符到第 m个字符；“-m”表示从第 1 个字符到第 m 个字符。
测试文件
[root@localhost ~]# cat student.txtID      Name    gender  Mark1       Liming  M       862       Sc      M       903       Tg      M       83用tab键隔开所有列，不要空格
测试
[root@localhost ~]# cut -f 2 student.txtNameLimingScTg#提取多列[root@localhost ~]# cut -f 2,3 student.txtName    genderLiming  MSc        MTg       M##cut 可以按照字符进行提取，需要注意“8-”代表的是提取所有行的第八个字符开始到行尾，而 “10-20”代表提取所有行的第十个字符到第二十个字符，而“-8”代表提取所有行从行首到第八个字符：[root@localhost ~]# cut -c 9- student.txtgender Mark M 8603[root@localhost ~]# cut -c -9 student.txtID Name g1 Liming2 Sc M 903 Tg M 83
awk编程
printf 格式化输出
[root@localhost ~]# printf ‘输出类型输出格式’ 输出内容输出类型：%ns：	输出字符串。n 是数字指代输出几个字符%ni：	输出整数。n 是数字指代输出几个数字%m.nf：	输出浮点数。m 和 n 是数字，指代输出的整数位数和小数位数。如%8.2f代表  共输出 8 位数，其中 2 位是小数，6 位是整数。输出格式：\a:	输出警告声音\b:	输出退格键，也就是 Backspace 键\f:	清除屏幕\n:	换行\r:	回车，也就是 Enter 键\t:	水平输出退格键，也就是 Tab 键\v:	垂直输出退格键，也就是 Tab 键
建立测试文件
vi student.txtID       Name    PHP     Linux   MySQL   Average1        Liming   82      95      86      87.662        Sc        74      96      87      85.663        Tg        99      83      93      91.66
不指定格式输出
[root@localhost ~]# printf &#x27;%s&#x27; $(cat student.txt)IDNamePHPLinuxMySQLAverage1Liming82958687.662Sc74968785.663Tg99839391.66#乱作一锅粥
指定格式输出
[root@localhost ~]# printf &#x27;%s\t %s\t %s\t %s\t %s\t %s\t \n&#x27; $(cat student.txt)ID       Name    PHP     Linux   MySQL   Average1        Liming  82      95      86      87.662        Sc      74      96      87      85.663        Tg      99      83      93      91.66
awk条件
[root@localhost ~]# awk &#x27;条件 1&#123;动作 1&#125; 条件 2&#123;动作 2&#125;…&#x27; 文件名
awk条件（Pattern）
一般使用关系表达式作为条件。这些关系表达式非常多，具体参考表 12-3 所示，例如：
x &gt; 10 判断变量 x 是否大于 10
x == y 判断变量 x 是否等于变量 y
A ~ B	判断字符串 A 中是否包含能匹配 B 表达式的子字符串
A !~ B 判断字符串 A 中是否不包含能匹配 B 表达式的子字符串
动作（Action）：
格式化输出 流程控制语句

awk内置变量

例子：
[root@localhost ~]# awk &#x27;&#123;printf $2 &quot;\t&quot; $6 &quot;\n&quot;&#125;&#x27; student.txt#输出第二列和第六列
假设我要提取根分区/dev/sda1 第五列的使用率[root@localhost ~]# df -hFilesystem               Size  Used Avail Use% Mounted ondevtmpfs                 475M     0  475M   0% /devtmpfs                    487M     0  487M   0% /dev/shmtmpfs                    487M  7.7M  479M   2% /runtmpfs                    487M     0  487M   0% /sys/fs/cgroup/dev/mapper/centos-root   50G  1.5G   49G   3% //dev/mapper/centos-home   47G   33M   47G   1% /home/dev/sda1               1014M  141M  874M  14% /boottmpfs                     98M     0   98M   0% /run/user/0[root@localhost ~]# df -h | grep &quot;/dev/sda1&quot;| awk &#x27;&#123;print $5&#125;&#x27;| cut -d &quot;%&quot; -f 114
Begin
[root@localhost ~]# awk &#x27;BEGIN&#123;printf &quot;11111111\n&quot; &#125; &#123;printf $2 &quot;\t&quot; $6 &quot;\n&quot;&#125;&#x27; student.txt11111111Name    AverageLiming  87.66Sc      85.66Tg      91.66
End
[root@localhost ~]# awk &#x27;END&#123;printf &quot;111111111\n&quot; &#125; &#123;printf $2 &quot;\t&quot; $6 &quot;\n&quot;&#125;&#x27; student.txtName    AverageLiming  87.66Sc      85.66Tg      91.66111111111
假设我想看看平均成绩大于等于 87 分的学员是谁
[root@localhost ~]# cat student.txtID      Name    PHP     Linux   MySQL   Average1       Liming  82      95      86      87.662       Sc      74      96      87      85.663       Tg      99      83      93      91.66[root@localhost ~]# cat student.txt | grep -v &quot;Name&quot; | awk &#x27;$6 &gt;= 87&#123;print $2&#125;&#x27;LimingTg

加入了条件之后，只有条件成立动作才会执行，如果条件不满足，则动作则不运行。通过这个实验，大家可以发现，虽然 awk 是列提取命令，但是也要按行来读入的。这个命令的执行过程是这样的：
1） 如果有 BEGIN 条件，则先执行 BEGIN 定义的动作
2） 如果没有 BEGIN 条件，则读入第一行，把第一行的数据依次赋予$0、$1、$2 等变量。其中$0代表此行的整体数据，$1 代表第一字段，$2 代表第二字段。
3） 依据条件类型判断动作是否执行。如果条件符合，则执行动作，否则读入下一行数据。如果没有条件，则每行都执行动作。
4） 读入下一行数据，重复执行以上步骤。

例子 2：[root@localhost ~]# awk &#x27;$2 ~ /Sc/ &#123;printf $6 &quot;\n&quot;&#125;&#x27; student.txt#如果第二字段中输入包含有“Sc”字符，则打印第六字段数据85.66
这里要注意在 awk 中，使用“//”包含的字符串，awk 命令才会查找。也就是说字符串必须用“//”包含，awk 命令才能正确识别。
如果要想让 awk 识别字符串，必须使用“//”包含，例如：
[root@localhost ~]# awk &#x27;/Liming/ &#123;print&#125;&#x27; student.txt1       Liming  82      95      86      87.66#打印 Liming 的成绩
当使用 df 命令查看分区使用情况是，如果我只想查看真正的系统分区的使用状况，而不想查看光盘和临时分区的使用状况，则可以：
[root@localhost ~]# df -h | awk &#x27;/sda[0-9]/ &#123;printf $1 &quot;\t&quot; $5 &quot;\n&quot;&#125; &#x27;/dev/sda1       14%#查询包含有 sda 数字的行，并打印第一字段和第五字段
查看/etc/passwd文件
[root@localhost ~]# useradd user1[root@localhost ~]# useradd user2[root@localhost ~]# cat /etc/passwd | grep &quot;/bin/bash&quot;root:x:0:0:root:/root:/bin/bashuser1:x:1000:1000::/home/user1:/bin/bashuser2:x:1001:1001::/home/user2:/bin/bash[root@localhost ~]# cat /etc/passwd | grep &quot;/bin/bash&quot; |  awk &#x27;&#123;FS=&quot;:&quot;&#125; &#123;print $1&#125;&#x27;root:x:0:0:root:/root:/bin/bashuser1user2#第一行root好像没有以冒号为分割读取第一列是因为awk 先把第一行数据读取了才进行&#123;FS=&quot;:&quot;&#125;直到上一步之后，才发现要以冒号作为分隔符后面的user1 user2 才正常###所以正确写法是BEGIN &#123;FS=&quot;:&quot;&#125;  在读取数据之前，先执行&#123;FS=&quot;:&quot;&#125;[root@localhost ~]# cat /etc/passwd | grep &quot;/bin/bash&quot; |  awk &#x27;BEGIN &#123;FS=&quot;:&quot;&#125; &#123;print $1&#125;&#x27;rootuser1user2
[root@localhost ~]# cat /etc/passwd | grep &quot;/bin/bash&quot; | awk &#x27;BEGIN&#123;FS=&quot;:&quot;&#125; $3==&quot;1000&quot; &#123;print $1&#125;&#x27;user1判断是否相等 要用双＝号
[root@localhost ~]# cat /etc/passwd | grep &quot;/bin/bash&quot; | \&gt; awk &#x27;BEGIN &#123;FS=&quot;:&quot;&#125; &#123;printf $1 &quot;\t&quot; $3 &quot;\t 行号：&quot; NR &quot;\t 字段数：&quot; NF &quot;\n&quot;&#125;&#x27;root    0        行号：1         字段数：7user1   1000     行号：2         字段数：7user2   1001     行号：3         字段数：7#如果我只想看sshd[root@localhost ~]# cat /etc/passwd | awk &#x27;BEGIN &#123;FS=&quot;:&quot;&#125; $1==&quot;sshd&quot; &#123;printf $1 &quot;\t&quot; $3 &quot;\t 行号：&quot;NR &quot;\t 字段数：&quot;NF &quot;\n&quot;&#125;&#x27;sshd    74       行号：20        字段数：7
Sed
sed命令
[root@localhost ~]# sed [选项] ‘[动作]’ 文件名选项：-n：  一般 sed 命令会把所有数据都输出到屏幕，如果加入此选择，则只会把经过 sed 命令处理		的行输出到屏幕。-e：	允许对输入数据应用多条 sed 命令编辑。-f     脚本文件名： 从 sed 脚本中读入 sed 操作。和 awk 命令的-f 非常类似。-r：	在 sed 中支持扩展正则表达式。-i：	用 sed 的修改结果直接修改读取数据的文件，而不是由屏幕输出动作：a \： 追加，在当前行后添加一行或多行。添加多行时，除最后 一行外，每行末尾需要用“\”代表数据未完结。c \： 行替换，用 c 后面的字符串替换原数据行，替换多行时，除最后一行外，每行末尾需用“\”代表数据未完结。i \：插入，在当期行前插入一行或多行。插入多行时，除最后 一行外，每行末尾需要用“\”代表数据未完结。d：删除，删除指定的行。
打印p
[root@localhost ~]# sed 2p student.txtID      Name    PHP     Linux   MySQL   Average1       Liming  82      95      86      87.661       Liming  82      95      86      87.662       Sc      74      96      87      85.663       Tg      99      83      93      91.66#你会发现第二行多打印了一遍，只有加-n才不会[root@localhost ~]# sed -n &#x27;2p&#x27; student.txt1       Liming  82      95      86      87.66
删除d（删除2到4行）
此操作并不会写入文件中，只是输出的时候删除了
[root@localhost ~]# sed &#x27;2,4d&#x27; student.txtID      Name    PHP     Linux   MySQL   Average[root@localhost ~]# cat student.txtID      Name    PHP     Linux   MySQL   Average1       Liming  82      95      86      87.662       Sc      74      96      87      85.663       Tg      99      83      93      91.66若想直接修改读取数据到文件的话加入 -i[root@localhost ~]# sed -i &#x27;2,4d&#x27; student.txt[root@localhost ~]# cat student.txtID      Name    PHP     Linux   MySQL   Average
前追加i
#在第二行前面插入内容[root@localhost ~]# sed &#x27;2i 前面新内容1  \&gt; 前面新内容（大量） 2222222222222222222 &#x27; student.txtID      Name    PHP     Linux   MySQL   Average前面新内容1前面新内容（大量） 22222222222222222221       Liming  82      95      86      87.662       Sc      74      96      87      85.663       Tg      99      83      93      91.66
后追加a
在第二行后追加内容[root@localhost ~]# sed -i &#x27;2a 新内容&#x27; student.txt[root@localhost ~]# cat student.txtID      Name    PHP     Linux   MySQL   Average1       Liming  82      95      86      87.66新内容2       Sc      74      96      87      85.663       Tg      99      83      93      91.66
替换c
#把2行新内容替换为其他数据[root@localhost ~]# cat student.txtID      Name    PHP     Linux   MySQL   Average1       Liming  82      95      86      87.662       Sc      74      96      87      85.663       Tg      99      83      93      91.66[root@localhost ~]# sed &#x27;2c 0 xiaoshan 83 55 11 44.2&#x27; student.txtID      Name    PHP     Linux   MySQL   Average0 xiaoshan 83 55 11 44.22       Sc      74      96      87      85.663       Tg      99      83      93      91.66#我前面是空格，若对齐用\t且\t后没有空格[root@localhost ~]# sed &#x27;2c 0 \txiaos\t83 \t55 \t11 \t44.2&#x27; student.txtID      Name    PHP     Linux   MySQL   Average0       xiaos   83      55      11      44.22       Sc      74      96      87      85.663       Tg      99      83      93      91.66
多命令执行e
[root@localhost ~]# cat student.txtID      Name    PHP     Linux   MySQL   Average前面新内容1前面新内容11       Liming  82      95      86      87.662       Sc      74      96      87      85.663       Tg      99      83      93      91.66##同时删除第二和第三行[root@localhost ~]# sed  -i -e &#x27;2d;3d&#x27; student.txt[root@localhost ~]# cat student.txtID      Name    PHP     Linux   MySQL   Average1       Liming  82      95      86      87.662       Sc      74      96      87      85.663       Tg      99      83      93      91.66
替换字符串 s/旧内容/新内容/g
#替换Liming为Lm[root@localhost ~]# sed -i &#x27;s/Liming/Lm/g&#x27; student.txt[root@localhost ~]# cat student.txtID      Name    PHP     Linux   MySQL   Average1       Lm      82      95      86      87.662       Sc      74      96      87      85.663       Tg      99      83      93      91.66#多命令执行，分号隔开[root@localhost ~]# sed -i &#x27;s/Sc/sc1/g; s/Tg/Eg/g&#x27; student.txt[root@localhost ~]# cat student.txtID      Name    PHP     Linux   MySQL   Average1       Lm      82      95      86      87.662       sc1     74      96      87      85.663       Eg      99      83      93      91.66#也可以多行执行[root@localhost ~]# sed -i &#x27;s/Sc/sc1/gs/Tg/Eg/g&#x27; student.txt#替换字符串为空值[root@localhost ~]# sed -i &#x27;s/Lm//g&#x27; student.txt[root@localhost ~]# cat student.txtID      Name    PHP     Linux   MySQL   Average1                 82      95      86      87.662       sc1     74      96      87      85.663       Eg      99      83      93      91.66
字符串处理命令
sort排序命令
[root@localhost ~]# sort [选项] 文件名选项：-f：	忽略大小写-b：	忽略每行前面的空白部分-n：	以数值型进行排序，默认使用字符串型排序-r：	反向排序-u：	删除重复行。就是 uniq 命令-t：	指定分隔符，默认是分隔符是制表符-k n[,m]： 按照指定的字段范围排序。从第 n 字段开始，m 字段结束（默认到行尾）
#默认按开头字符排序[root@localhost ~]# sort /etc/passwdabrt:x:173:173::/etc/abrt:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinbin:x:1:1:bin:/bin:/sbin/nologinchrony:x:997:995::/var/lib/chrony:/sbin/nologin#反向排序[root@localhost ~]# sort -r /etc/passwduser2:x:1001:1001::/home/user2:/bin/bashuser1:x:1000:1000::/home/user1:/bin/bashtcpdump:x:72:72::/:/sbin/nologinsystemd-network:x:192:192:systemd Network Management:/:/sbin/nologinsync:x:5:0:sync:/sbin:/bin/sync
指定字段排序
#如UID[root@localhost ~]# sort -t &quot;:&quot; -k 3,3 /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologinuser1:x:1000:1000::/home/user1:/bin/bashuser2:x:1001:1001::/home/user2:/bin/bashoperator:x:11:0:operator:/root:/sbin/nologingames:x:12:100:games:/usr/games:/sbin/nologinftp:x:14:50:FTP User:/var/ftp:/sbin/nologinabrt:x:173:173::/etc/abrt:/sbin/nologinsystemd-network:x:192:192:systemd Network Management:/:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologin#你会发现他先排第一位数字1，再排2。、#就是1  12  13  到  2  22 23 并没有123456排下去#所以要加上 -n    -n：以数值型进行排序，默认使用字符串型排序[root@localhost ~]# sort -nt &quot;:&quot; -k 3,3 /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologinsync:x:5:0:sync:/sbin:/bin/syncshutdown:x:6:0:shutdown:/sbin:/sbin/shutdownhalt:x:7:0:halt:/sbin:/sbin/haltmail:x:8:12:mail:/var/spool/mail:/sbin/nologinoperator:x:11:0:operator:/root:/sbin/nologingames:x:12:100:games:/usr/games:/sbin/nologin
uniq取消重复行
uniq 命令是用来取消重复行的命令，其实和“sort -u”选项是一样的。命令格式如下
[root@localhost ~]# uniq [选项] 文件名选项：-i：	忽略大小写
[root@localhost ~]# cat student.txtID      Name    PHP     Linux   MySQL   Average1               82      95      86      87.661               82      95      86      87.662       sc1     74      96      87      85.663       Eg      99      83      93      91.66[root@localhost ~]# uniq student.txtID      Name    PHP     Linux   MySQL   Average1               82      95      86      87.662       sc1     74      96      87      85.663       Eg      99      83      93      91.66
wc统计命令
[root@localhost ~]# wc [选项] 文件名选项：-l：	只统计行数-w：	只统计单词数-m：	只统计字符数
[root@localhost ~]# cat student.txtID      Name    PHP     Linux   MySQL   Average1               82      95      86      87.661               82      95      86      87.662       sc1     74      96      87      85.663       Eg      99      83      93      91.66[root@localhost ~]# wc -l student.txt5 student.txt[root@localhost ~]# wc -w student.txt28 student.txt[root@localhost ~]# wc -m student.txt109 student.txt
条件判断（test命令）

按照文件类型进行判断(test)




测试选项
作	用




-b 文件
判断该文件是否存在，并且是否为块设备文件（是块设备文件为真）


-c 文件
判断该文件是否存在，并且是否为字符设备文件（是字符设备文件为真）


-d 文件
判断该文件是否存在，并且是否为目录文件（是目录为真）


-e 文件
判断该文件是否存在（存在为真）


-f 文件
判断该文件是否存在，并且是否为普通文件（是普通文件为真）


-L 文件
判断该文件是否存在，并且是否为符号链接文件（是符号链接文件为真）


-p 文件
判断该文件是否存在，并且是否为管道文件（是管道文件为真）


-s 文件
判断该文件是否存在，并且是否为非空（非空为真）


-S 文件
判断该文件是否存在，并且是否为套接字文件（是套接字文件为真）



[root@localhost ~]# lsanaconda-ks.cfg  file.txt  file.txtn  netstart.bak  student.txt第一种格式：#-e 判断该文件是否存在（存在为真）[root@localhost ~]# test -e file.txt#输出为0即为真[root@localhost ~]# echo $?0第一种格式：#-e 判断不存在的文件[root@localhost ~]# [ -e abc ][root@localhost ~]# echo $?1#输出为非0即为假
###判断该文件是否存在，并且是否为目录文件[root@localhost ~]# [ -d student.txt ] &amp;&amp; echo yes || echo nono###判断该文件是否存在，并且是否为普通文件[root@localhost ~]# [ -f student.txt ] &amp;&amp; echo yes || echo noyes
###判断文件是否有数据[root@localhost ~]# [ -s abc ] &amp;&amp; echo yes || echo nono[root@localhost ~]# echo 111 &gt;&gt; abc[root@localhost ~]# [ -s abc ] &amp;&amp; echo yes || echo noyes

按照文件权限判断




测试选项
作    用




-r 文件
判断该文件是否存在，并且是否该文件拥有读权限（有读权限为真）


-w 文件
判断该文件是否存在，并且是否该文件拥有写权限（有写权限为真）


-x 文件
判断该文件是否存在，并且是否该文件拥有执行权限（有执行权限为真）


-u 文件
判断该文件是否存在，并且是否该文件拥有  SUID 权限（有 SUID 权限为真）


-g 文件
判断该文件是否存在，并且是否该文件拥有  SGID 权限（有 SGID 权限为真）


-k 文件
判断该文件是否存在，并且是否该文件拥有  SBit 权限（有 SBit 权限为真）



##判断文件是否有写的权限[root@localhost ~]# [ -w abc ] &amp;&amp; echo yes || echo noyes[root@localhost ~]# [ -u abc ] &amp;&amp; echo yes || echo nono##判断文件是否有SUID的权限[root@localhost ~]# chmod u+s abc[root@localhost ~]# [ -u abc ] &amp;&amp; echo yes || echo noyes[root@localhost ~]# lltotal 24-rwSr--r--  1 root root    4 May  2 03:02 abc

两个文件之间进行比较




测试选项
作    用




文件 1 -nt 文件 2
判断文件 1 的修改时间是否比文件 2 的新（如果新则为真）


文件 1 -ot 文件 2
判断文件 1 的修改时间是否比文件 2 的旧（如果旧则为真）


文件 1 -ef 文件 2
判断文件 1 是否和文件 2 的 Inode 号一致，可以理解为两个文件是否  为同一个文件。这个判断用于判断硬链接是很好的方法



##判断两个文件是否是硬链接[root@localhost ~]# ln /root/abc /tmp/abc[root@localhost ~]# ll /tmp/abc-rw-r--r-- 2 root root 4 May  2 03:02 /tmp/abc[root@localhost ~]# [ /root/abc -ef /tmp/abc ] &amp;&amp; echo yes || echo noyes

两个整数之间比较




测试选项
作    用




整数 1 -eq 整数 2
判断整数 1 是否和整数 2 相等（相等为真）


整数 1 -ne 整数 2
判断整数 1 是否和整数 2 不相等（不相等位置）


整数 1 -gt 整数 2
判断整数 1 是否大于整数 2（大于为真）


整数 1 -lt 整数 2
判断整数 1 是否小于整数 2（小于位置）


整数 1 -ge 整数 2
判断整数 1 是否大于等于整数 2（大于等于为真）


整数 1 -le 整数 2
判断整数 1 是否小于等于整数 2（小于等于为真）



-eq: equal : 相等               -ne: not equal : 不相等
-gt: greater than : 大于       -lt: less than : 小于
##判断整数1是否等于整数2[root@localhost ~]# [ 22 -eq 22 ] &amp;&amp; echo yes || echo noyes##判断整数1是否小于整数2[root@localhost ~]# [ 22 -lt 22 ] &amp;&amp; echo yes || echo nono[root@localhost ~]# [ 21 -lt 22 ] &amp;&amp; echo yes || echo noyes

字符串判断




测试选项
作    用




-z 字符串
判断字符串是否为空（为空返回真）


-n 字符串
判断字符串是否为非空（非空返回真）


字串 1 ==字串 2
判断字符串  1 是否和字符串 2 相等（相等返回真）


字串 1 != 字串 2
判断字符串  1 是否和字符串 2 不相等（不相等返回真）



##判断两个字符串相等[root@localhost ~]# aa=11[root@localhost ~]# bb=22[root@localhost ~]# [ &quot;$aa&quot; == 8 ] &amp;&amp; echo yes || echo nono[root@localhost ~]# [ $aa == $bb ] &amp;&amp; echo yes || echo nono[root@localhost ~]# [ &quot;$aa&quot; == 11 ] &amp;&amp; echo yes || echo noyes##判断字符是否为空[root@localhost ~]# name=xx[root@localhost ~]# [ -z $name ] &amp;&amp; echo yes || echo nono

多重条件判断




测试选项
作    用




判断 1 -a 判断 2
逻辑与，判断  1 和判断 2 都成立，最终的结果才为真


判断 1 -o 判断 2
逻辑或，判断  1 和判断 2 有一个成立，最终的结果就为真


！判断
逻辑非，使原始的判断式取反



#先给aa赋值[root@localhost ~]# aa=24#-n先判断是否为空，明显不为空则为真，真就继续判断是否大于23[root@localhost ~]# [ -n &quot;$aa&quot; -a &quot;$aa&quot; -gt 23 ] &amp;&amp; echo yes || echo noyes
##逻辑非[root@localhost ~]# [ ! -n &quot;$aa&quot; ] &amp;&amp; echo &quot;yes&quot; || echo &quot;no&quot;no#本来“-n”选项是变量 aa 不为空，返回值就是真。#加入！之后，判断值就会取反，所以当变量 aa 有值时，返回值是假
流程控制
if条件判断

单分支 if 条件语句

单分支条件语句最为简单，就是只有一个判断条件，如果符合条件则执行某个程序，否则什么事情都不做。语法如下：
if [ 条件判断式 ];then     程序fi
单分支条件语句需要注意几个点：
·if 语句使用 fi 结尾，和一般语言使用大括号结尾不同
· [ 条件判断式 ]就是使用 test 命令判断，所以中括号和条件判断式之间必须有空格
· then 后面跟符合条件之后执行的程序，可以放在[]之后，用“；”分割。也可以换行写入，就不需要“；”了，比如单分支 if 语句还可以这样写：
if [ 条件判断式 ]   then     程序fi
例子：判断sda1并设置警告信息
[root@localhost ~]# df -hFilesystem               Size  Used Avail Use% Mounted on/dev/sda1               1014M  141M  874M  14% /boot[root@localhost ~]# df -h | grep &quot;/dev/sda1&quot; | awk &#x27;&#123;print $5&#125;&#x27; | cut -d &quot;%&quot; -f 114[root@localhost ~]# vi if1.sh#!/bin/bash#统计分区使用率rate=$(df -h | grep &quot;/dev/sda1&quot; | awk &#x27;&#123;print $5&#125;&#x27;| cut -d &quot;%&quot; -f 1)#把根分区使用率作为变量值赋予变量rateif [ $rate -ge 14 ]#判断 rate 的值如果大于等于 14，则执行then 程序thenecho &quot;Warning! /dev/sda1 快满了!&quot;#打印警告信息。在实际工作中，也可以向管理员发送邮件。fi[root@localhost ~]# sh if1.shWarning! /dev/sda1 快满了!

双分支if语句

if [ 条件判断式 ]       then		条件成立时，执行的程序	else		条件不成立时，执行的另一个程序fi
数据备份的例子
#备份mysql数据库（并不完善，但可以使用）[root@localhost ~]# vi sh/bakmysql.sh #!/bin/bash#备份 mysql 数据库。#同步系统时间ntpdate asia.pool.ntp.org &amp;&gt;/dev/null#把当前系统时间按照“年月日”格式赋予变量datedate=$(date +%y%m%d)#统计 mysql 数据库的大小，并把大小赋予size 变量size=$(du -sh /var/lib/mysql)if [ -d /tmp/dbbak ]    #判断备份目录是否存在，是否为目录  then    #如果判断为真，执行以下脚本	echo &quot;Date : $date!&quot; &gt; /tmp/dbbak/dbinfo.txt	#把当前日期写入临时文件	echo &quot;Data size : $size&quot; &gt;&gt; /tmp/dbbak/dbinfo.txt	#把数据库大小写入临时文件	cd /tmp/dbbak	#进入备份目录	tar -zcf mysql-lib-$date.tar.gz /var/lib/mysql dbinfo.txt &amp;&gt;/dev/null	#打包压缩数据库与临时文件，把所有输出丢入垃圾箱（不想看到任何输出）	rm -rf /tmp/dbbak/dbinfo.txt	#删除临时文件  else	mkdir /tmp/dbbak	#如果判断为假，则建立备份目录	echo &quot;Date : $date!&quot; &gt; /tmp/dbbak/dbinfo.txt echo &quot;Data size : $size&quot; &gt;&gt; /tmp/dbbak/dbinfo.txt #把日期和数据库大小保存如临时文件	cd /tmp/dbbak	tar -zcf mysql-lib-$date.tar.gz dbinfo.txt /var/lib/mysql &amp;&gt;/dev/null	#压缩备份数据库与临时文件	rm -rf /tmp/dbbak/dbinfo.txt	#删除临时文件fi
常用字符表
echo
在 echo 命令中如果使用了“-e”选项，则可以支持控制字符，如表 11-2 所示：



控制字符
作	用




\
输出\本身


\a
输出警告音


\b
退格键，也就是向左删除键


\c
取消输出行末的换行符。和“-n”选项一致


\e
ESCAPE 键


\f
换页符


\n
换行符


\r
回车键


\t
制表符，也就是 Tab 键


\v
垂直制表符


\0nnn
按照八进制 ASCII 码表输出字符。其中 0 为数字零，nnn 是三位八进制数


\xhh
按照十六进制 ASCII 码表输出字符。其中 hh 是两位十六进制数



Bash常用快捷键



快捷键
作	用




ctrl+A
把光标移动到命令行开头。如果我们输入的命令过长，想要把光标移动到命令行开头时使用。


ctrl+E
把光标移动到命令行结尾。


ctrl+C
强制终止当前的命令。


ctrl+L
清屏，相当于 clear 命令。


ctrl+U
删除或剪切光标之前的命令。我输入了一行很长的命令，不用使用退格键一个一个字符的删除，使用这个快捷键会更加方便


ctrl+K
删除或剪切光标之后的内容。


ctrl+Y
粘贴 ctrl+U 或 ctrl+K 剪切的内容。


ctrl+R
在历史命令中搜索，按下ctrl+R 之后，就会出现搜索界面，只要输入搜索内容，就会从历史命令中搜索。






ctrl+D
退出当前终端。




ctrl+Z
暂停，并放入后台。这个快捷键牵扯工作管理的内容，我们在系统管理章节详细介绍。


ctrl+S
暂停屏幕输出。


ctrl+Q
恢复屏幕输出。



基础正则表达式



元字符
作	用




*
前一个字符匹配 0 次或任意多次。


.
匹配除了换行符外任意一个字符。


^
匹配行首。例如：^hello 会匹配以 hello 开头的行。


$
匹配行尾。例如：hello&amp;会匹配以 hello 结尾的行。


[]
匹配中括号中指定的任意一个字符，只匹配一个字符。例如：[aoeiu] 匹配任意一个元音字母，[0-9] 匹配任意一位数字，[a-z][0-9]匹配小写字和一位数字构成的两位字符。


[^]
匹配除中括号的字符以外的任意一个字符。例如：[^0-9] 匹配任意一位非数字字符，[^a-z] 表示任意一位非小写字母。


\
转义符。用于取消讲特殊符号的含义取消。


{n}
表示其前面的字符恰好出现 n 次。例如：[0-9]{4} 匹配 4 位数字，[1][3-8][0-9]{9} 匹配手机号码。


{n,}
表示其前面的字符出现不小于 n 次。例如： [0-9]{2,} 表示两位及以上的数字。


{n,m}
表示其前面的字符至少出现n 次，最多出现m 次。例如：[a-z]{6,8}匹配 6 到 8 位的小写字母。



扩展正则



扩展元字符
作	用




+
前一个字符匹配 1 次或任意多次。如“go+gle”会匹配“gogle”、“google”或“gooogle”，当然如果“o”有更多个，也能匹配。


？
前一个字符匹配 0 次或 1 次。如“colou?r”可以匹配“colour”或“color”。


|
匹配两个或多个分支选择。如“was|his”会匹配既包含“was”的行，也匹配包含“his”的行。


（）
匹配其整体为一个字符，即模式单元。可以理解为由多个单个字符组成的大字符。如“(dog)+”会匹配“dog”、“dogdog”、“dogdogdog”等，因为被（）包含的字符会当成一个整体。但“hello （world|earth）”会匹配“hello world”及“hello earth”。



grep 参数列表

-i : 忽略大小写
-v : 查找不包含指定字符串的所有行（取反）
-r : 递归查找文件夹下的文件
-n : 显示匹配行所在位置（行号）
-l : 只显示包含搜索字符串的文件名，而非每个匹配行
-c : 统计符合条件的行数
-e pattern : 指定要查找的正则表达式模式
-w : 匹配整个单词，即只匹配独立的单词而非单词内的字符
-A num : 输出匹配行后 N 行内容
-B num : 输出匹配行前 N 行内容
-C[num] 或者 --context[=num]: 输出匹配行前后总共 N 行内容。
--exclude : 排除指定文件类型，多个文件类型用 “,” 隔开

cut
[root@localhost ~]# cut [选项] 文件名选项：-f 列号：          提取第几列-d 分隔符：	     按照指定分隔符分割列-c 字符范围：    不依赖分隔符来区分列，而是通过字符范围（行首为 0）来进行字段提取。“n-”表示从第 n 个字符到行尾；“n-m”从第 n 个字符到第 m个字符；“-m”表示从第 1 个字符到第 m 个字符。
awk条件（Pattern）

awk内置变量


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>ZeroTier免费远控工具</title>
    <url>/posts/a735afbd/</url>
    <content><![CDATA[
ZeroTier免费远控工具
ZeroTier，分分钟异地实现组网的远控工具，这个工具有免费计划的
免费计划里有3个网络和10个设备。
例子：在ZeroTier网页后台添加一个网络，就是在两个设备下载ZeroTier客户端，两个设备都加入到网络，就形成了一个局域网，最后在网页后台进行设备加入的授权加入，就会形成一个ip，只需要输入这个ip就可以实现远控了。
这个网络，相当于一个巨型的局域网，在免费计划里可以加入10个设备。

注册ZeroTier
在Sign in to ZeroTier注册一个账号，有很多方式，自己选一个就行了

创建网络
点击创建网络，下面就会有一个网络id，这个id是所有设备加入网络需要的输入的id

各个设备下载客户端
在你需要远控的各个设备，下载客户端Download - ZeroTier
你的设备是什么系统，你就下载对应系统的软件

我这里双方都是windows，linux有教程，你可以自己看看
windows安装之后是没有桌面图标的，你要在开始菜单里才能看见
        
打开之后再小托盘里就可以看见了
客户端加入网络
复制这个网络id，然后在小托盘右键ZeroTier，点击Join New Network输入网络id就可以成功加入网络，各个需要远控的设备都要安装ZeroTier和输入网络id，才能成功加入这个网络



授权设备

进行远控
按win+r输入mstsc

然后输入你要控制对方的托管ip即可远控

登入一下就可以了

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>组网</category>
      </categories>
  </entry>
  <entry>
    <title>基于Docker部署Hexo框架</title>
    <url>/posts/25850/</url>
    <content><![CDATA[基于Docker部署Hexo框架
创建dockerfile文件夹
mkdir dockerfile
使用vim创建和编辑Dockerfile
vim dockerfile/Dockerfile
# 使用最新的node镜像作为基础环境  FROM node:latest  # 设置临时工作目录  WORKDIR /usr/blog# 配置 npm 镜像站点  RUN npm config set registry https://registry.npm.taobao.org  # 安装 hexo-cli  RUN npm install hexo-cli -g  # 初始化 hexo blog  RUN hexo init  # hexo 默认端口号 4000  EXPOSE 4000
构建镜像
docker build -t hexo-image /root/dockerfile/
创建容器
docker run -itd -v /root/blog:/usr/blog -p 4000:4000 --name hexo-blog hexo-image
-v /root/blog:/usr/blog  可以实现把容器中的/usr/blog挂载到宿主机的/root/blog，非常方便
docker update hexo-blog --restart=always #增加开机自启动属性
查看容器id
docker ps -a
加入容器
docker exec -it 容器id /bin/bash#也可以是名字docker exec -it hexo-blog /bin/bash
开启服务
hexo s

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Centos 8</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Mysql笔记</title>
    <url>/posts/43761/</url>
    <content><![CDATA[Mysql笔记
数据库的创建和管理
创建数据库
create database 数据库名字default character set 字符集名字Default collate 排序规则名 ;例子：create database webinfodefault character set utf8mb4Default collate utf8mb4_general_ci;
显示当前所有的数据库列表
Show databases;
指定默认的数据库列表
Use 数据库名字;
use stuinfo;
使用alter database 语句修改数据库
Alter database 数据库名字Default character set 字符集名Default collate 排序规则名字;例子：Alter database stuinfoDefault character set utf8Default collate utf8_general_ci;
删除数据库
Drop database 数据库名字;
数据表的创建和管理
使用create table 语句创建数据表
Create table 表名 (字段名1 数据类型 \[属性\] \[索引\] ,字段名1 数据类型 \[属性\] \[索引\],.......字段名1 数据类型 \[属性\] \[索引\]);例子：Create table student (Id int unsigned not null auto_increment comment &#x27;学生ID&#x27;,sNo char(10) not null comment &#x27;学号&#x27;,sName varchar(20) not null comment &#x27;姓名&#x27;,sex char(1) not null default &#x27;男&#x27; comment &#x27;性别&#x27;,birthday Date not null comment &#x27;出生日期&#x27;,depyName varchar(30) not null comment &#x27;班级名称&#x27;,remark Varchar(80) comment &#x27;备注&#x27;,primary key (id), /    \*设置id为主键\*/unique (sNo), /        \*设置sNo为唯一性索引\*/index (sName) /         \*设置sName为普通索引\*/); ENGINE=InnoDB;
Not null 不为空 unsigned无符号 auto_increment自动增加
查看数据表
Show tables;
复制数据表
Create table 新表名 like 旧表名;
创建临时表
新建之后，当mysql关闭后会自动删除
Create temporary table student (Id int unsigned not null auto_increment comment &#x27;学生ID&#x27;,sNo char(10) not null comment &#x27;学号&#x27;,sName varchar(20) not null comment &#x27;姓名&#x27;,sex char(1) not null default &#x27;男&#x27; comment &#x27;性别&#x27;,birthday Date not null comment &#x27;出生日期&#x27;,depyName varchar(30) not null comment &#x27;班级名称&#x27;,remark Varchar(80) comment &#x27;备注&#x27;,primary key (id), /            \*设置id为主键\*/unique (sNo), /                \*设置sNo为唯一性索引\*/index (sName) /                \*设置sName为普通索引\*/); ENGINE=InnoDB;
Not null 不为空 unsigned无符号 auto_increment自动增加
查看表结构
Describe | desc 表名;
例子：
desc 表名;
查看表的结构  sql语句
Show create table 表名;
修改表结构
使用alter table 修改表结构
Alter table 表名
Add字段名 数据类型 [属性] [索引] [First | after 字段名] --添加新字段
Modify 字段名 数据类型 [属性] [索引] ---更改指定数据类型
Change 字段名 新字段名 数据类型 [属性] [索引] ---更改指定数据类型同时更改名字
Drop 字段名 ----删除指定字段
Rename as 新表名 ---用来给数据表重新命名
例子：

使用insert操作表中数据
Insert into 表名 (字段名1，字段名2，字段名3)
Values (值1, 值2, 值3 ), (值1, 值2, 值3 ), (值1, 值2, 值3 )
例子：



使用update修改表中数据
Update 表名
Set 字段名1=值1 , 字段名2=值2[,…]
[where 条件]

使用delete删除表中数据
Delete from 表名
[where 条件]
例子：
Delete from student where sNo=&#x27;1308013105&#x27;;

会删除所在字段（sNo）的那一行的数据，不是只单单删除sNo

使用truncate语句清空表中数据
Truncate [table] 表名
删除表
Use 数据库名字;Drop table 表名;
创建索引
定义
MySQL 的索引类型主要有以下几种。
• 普通索引（NDEX）：最基本的索引，它没有任何限制，是用来提升数据库性能、提高
数据查询效率的一项重要的技术。
• 唯一性索引（UNIQUE)）：索引列的值必须唯一，但允许有空值。一张表中可以有多个
唯一性索引。如果是组合索引，则列值的组合必须唯一。
• 主键索引（PRIMARY KEY)：一种特殊的唯一性索引，但不允许有空值。一张表中只能
有一个主键。为了有效实现数据的管理，每张表都应该有自己的主键，一般是在建表的
同时创建主键索引。
• 全文索引（FULLITEXT)：主要用来查找文本中的关键字，而不是直接与索引中的值相
比较。全文索引跟其他索引大不相同，它更像是一个搜索引擎，而不是简单的 WHERE
语句的参数匹配。全文索引配合 MATCH AGAINST 操作使用，而不是一般的 WHERE
语句加 LIKE。目前只有 CHAR、VARCHAR、TEXT 列上可以创建全文索引。
在create table创建索引
Create table 表名 (
字段名1 | 索引项…,
)
索引项的语法：
Primary key 索引名 (字段名)
Unique 索引名 (字段名)
Index | key 索引名 (字段名)
Fulltext 索引名 (字段名)
例子
Create table student (Id int unsigned not null auto_increment comment &#x27;学生ID&#x27;,sNo char(10) not null comment &#x27;学号&#x27;,sName varchar(20) not null comment &#x27;姓名&#x27;,sex char(1) not null default &#x27;男&#x27; comment &#x27;性别&#x27;,birthday Date not null comment &#x27;出生日期&#x27;,depyName varchar(30) not null comment &#x27;班级名称&#x27;,remark Varchar(80) comment &#x27;备注&#x27;,primary key (id), /         \*设置id为主键\*/unique (sNo), /             \*设置sNo为唯一性索引\*/index (sName) /             \*设置sName为普通索引\*/); ENGINE=InnoDB;
Not null 不为空 unsigned无符号 auto_increment自动增加
使用alter table 语句创建索引
Alter table 表名
​	Add 索引项;
例子:
Alter table course    Add unique ux_cNo(cNo),   Add index ix_cName(cName);
使用create index语句创建索引
Create [unique] | [fulltext] index 索引名
On 表名 (字段名)
例子：
在成绩表上创建唯一性索引（组合索引）
Create unique index ux_sId_cId  ON score (sId,cId);
使用Show index 语句查看索引
Show index from &lt;表名&gt; [from &lt;数据库名字&gt;]
例子：
Show index from student;
查看学生表中的索引
Select查询
应该是到这会用到sql语句，[点此下载]
选择字段进行查询
Select 字段1 [,字段2,字段3] from 表名;
例子：
Select deptname,name,sNo,sex from student;
定义字段别名
Select sNo AS &#x27;学号&#x27; , sName AS &#x27;姓名&#x27; from student;
条件查询



使用like模糊查询

使用in 进行范围查询

使用order by 子句对查询结果
降序

中文名拼音排序

先按班级升序排列，同一个班级内出生日期降序排列

使用limit子句限制返回记录的行数

使用distinct关键字过滤重复记录

内连接

使用统计函数对数据进行统计汇总


使用group by进行分组查询

使用having子句对分组汇总结果进行筛选

使用exists关键字创建子查询
查询选修课程的女生名单

薄弱盲区
复制表到新表

向表中插入子查询结果

带子查询的修改语句

带子查询的删除语句


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s部署Prometheus+Grafana</title>
    <url>/posts/25424/</url>
    <content><![CDATA[
k8s部署Prometheus+Grafana




主机名
ip1（NAT）
系统
磁盘
内存




master1
192.168.48.101
Centos7.9
100G
4G


master2
192.168.48.102
Centos7.9
100G
4G


master3
192.168.48.103
Centos7.9
100G
4G


node01
192.168.48.104
Centos7.9
100G
6G


node02
192.168.48.105
Centos7.9
100G
6G



做这个之前你要拥有一个k8s集群，参考以下教程
K8S高可用集群（内部etcd） - 严千屹博客 (qianyios.top)
K8S高可用集群（外部etcd） - 严千屹博客 (qianyios.top)
部署Prometheus
创建命名空间
操作节点[master1]
kubectl create namespace prometheus-work
部署Prometheus deploy
操作节点[master1]
cat &gt;prome_deploy.yml&lt;&lt; &quot;EOF&quot;apiVersion: apps/v1kind: Deploymentmetadata:  name: prometheus  namespace: prometheus-work  labels:    app: prometheusspec:  selector:    matchLabels:      app: prometheus  template:    metadata:      labels:        app: prometheus    spec:      securityContext:                                   #指定运行的用户为root        runAsUser: 0      serviceAccountName: prometheus      containers:      - image: prom/prometheus:v2.30.2        name: prometheus        args:        - &quot;--config.file=/etc/prometheus/prometheus.yml&quot; #通过volume挂载prometheus.yml        - &quot;--storage.tsdb.path=/prometheus&quot;              #通过vlolume挂载目录/prometheus        - &quot;--storage.tsdb.retention.time=24h&quot;        - &quot;--web.enable-admin-api&quot;                       #控制对admin HTTP API的访问        - &quot;--web.enable-lifecycle&quot;                       #支持热更新，直接执行localhost:9090/-/reload立即生效        ports:        - containerPort: 9090          name: http        volumeMounts:        - mountPath: &quot;/etc/prometheus&quot;          name: config-volume        - mountPath: &quot;/prometheus&quot;          name: data        resources:          requests:            cpu: 100m            memory: 512Mi          limits:            cpu: 100m            memory: 512Mi      volumes:      - name: data        persistentVolumeClaim:          claimName: prometheus-data  #本地存储      - name: config-volume        configMap:          name: prometheus-config     #定义的prometeus.yamlEOFkubectl apply -f prome_deploy.yml
部署Prometheus service
操作节点[master1]
cat&gt; prome_svc.yml&lt;&lt; &quot;EOF&quot;apiVersion: v1kind: Servicemetadata:  name: prometheus  namespace: prometheus-work  labels:    app: prometheusspec:  selector:    app: prometheus  type: NodePort  ports:    - name: web      port: 9090      targetPort: httpEOFkubectl apply -f prome_svc.yml
部署configmap
操作节点[master1]
cat &gt; prome_cfg.yml &lt;&lt; &quot;EOF&quot;apiVersion: v1kind: ConfigMapmetadata:  name: prometheus-config  namespace: prometheus-workdata:  prometheus.yml: |    global:      scrape_interval: 15s      scrape_timeout: 15s    scrape_configs:    - job_name: &#x27;prometheus&#x27;      static_configs:      - targets: [&#x27;localhost:9090&#x27;]EOF kubectl apply -f prome_cfg.yml
部署PV，PVC
操作节点[node01]
#在node01节点上执行mkdir /data/k8s/prometheus -p
操作节点[master1]
cat &gt; prome_pvc.yml &lt;&lt; &quot;EOF&quot;apiVersion: v1kind: PersistentVolumemetadata:  name: prometheus-local  labels:    app: prometheusspec:  accessModes:  - ReadWriteOnce  capacity:    storage: 5Gi  storageClassName: local-storage  local:    path: /data/k8s/prometheus  #在node01节点创建此目录  nodeAffinity:    required:      nodeSelectorTerms:      - matchExpressions:        - key: kubernetes.io/hostname          operator: In          values:          - node01   #指定运行在node节点  persistentVolumeReclaimPolicy: Retain---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: prometheus-data  namespace: prometheus-workspec:  selector:    matchLabels:      app: prometheus  accessModes:  - ReadWriteOnce  resources:    requests:      storage: 5Gi  storageClassName: local-storageEOFkubectl apply -f prome_pvc.yml
配置rabc
操作节点[master1]
cat &gt; prome_rabc.yml &lt;&lt; &quot;EOF&quot;apiVersion: v1kind: ServiceAccountmetadata:  name: prometheus  namespace: prometheus-work---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRole        #创建一个clusterrolemetadata:  name: prometheusrules:- apiGroups:  - &quot;&quot;  resources:  - nodes  - services  - endpoints  - pods  - nodes/proxy  verbs:  - get  - list  - watch- apiGroups:  - &quot;extensions&quot;  resources:    - ingresses  verbs:  - get  - list  - watch- apiGroups:  - &quot;&quot;  resources:  - configmaps  - nodes/metrics  verbs:  - get- nonResourceURLs:  - /metrics  verbs:  - get---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: prometheusroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: prometheussubjects:- kind: ServiceAccount  name: prometheus  namespace: prometheus-workEOFkubectl apply -f prome_rabc.yml
查看部署的Prometheus服务
操作节点[master1]
[root@master1 ~]# kubectl get pod,svc,configmap,sa -n prometheus-workNAME                             READY   STATUS    RESTARTS   AGEpod/prometheus-db4b5c549-6gb7d   1/1     Running   0          4m39sNAME                 TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGEservice/prometheus   NodePort   10.103.99.200   &lt;none&gt;        9090:30512/TCP   15m    #注意这个30512是后面要访问的端口NAME                          DATA   AGEconfigmap/kube-root-ca.crt    1      17mconfigmap/prometheus-config   1      14mNAME                        SECRETS   AGEserviceaccount/default      0         17mserviceaccount/prometheus   0         12m
在浏览器访问Prometheus
访问地址是node节点IP加上service的nodeport端口
192.168.48.104:30512

部署grafana
部署deployment
cat &gt;grafana.yml &lt;&lt;&quot;EOF&quot;kind: DeploymentapiVersion: apps/v1metadata:  labels:    app: grafana  name: grafana  namespace: prometheus-workspec:  replicas: 1  revisionHistoryLimit: 10  selector:    matchLabels:      app: grafana  template:    metadata:      labels:        app: grafana    spec:      securityContext:        runAsNonRoot: true        runAsUser: 10555        fsGroup: 10555      containers:        - name: grafana          image: grafana/grafana:8.4.4          imagePullPolicy: IfNotPresent          env:            - name: GF_AUTH_BASIC_ENABLED              value: &quot;true&quot;            - name: GF_AUTH_ANONYMOUS_ENABLED              value: &quot;false&quot;          readinessProbe:            httpGet:              path: /login              port: 3000          volumeMounts:            - mountPath: /var/lib/grafana              name: grafana-data-volume          ports:            - containerPort: 3000              protocol: TCP      volumes:        - name: grafana-data-volume          emptyDir: &#123;&#125;EOFkubectl apply -f grafana.yml
部署svc
cat &gt;grafana_svc.yml&lt;&lt;&quot;EOF&quot;kind: ServiceapiVersion: v1metadata:  labels:    app: grafana  name: grafana-service  namespace: prometheus-workspec:  ports:    - port: 3000      targetPort: 3000  selector:    app: grafana  type: NodePortEOFkubectl apply -f grafana_svc.yml
查看服务
[root@master1 ~]# kubectl get pod,svc -n prometheus-work |grep grafanapod/grafana-5d475d9d7-ctb2t      1/1     Running   0          5m18sservice/grafana-service   NodePort   10.99.157.212   &lt;none&gt;        3000:31163/TCP   5m12s#查看grafana的pod在哪个节点[root@master1 1]# kubectl describe pod -n prometheus-work grafana-5d475d9d7-ctb2t | grep Node:Node:             node02/192.168.48.105[root@master1 1]#
访问页面http://192.168.48.105:31163
首次登录grafana，用户名和密码都是admin，登陆之后会要求修改admin的密码，也可以不修改




监控开始
以下测试均已监控node01来做测试

GitHub - prometheus/node_exporter: Exporter for machine metrics
监控linux服务器
主要监控指标：

CPU
内存
硬盘
网络流量
系统负载
系统服务

安装采集器
从上图得知监控linux需要使用Node exporter采集器
下载Node exporter采集器

操作节点[node01]
wget -P /usr/local/ https://github.com/prometheus/node_exporter/releases/download/v1.7.0/node_exporter-1.7.0.linux-amd64.tar.gz#也可以自己先下载上传到/usr/local/ 下
cd /usr/local/tar -xvf node_exporter-1.7.0.linux-amd64.tar.gzmv node_exporter-1.7.0.linux-amd64 node_exportercd /usr/local/node_exporter./node_exporter
出现以下没关系

访问采集器
http://192.168.48.104:9100/

设置采集器开机自启动
加入systemd管理
cat &gt; /usr/lib/systemd/system/node_exporter.service &lt;&lt; &quot;EOF&quot;[Unit]Description=node_exporter[Service]#ExecStart=/usr/local/node_exporter/node_exporter --web.config=/usr/local/node_exporter/config.ymlExecStart=/usr/local/node_exporter/node_exporterExecReload=/bin/kill -HUP $MAINPIDKillMode=processRestart=on-failure[Install]WantedBy=multi-user.targetEOF# 重新加载systemctl daemon-reload# 启动systemctl start node_exporter# 加入开机自启systemctl enable node_exporter
Prometheus配置
配置文件修改
操作节点[master1]
在Prometheus配置文件添加被监控端cat &gt; prome_cfg.yml &lt;&lt; &quot;EOF&quot;apiVersion: v1kind: ConfigMapmetadata:  name: prometheus-config  namespace: prometheus-workdata:  prometheus.yml: |    global:      scrape_interval: 15s      scrape_timeout: 15s    scrape_configs:    - job_name: &#x27;prometheus&#x27;      static_configs:      - targets: [&#x27;localhost:9090&#x27;]    - job_name: &#x27;test&#x27;      static_configs:      - targets: [&#x27;192.168.48.104:9100&#x27;]EOFkubectl apply -f prome_cfg.yml
热加载更新
操作节点[node01]加的是哪个节点就在哪个节点进行热加载
#实现Prometheus热更新curl -X POST http://192.168.48.104:30512/-/reload
访问prometheus页面查看http://192.168.48.104:30512

可以看到有记录了
Grafana展示
展示数据需要配置仪表盘，仪表盘可以自己制作导入，也可以从官方下载使用。
进入grafana  http://192.168.48.105:31163

这里直接输入9276仪表盘id，然后点击load



默认情况下网络这部分是没有数据的



点击右上角的save保存



监控多台linux

复制安装文件

远程复制到新增的机器
假设我还要监控node02
操作节点[node01]
scp -r /usr/local/node_exporter root@192.168.48.105:/usr/local/scp /usr/lib/systemd/system/node_exporter.service root@192.168.48.105:/usr/lib/systemd/system/
操作节点[node02]
# 重新加载systemctl daemon-reload# 启动systemctl start node_exporter# 加入开机自启systemctl enable node_exporter
添加采集器信息
操作节点[master1]
在Prometheus配置文件添加被监控端cat &gt; prome_cfg.yml &lt;&lt; &quot;EOF&quot;apiVersion: v1kind: ConfigMapmetadata:  name: prometheus-config  namespace: prometheus-workdata:  prometheus.yml: |    global:      scrape_interval: 15s      scrape_timeout: 15s    scrape_configs:    - job_name: &#x27;prometheus&#x27;      static_configs:      - targets: [&#x27;localhost:9090&#x27;]    - job_name: &#x27;test&#x27;      static_configs:      - targets: [&#x27;192.168.48.104:9100&#x27;,&#x27;192.168.48.105:9100&#x27;]#以逗号分割添加新的node02地址的采集器EOFkubectl apply -f prome_cfg.yml
热加载更新
操作节点[node02]
#实现Prometheus热更新curl -X POST http://192.168.48.104:30512/-/reload



 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>Centos 7</tag>
        <tag>K8s</tag>
        <tag>Prometheus</tag>
        <tag>Grafana</tag>
      </tags>
  </entry>
  <entry>
    <title>基于K8S1.28.2实验rook部署ceph</title>
    <url>/posts/58032/</url>
    <content><![CDATA[
基于K8S1.28.2实验rook部署ceph
Rook 支持 Kubernetes v1.22 或更高版本。
rook版本1.12.8
K8S版本1.28.2
部署出来ceph版本是quincy版本



主机名
ip1（NAT）
系统
新硬盘
磁盘
内存




master1
192.168.48.101
Centos7.9
100G
100G
4G


master2
192.168.48.102
Centos7.9

100G
4G


master3
192.168.48.103
Centos7.9

100G
4G


node01
192.168.48.104
Centos7.9
100G
100G
6G


node02
192.168.48.105
Centos7.9
100G
100G
6G



我这里是五台机，本应该ceph（三节点）是需要部署在三台node上的，这里为了测试方便，仅部署在master1，node01，node02上所以需要给这三台加一个物理硬盘
注意！使用之前，请确定是否去掉master节点的污点
【去污点方法】
以下所有操作都在master进行
前期准备
克隆仓库
git clone --single-branch --branch v1.12.8 https://github.com/rook/rook.gitcd rook/deploy/examples
查看所需镜像
[root@master1 examples]# cat operator.yaml | grep IMAGE:  # ROOK_CSI_CEPH_IMAGE: &quot;quay.io/cephcsi/cephcsi:v3.9.0&quot;  # ROOK_CSI_REGISTRAR_IMAGE: &quot;registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0&quot;  # ROOK_CSI_RESIZER_IMAGE: &quot;registry.k8s.io/sig-storage/csi-resizer:v1.8.0&quot;  # ROOK_CSI_PROVISIONER_IMAGE: &quot;registry.k8s.io/sig-storage/csi-provisioner:v3.5.0&quot;  # ROOK_CSI_SNAPSHOTTER_IMAGE: &quot;registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2&quot;  # ROOK_CSI_ATTACHER_IMAGE: &quot;registry.k8s.io/sig-storage/csi-attacher:v4.3.0&quot;  # ROOK_CSIADDONS_IMAGE: &quot;quay.io/csiaddons/k8s-sidecar:v0.7.0&quot;  [root@master1 examples]# cat operator.yaml | grep image:          image: rook/ceph:v1.12.8


基本都是国外的镜像，在这里通过阿里云+github方式构建镜像仓库解决（以下是添加为自己私人构建的镜像）
sed -i &#x27;s/# ROOK_CSI_CEPH_IMAGE: &quot;quay.io\/cephcsi\/cephcsi:v3.9.0&quot;/ROOK_CSI_CEPH_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com\/qianyios\/cephcsi:v3.9.0&quot;/g&#x27; operator.yamlsed -i &#x27;s/# ROOK_CSI_REGISTRAR_IMAGE: &quot;registry.k8s.io\/sig-storage\/csi-node-driver-registrar:v2.8.0&quot;/ROOK_CSI_REGISTRAR_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com\/qianyios\/csi-node-driver-registrar:v2.8.0&quot;/g&#x27; operator.yamlsed -i &#x27;s/# ROOK_CSI_RESIZER_IMAGE: &quot;registry.k8s.io\/sig-storage\/csi-resizer:v1.8.0&quot;/ROOK_CSI_RESIZER_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com\/qianyios\/csi-resizer:v1.8.0&quot;/g&#x27; operator.yamlsed -i &#x27;s/# ROOK_CSI_PROVISIONER_IMAGE: &quot;registry.k8s.io\/sig-storage\/csi-provisioner:v3.5.0&quot;/ROOK_CSI_PROVISIONER_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com\/qianyios\/csi-provisioner:v3.5.0&quot;/g&#x27; operator.yamlsed -i &#x27;s/# ROOK_CSI_SNAPSHOTTER_IMAGE: &quot;registry.k8s.io\/sig-storage\/csi-snapshotter:v6.2.2&quot;/ROOK_CSI_SNAPSHOTTER_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com\/qianyios\/csi-snapshotter:v6.2.2&quot;/g&#x27; operator.yamlsed -i &#x27;s/# ROOK_CSI_ATTACHER_IMAGE: &quot;registry.k8s.io\/sig-storage\/csi-attacher:v4.3.0&quot;/ROOK_CSI_ATTACHER_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com\/qianyios\/csi-attacher:v4.3.0&quot;/g&#x27; operator.yamlsed -i &#x27;s/# ROOK_CSIADDONS_IMAGE: &quot;quay.io\/csiaddons\/k8s-sidecar:v0.7.0&quot;/ROOK_CSIADDONS_IMAGE: &quot;registry.cn-hangzhou.aliyuncs.com\/qianyios\/k8s-sidecar:v0.7.0&quot;/g&#x27; operator.yamlsed -i &#x27;s/image: rook\/ceph:v1.12.8/image: registry.cn-hangzhou.aliyuncs.com\/qianyios\/ceph:v1.12.8/g&#x27; operator.yaml
开启自动发现磁盘（用于后期扩展）
sed -i &#x27;s/ROOK_ENABLE_DISCOVERY_DAEMON: &quot;false&quot;/ROOK_ENABLE_DISCOVERY_DAEMON: &quot;true&quot;/&#x27; /root/rook/deploy/examples/operator.yaml
建议提前下载镜像
docker pull registry.cn-hangzhou.aliyuncs.com/qianyios/cephcsi:v3.9.0docker pull registry.cn-hangzhou.aliyuncs.com/qianyios/csi-node-driver-registrar:v2.8.0docker pull registry.cn-hangzhou.aliyuncs.com/qianyios/csi-resizer:v1.8.0docker pull registry.cn-hangzhou.aliyuncs.com/qianyios/csi-provisioner:v3.5.0docker pull registry.cn-hangzhou.aliyuncs.com/qianyios/csi-snapshotter:v6.2.2docker pull registry.cn-hangzhou.aliyuncs.com/qianyios/csi-attacher:v4.3.0docker pull registry.cn-hangzhou.aliyuncs.com/qianyios/k8s-sidecar:v0.7.0docker pull registry.cn-hangzhou.aliyuncs.com/qianyios/ceph:v1.12.8
安装rook+ceph集群
开始部署

创建crd&amp;common&amp;operator

kubectl create -f crds.yaml -f common.yaml -f operator.yaml


创建cluster（ceph）

修改配置：等待operator容器和discover容器启动，配置osd节点
先注意一下自己的磁盘（lsblk）根据自身情况修改下面的配置文件

#更改为国内镜像sed -i &#x27;s#image: quay.io/ceph/ceph:v17.2.6#image: registry.cn-hangzhou.aliyuncs.com/qianyios/ceph:v17.2.6#&#x27; cluster.yaml
vim cluster.yaml------------------------------------- - 修改镜像    image: registry.cn-hangzhou.aliyuncs.com/qianyios/ceph:v17.2.6 - 改为false，并非使用所有节点所有磁盘作为osd- 启用deviceFilter- 按需配置config- 会自动跳过非裸盘  storage: # cluster level storage configuration and selection    useAllNodes: false    useAllDevices: false    deviceFilter:    config:    nodes:      - name: &quot;master1&quot;        deviceFilter: &quot;sda&quot;      - name: &quot;node01&quot;        deviceFilter: &quot;sda&quot;      - name: &quot;node02&quot;        deviceFilter: &quot;^sd.&quot;  #自动匹配sd开头的裸盘
这里的三个节点，是我们开头讲到的三台机，自行根据修改调整，注意这里的名字是k8s集群的名字可以在kubectl get nodes查看


部署cluster
kubectl create -f cluster.yaml
查看状态
- 实时查看pod创建进度kubectl get pod -n rook-ceph -w - 实时查看集群创建进度kubectl get cephcluster -n rook-ceph rook-ceph -w - 详细描述kubectl describe cephcluster -n rook-ceph rook-ceph

安装ceph客户端工具
- 进入工作目录cd rook/deploy/examples/- 查看所需镜像[root@master1 examples]# cat toolbox.yaml | grep image:          image: quay.io/ceph/ceph:v17.2.6- 更改为国内镜像sed -i &#x27;s#image: quay.io/ceph/ceph:v17.2.6#image: registry.cn-hangzhou.aliyuncs.com/qianyios/ceph:v17.2.6#&#x27; toolbox.yaml- 创建toolboxkubectl  create -f toolbox.yaml -n rook-ceph - 查看podkubectl  get pod -n rook-ceph -l app=rook-ceph-tools - 进入podkubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash - 查看集群状态ceph status - 查看osd状态ceph osd status - 集群空间用量ceph df


暴露dashboard
cat &gt; rook-dashboard.yaml &lt;&lt; EOF---apiVersion: v1kind: Servicemetadata:  labels:    app: rook-ceph-mgr    ceph_daemon_id: a    rook_cluster: rook-ceph  name: rook-ceph-mgr-dashboard-np  namespace: rook-cephspec:  ports:  - name: http-dashboard    port: 8443    protocol: TCP    targetPort: 8443    nodePort: 30700  selector:    app: rook-ceph-mgr    ceph_daemon_id: a  sessionAffinity: None  type: NodePortEOFkubectl apply -f rook-dashboard.yaml
查看dashboard密码
kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=&quot;&#123;[&#x27;data&#x27;][&#x27;password&#x27;]&#125;&quot; | base64 --decode &amp;&amp; echoQmu/!$ZvfQTAd-aCuHF+
访问dashboard
https://192.168.48.200:30700

如果出现以下报错（可以按下面解决，反之跳过）
消除HEALTH_WARN警告

查看警告详情

AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED: mons are allowing insecure global_id reclaim
MON_DISK_LOW: mons a,b,c are low on available space





官方解决方案：https://docs.ceph.com/en/latest/rados/operations/health-checks/


AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED

方法一：- 进入toolboxkubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bashceph config set mon auth_allow_insecure_global_id_reclaim false方法二：kubectl get configmap rook-config-override -n rook-ceph -o yamlkubectl edit configmap rook-config-override -n rook-ceph -o yamlconfig: |    [global]    mon clock drift allowed = 1    #删除podkubectl -n rook-ceph delete pod $(kubectl -n rook-ceph get pods -o custom-columns=NAME:.metadata.name --no-headers| grep mon)#显示一下信息pod &quot;rook-ceph-mon-a-557d88c-6ksmg&quot; deletedpod &quot;rook-ceph-mon-b-748dcc9b89-j8l24&quot; deletedpod &quot;rook-ceph-mon-c-5d47c664-p855m&quot; deleted#最后查看健康值ceph -s


MON_DISK_LOW：根分区使用率过高，清理即可。


Ceph存储使用
三种存储类型



存储类型
特征
应用场景
典型设备




块存储（RBD）
存储速度较快 不支持共享存储 [ReadWriteOnce]
虚拟机硬盘
硬盘 Raid


文件存储（CephFS）
存储速度慢（需经操作系统处理再转为块存储） 支持共享存储 [ReadWriteMany]
文件共享
FTP NFS


对象存储（Object）
具备块存储的读写性能和文件存储的共享特性 操作系统不能直接访问，只能通过应用程序级别的API访问
图片存储 视频存储
OSS



块存储
创建CephBlockPool和StorageClass

文件路径：/root/rook/deploy/examples/csi/rbd/storageclass.yaml
CephBlockPool和StorageClass都位于storageclass.yaml 文件
配置文件简要解读：

cd /root/rook/deploy/examples/csi/rbd[root@master1 rbd]# grep -vE &#x27;^\s*(#|$)&#x27; storageclass.yamlapiVersion: ceph.rook.io/v1kind: CephBlockPoolmetadata:  name: replicapool  namespace: rook-ceph # namespace:clusterspec:  failureDomain: host              # host级容灾  replicated:    size: 3                              # 默认三个副本    requireSafeReplicaSize: true---apiVersion: storage.k8s.io/v1kind: StorageClass                 # sc无需指定命名空间metadata:  name: rook-ceph-blockprovisioner: rook-ceph.rbd.csi.ceph.com    # 存储驱动parameters:  clusterID: rook-ceph # namespace:cluster  pool: replicapool                  # 关联到CephBlockPool  imageFormat: &quot;2&quot;  imageFeatures: layering  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph # namespace:cluster  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph # namespace:cluster  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # namespace:cluster  csi.storage.k8s.io/fstype: ext4allowVolumeExpansion: true                          # 是否允许扩容reclaimPolicy: Delete                                    # PV回收策略[root@master1 rbd]#
创建CephBlockPool和StorageClass
kubectl create -f storageclass.yaml
查看
- 查看sckubectl get sc - 查看CephBlockPool（也可在dashboard中查看）kubectl get cephblockpools -n rook-ceph


块存储使用示例

Deployment单副本+PersistentVolumeClaim

cat &gt; nginx-deploy-rbd.yaml &lt;&lt; &quot;EOF&quot;apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: nginx-deploy-rbd  name: nginx-deploy-rbdspec:  replicas: 1  selector:    matchLabels:      app: nginx-deploy-rbd  template:    metadata:      labels:        app: nginx-deploy-rbd    spec:      containers:      - image: registry.cn-hangzhou.aliyuncs.com/qianyios/nginx:latest        name: nginx        volumeMounts:        - name: data          mountPath: /usr/share/nginx/html      volumes:      - name: data        persistentVolumeClaim:          claimName: nginx-rbd-pvc---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: nginx-rbd-pvcspec:  storageClassName: &quot;rook-ceph-block&quot;   #就是这里指定了前面的创建的sc  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 5GiEOF
kubectl create -f nginx-deploy-rbd.yamlkubectl exec -it nginx-deploy-rbd-7886bf6666-qhw74 bashecho &quot;hello,nginx-deploy-rbd&quot; &gt; /usr/share/nginx/html/index.htmlexitkubectl get pod -o wide | grep nginx#测试完就删除kubectl delete -f nginx-deploy-rbd.yaml




StatefulSet多副本+volumeClaimTemplates

cat &gt; nginx-ss-rbd.yaml &lt;&lt; &quot;EOF&quot; apiVersion: apps/v1kind: StatefulSetmetadata:  name: nginx-ss-rbdspec:  selector:    matchLabels:      app: nginx-ss-rbd   serviceName: &quot;nginx&quot;  replicas: 3   template:    metadata:      labels:        app: nginx-ss-rbd     spec:      containers:      - name: nginx        image: registry.cn-hangzhou.aliyuncs.com/qianyios/nginx:latest        ports:        - containerPort: 80          name: web        volumeMounts:        - name: www          mountPath: /usr/share/nginx/html  volumeClaimTemplates:  - metadata:      name: www    spec:      accessModes: [ &quot;ReadWriteOnce&quot; ]      storageClassName: &quot;rook-ceph-block&quot;  #就是这里指定了前面的创建的sc      resources:        requests:          storage: 2GiEOF
部署
kubectl create -f nginx-ss-rbd.yamlkubectl get pod -o wide | grep nginx-sskubectl exec -it nginx-ss-rbd-0 bashecho &quot;hello,nginx-ss-rbd-0&quot; &gt; /usr/share/nginx/html/index.html &amp;&amp; exitkubectl exec -it nginx-ss-rbd-1 bashecho &quot;hello,nginx-ss-rbd-1&quot; &gt; /usr/share/nginx/html/index.html &amp;&amp; exitkubectl exec -it nginx-ss-rbd-2 bashecho &quot;hello,nginx-ss-rbd-2&quot; &gt; /usr/share/nginx/html/index.html &amp;&amp; exit#测试完就删除kubectl delete -f nginx-ss-rbd.yaml这里可能需要手动删除一下pvc[root@master1 ~]# kubectl get pvcNAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGEwww-nginx-ss-rbd-0   Bound    pvc-4a75f201-eec0-47fa-990c-353c52fe14f4   2Gi        RWO            rook-ceph-block   6m27swww-nginx-ss-rbd-1   Bound    pvc-d5f7e29f-79e4-4d1e-bcbb-65ece15a8172   2Gi        RWO            rook-ceph-block   6m21swww-nginx-ss-rbd-2   Bound    pvc-8cce06e9-dfe4-429d-ae44-878f8e4665e0   2Gi        RWO            rook-ceph-block   5m53s[root@master1 ~]# kubectl delete  pvc www-nginx-ss-rbd-0persistentvolumeclaim &quot;www-nginx-ss-rbd-0&quot; deleted[root@master1 ~]# kubectl delete  pvc www-nginx-ss-rbd-1persistentvolumeclaim &quot;www-nginx-ss-rbd-1&quot; deleted[root@master1 ~]# kubectl delete  pvc www-nginx-ss-rbd-2persistentvolumeclaim &quot;www-nginx-ss-rbd-2&quot; deleted


共享文件存储
部署MDS服务
创建Cephfs文件系统需要先部署MDS服务，该服务负责处理文件系统中的元数据。

文件路径：/root/rook/deploy/examples/filesystem.yaml

配置文件解读
cd /root/rook/deploy/examples[root@master1 examples]# grep -vE &#x27;^\s*(#|$)&#x27; filesystem.yamlapiVersion: ceph.rook.io/v1kind: CephFilesystemmetadata:  name: myfs  namespace: rook-ceph # namespace:clusterspec:  metadataPool:    replicated:      size: 3            # 元数据副本数      requireSafeReplicaSize: true    parameters:      compression_mode:        none  dataPools:    - name: replicated      failureDomain: host      replicated:        size: 3             # 存储数据的副本数        requireSafeReplicaSize: true      parameters:        compression_mode:          none  preserveFilesystemOnDelete: true  metadataServer:    activeCount: 1        # MDS实例的副本数，默认1，生产环境建议设置为3    activeStandby: true  ......省略kubectl create -f filesystem.yamlkubectl get pod -n rook-ceph | grep mds

- 进入podkubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash - 查看集群状态ceph status

配置存储(StorageClass)
配置文件：/root/rook/deploy/examples/csi/cephfs/storageclass.yaml
cd /root/rook/deploy/examples/csi/cephfskubectl apply -f storageclass.yaml

共享文件存储使用示例
cat &gt; nginx-deploy-cephfs.yaml &lt;&lt; &quot;EOF&quot; apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: nginx-deploy-cephfs  name: nginx-deploy-cephfsspec:  replicas: 3  selector:    matchLabels:      app: nginx-deploy-cephfs  template:    metadata:      labels:        app: nginx-deploy-cephfs    spec:      containers:      - image: registry.cn-hangzhou.aliyuncs.com/qianyios/nginx:latest        name: nginx        volumeMounts:        - name: data          mountPath: /usr/share/nginx/html      volumes:      - name: data        persistentVolumeClaim:          claimName: nginx-cephfs-pvc---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: nginx-cephfs-pvcspec:  storageClassName: &quot;rook-cephfs&quot;  accessModes:    - ReadWriteMany  resources:    requests:      storage: 1GiEOFkubectl apply -f nginx-deploy-cephfs.yamlkubectl get pod -o wide | grep cephfskubectl exec -it nginx-deploy-cephfs-6dc8797866-4s564 bashecho &quot;hello cephfs&quot; &gt; /usr/share/nginx/html/index.html &amp;&amp; exit#测试完删除 kubectl delete -f nginx-deploy-cephfs.yaml

在K8S中直接调用出ceph命令
#安装epel源yum install epel-release -y#安装ceph仓库yum install https://mirrors.aliyun.com/ceph/rpm-octopus/el7/noarch/ceph-release-1-1.el7.noarch.rpm -yyum list ceph-common  --showduplicates |sort -r#安装ceph客户端yum install ceph-common -y
同步ceph中的认证文件
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash[root@master1 ~]# kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bashbash-4.4$  cat /etc/ceph/ceph.conf[global]mon_host = 10.97.121.57:6789,10.104.235.210:6789,10.96.136.90:6789[client.admin]keyring = /etc/ceph/keyringbash-4.4$ cat /etc/ceph/keyring[client.admin]key = AQC241lltDbVKBAANrzwgqZd1A2eY+8h1A+BOg==bash-4.4$注意这两个文件，复制内容之后exit退出

直接在master1创建这两个文件（这里的master1是指我要在master1可以调用ceph的客户端）cat &gt; /etc/ceph/ceph.conf &lt;&lt; &quot;EOF&quot;[global]mon_host = 10.97.121.57:6789,10.104.235.210:6789,10.96.136.90:6789[client.admin]keyring = /etc/ceph/keyringEOFcat &gt; /etc/ceph/keyring &lt;&lt; &quot;EOF&quot;[client.admin]key = AQC241lltDbVKBAANrzwgqZd1A2eY+8h1A+BOg==EOF
当你添加完之后直接调用ceph的命令

删除pvc，sc及对应的存储资源
- 按需删除pvc、pvkubectl get pvc -n [namespace] | awk &#x27;&#123;print $1&#125;;&#x27; | xargs kubectl delete pvc -n [namespace]kubectl get pv | grep Released | awk &#x27;&#123;print $1&#125;;&#x27; | xargs kubectl delete pv - 删除块存储及SCkubectl delete -n rook-ceph cephblockpool replicapoolkubectl delete storageclass rook-ceph-block

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>Ceph</tag>
        <tag>K8s</tag>
      </tags>
  </entry>
  <entry>
    <title>先电IaaS2.2部署OpenStack</title>
    <url>/posts/52877/</url>
    <content><![CDATA[
先电IaaS2.2私有云部署OpenStack
前期准备
ip拓扑



主机名
ip1（NAT）
ip2（仅主机）
硬盘
内存




controller-48
192.168.48.10
192.168.148.10
100G
8G


computer-48
192.168.48.20
192.168.148.20
100G
3G



基础镜像
CentOS-7-x86_64-DVD-2009.iso
chinaskills_cloud_iaas.iso
[root@localhost ~]# lltotal 4758568-rw-r--r--  1 root root  861155328 Jun 19 03:06 CentOS-7-x86_64-DVD-2009.iso-rw-r--r--  1 root root 3799093248 Jun 19 03:06 chinaskills_cloud_iaas.iso
虚拟机硬件配置

controller

computer

sdb和sdc是分别作为cinder服务和swift服务的存储磁盘。
配置主机名和hosts，关闭防火墙和永久关闭selinux
controller
hostnamectl set-hostname controller &amp;&amp; bashsystemctl disable firewalld --nowsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/sysconfig/selinuxsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/configecho &quot;192.168.48.10 controller&quot; &gt;&gt; /etc/hostsecho &quot;192.168.48.20 computer&quot; &gt;&gt; /etc/hostssetenforce 0
computer
hostnamectl set-hostname computer &amp;&amp; bashsystemctl disable firewalld --nowsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/sysconfig/selinuxsed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/configecho &quot;192.168.48.10 controller&quot; &gt;&gt; /etc/hostsecho &quot;192.168.48.20 computer&quot; &gt;&gt; /etc/hostssetenforce 0
确保两者可以互通！
格式化compute磁盘（在compute中操作）
mkfs.xfs /dev/sdbmkfs.xfs /dev/sdc
配置yum源
controller
rm -rf /etc/yum.repos.d/*cat &gt;&gt; /etc/yum.repos.d/centos.repo &lt;&lt; EOF[centos]name=centosbaseurl=file:///qianyios/centosgpgcheck=0enabled=1[iaas]name=iaasbaseurl=file:///qianyios/iaas-repogpgcheck=0enabled=1EOF
computer
mkdir repo.bakmv /etc/yum.repos.d/* repo.bak/cat &gt;&gt; /etc/yum.repos.d/centos.repo &lt;&lt; EOF[centos]name=centosbaseurl=ftp://192.168.48.10/centosgpgcheck=0enabled=1[iaas]name=iaasbaseurl=ftp://192.168.48.10/iaas-repogpgcheck=0enabled=1EOF
挂载iso镜像文件
#控制节点mkdir /qianyios/centos -pmount -o loop CentOS-7-x86_64-DVD-2009.iso /mnt/cp -rvf /mnt/* /qianyios/centos/umount /mnt/mount -o loop chinaskills_cloud_iaas.iso /mnt/cp -rvf /mnt/* /qianyios/umount /mnt/yum clean all &amp;&amp; yum makecache
搭建ftp服务器
控制节点yum install vsftpd -ycat&gt;&gt; /etc/vsftpd/vsftpd.conf &lt;&lt;EOFanon_root=/qianyios/EOFsystemctl start vsftpd &amp;&amp; systemctl enable vsftpd
验证操作
1.清理yum缓存
#各节点yum clean all &amp;&amp; yum makecache
2.在文件资源管理器输入ftp://192.168.48.10/

编辑xiandian变量
#各节点yum install iaas-xiandian -y#使用sed命令批量去除第一个#注释符为空sed -i -e &#x27;s/^#//&#x27;g /etc/xiandian/openrc.sh使用sed命令批量修改密码默认000000sed -i -e &#x27;s/PASS=/PASS=000000/&#x27;g /etc/xiandian/openrc.sh
grep -v &quot;^#&quot; /etc/xiandian/openrc.sh | grep -v &quot;^$&quot;HOST_IP=192.168.48.10HOST_PASS=123456#ssh登入密码HOST_NAME=controllerHOST_IP_NODE=192.168.48.20HOST_PASS_NODE=123456#ssh登入密码HOST_NAME_NODE=computernetwork_segment_IP=192.168.48.0/24RABBIT_USER=openstackRABBIT_PASS=000000DB_PASS=000000DOMAIN_NAME=demoADMIN_PASS=000000DEMO_PASS=000000KEYSTONE_DBPASS=000000GLANCE_DBPASS=000000GLANCE_PASS=000000NOVA_DBPASS=000000NOVA_PASS=000000NEUTRON_DBPASS=000000NEUTRON_PASS=000000METADATA_SECRET=000000INTERFACE_IP=192.168.48.10#复制到第二台机的时候，记得改成192.168.48.20INTERFACE_NAME=ens36#第二块网卡的名字Physical_NAME=providerminvlan=2maxvlan=300CINDER_DBPASS=000000CINDER_PASS=000000BLOCK_DISK=sdbSWIFT_PASS=000000OBJECT_DISK=sdcSTORAGE_LOCAL_NET_IP=192.168.48.20HEAT_DBPASS=000000HEAT_PASS=000000ZUN_DBPASS=000000ZUN_PASS=000000KURYR_DBPASS=000000KURYR_PASS=000000CEILOMETER_DBPASS=000000CEILOMETER_PASS=000000AODH_DBPASS=000000AODH_PASS=000000BARBICAN_DBPASS=000000BARBICAN_PASS=000000
scp /etc/xiandian/openrc.sh computer:/etc/xiandian/openrc.sh #以下在计算节点sed -i &#x27;s/INTERFACE_IP=192.168.48.10/INTERFACE_IP=192.168.48.20/g&#x27; /etc/xiandian/openrc.sh
安装平台基本服务
[root@controller ~]# iaas-pre-host.sh [root@compute ~]# iaas-pre-host.sh #双节点reboot# 可同时执行，执行完毕后脚本会提示重启，不然rabbitmq服务会报错！！！此脚本会初始化虚拟机环境，如修改主机名、主机映射、时间同步等，自己可去尝试解读脚本，手工搭建平台！！！！，由于重启可能导致平台出现问题可用ssh连接自己的IP地址重新登陆，只要出现屏幕登录时间以及看到屏幕欢迎界面即可Controller节点iaas-install-mysql.shiaas-install-keystone.shiaas-install-glance.shController节点iaas-install-nova-controller.shCompute节点iaas-install-nova-compute.shController节点sed -i &#x27;s/yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables -y/yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables -y --skip-broken/g&#x27; /usr/local/bin/iaas-install-neutron-controller.shiaas-install-neutron-controller.shCompute节点iaas-install-neutron-compute.shController节点iaas-install-dashboard.sh从Ftp服务器上下载镜像到本地。（在controller中操作）source /etc/keystone/admin-openrc.shglance image-create --name CentOS7.5 --disk-format qcow2 --container-format bare --progress &lt; /qianyios/images/CentOS_7.5_x86_64_XD.qcow2
访问页面http://192.168.48.10/dashboard/

更多服务
#所以安装脚本都在/usr/local/bin/目录下#按照自己需求安装安装Swift对象存储服务iaas-install-swift-controller.sh安装Heat编配服务iaas-install-heat.sh安装Zun服务iaas-install-zun-controller.sh安装Ceilometer监控服务iaas-install-ceilometer-controller.sh安装Aodh监控服务iaas-install-aodh.sh

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>Centos 7</tag>
        <tag>OpenStack</tag>
        <tag>Iaas</tag>
      </tags>
  </entry>
  <entry>
    <title>基于CentOS Stream 8一键安装OpenStack Yoga版本</title>
    <url>/posts/11354/</url>
    <content><![CDATA[
基于CentOS Stream 8一键安装OpenStack Yoga版本
主机拓扑



主机名
ip
内存
cpu
硬盘
OS




openstack
192.168.48.100
8G
2v2c
100G+100G
CentOS Stream 8




本机镜像可以进入这里下载CentOS-Stream-8-x86_64-latest-boot.iso
注意！Centos Stream 8已经停止更新了，此实验可以用作测试，不可用于生产
网络配置
[root@localhost ~]# vi /etc/sysconfig/network-scripts/ifcfg-ens160TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=eui64NAME=ens160UUID=025bf07f-8fc9-41eb-b26e-13218b0d434bDEVICE=ens160ONBOOT=yesIPADDR=192.168.48.100PREFIX=24GATEWAY=192.168.48.2DNS1=192.168.48.2DNS2=114.114.114.114
基础配置
#设置主机名hostnamectl set-hostname openstack &amp;&amp; bash#添加本地名称解析cat &gt;&gt;/etc/hosts &lt;&lt; &quot;EOF&quot;192.168.48.100 openstack EOF#关闭防火墙systemctl disable firewalld --nowsed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsetenforce 0#值得需要注意的是，openstack不允许使用NetworkManager，而是需要使用Network，所以我们还需要安装Network的服务。dnf install -y network-scripts systemctl disable --now NetworkManagersystemctl restart networksystemctl enable --now network#启用powertools库dnf config-manager --enable powertools#重建以下缓存dnf clean all &amp;&amp; dnf makecache #升级软件包dnf -y updatereboot
安装openstack
[root@openstack ~]#  dnf search release-openstack........centos-release-openstack-yoga.noarch : OpenStack from the CentOS Cloud SIG repo configs#安装最新的yoga版本dnf install -y centos-release-openstack-yoga.noarch#安装packstack软件包并生成应答文件dnf install -y openstack-packstackpackstack --gen-answer-file /root/openstack-answer.txt#修改应答文件#一键替换，若有其他需要根据需要自行修改其中的值sed -i &#x27;s/CONFIG_HEAT_INSTALL=n/CONFIG_HEAT_INSTALL=y/g; s/CONFIG_PROVISION_DEMO=y/CONFIG_PROVISION_DEMO=n/g; s/CONFIG_NEUTRON_OVN_BRIDGE_IFACES=/CONFIG_NEUTRON_OVN_BRIDGE_IFACES=br-ex:ens160/g&#x27; /root/openstack-answer.txtsed -i &#x27;s/CONFIG_KEYSTONE_ADMIN_PW=.*/CONFIG_KEYSTONE_ADMIN_PW=admin/g&#x27; /root/openstack-answer.txt#也可以手动替换vi /root/openstack-answer.txt-------------------------CONFIG_HEAT_INSTALL=y           #安装heat模板服务CONFIG_PROVISION_DEMO=n         #我们不要提供的demo项目CONFIG_KEYSTONE_ADMIN_PW=admin  #设置登陆密码CONFIG_NEUTRON_OVN_BRIDGE_IFACES=br-ex:ens160            #OVN端口映射，就是云主机连接外网时通过那块网卡进行数据包的转发-------------------------#开始安装packstack --answer-file /root/openstack-answer.txt 
然后你就可以通过http://192.168.48.100/dashboard访问页面了

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>Centos 8 stream</tag>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title>从零开始搭建你的免费图床系统</title>
    <url>/posts/aaddb3d/</url>
    <content><![CDATA[
从零开始搭建你的免费图床系统（Cloudflare R2 + WebP Cloud + 图床工具）
文章来源自：从零开始搭建你的免费图床系统（Cloudflare R2 + WebP Cloud + PicGo） · Pseudoyu
背景
陆陆续续两三年了，建立博客至今也有诸多文章了，有一个问题就是图片如果放在本地显示会很慢，特别是放在服务器，如果没有cdn的话，就没有如今的效果，我现在的模式就是hexo（github page+公益cdn），后话了，先回到这这里，由于图片一多，造成发送图片源位置移动的话，还要重新找，特别是我在hexo里static下的img文件夹，本身markdown显示的都是相对路径，管理起来很麻烦，如今我了解到图床的概念，不但可以统一进行管理，还能减少博客仓库的体积，提升网站速度，（虽然我现在依然保持着，源图和文档在一起，源图片直接上传到github，毕竟上传到别人服务器的数据，就怕突然有一天倒闭了，就没了，所以我依然有着备份原图片在本地的习惯），所以我接下来讲的，只是适用于，不那么重要的项目
阿里云OSS+Picgo
阿里云oss，我个人使用来说，很方便，速度很快，结合图床来使用也是很方便的，就是唯一的缺点就是大多都是按量付费，如果内容一多，对于本身就是非盈利性的我来说，就很不妥当，通过picgo或兰空图床进行上传图片到阿里云oss，访问速度也很快，针对国内用户的访问速度明显提升（费用：存储加公网访问流量费用）
还有一个办法就是设置反向代理，你需要购买一台同区域的esc，保持和你存储桶是一个区域的，这样经过的流量就是内网流量，不会计费，但是会计算存储费用
图床搭建说明
Cloudflare R2 + WebP Cloud + 图床工具 的方案尽管牵扯到了多个组件和平台，但所有操作都在 Free Plan 中，也是我最终选定的方案，下面将从零开始介绍如何搭建这个免费图床系统。
Cloudflare R2
R2 是 Cloudflare 推出的免费对象存储服务，需要免费注册一个 Cloudflare 账号才能使用，注册登录后，点击左侧边栏的 R2 访问服务，但需要注意的是开通 R2 服务需要绑定信用卡（国内外主流信用卡皆可），但并不会扣费，主要是为了验证用户身份使用。但是我最近发现他有了paypal验证方式，有了paypal，可以直接绑定国内储蓄卡，就可以通过R2验证的银行卡验证了

创建存储桶
开通 R2 服务后，点击右上角「创建存储桶」按钮进行创建。

进入创建配置界面后，需要填写存储桶（Bucket）名称，建议有一些辨识度，后续在配置上传时会用到。
位置则选择「自动」，但可以额外多配置一个位置提示，由于我后续还将使用「WebP Cloud」服务的美西机房进行图片代理优化，所以在此处选择的是「北美洲西部（WNAM）」，根据需求选其他区域也可以，但 Cloudflare 并不保证一定会分配到所指定的区域。

进入设置

首先我们需要打开「R2.dev 子域」，这是为了后续访问图片时需要的公网地址，点击「允许访问」，并按照提示输入「allow」即可开启。

完成后会显示一个以 r2.dev 结尾的公网网址，即我们后续访问图片的网址。
自定义图床域名（可选）
但是分配的网址比较长，不易于记忆，我们可以通过「自定义域」来绑定我们的专属域名，点击「连接域」按钮，填入想要访问的域名

然后在你域名服务商那添加dns解析，由于我是将域名托管到了cloudflare，直接自动添加解析了

效果就是这样了


配置存储桶访问api
当我们回到对象页面，上传一张测试图片

就有链接可以进行访问了

但是如果每次都打开cloudflare手动上传图片的话都会很麻烦。R2 提供了 S3 兼容的 API，可以方便地使用一些客户端/命令行工具进行上传、删除等操作。



这时候就会有api令牌信息，这里的内容只会出现一次，可以记下来，但是不要给别人哦

至此，我们需要在 Cloudflare R2 上配置的部分就完成了，接下来我们需要配置图床工具吧。
PicGo（可选，无需服务器）
PicGo 是一个用于快速上传并获取图片 URL 的工具软件，有着较为丰富的插件生态，支持多种图床服务，其 GitHub 仓库为「GitHub - Molunerfinn/PicGo」，可以下载对应平台客户端使用。
配置 R2 图床
PicGo 本体并不包括 S3 图床，但可以通过「GitHub - wayjam/picgo-plugin-s3」插件来支持。
windows下的稳定版2.3.1会出现搜索不到插件的情况,下载里面的beta版本就行了
Bug: 插件设置页无法搜索到插件 · Issue #1297 · Molunerfinn/PicGo
我只能安装1.0.2的，第一个安装不了，无关紧要

然后填写信息

这里有几项配置需要尤其注意。

应用密钥 ID，填写 R2 API 中的 Access Key ID（访问密钥 ID）
应用密钥，填写 R2 API 中的 Secret Access Key（机密访问密钥）
桶名，填写 R2 中创建的 Bucket 名称，如我上文的 blog-r2
文件路径，上传到 R2 中的文件路径，我选择使用 &#123;fileName&#125;.&#123;extName&#125; 来保留原文件的文件名和扩展名。
自定义节点，填写 R2 API 中的「为 S3 客户端使用管辖权地特定的终结点」，即 xxx.r2.cloudflarestorage.com 格式的 S3 Endpoint
自定义域名，填写上文生成的 xxx.r2.dev 格式的域名或自定义域名，如我配置的 image.qianyisky.cn


其他配置保持默认即可，确认参数无误后点击「确定」与「设置为默认图床」即可。
图片上传

完成上述配置后，我们就可以在「上传区」直接拖入文件进行图片上传了，如上传后显示无误则为配置成功，生成的链接会自动在系统剪贴板中，直接在需要的地方粘贴即可。
根据你想要的链接格式，我这里是markdown，就选择markdown格式，然后在相册里复制链接，在markdown文档看看


Lsky Pro图床（可选，需要服务器）
官网：Lsky Pro+ - 属于您自己的云上相册。
部署教程：Lsky Pro图床 | 严千屹博客
我这里就展示自己搭建好lsky pro图床进行演示了
创建存储策略


确认即可
图片上传
设置上传图片的限制大小为10M


点击保存即可


两者路径修改位置

这就是统一管理的方便性
这是lsky pro设置上传路径的位置

这是PicGo设置上传路径的位置

再去测试上传看看

效果

当然你可以在设置里选择这两个



WebP Cloud 图片优化
至此我们已经完成了整个图床的搭建、配置和上传，但通常我们本地截图或是相机拍摄的图片体积较大，对于访客来说加载时间会较长，并不直接适合互联网发布。
根据从零开始搭建你的免费图床系统（Cloudflare R2 + WebP Cloud + PicGo） · Pseudoyu写到WebP Cloud的方法
它可以实现图片的压缩，加快页面显示的速度

简单来说这是一个类 CDN 的图片代理 SaaS 服务，可以在几乎不改变画质的情况下大幅缩小图片体积，加快整体站点加载速度。发展到现在除了图片体积减少外，还提供了缓存、添加水印、图片滤镜等更多实用的功能，并提供了自定义 Header 等配置选项。
配置webp cloud
首先通过 GitHub 授权登录 WebP Cloud 平台。


回到主页，点击创建代理


为了优化国内访问，我「Proxy Region」选择的是美西「Hillsboro, OR」区域
「Proxy Name」填写一个自定义名称即可
「Proxy Origin URL」，比较重要，需要填写上文我们配置好的 R2 自定义域名，如我填写的是 image.qianyisky.cn，如果没配置自定义域名则填写 R2 提供的 xxx.r2.dev 格式的域名


之后点击你创建的代理，将代理地址更新到存储策略的配置里
PicGo配置

Lsky Pro配置

再进行上传图片的测试

就是我们通过图床工具上传的图片上传了之后，就可以通过这个WebP Cloud Services提供的代理节点进行访问
如果觉得默认代理地址不好看，可以进行设置自定义域名



最后确认，等待十分钟这样子，点击刷新按钮，就会更新代理地址了，然后点击启用


这时候你就可以去修改之前的图床工具的代理地址了
https://images.qianyisky.cn/PicGo/test.png
WebP Cloud 用量
免费用户每天有 3000 Free Quota，即能够代理 3000 次图片访问请求，并提供 200M 的图片缓存，对于一般用户来说完全够用，如有一些流量较大的特定时期也可以购买额外 Quota，价格很便宜。
如超过了 Quota，访问则会被 301 转发到源站图片地址，不经 WebP Cloud 服务压缩，但依然可用；超过 200M 的缓存则会按照 LRU 算法清理，所以依然能够保障一些高频请求的图片能够有较好的访问体验。
总结
以上就是Pseudoyu的图床系统搭建方案，如果你的项目是在国内的服务器上，用这个方法勉强够用哦，基本都是免费订阅的
那么接下来就要讲到安全这几块了
以下内容来自使用 WebP Cloud 与 Cloudflare WAF 为你的图床添加隐私和版权保护 · Pseudoyu
可以看看作者的一个图片被盗用的经历，就可以知道隐私保护的重要性
使用 WebP Cloud 与 Cloudflare WAF 为你的图床添加隐私和版权保护
在使用 WebP Cloud 的过程中，Pseudoyu发现它还提供了自定义 Proxy User Agent、水印等功能，于是萌生了一个想法，是不是可以通过 WebP Cloud 对Pseudoyu的图床源站链接进行保护，使 WebP Cloud 的代理链接成为访问我所有图片的唯一入口，并统一添加Pseudoyu的专属版权水印。
本文是对这一实践的记录，也算是图床搭建番外篇了。
需求分析

理论上两个地址都可以进行访问到我图床里的图片，但是前者是代理后经过处理的图，后者是直接访问cloudflare R2的原图
隐私保护
事实上我们通过手机、数码相机等设备拍摄的照片都会携带 EXIF(EXchangeable Image File Format) 信息，通常会包含拍摄设备、时间和地点等敏感信息，我们可以通过一些技术方式手动去除这些元数据，但操作十分繁琐且容易遗漏。
Pseudoyu查阅了一下 WebP Cloud 的文档，发现它果然提供自动擦除 EXIF 信息的功能，无须额外配置操作，但其实访客依然可以可以通过 Cloudflare R2 暴露出的源站信息访问到原图，为了避免这一点，我们需要限制用户只能通过 WebP Cloud 代理链接进行请求，访问 Cloudflare R2 源站链接时获取不到任何有用信息。
版权保护
这是作者图片被盗用的经历

实现方案
需求清晰了，其实主要分为两部分：

让用户只能通过 WebP Cloud 代理链接访问到我的图片，禁止直接访问原图链接
在 WebP Cloud 代理层面为所有的图片统一自动添加自己的版权水印，无须手动操作

以下是我的实现方案与详细步骤。
WebP 自定义 User Agent + Cloudflare WAF
WebP Cloud 配置
当我们访问互联网上的网页或图片链接时，请求通常会包含一个 User Agent 字段，一般包含浏览器版本等信息，网站可针对不同的 User Agent 进行一些特定逻辑处理。
WebP Cloud 默认会使用 WebP Cloud Services/1.0 作为值，也就是不论用户访问图片时使用的是什么终端设备和浏览器，请求到 Cloudflare R2 时都会被统一为 WebP Cloud 定义的 User Agent 值，而这个值又是用户可以自定义的。

因此，我们登录 WebP Cloud 的控制台，将「Proxy User Agent」字段设置为一个自定义值，如 ABC.com/1.0。
随后更新代理
Cloudflare WAF 配置
WAF（Web Application Firewall） 是 Cloudflare 提供的一个防火墙服务，可以自定义规则来限制特定请求以保护网站安全，登录 Cloudflare 后在左侧边栏点击「网站」，点击进入需要保护的域名，选择侧边栏「安全性」 - 「WAF」即可免费使用（注：不是最外层的账户级 WAF），免费账户可设定五个自定义规则。


点击右侧的「编辑表达式」，填入以下规则：
(http.user_agent ne &quot;ABC.com/1.0&quot;) and (http.host eq &quot;image.qianyisky.cn&quot;)

其他默认，然后点击保存

方便理解：
1.代理链接的User Agent会包含在网络请求里，也就是设置了ABC.com/1.0（A钥匙）
2.cloudflare WAF：设置了只有A钥匙才可以访问
3.一般用户浏览网页的User Agent是这样的
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36
也就是说没有A钥匙，是访问不了的
4.所以在代理链接设置了A钥匙，那么他会把访问原地址的流量请求加入A钥匙，达到访问的目的，如果没有A钥匙直接访问原地址就会出现如下图


所以完成配置后，当我们再次访问原地址image.qianyisky.cn的时候就会被拦截，例如：
原地址：https://image.qianyisky.cn/PicGo/test.png
代理地址：https://images.qianyisky.cn/PicGo/test.png
完美实现了我们的需求。
设置水印
我这里就设置了一个灰色的水印


设置好后保存

由于，刚设置，只会对新上传的图片添加水印，如果你要之前上传的图片都要打上水印，你就要清除缓存，最后重新访问图片就会有水印了

缓存的规则：目前所有代理都会有 200.00 MiB 缓存，如果超出这个限制，我们将使用“最近最少使用”（LRU）算法进行驱逐，也就是说，最少请求的内容将从缓存中驱逐。
代理地址：https://images.qianyisky.cn/PicGo/test.png


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>杂项</category>
      </categories>
  </entry>
  <entry>
    <title>华为ensp模拟器安装教程</title>
    <url>/posts/f8bb3dfe/</url>
    <content><![CDATA[
华为ensp模拟器安装教程
请留意：经过多年，Windows系统已经推出多个新版本，因此可能会出现各种不相容的状况，故此此文仅供参考，如有疑问，请自行处理。eNSP模拟器从2019年开始不再有新版本更新，最近的版本是1.3.00.100 V100R003C00SPC100。
安装准备
下载地址1：自建   速度较慢
下载地址2：ensp及依赖安装包
这个下载之后是ensp及依赖安装包.exe123，重命名把后面的123删除即可
安装环境检查
检查之前是否已经安装过 eNSP 和依赖软件，如果有先请卸载，包括依赖软件一起卸载。
如果之前安装过 eNSP ，请使用注册表清理工具清理一下注册表【此步骤谨慎操作，注册表比较重要，请在有备份的情况下清理，清理出错可能会导致系统不可用】。
eNSP安装和使用过程中请将 Windows 防火墙关闭。
Windows11安装需要关闭内核隔离选项。

安装winPcap
双击安装包，一直下一步到这，保持默认，继续下一步


点击finish即可
安装Wireshark
双击安装包打开，一直下一步，到这，保持默认，继续下一步

保持默认，继续下一步

可以自行选择安装路径，但是不建议,我这里是c盘内存不多了

保持默认，下一步

保持默认，下一步

安装过程中，会跳出一个安装节目，保持默认，下一步即可

都不选下一步

最后点击finish，返回wireshark安装界面
啥也不选，点击finish

安装VirtualBox
双击安装包打开，一直下一步，到这，保持默认，继续下一步（自行选择安装路径，也可以默认）



然后点击安装到这




安装ensp




然后一直下一步，安装即可

打开ensp


有报错了，启动一个AR


因为刚刚安装VirtualBox最后没有重启电脑，你重启一下电脑就行了
要确保是这个地址

然后你再去启动就可以启动了
如果还是不行，AR路由器还是启动不了，你才执行下面的操作
按win键搜索运行输入regedit

你直接导航到
计算机\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\VBoxDrv
将Start的值改成3，然后重启电脑


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>eNSP</tag>
      </tags>
  </entry>
  <entry>
    <title>公共留言区</title>
    <url>/posts/22484/</url>
    <content><![CDATA[


        
            
        
        
            千屹留言区
        
                
            
        
    
 
    这里是公共留言区，有什么问题和建议都可以畅所欲言，在下面留言哦！


]]></content>
      <tags>
        <tag>Public</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Centos7部署k3s</title>
    <url>/posts/df961508/</url>
    <content><![CDATA[
基于Centos7部署k3s
主机拓扑



主机名
ip
CPU
内存




Server1
192.168.48.101
≧2
2G


Server2
192.168.48.102
≧2
2G


Server3
192.168.48.103
≧2
2G


Agent1
192.168.49.104
≧1
512MB



如果你是只用单serve1只需要创建server1和若干台agent，部署教程请跳转3.基础k3s
如果你是高可用一定需要保留≥3台的server节点，部署教程请跳转6.高可用k3s
前提是要完成2.基础配置
基础配置
高可用和基础k3s都要运行
系统初始化
操作节点:[所有节点]
sudo vi system_init.sh
#!/bin/bashif [ $# -eq 2 ];then  echo &quot;设置主机名为：$1&quot;  echo &quot;ens33设置IP地址为：192.168.48.$2&quot;else  echo  &quot;使用方法：sh $0 主机名 主机位&quot;  exit 2fiecho &quot;--------------------------------------&quot;echo &quot;1.正在设置主机名：$1&quot;hostnamectl set-hostname $1echo &quot;2.正在关闭firewalld、selinux&quot;systemctl disable firewalld &amp;&gt; /dev/nullsystemctl stop firewalldsed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsetenforce 0echo &quot;3.正在设置ens33：192.168.48.$2&quot;cat &gt; /etc/sysconfig/network-scripts/ifcfg-ens33 &lt;&lt;EOFTYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noNAME=ens33UUID=53b402ff-5865-47dd-a853-7afcd6521738DEVICE=ens33ONBOOT=yesIPADDR=192.168.48.$2GATEWAY=192.168.48.2PREFIX=24DNS1=192.168.48.2DNS2=114.114.114.114EOFnmcli c reloadnmcli c up ens33echo &quot;4.更新yum源软件包缓存&quot;yum clean all &amp;&amp; yum makecacheecho &quot;5.添加hosts解析&quot;cat &gt; /etc/hosts &lt;&lt;EOF127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.48.101 Server1192.168.48.102 Server2192.168.48.103 Server3192.168.48.104 Agent1EOFecho &quot;6.必备工具安装&quot;yum install wget psmisc vim net-tools telnet socat device-mapper-persistent-data lvm2 git gcc -yecho &quot;7.重启系统&quot;reboot
sudo chmod +x system_init.sh[Server1] sudo sh system_init.sh Server1 101[Server2] sudo sh system_init.sh Server2 102[Server3] sudo sh system_init.sh Server3 103[Agent1] sudo sh system_init.sh Agent1 104
安装容器工具
请你考虑好，你的集群要以什么为运行时，下面提供了，docker和containerd，自行选择
只能二选一！！！
只能二选一！！！
只能二选一！！！
安装docker
操作节点:[所有节点]
sudo curl -L &quot;https://qygit.qianyisky.cn/https://github.com/docker/compose/releases/download/v2.35.1/docker-compose-linux-x86_64&quot; -o /usr/local/bin/docker-compose#卸载旧版本sudo yum -y remove docker \                  docker-client \                  docker-client-latest \                  docker-common \                  docker-latest \                  docker-latest-logrotate \                  docker-logrotate \                  docker-selinux \                  docker-engine-selinux \                  docker-enginesudo yum remove docker-ce docker-ce-cli containerd.io docker-compose docker-machine docker-swarm -ysudo rm /etc/yum.repos.d/docker-ce.reposudo rm -rf /var/lib/dockersudo yum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposudo yum install docker-ce -ysudo systemctl enable --now dockersudo chmod +x /usr/local/bin/docker-composesudo tee /etc/docker/daemon.json &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;&#123;  &quot;registry-mirrors&quot;: [    &quot;https://docker.xuanyuan.me&quot;,    &quot;https://docker.m.daocloud.io&quot;,    &quot;https://docker.1ms.run&quot;,    &quot;https://docker.1panel.live&quot;,    &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,    &quot;https://docker.qianyios.top&quot;  ],  &quot;max-concurrent-downloads&quot;: 10,  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-level&quot;: &quot;warn&quot;,  &quot;log-opts&quot;: &#123;    &quot;max-size&quot;: &quot;10m&quot;,    &quot;max-file&quot;: &quot;3&quot;  &#125;,  &quot;data-root&quot;: &quot;/var/lib/docker&quot;&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart dockersudo docker-compose --versionsudo docker version
安装containerd
操作节点:[所有节点]
sudo yum install -y yum-utilssudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposudo yum clean all &amp;&amp; yum makecachesudo yum install -y containerd.iosudo mkdir -p /etc/containerd/certs.d/docker.iosudo mkdir -p /etc/containerd/certs.d/registry.k8s.iosudo mkdir -p /etc/containerd/certs.d/k8s.gcr.iosudo mkdir -p /etc/containerd/certs.d/ghcr.iosudo mkdir -p /etc/containerd/certs.d/gcr.iosudo mkdir -p /etc/containerd/certs.d/quay.iosudo mkdir -p /etc/containerd/certs.d/registry-1.docker.iosudo tee /etc/containerd/certs.d/docker.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://docker.io&quot; [host.&quot;https://registry.cn-hangzhou.aliyuncs.com/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.xuanyuan.me&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.m.daocloud.io&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.1ms.run&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.1panel.live&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.qianyios.top/&quot;]   capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://reg-mirror.giniu.com&quot;]   capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/registry-1.docker.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://registry-1.docker.io&quot;[host.&quot;https://registry.cn-hangzhou.aliyuncs.com/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.xuanyuan.me&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.m.daocloud.io&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.1ms.run&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.1panel.live&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.qianyios.top/&quot;]   capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://reg-mirror.giniu.com&quot;]   capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/k8s.gcr.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://k8s.gcr.io&quot;[host.&quot;https://registry.aliyuncs.com/google_containers&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/ghcr.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://ghcr.io&quot;[host.&quot;https://ghcr.m.daocloud.io/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/gcr.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://gcr.io&quot;[host.&quot;https://gcr.m.daocloud.io/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;registry.k8s.io&quot;[host.&quot;k8s.m.daocloud.io&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;, &quot;push&quot;][host.&quot;https://registry.aliyuncs.com/v2/google_containers&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/quay.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://quay.io&quot;[host.&quot;https://quay.tencentcloudcr.com/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo sh -c &#x27;containerd config default &gt; /etc/containerd/config.toml&#x27;sudo sed -i &#x27;s#sandbox_image = &quot;registry.k8s.io/pause:.*&quot;#sandbox_image = &quot;registry.aliyuncs.com/google_containers/pause:3.10&quot;#&#x27; /etc/containerd/config.tomlsudo sed -i &#x27;s/SystemdCgroup = false/SystemdCgroup = true/g&#x27; /etc/containerd/config.tomlsed -i &#x27;/\[plugins\.&quot;io\.containerd\.grpc\.v1\.cri&quot;\.registry\]/!b;n;s/config_path = &quot;&quot;/config_path = &quot;\/etc\/containerd\/certs.d&quot;/&#x27; /etc/containerd/config.toml# 重启 containerd 服务sudo systemctl daemon-reloadsudo systemctl restart containerd.servicesudo ctr image ls
添加镜像源
操作节点:[所有节点]
sudo mkdir -p /etc/rancher/k3ssudo tee /etc/rancher/k3s/registries.yaml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;mirrors:  docker.io:    endpoint:      - &quot;https://registry.cn-hangzhou.aliyuncs.com/&quot;       - &quot;https://docker.xuanyuan.me&quot;       - &quot;https://docker.m.daocloud.io&quot;       - &quot;https://docker.1ms.run&quot;       - &quot;https://docker.1panel.live&quot;       - &quot;https://hub.rat.dev&quot;       - &quot;https://docker-mirror.aigc2d.com&quot;       - &quot;https://docker.qianyios.top/&quot;  quay.io:    endpoint:      - &quot;https://quay.tencentcloudcr.com/&quot;     registry.k8s.io:    endpoint:      - &quot;https://registry.aliyuncs.com/v2/google_containers&quot;     gcr.io:    endpoint:      - &quot;https://gcr.m.daocloud.io/&quot;     k8s.gcr.io:    endpoint:      - &quot;https://registry.aliyuncs.com/google_containers&quot;     ghcr.io:    endpoint:      - &quot;https://ghcr.m.daocloud.io/&quot;   EOF
建议在这里打个快照
基础k3s
确保所有节点都下载了k3s安装脚本
sudo wget https://rancher-mirror.rancher.cn/k3s/k3s-install.shsudo chmod +x k3s-install.sh
这里目前只需要用到Server1和Agent1，也就是单server1情况
部署Server
操作节点：[Server1]
如需在单个服务器上安装 K3s，可以在 server 节点上执行如下操作：
要注意你选的是哪个容器工具哈
要注意你选的是哪个容器工具哈
要注意你选的是哪个容器工具哈
基于docker
如果有参数的值数错了，可以改一下，然后重新运行命令即可
--docke一定要放在所有参数的最前面
sudo \INSTALL_K3S_MIRROR=cn \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \./k3s-install.sh --docker \--system-default-registry &quot;registry.cn-hangzhou.aliyuncs.com&quot;

自行完成 9.解决非root用户使用kubectl等命令显示无命令的办法
基于containerd
如果有参数的值数错了，可以改一下，然后重新运行命令即可
sudo \INSTALL_K3S_MIRROR=cn \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \./k3s-install.sh \--system-default-registry &quot;registry.cn-hangzhou.aliyuncs.com&quot;

自行完成 9.解决非root用户使用kubectl等命令显示无命令的办法
部署Agent节点
操作节点：[Server1]
查看token
sudo cat /var/lib/rancher/k3s/server/node-token

K107eca1d1c601a2d308c7dd0b639ef08fa9414f729c720c0cc04337126aa966884::server:035b03761950e59b30e9f5310b92310c

要注意你选的是哪个容器工具哈
要注意你选的是哪个容器工具哈
要注意你选的是哪个容器工具哈
基于docker
操作节点：[Agent1]
如果有参数的值数错了，可以改一下，然后重新运行命令即可
--docke一定要放在所有参数的最前面
sudo \INSTALL_K3S_MIRROR=cn \K3S_URL=https://192.168.48.101:6443  \K3S_TOKEN=035b03761950e59b30e9f5310b92310c \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \./k3s-install.sh --docker
· 035b03761950e59b30e9f5310b92310c 是前面server1获取的token
· 192.168.48.101是server1的ip

这时候在server1查看是否成功加入集群
sudo kubectl get nodes

如果需要部署dashboard请跳转7.安装dashboard
基于containerd
操作节点：[Agent1]
如果有参数的值数错了，可以改一下，然后重新运行命令即可
sudo \INSTALL_K3S_MIRROR=cn \K3S_URL=https://192.168.48.101:6443  \K3S_TOKEN=035b03761950e59b30e9f5310b92310c \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \./k3s-install.sh 
· 035b03761950e59b30e9f5310b92310c 是前面server1获取的token
· 192.168.48.101是server1的ip

这时候在server1查看是否成功加入集群
sudo kubectl get nodes

如果需要部署dashboard请跳转7.安装dashboard
高可用k3s
确保所有节点都下载了k3s安装脚本
sudo wget https://rancher-mirror.rancher.cn/k3s/k3s-install.shsudo chmod +x k3s-install.sh
配置集群负载均衡器
官方教程：集群负载均衡器
按理来说我们需要两台额外的节点来做负载均衡和高可用vip节点，但是为了测试方便，我们直接部署在
server节点，也就是图中的第二种方法

操作节点：[所有的server]
sudo yum install -y haproxy keepalivedsudo tee /etc/haproxy/haproxy.cfg &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;frontend k3s-frontend    bind *:16443    mode tcp    option tcplog    default_backend k3s-backendbackend k3s-backend    mode tcp    option tcp-check    balance roundrobin    timeout connect 5s    timeout server 30s    timeout client 30s    default-server inter 10s downinter 5s    server server-1 192.168.48.101:6443 check    server server-2 192.168.48.102:6443 check    server server-3 192.168.48.103:6443 checkEOF
操作节点:[Server1]
sudo tee /etc/keepalived/keepalived.conf &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;global_defs &#123;  enable_script_security  script_user root&#125;vrrp_script chk_haproxy &#123;    script &#x27;killall -0 haproxy&#x27;    interval 2&#125;vrrp_instance haproxy-vip &#123;    interface ens33  #这里要改，是你的网卡    state MASTER    #这里要改 server1是Master 其他都是Backup    priority 200    virtual_router_id 51    virtual_ipaddress &#123;        192.168.48.200/24      #高可用ip    &#125;    track_script &#123;        chk_haproxy    &#125;&#125;EOF
操作节点:[Server2]
sudo tee /etc/keepalived/keepalived.conf &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;global_defs &#123;  enable_script_security  script_user root&#125;vrrp_script chk_haproxy &#123;    script &#x27;killall -0 haproxy&#x27;    interval 2&#125;vrrp_instance haproxy-vip &#123;    interface ens33  #这里要改，是你的网卡    state BACKUP    #这里要改 server1是Master 其他都是Backup    priority 150    virtual_router_id 51    virtual_ipaddress &#123;        192.168.48.200/24      #高可用ip    &#125;    track_script &#123;        chk_haproxy    &#125;&#125;EOF
操作节点:[Server3]
sudo tee /etc/keepalived/keepalived.conf &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;global_defs &#123;  enable_script_security  script_user root&#125;vrrp_script chk_haproxy &#123;    script &#x27;killall -0 haproxy&#x27;    interval 2&#125;vrrp_instance haproxy-vip &#123;    interface ens33  #这里要改，是你的网卡    state BACKUP    #这里要改 server1是Master 其他都是Backup    priority 100    virtual_router_id 51    virtual_ipaddress &#123;        192.168.48.200/24      #高可用ip    &#125;    track_script &#123;        chk_haproxy    &#125;&#125;EOF
操作节点：[所有的Server]
sudo systemctl restart haproxy keepalivedsudo systemctl enable --now haproxy keepalived
现在来查看vip是否生成
操作节点：[Server1]
ip a

初始化第一个server
操作节点:[server1]
要注意你选的是哪个容器工具哈
要注意你选的是哪个容器工具哈
要注意你选的是哪个容器工具哈
基于docker
如果有参数的值数错了，可以改一下，然后重新运行命令即可

--docke一定要放在所有参数的最前面

sudo \INSTALL_K3S_MIRROR=cn \K3S_TOKEN=qianyiosQianyios12345 \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \./k3s-install.sh --docker --cluster-init - server  \--system-default-registry &quot;registry.cn-hangzhou.aliyuncs.com&quot; \--tls-san= 192.168.48.200 
qianyiosQianyios12345是作为集群间的共享密钥，可自定义

自行完成 9.解决非root用户使用kubectl等命令显示无命令的办法
基于containerd
如果有参数的值数错了，可以改一下，然后重新运行命令即可
sudo \INSTALL_K3S_MIRROR=cn \K3S_TOKEN=qianyiosQianyios12345 \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \./k3s-install.sh  --cluster-init - server \--system-default-registry &quot;registry.cn-hangzhou.aliyuncs.com&quot; \--tls-san= 192.168.48.200
qianyiosQianyios12345是作为集群间的共享密钥，可自定义
自行完成 9.解决非root用户使用kubectl等命令显示无命令的办法
其他server加入集群
基于docker
如果有参数的值数错了，可以改一下，然后重新运行命令即可
--docke一定要放在所有参数的最前面
sudo \INSTALL_K3S_MIRROR=cn \K3S_TOKEN=qianyiosQianyios12345 \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \./k3s-install.sh --docker \--server https://192.168.48.101:6443  \--system-default-registry &quot;registry.cn-hangzhou.aliyuncs.com&quot; \--tls-san= 192.168.48.200
qianyiosQianyios12345是作为第一个server1共享出来的密钥
–server https://192.168.48.101:6443 改成serve1的ip地址即可
自行完成 9.解决非root用户使用kubectl等命令显示无命令的办法
基于containerd
如果有参数的值数错了，可以改一下，然后重新运行命令即可
sudo \INSTALL_K3S_MIRROR=cn \K3S_TOKEN=qianyiosQianyios12345 \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \./k3s-install.sh \--system-default-registry &quot;registry.cn-hangzhou.aliyuncs.com&quot; \--server https://192.168.48.101:6443  \--tls-san= 192.168.48.200
qianyiosQianyios12345是作为第一个server1共享出来的密钥
–server https://192.168.48.101:6443 改成serve1的ip地址即可
自行完成 9.解决非root用户使用kubectl等命令显示无命令的办法
其他Agent加入集群
基于docker
如果有参数的值数错了，可以改一下，然后重新运行命令即可
--docke一定要放在所有参数的最前面
sudo \INSTALL_K3S_MIRROR=cn \K3S_TOKEN=qianyiosQianyios12345 \K3S_URL=https://192.168.48.101:6443  \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \./k3s-install.sh --docker - agent 
基于containerd
如果有参数的值数错了，可以改一下，然后重新运行命令即可
sudo \INSTALL_K3S_MIRROR=cn \K3S_TOKEN=qianyiosQianyios12345 \K3S_URL=https://192.168.48.101:6443  \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \./k3s-install.sh - agent
这时候在Server可以查看node情况
sudo kubectl get nodes

安装dashboard
在基础或高可用k3s都可以使用
操作节点:[Server1]
sudo wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml &amp;&amp; \sudo sed -i &#x27;s/kubernetesui\/dashboard:v2.7.0/registry.cn-hangzhou.aliyuncs.com\/qianyios\/dashboard:v2.7.0/g&#x27; recommended.yaml sleep 3sudo sed -i &#x27;s/kubernetesui\/metrics-scraper:v1.0.8/registry.cn-hangzhou.aliyuncs.com\/qianyios\/metrics-scraper:v1.0.8/g&#x27; recommended.yaml sudo sed -i &#x27;/targetPort: 8443/a\      nodePort: 30001&#x27; recommended.yaml sudo sed -i &#x27;/nodePort: 30001/a\  type: NodePort&#x27; recommended.yaml
运行pod
sudo kubectl apply -f recommended.yaml
创建token
#创建service account并绑定默认cluster-admin管理员群角色#创建用户sudo kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard#用户授权sudo kubectl create clusterrolebinding dashboard-admin \--clusterrole=cluster-admin \--serviceaccount=kubernetes-dashboard:dashboard-admin#临时获取用户Token（默认只有 30 分钟 ）sudo kubectl create token dashboard-admin -n kubernetes-dashboard#永久获取用户Tokensudo cat &lt;&lt;EOF | sudo kubectl apply -f -apiVersion: v1kind: Secretmetadata:  name: dashboard-admin-token  namespace: kubernetes-dashboard  annotations:    kubernetes.io/service-account.name: dashboard-admintype: kubernetes.io/service-account-tokenEOFKUBECONFIG_FILE=&quot;dashboard-kubeconfig.yaml&quot;# 自动获取 API Server 地址APISERVER=$(sudo kubectl config view --minify -o jsonpath=&#x27;&#123;.clusters[0].cluster.server&#125;&#x27;)# 自动获取 CA 证书CA_CERT=$(sudo kubectl config view --raw -o jsonpath=&#x27;&#123;.clusters[0].cluster.certificate-authority-data&#125;&#x27;)# 自动从 Secret 获取 Token（你提到的正确方式）TOKEN=$(sudo kubectl get secret dashboard-admin-token -n kubernetes-dashboard -o jsonpath=&#x27;&#123;.data.token&#125;&#x27; | base64 --decode)# 生成 kubeconfig 文件sudo cat &lt;&lt;EOF &gt; $&#123;KUBECONFIG_FILE&#125;apiVersion: v1kind: Configclusters:  - name: kubernetes    cluster:      server: $&#123;APISERVER&#125;      certificate-authority-data: $&#123;CA_CERT&#125;users:  - name: dashboard-admin    user:      token: $&#123;TOKEN&#125;contexts:  - name: dashboard-context    context:      cluster: kubernetes      user: dashboard-admincurrent-context: dashboard-contextEOFecho &quot;✅ kubeconfig 文件已生成：$&#123;KUBECONFIG_FILE&#125;&quot;
这时候就会提示你
✅ kubeconfig 文件已生成：dashboard-kubeconfig.yaml
你就把这个文件上传到dashboard的kubeconfig就可以免密登入了
这个192.168.48.200是高可用地址

高可用模拟宕机测试
查看dashboard部署在哪个节点
sudo kubectl get pods -A -l k8s-app=kubernetes-dashboard -o widesudo kubectl get nodes
我这里显示的是dashboard部署在Server1
那么我们就对Server1进行powerof关机，来模拟宕机看看dashboard能否被k3s自动调度到其他节点
但是我发现pod还在running的状态,且server1的状态还是ready


server2 已经被标记为 NotReady
说明 Kubernetes 已感知到它不可用（可能是关机、网络不通或 kubelet 崩溃等），但：

如果 Pod 的副本数是 1，Kubernetes 不会自动创建新的 Pod 。
默认的节点失联容忍时间较长（5分钟），所以即使节点 NotReady，也不会立刻触发 Pod 驱逐。


方案一 等待五分钟
经过漫长等待，dashboard的pod进行了重新分配

kubectl get pods -A -l k8s-app=kubernetes-dashboard -o wide
经过查看已经被调度到了Agent1节点

而且，原本server1的高可用ip，现已经漂移到了server2了，同样也可以用高可用ip访问k3s内的所有pod

结论：高可用实验，实验成功，且页面可以正常访问
方案二 手动删除 Pod 强制重建（推荐测试）
由于刚刚经过方案一的测试，被调度到了Agent1，所以这次对Agent1进行模拟宕机，然后手动删除pod
sudo kubectl delete pod -n kubernetes-dashboard pod名字

经过手动删除，立马触发自动调度，已经被调度到了Server3节点
结论：高可用实验，实验成功，且页面可以正常访问
方案三 缩短节点失联容忍时间（适用于生产环境）
如果你希望 Kubernetes 更快地响应节点故障，可以在 K3s 启动参数中添加以下内容：
--node-monitor-grace-period=20s \--pod-eviction-timeout=30s
⚠️ 注意：这会影响整个集群的行为，适用于生产环境或需要快速故障恢复的场景。
修改启动参数
如果你在安装的时候有些参数输入错了，或者想改，可以在这里改
首先，停止 K3s 服务以避免在更新过程中出现冲突：
sudo systemctl stop k3s
修改k3s启动参数
sudo vi /etc/systemd/system/k3s.service

假设你的–tls-san的高可用地址输入错了，要改成别的，你就改完，记得保存
然后删除旧证书
sudo rm -f /var/lib/rancher/k3s/server/tls/serving-kube-apiserver*sudo rm -f /var/lib/rancher/k3s/server/tls/server*
重启服务
sudo systemctl daemon-reloadsudo systemctl start k3s
卸载K3S
官方教程：Uninstalling K3s | K3s
卸载Server
要从服务器节点卸载 K3s，请运行：
/usr/local/bin/k3s-uninstall.sh
卸载Agent
要从代理节点卸载 K3s，请运行：
/usr/local/bin/k3s-agent-uninstall.sh
解决非root用户使用kubectl等命令显示无命令的办法
这时候运行查看节点命令，提示找不到命令
sudo kubectl get nodes
一看发现只有具体到指定路径才可以正常运行，并且用户和权限组都是root

这时候在普通用户查看visudo
sudo visudo

一看地址，他并没有/usr/local/bin的路径，所以普通用户是没办法继承root的路径的，所以你要设置普通用户默认的环境变量（生成环境，建议仔细斟酌要不要添加，不然就只能用绝对路径）
/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

这时候再次运行k3s命令
sudo kubectl get nodessudo crictl images


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>Centos 7</tag>
        <tag>K3s</tag>
      </tags>
  </entry>
  <entry>
    <title>基于阿里云容器服务构建私人docker镜像</title>
    <url>/posts/54563/</url>
    <content><![CDATA[
基于阿里云容器服务构建私人docker镜像
前情提要
你是否被国外镜像拉取速度慢的情况所折磨，甚至一个小时都未必能下载好或者下载不到。
接下来我们通过阿里云容器服务构建这些海外镜像为私人镜像

使用之前，要有自己的阿里云账户，且要有一定的dockerfile知识
Docker笔记 - 严千屹博客 (qianyios.top)
原理图：

创建海外镜像Dockerfile
Codeup · 企业级代码管理平台 (aliyun.com)
注册账号过程不解释，自行探索

添加文件

创建dockerfile
假设这里是你遇到的海外镜像，拉取很慢的镜像

提交之后我们就可以看见文件了

去个人中心设置里设置你的克隆密码


创建个人访问令牌


下面构建镜像要用到这个个人访问令牌（这个只会出现一次，要记下来）
阿里容器服务构建镜像
容器镜像服务 (aliyun.com)
同样，注册账号过程不做解释
创建个人实例，并点击

创建命名空间

创建镜像仓库（仓库名称=docker镜像名称），并点击进去

绑定代码源（一定要勾选海外构建镜像）

用codeup获取的克隆账号


填写信息

构建镜像，过一会他会自己构建好

获取镜像地址

registry.cn-hangzhou.aliyuncs.com/qianyios/pause:3.1
验证
docker pull registry.cn-hangzhou.aliyuncs.com/qianyios/pause:3.1

你会发现已经下载好了！还很快！
以下是一个k8s部署实例文件，有时候会因为镜像拉取慢，我们就可以替换为我们自己构建的镜像


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>华为配置专题</title>
    <url>/posts/1c79e7d0/</url>
    <content><![CDATA[
华为配置专题
基础配置（送分题）：system-view、sysname、interface vlanif、写个IP地址
高频配置（一定要会）：VLAN 、DHCP、ACL、策略路由、NAT、静态/默认/RIP/OSPF
偏门的考点（尽力而为）：IPv6、4G、WLAN（缺少这个案例）、IPSEC（不全，少了动态配置）
topo下载：华为配置专题案例
基础配置
基础配置视图

华为登入配置
&lt;AR&gt; system-view //进入系统模式
[AR]sysname Huawei //设备命名为Huawei
[Huawei] telnet server enable //开启设备telnet功能
[Huawei] user-interface vty 0 4 //开启登录端口0-4
[Huawei-ui-vty0-4] protocol inbound telnet //通过telnet协议登录
[Huawei-ui-vty0-4] authentication-mode aaa //认证方式为aaa
[Huawei] aaa //启用aaa
[Huawei-aaa] local-user admin123 password admin123 //配置用户名密码
[Huawei-aaa] local-user admin123 service-type telnet //用户用于telnet
[Huawei-aaa] local-user admin123 privilege level 15 //用户等级为15
[Huawei-aaa] quit //退出来
[Huawei]
VLAN与VLANIF地址配置
&lt;HUAWEI&gt; system-view //进入系统模式
[HUAWEI] sysname Switch //交换机重命名
[Switch] vlan 100 //创建vlan 100（批量创建命令：vlan batch 10 20）
[Switch-vlan100] quit //退出vlan模式
[Switch] interface gigabitethernet 0/0/1 //进入接口
[Switch-GigabitEthernet0/0/1] port link-type access //把交换机接口模式设置为access
[Switch-GigabitEthernet0/0/1] port default vlan 100 //把接口划入vlan100
[Switch-GigabitEthernet0/0/1] quit //退出
[Switch] interface vlanif 100 //进入三层vlanif接口
[Switch-Vlanif100] ip address 172.16.1.1 24 //配置IP地址
[Switch-Vlanif100] quit //退出
[Switch]
DHCP配置命令
&lt;SwitchA&gt; system-view //进入系统模式
[SwitchA] dhcp enable //启用dhcp服务
[SwitchA] ip pool 1 //系统视图下创建IP地址池1
[SwitchA-ip-pool-1] network 10.1.1.0 mask 255.255.255.128 //配置地址池范围
[SwitchA-ip-pool-1] dns-list 10.1.1.1 //配置DNS
[SwitchA-ip-pool-1] gateway-list 10.1.1.1 //配置PC电脑网关
[SwitchA-ip-pool-1] excluded-ip-address 10.1.1.2 //保留IP地址
[SwitchA-ip-pool-1] excluded-ip-address 10.1.1.4 //保留IP地址
[SwitchA-ip-pool-1] lease 10 // 配置租期
[SwitchA-ip-pool-1] quit //退出
配置VLANIF10接口下的客户端从全局地址池ip pool 1中获取IP地址。
[SwitchA] interface vlanif 10 //进入VLAN10接口
[SwitchA-Vlanif10] ip address 10.1.1.254 255.255.255.128 //配置VLAN网关
[SwitchA-Vlanif10] dhcp select global/interface //全局或接口dhcp服务器
ACL访问控制列表配置
//配置时间段，周一到周五早上8:30到下午18:00
[Huawei] time-range workday 8:30 to 18:00 working-day
//启用编号为2000的ACL
[Huawei] acl 2000
//只允许192.168.1.10这一个用户在工作日可以telnet交换机
[Huawei-acl-basic-2000] rule permit source 192.168.1.10 0 time-range workday
//这个地方rule deny可以不用写，acl在这种场景下最后隐含有一条deny any的语句
[Huawei-acl-basic-2000] rule deny
//进入虚拟接口0-4
[Huawei] user-interface vty 0 4
//应用ACL，只允许匹配acl数据流的的用户telent登陆交换机，没有被permit的全部被deny
[Huawei-ui-vty0-4] acl 2000 inbound
NAT地址转换配置
[Router] nat address-group 1 2.2.2.100 2.2.2.200 //配置NAT地址池1
[Router] nat address-group 2 2.2.2.80 2.2.2.83 //配置NAT地址池2
[Router] acl 2000 //配置ACL2000
//设置ACL200编号为5的规则，允许上述源地址通过
[Router-acl-basic-2000] rule 5 permit source 192.168.20.0 0.0.0.255
[Router-acl-basic-2000] quit
[Router] acl 2001 //配置ACL2001
//设置ACL2001中编号为5的规则，允许上述地址通过
[Router-acl-basic-2001] rule 5 permit source 10.0.0.0 0.0.0.255
[Router-acl-basic-2001] quit
[Router] interface gigabitethernet 3/0/0 //进入接口
//将设置ACL2000匹配的源地址，转换为地址池1的地址，并且不开启端口NAT
[Router-GigabitEthernet3/0/0] nat outbound 2000 address-group 1 no-pat
//将设置ACL2001匹配的源地址，转换为地址池2的地址
[Router-GigabitEthernet3/0/0] nat outbound 2001 address-group 2
[Router-GigabitEthernet3/0/0] quit
[Router]
动态主机配置协议 DHCP


DHCP概念和工作原理



基于接口地址池的DHCP配置

基于全局地址池的DHCP配置

DHCP接口地址池配置案例

DHCP全局地址池配置案例

DHCP案例
三层交换机跨vlan通信案例


system-viewsysname SWvlan batch 10 20interface vlanif10ip address 192.168.10.254 24interface vlanif20ip address 192.168.20.254 24interface GigabitEthernet 0/0/1port link-type accessport default vlan 10interface GigabitEthernet 0/0/2port link-type accessport default vlan 20

DHCP接口和全局配置
还是上面那个图，左边实现dhcp接口地址池，右边实现全局配置
我们就用SW交换机来做dhcp服务端给下面的客户端分配IP地址

基础vlan配置
system-viewsysname SWvlan batch 10 20interface vlanif10ip address 192.168.10.254 24interface vlanif20ip address 192.168.20.254 24interface GigabitEthernet 0/0/1port link-type accessport default vlan 10interface GigabitEthernet 0/0/2port link-type accessport default vlan 20
vlanif10 实现接口dhcp
system-viewdhcp enableinterface vlanif 10dhcp select interfacedhcp server dns-list 8.8.8.8#排除192.168.10.101到192.168.10.253这些地址，其他可分配dhcp server excluded-ip-address 192.168.10.101 192.168.10.253dhcp server lease day 8

对PC1的E0/0/1接口进行抓包，并且设置dhcp模式并且应用


这时候他就可以从可选地址里从大到小分配地址，这是华为才会的，思科则会从小到大进行分配

vlanif20 实现全局dhcp
system-viewinterface vlanif 20dhcp select globalqip pool 20network 192.168.20.0 mask 24gateway-list 192.168.20.254dns-list 9.9.9.9lease 10excluded-ip-address 192.168.20.151 192.168.20.253qdis cu

对pc2进行设置自动获取ip模式


成功，而且两台机子互通

对左边的接口dhcp进行修改成全局dhcp
system-viewinterface vlan 10undo dhcp select interface#输入ydhcp select globalqip pool 10network 192.168.10.0 mask 24gateway-list 192.168.10.254dns-list 8.8.8.8lease 8


这次我没有排除ip地址，所以他会从这个网段下的所有可用地址进行分配
扩展知识点
7.8.4 Option 43 应用举例
7.8.7 DHCP中继（DHCP Relay）
7.8.8 DHCP Snooping
ACL原理与配置
ACL概述


ACL工作原理
ACL由若干条permit或deny语句组成。
• 每条语句就是该ACL的一条规则，每条语句中的permit或deny就是与这条规则相对应的处理动作。

规则编号

匹配符


ACL分类

基本ACL和高级ACL

ACL的匹配机制

ACL的匹配顺序及匹配结果

ACL的应用位置

入站 (Inbound)及出站 (Outbound)方向

ACL配置及应用


高级ACL命令（1）


进阶案例：使用高级ACL限制不同网段的用户互访 (1)


1.下列选项中，哪一项才是一条合法的基本ACL的规则?©
A. rule permit ip
B. rule deny ip
C. rule permit source any
D. rule deny tcp source any
2.高级ACL可以基于哪些条件来定义规则?

源IP地址
目的IP地址
源端口号
目的端口号
协议类型（如TCP、UDP、ICMP等）
数据包优先级（DSCP、ToS等）
ICMP消息类型和代码
时间范围（基于时间的ACL）
片段标记（非首片分片）
数据包长度
TCP标志位（如SYN、ACK、FIN等）
VLAN ID
源MAC地址
目的MAC地址
以太网协议类型

章节总结
ACL是一种应用非常广泛的网络技术。它的基本原理是:配置了ACL的网络设备根据事先设定好的报文匹配规则对经过该设备的报文进行匹配，然后对匹配上的报文执行事先设定好的处理动作。这些匹配规则及相应的处理动作是根据具体的网络需求而设定的。处理动作的不同以及匹配规则的多样性，使得ACL可以发挥出各种各样的功效。
ACL技术总是与防火墙、路由策略、QoS、流量过滤等其他技术结合使用。
在本章节中，主要介绍了ACL的相关技术知识，包括:ACL的作用，ACL的组成、匹配和分类、通配符的使用方法，以及ACL的基本配置及应用。
案例

某公司为保证财务数据安全，禁止研发部门访问财务服务器，但总裁办不受限制
中间的路由器添加了一个百兆网卡接口




配置路由器各个接口的ip地址
Router
system-viewsysname Routerinterface GigabitEthernet 0/0/0ip address 192.168.1.254 24interface GigabitEthernet 0/0/1ip address 192.168.2.254 24interface GigabitEthernet 0/0/2ip address 1.1.1.254 24interface Ethernet 4/0/0ip address 192.168.3.254 24
互联网
system-viewsysname hlwinterface GigabitEthernet 0/0/0ip address 1.1.1.1 24
为啥ping不通

因为pc1的流量可以走到互联网这台路由器，但是回来的流量没有啊，因为互联网这台路由器里没有pc1网段的路由

system-viewip route-static 0.0.0.0 0 1.1.1.254


现在所有网络都通了，接下来要配置acl实现访问策略了

某公司为保证财务数据安全，禁止研发部门访问财务服务器，但总裁办不受限制

Router
system-viewacl 3000#禁止源网段192.168.1.0访问192.168.3.100rule 10 deny ip source 192.168.1.0 0.0.0.255 destination 192.168.3.100 0.0.0.0#允许源网段192.168.2.0访问192.168.3.100rule 20 permit ip source 192.168.2.0 0.0.0.255 destination 192.168.3.100 0.0.0.0#禁止互联网的所有流量访问192.168.3.100rule 30 deny ip source any destination 192.168.3.100 0.0.0.0
配置完后测试一下PC1能不能访问财务部

还能访问为什么，因为你的acl还没有应用到接口上

Router
system-viewinterface Ethernet 4/0/0traffic-filter outbound acl 3000

目的达成，且两台pc访问互联网是没有影响的

网络地址转换NAT
技术背景
随着Internet的发展和网络应用的增多，有限的IPv4公有地址已经成为制约网络发展的瓶颈。为解决
这个问题，NAT（Network Address Translation，网络地址转换）技术应需而生。
NAT技术主要用于实现内部网络的主机访问外部网络。
本章节我们将了解NAT的技术背景， 学习不同类型NAT的技术原理、使用场景。
NAT概述

私有IP地址

NAT技术原理

静态NAT
静态NAT原理

静态NAT转换示例

静态NAT配置介绍

静态NAT配置示例

动态NAT
动态NAT原理

动态NAT转换示例 (出去流量)

动态NAT转换示例 (回程流量)

动态NAT配置介绍

动态NAT配置示例

NAPT、Easy-IP
NAPT原理


NAPT转换示例 (出去流量)

NAPT转换示例 (回程流量)

NAPT配置示例

Easy IP

Easy IP配置示例

NAT Server
NAT Server使用场景

NAT Server转换示例

NAT Server配置示例

何种NAT转换可以让外部网络主动访问内网服务器？
1.静态NAT通过固定映射内网IP到公网IP实现外网访问，适合长期对外服务的服务器；
2.端口映射(NAPT)则通过指定公网IP和端口号将流量转发到内网服务器，节省公网IP资源且灵活性高。
NAPT相比较于No-PAT有哪些优点？
1.NAPT相比No-PAT的优点在于能够通过端口复用技术让多个内网设备共享一个公网IP地址，从而大幅节省公网IP资源，同时支持更多内网设备访问外网。而No-PAT仅进行IP地址转换，每个内网设备需占用一个独立的公网IP，资源利用率低。
章节总结

NAT案例


基本配置
AR1
system-viewun in ensysname AR1interface GigabitEthernet 0/0/1ip address 12.1.1.1 24interface GigabitEthernet 0/0/0ip address 192.168.1.254 24
AR2
system-viewun in ensysname AR2interface GigabitEthernet 0/0/0ip address 12.1.1.254 24
静态NAT
功能就是内网地址和公网地址一一对应
有两个方法
1.在接口下配
AR1
system-viewun in eninterface GigabitEthernet 0/0/1 nat static global 12.1.1.2 inside 192.168.1.1
2.不在接口下配，也就是系统视图下,也就是说你虽然创建了静态nat规则，但是你没有在接口应用
system-viewun in ennat static global 12.1.1.2 inside 192.168.1.1interface GigabitEthernet 0/0/1 nat static enable
二者的区别就是，你要是在接口试图下配置nat是不用输入nat static enable
12.1.1.2是公网地址，这里的12.1.1.0/24是公网网段，也就是说 global我定义这个网段的哪一个地址都行，生产中没有这么多地址，这里就以12.1.1.2到12.1.1.10为公网地址

动态nat
有公网地址池概念，也是内网地址和公网一一对应，但是公网地址会有空闲的
删掉静态NAT的配置
AR1
在接口下配
system-viewun in eninterface GigabitEthernet 0/0/1 undo nat static global 12.1.1.2 inside 192.168.1.1
不在接口下配
system-viewun in enundo nat static global 12.1.1.2 inside 192.168.1.1interface GigabitEthernet 0/0/1 undo nat static enable
快开始配置NAT
AR1
1.创建地址池，编号为1，范围是12.1.1.2到12.1.1.10
2.创建地址转换acl
3.在接口下应用acl,并且设置为no-pat（非端口地址转换）
system-viewun in ennat address-group 1 12.1.1.2 12.1.1.10acl 2000rule 10 permit source 192.168.1.0 0.0.0.255interface GigabitEthernet 0/0/1nat outbound 2000 address-group 1 no-pat

NAPT
接口地址转换，带端口的,这种情况只适合公网地址不多的情况，一般小公司只有一个公网地址的情况，要给公司所有内网电脑去使用，咋办呢，这时候就有了带端口的地址转换
删除动态nat的配置，前面加个undo就行了
AR1
system-viewun in ennat address-group 1 12.1.1.2 12.1.1.2acl 2000rule 10 permit source 192.168.1.0 0.0.0.255interface GigabitEthernet 0/0/1nat outbound 2000 address-group 1


Easy-Ip
适用于没有固定的公网ip的网络，原理跟NAPT一样也是基于端口转换,把规则应用到出接口就行了，以出接口的地址为公网地址做转换
system-viewun in enacl 2000rule 10 permit source 192.168.1.0 0.0.0.255interface GigabitEthernet 0/0/1nat outbound 2000
pc1一样可以ping外网

NAT Server
上面几种方法都是内访问外，我们内网主动去访问外网的，而且有了nat技术，可以保护内网地址，这样外网就不知道我们的内网地址，只知道一个公网的nat地址，那么接下来就有一种是外访问内的
一般叫它端口映射，比如说pc2是一台服务器，你部署了一个网页，那我们就把内网80端口映射到外网的80端口，这样用户就可以通过公网：80访问到我们的服务器
system-view un in eninterface GigabitEthernet 0/0/1nat server protocol tcp global 12.1.1.2 80 inside 192.168.1.2 80

VRRP
VRRP概述与原理
VRRP技术背景：单网关面临的问题

VRRP概述

VRRP的基本概念



VRRP典型应用
VRRP负载分担

VRRP与MSTP结合应用

VRRP监视上行端口

VRRP与BFD联动

VRRP基本配置




VRRP案例

设置pc

acsw
system-viewun in ensysname acswvlan 10interface GigabitEthernet 0/0/3port link-type accessport default vlan 10interface GigabitEthernet 0/0/1port link-type trunkport trunk allow-pass vlan allinterface GigabitEthernet 0/0/2port link-type trunkport trunk allow-pass vlan all
coresw1
system-viewun in ensysname coresw1vlan batch 10 100interface GigabitEthernet 0/0/1port link-type trunkport trunk allow-pass vlan allinterface GigabitEthernet 0/0/3port link-type trunkport trunk allow-pass vlan allinterface GigabitEthernet 0/0/2port link-type accessport default vlan 100interface vlanif 10ip address 192.168.10.252 24interface vlanif 100ip address 192.168.100.1 30
coresw2
system-viewun in ensysname coresw2vlan batch 10 200interface GigabitEthernet 0/0/1port link-type trunkport trunk allow-pass vlan allinterface GigabitEthernet 0/0/3port link-type trunkport trunk allow-pass vlan allinterface GigabitEthernet 0/0/2port link-type accessport default vlan 200interface Vlanif 10ip address 192.168.10.253 24interface Vlanif 200ip address 192.168.200.1 30
配置VRRP
coresw1
interface Vlanif 10vrrp vrid 10 virtual-ip 192.168.10.254 vrrp vrid 10 priority 120vrrp vrid 10 preempt-mode timer delay 20
coresw2
这里就配置优先级了，默认100
interface Vlanif 10vrrp vrid 10 virtual-ip 192.168.10.254 

AR1配置ip地址
system-viewsysname AR1un in eninterface GigabitEthernet 0/0/0ip address 100.1.1.2 30interface GigabitEthernet 0/0/1ip address 192.168.100.2 30interface GigabitEthernet 0/0/2ip address 192.168.200.2 30
互联网配置ip地址
system-viewsysname hlwun in eninterface GigabitEthernet 0/0/0ip address 100.1.1.1 30
测试pc1是否能与互联网互通

发现不能互通，经tracert检测，路由只到了coresw1，这时候查看coresw1的路由表

发现并没有指向100.1.1.0/30网段的路由，也就是说他只停留在了coresw1，这时候可以配一个默认路由，将所有流量指向AR1的入接口，两个核心交换机都要指向出口路由器AR1
coresw1
ip route-static 0.0.0.0 0 192.168.100.2
coresw2
ip route-static 0.0.0.0 0 192.168.200.2
测试pc1是否能与互联网互通


结论：抓包信息可以看到，路由可以到达互联网，但是互联网没有返程路由也就是pc1的路由，那就要给互联网配置返程路由指向AR1的如接口
hlw
ip route-static 192.168.10.0 24 100.1.1.2 
测试pc1是否能与互联网互通

结论：返程路由192.168.10.0到了AR1，查询自己的路由表没有这个表项就丢弃了，所以要在AR1配置静态路由指向coresw1，而且也要指向coresw2
AR1
ip route-static 192.168.10.0 24 192.168.100.1ip route-static 192.168.10.0 24 192.168.200.1
测试pc1是否能与互联网互通

测试给coresw1设置到90优先级
interface vlanif10vrrp vrid 10 priority 90

测试延迟抢占
当优先级变回120时，coresw1并没有那么快进行抢占，因为前面配置了一个20秒的延迟，等延迟过后，就会变回master
vrrp vrid 10 preempt-mode timer delay 20

测试监视上行接口
coresw1
监视g0/0/2，如果g0/0/2  down掉了，那么coresw1的优先级就会减30变成90，则coresw2的优先级默认就是100就会大于coresw1，变成master
interface vlanif10vrrp vrid 10 track interface g0/0/2 reduced 30interface GigabitEthernet 0/0/2shutdown

浮动路由与BFD监视
目的：在出口路由器配置默认路由指向联通和电信，主要实现，当电信挂了，默认路由就会指向电信路由器，以此达到浮动路由的目的

配置IP地址
route2000
system-viewsysname routeun in eninterface GigabitEthernet 0/0/0ip address 192.168.10.254 24interface GigabitEthernet 0/0/1ip address 192.168.20.254 24interface GigabitEthernet 0/0/2ip address 12.1.1.1 30interface GigabitEthernet 4/0/0ip address 13.1.1.1 30
dianxin2001
system-viewsysname dianxinun in eninterface GigabitEthernet 0/0/0ip address 12.1.1.2 30interface GigabitEthernet 0/0/1ip address 100.1.1.1 30
liantong2002
system-viewsysname liantongun in eninterface GigabitEthernet 0/0/0ip address 13.1.1.2 30interface GigabitEthernet 0/0/1ip address 200.1.1.1 30
互联网2003
system-viewsysname hlwun in eninterface GigabitEthernet 0/0/0ip address 100.1.1.2 30interface GigabitEthernet 0/0/1ip address 200.1.1.2 30interface LoopBack 0ip address 22.22.22.22 32
配置OSPF
dianxing，互联网，liantong
这样做的目的，会把自己直连的路由宣告出去
ospf 1area 0network 0.0.0.0 0.0.0.0qdis ospf peer brief


电信路由器，已经学到了ospf区域内设备的直连路由，其他设备也一样
配置nat
接着，在route（角色：出口路由器）配置nat，假设他不具备固定的公网ip地址，把内网路由转换到出接口
route
acl 2000rule 10 permit source 192.168.10.0 0.0.0.255rule 20 permit source 192.168.20.0 0.0.0.255qinterface GigabitEthernet 0/0/2nat outbound 2000interface GigabitEthernet 4/0/0nat outbound 2000
还要配两条静态路由指向运营商，考点：出口路由器都要配置默认路由
ip route-static 0.0.0.0 0 12.1.1.2ip route-static 0.0.0.0 0 13.1.1.2

由于我使用的路由器是AR2000系列的，中低端路由器，他是不支持基于包来做流量的负载均衡，所以，当进行tracert 22.22.22.22你会发现，他的每一次路径都是走联通的，了解一下就行了
配置浮动路由
原先配置了两条等价路由，现在为了达到，开头的目的，就必须先把走向联通的路由器的优先级调高，这样越小越优先，就会走向电信路由器
route
ip route-static 0.0.0.0 0 13.1.1.2 preference 100dis ip rou

这时候你会发现，只有一条指向电信的路由了，这就是浮动路由，调成100的优先级就不优先了，就会被隐藏起来，当你电信的默认路由，掉了，指向联通的路由才会显现出来，接下来就模拟这个浮动路由的实验
int g0/0/2shutdowndis ip rou

这时候指向联通路由就会出来，保证了业务的连续性
int g0/0/2undo shutdown

等几秒就会回来了
而且，不管是以下图片哪个接口down了，他都会实现浮动路由到联通

BFD监测网络状态
背景
背景：我们前面不是配置了指向电信的默认路由吗，但是现实情况，不可能说出口路由器直接与运营商去互联的，中间肯定经过很多层交换机，路由器啥的，现在就来模拟这个实验，就会影响我们的浮动路由

这时候将电信路由器的g0/0/0口断掉，查看情况
dianxin
sysun in eninterface GigabitEthernet 0/0/0shutdown

配置bfd
接下来就在dianxin和route配置bfd，来达到双向检测，如果监测断掉之后，就可以实现浮动路由
route
bfdbfd 1 bind peer-ip 12.1.1.2 source-ip 12.1.1.1 autocommitdis bfd session all
dianxin
配置这个之前，你要把原来的g0/0/0接口打开，不然即使创建了规则，也会报错
int g0/0/0undo shutdownbfdbfd 1 bind peer-ip 12.1.1.1 source-ip 12.1.1.2 autobfd 1commitdis bfd session all
接口没打开，你配置了就会报这个错，接口开启来就行了，没事


接下来在指向电信路由器的默认路由配置bfd监视，当这个bfd 1的监视器出现了down的情况，那么这个默认路由就会被删掉，从而实现浮动路由
route
ip route-static 0.0.0.0 0 12.1.1.2 track bfd-session 1
当dianxin的g0/0/0接口断掉之后

走的是联通服务器

路由综合实验RIP OSPF BGP
目的：中间通过ospf或rip，然后经过bgp之间学习路由，达到PC可以互通

配置IP地址
PC1、PC2

R1
system-viewsysname R1un in eninterface GigabitEthernet 0/0/0ip address 100.1.1.254 24interface GigabitEthernet 0/0/1ip add 12.1.1.1 30int g0/0/2ip add 13.1.1.1 30dis ip in br

R2
syssysname R2un in enint g0/0/1ip add 12.1.1.2 30int g0/0/2ip add 23.1.1.1 30dis ip in br

R3
syssysname R3un in enint g0/0/2ip add 23.1.1.2 30int g0/0/1ip add 13.1.1.2 30int g0/0/0ip add 34.1.1.1 30dis ip in br
R4
sys sysname R4un in enint g0/0/0ip add 34.1.1.2 30int g0/0/1ip add 45.1.1.1 30dis ip in br
R5
sys sysname R5un in enint g0/0/0ip add 200.1.1.254 24int g0/0/1ip add 45.1.1.2 30dis ip in br
配置RIP(与osfp二选一)
配置rip宣告网段时，例如R2宣告12.1.1.0的网段时，会报错，一般生产中都会配置ripv2，由于他的路由自动汇总，他会把12.1.1.0汇总到更大的网络12.0.0.0，因为这些地址都是A类地址，所以rip就默认自动汇总到12.0.0.0，目的是减少路由表的大小，提高网络的可扩展性。
A类地址范围：1.0.0.0-126.0.0.0   /8
B类地址范围：128.0.0.0-191.255.0.0  /16
C类地址范围：192.0.0.0-223.255.255.0  /24
R1
sysrip 1version 2network 100.0.0.0network 12.0.0.0network 13.0.0.0
R2
sysrip 1version 2network 12.0.0.0network 23.0.0.0
R3
sysrip 1version 2network 13.0.0.0network 23.0.0.0
配置完检查一下3个路由的路由表
dis ip rou



RIP区域内的路由器，通过rip学习到各个路由器宣告的路由，但是如果其中有自己直连的路由信息，就没有rip路由显示，本身就是直连的嘛，只会学习到没有的
配置OSPF(与RIP二选一)
由于我前面配置了rip，这里要演示ospf我就删掉前面的rip配置，如果你没有敲前面的rip就不用执行下面的删除rip配置的命令
R1,R2,R3
sysundo rip 1#会提示你是否删除，你输入Y就行了Warning: The RIP process will be deleted. Continue?[Y/N]y
接下来正式配置ospf,它宣告网段的方式和rip是不一样的，要加上反掩码，而且没有rip的自动汇总，是可以直接写网段的
R1
sysospf 1area 0network 100.1.1.0 0.0.0.255network 13.1.1.0 0.0.0.3network 12.1.1.0 0.0.0.3
R2
sysospf 1area 0network 23.1.1.0 0.0.0.3network 12.1.1.0 0.0.0.3
R3
sysospf 1area 0network 23.1.1.0 0.0.0.3network 13.1.1.0 0.0.0.3
dis ospf peer brief



这时候查看路由表
dis ip rou



ospf也和rip一样，就可以学习到其他路由器宣告的，非直连的路由
以上是RIP和OSPF的配置过程，就是了配置IGP协议进行一个自治系统（AS）内部交换路由信息的协议
IGP可以进一步划分为两类：距离矢量路由协议和链路状态路由协议。常见的IGP协议包括RIP（路由信息协议）、OSPF（开放最短路径优先）和IS-IS（中间系统到中间系统）等。

RIP：使用跳数作为路径选择的度量标准，适用于小型网络。
OSPF：基于链路状态的协议，使用Dijkstra算法来计算最短路径，适用于中型到大型网络。
IS-IS：也是一种基于链路状态的协议，类似于OSPF，通常用于ISP和大型企业网络中。

配置bgp

BGP：边界网关协议（Border Gateway Protocol）是一种具体的外部网关协议，是EGP的一种实现。BGP是互联网中使用最广泛的外部网关协议，用于在自治系统之间进行路由信息交换。取代EGP

R3 他在BGP AS 100 ,要填写领居也就是对端的信息，构建领居关系
sysbgp 100peer 34.1.1.2 as-number 200
R4
sysbgp 200peer 34.1.1.1 as-number 100peer 45.1.1.2 as-number 200
R4
sysbgp 200peer 45.1.1.1 as-number 200
等待片刻，查看R4是否与R3和R5建立领居关系

Established相当于ospf的full，图中已经成功建立了，查看R4的路由表
dis ip rou

这时候并没有bgp的路由，为啥，因为bgp的特殊性，需要手动宣告和将其他路由器学到的路由导入到本路由器才行
接下来会演示两种方法
手动宣告路由
R5将200.1.1.0的网段宣告进IBGP里，在BGP中，通过ipv4-family unicast命令进入IPv4单播地址族视图后，可以使用network命令来宣告哪些IPv4网络将被加入到BGP的路由表中，并向其他BGP对等体发布。这样其他bgp路由器就可以学习到这个网段
sysbgp 200ipv4-family unicastnetwork 200.1.1.0 24


一看，R3和R4都学到了这个路由
引进路由
首先我们可以先，pc各自的网关，是否有对方网段的路由
dis ip rou


也就是说，pc1无法ping   pc2的，各自的网关都没有对端的路由。
但是R3刚刚不是经过R5手动宣告路由学到了200.1.1.0的路由吗，这是R3要将右边学到的bgp路由信息引入到ospf里
sysospf 1import-route bgp
如果是rip，就进入到rip version2里一样的输入import-route bgp
这时候查看R1的路由表,已经学习到了200的路由了
dis ip rou

但是我们pc1要ping到pc2，pc2的网关R5没有100的路由啊，是ping不了的
dis ip rou

所以我们要在R3上，将ospf 进程1的路由引入到bgp里
sysbgp 100import-route ospf 1
查看R4,R5的路由表，发现下一跳不可达到，就没办法加入到R5的路由表

这时候有两个办法，一个是配置R5配置静态路由指向34.1.1.0网段或者R4在bgp里设置R5的下一跳为45.1.1.2口
第一种R5
ip route-static 34.1.1.0 30 45.1.1.1
这时候就能学到100的路由了

第二种R4
R5删除静态路由
undo ip route-static 34.1.1.0 30 45.1.1.1
R4
bgp 200peer 45.1.1.2 next-hop-local

这时候R5已经学到100的路由了
这里就有个疑问，为什么我配置的是啥45.1.1.2为下一跳，而表里显示的45.1.1.1
因为45.1.1.2指的是R4这个IBGP里的bgp对等体，就是直接把45.1.1.2当做了R4，经过R4配置了peer 45.1.1.2 next-hop-local那么R4在把自己学到的ospf里路由通告给R5时，会把R5里的下一跳改成自己R4接口（45.1.1.1）
验证pc互通


当我们对R1的g0/0/2进行断开之后
R1
sysint g0/0/2shutdown

这时候就会多了两条路径，就是走的是R2
实验成功
综合实验[DHCP NAT BFD 策略路由等]


配置IP地址和vlan，dhcp

acsw
system-viewsysname acseundo info-center enablevlan batch 10 20interface GigabitEthernet 0/0/1port link-type accessport default vlan 10interface GigabitEthernet 0/0/2port link-type accessport default vlan 20interface GigabitEthernet 0/0/3port link-type trunkport trunk allow-pass vlan 10 20
coresw
system-viewsysname coreswundo info-center enablevlan batch 10 20 30interface GigabitEthernet 0/0/3port link-type trunkinterface GigabitEthernet 0/0/1port link-type accessport default vlan 30interface vlanif 10 ip address 192.168.10.254 24interface vlanif 20ip address 192.168.20.254 24interface vlanif 30ip address 192.168.30.254 24
coresw
配置dhcp服务器
排除192.168.10.2-192.168.10.253，因为我们pc1只有一台嘛，我们只想让他分配一个地址
dhcp enableip pool 10network 192.168.10.0 mask 24gateway-list 192.168.10.254dns-list 8.8.8.8lease day 8excluded-ip-address 192.168.10.2 192.168.10.253interface vlanif10dhcp select globalqip pool 20network 192.168.20.0 mask 24gateway-list 192.168.20.254dns-list 114.114.114.114lease day 8excluded-ip-address 192.168.20.2 192.168.20.253interface vlanif20dhcp select global

也能互通

pc1  ping pc2的过程
1.首先判断是否在同一网段
很明显不在同一网段，那么acsw就会把路由交给网关（核心交换机coresw）
2.那么核心交换机就会查自己的路由表，发现有目的网段，就会把流量转发过去

route
system-viewsysname routeun in enint g0/0/1ip address 192.168.30.3 24int g0/0/0ip address 12.1.1.3 24int g0/0/2ip address 23.1.1.3 24
dianxin
system-viewsysname dianxinun in enint g0/0/1ip address 100.1.1.1 24int g0/0/0ip address 12.1.1.1 24
liantong
system-viewsysname liantongun in enint g0/0/1ip address 100.1.1.2 24int g0/0/2ip address 23.1.1.2 24
配置静态路由或动态路由(三选一)
背景
此时测试pc1 ping route

因为流量虽然能到route但是没有返程路由，就ping不通

以下有三种方法三选一
静态路由
这时候要配置静态ip指向10网段的路由，下一跳是192.168.30.254
route
ip route-static 192.168.10.0 24 192.168.30.254

pc1能ping通route了

这是pc2也不能ping了

这时候也要配一个静态路由指向20的网段
ip route-static 192.168.20.0 24 192.168.30.254

配置Rip
route
rip 1version 2network 192.168.30.0
coresw
rip 1version 2network 192.168.10.0network 192.168.20.0network 192.168.30.0
这时候route通过coresw宣告的路由学习到了10和20到网段里


两个都能ping通
配置ospf
route
sysospf 1area 0network 192.168.30.0 0.0.0.255
coresw
sysospf 1area 0network 192.168.10.0 0.0.0.255network 192.168.20.0 0.0.0.255network 192.168.30.0 0.0.0.255
现在二者已经构建了领居关系

这时候route已经通过ospf学习到了路由


配置运营商之间的动态路由
电信路由器和联通路由器配置环回口，相当于，各种接了一台服务器
dianxin
sysint loopback 0ip address 1.1.1.1 24
liantong
sysint loopback 0ip address 2.2.2.2 24
由于我们在测试，dianxin和liantong的路由器里的其他路由，如果互ping肯定是不通的，现实环境下，两个肯定互通，因为可能运行着bgp等协议，我们这里测试，我们就配一个动态路由，让彼此学习到各自的路由就行了，可以选择rip，ospf，bgp
配置RIP
dianxin 由于是a类地址，只能宣告a类网段，这是rip的规则，我前面一个实验也有说过
rip 1version 2network 100.0.0.0network 12.0.0.0network 1.0.0.0
liantong
rip 1version 2network 100.0.0.0network 23.0.0.0network 2.0.0.0
各自都学到了自己的路由

配置nat
在出口路由器配置nat的easyip
route
sysacl 2000rule 10 permit source 192.168.10.0 0.0.0.255rule 20 permit source 192.168.20.0 0.0.0.255qint g0/0/0nat outbound 2000int g0/0/2nat outbound 2000
这时候我们去跟踪路由信息，pc ping 电信的1.1.1.1环回口

发现他只能到网关，因为网关的路由表里没有目的1.1.1.1的路由，到了网关这里无路可走，就会丢弃，但是我们要让他可走，就要把流量丢给他的老大也就是出口路由器

一般我们在生产中会在网关配置默认路由到出口路由器的

coresw
ip route-static 0.0.0.0 0 192.168.30.3

配置了不通，但是至少，我们的流量已经到了出口路由器了，但是出口路由器的路由表也没有目的1.1.1.1的路由

考试重点：在出口路由器都要配置默认路由指向运营商的


这时候我们回顾一下，这里的场景一，默认走电信，出故障走联通

配置场景1
所以我们要配置两个静态路由，则设置电信的优先级最小，因为越小越优先，普通的默认路由优先级是60
route
#指向电信的默认路由，设置低于60的优先级ip route-static 0.0.0.0 0 12.1.1.1 preference 50#指向联通的默认路由ip route-static 0.0.0.0 0 23.1.1.2

这里只有一条，当这条指向电信的默认路由故障了，才会走向联通，但是还要配置bfd去进行监测才能实现故障转移
配置BFD
dianxin和route配置bfd检测对端接口活动情况
route
undo ip route-static 0.0.0.0 0 12.1.1.1bfdbfd dianxin bind peer-ip 12.1.1.1 source 12.1.1.3 autocommit
dianxin
bfdbfd dianxin bind peer-ip 12.1.1.3 source 12.1.1.1 autocommit
等待几秒建立连接
dis bfd session all

route配置bfd策略
如果 BFD 检测到链路故障下一跳12.1.1.1不可达，这条静态路由会自动从路由表中移除，从而避免流量被错误转发。
ip route-static 0.0.0.0 0 12.1.1.1 preference 50 track bfd-session dianxin
模拟一下12.1.1.1  down状态
dianxin
int g0/0/0shutdown
route
dis ip rou

这时候走向电信的默认路由因为出现故障而被bfd自动删除，取而代之的就是走向联通的路由
配置场景2
教学楼流量走电信出口，宿舍楼流程走联通出口。
做这个要删掉前面配置的静态路由哦，每个场景都是独立的
route
undo ip route-static 0.0.0.0 0 12.1.1.1undo ip route-static 0.0.0.0 0 23.1.1.2
配置acl，匹配流量
以下都在出口路由器配置
acl 2010rule 10 permit source 192.168.10.0 0.0.0.255acl 2020rule 10 permit source 192.168.20.0 0.0.0.255
流分类
分类出两个流量，一个是教学的一个是宿舍的，每个流量应用相应的acl
traffic classifier jiaoxueif-match acl 2010traffic classifier susheif-match acl 2020
流行为
创建两种种行为，重定向电信的下一跳是12.1.1.1，重定向联通的下一跳是23.1.1.2
traffic behavior re-dianxinredirect ip-nexthop 12.1.1.1traffic behavior re-liantongredirect ip-nexthop 23.1.1.2
流策略
我们要将流行为和流策略绑定在一起
traffic policy pclassifier jiaoxue behavior re-dianxinclassifier sushe behavior re-liantong
入接口应用策略路由
interface GigabitEthernet 0/0/1traffic-policy p inbound

总结一下流量过程，假设我们是教学楼，我们去ping电信服务器1.1.1.1时，流量到了出口路由器的入接口，那么经过流分类中的教学里的acl识别到是教学楼10网段的流量，就成功匹配，然后进行流行为，重定向流量到dianxin，下一跳指向12.1.1.1（电信接口）。其他流量也一样
配置场景3
访问电信的服务器走电信出口，访问联通服务器走联通出口。
route
删除前面的配置
interface GigabitEthernet 0/0/1undo traffic-policy inboundqundo traffic policy pundo traffic behavior re-dianxinundo traffic behavior re-liantongundo traffic classifier jiaoxueundo traffic classifier susheundo acl 2010undo acl 2020
就其实和前面那个场景是差不多，唯一不同的是在acl我们只需要定义目的地址是联通或者电信的就行了，源流量设置为any，这样教学楼和宿舍不管源地址是啥，只要ping哪个服务器，就会被带到哪个路由器
acl 3010rule 10 permit ip source any destination 1.1.1.0 0.0.0.255acl 3020rule 10 permit ip source any destination 2.2.2.2 0.0.0.255traffic classifier dianxinif-match acl 3010traffic classifier liantongif-match acl 3020traffic behavior re-dianxinredirect ip-nexthop 12.1.1.1traffic behavior re-liantongredirect ip-nexthop 23.1.1.2traffic policy pclassifier dianxin behavior re-dianxinclassifier liantong behavior re-liantongint g0/0/1traffic-policy p inbound

IPsec实验





看着上面的表配，这里就不说了
配置静态路由和NAT

一般出口路由器都要配置一条默认路由指向运营商

R1
sysun in enip route-static 0.0.0.0 0 100.1.1.2acl 2000rule 10 permit source 192.168.10.0 0.0.0.255qint g0/0/1nat outbound 2000
R2
sysun in enip route-static 0.0.0.0 0 200.1.1.2acl 2000rule 10 permit source 192.168.20.0 0.0.0.255qint g0/0/1nat outbound 2000

可以ping通互联网，但是不能互通pc，这时候我们要通过配置ipsec把这两个内网打通
手动配置ipsec
配置acl
定义需要保护的数据流，那条路的路由需要保护，就圈起来
R1
sysacl 3000rule 10 permit ip source 192.168.10.0 0.0.0.255 destination 192.168.20.0 0.0.0.255 
R2
sysacl 3000rule 10 permit ip source 192.168.20.0 0.0.0.255 destination 192.168.10.0 0.0.0.255
配置ipsec提议
对你即将要建立的隧道，设置认证和加密算法，采用什么算法和方式进行保护，一般默认都是隧道模式，加密用的是加密算法
R1
ipsec提议名字cd（成都）
认证算法采用sha2-256
加密算法采用des
ipsec proposal cdesp authentication-algorithm sha2-256esp encryption-algorithm desdis ipsec proposal

R2
ipsec prpoosal bjesp authentication-algorithm sha2-256esp encryption-algorithm desdis ipsec proposal

配置ipsec手动安全策略

这里涉及两个SA编号，在R1的角度上入方向是54321，出方向是12345
那到了R2，你就不能按照R1的来了，你要反着写，入方向是12345，出方向是54321。
都是反着来，一一对应的，千万别搞错
R1
sysipsec policy chengdu 10 manualsecurity acl 3000proposal cdtunnel local 100.1.1.1tunnel remote 200.1.1.1sa spi inbound esp 54321sa string-key inbound esp cipher qianyiossa spi outbound esp 12345sa string-key outbound esp cipher qianyios
//配置IPSEC策略chegndu，方式为手动
//包含acl3000的流量
//采用ipsec 提议 cd
//配置隧道本地地址100.1.1.1
//配置隧道远端地址 200.1.1.1
//配置入方向SA编号54321
//配置入方向SA的认证密钥为qianyios
//配置出方向SA编号12345
//配置出方向SA的认证密钥为qianyios
R2
sysipsec policy beijing 10 manualsecurity acl 3000proposal bjtunnel local 200.1.1.1tunnel remote 100.1.1.1sa spi inbound esp 12345sa string-key inbound esp cipher qianyiossa spi outbound esp 54321sa string-key outbound esp cipher qianyios
//配置IPSEC策略beijing，方式为手动
//包含acl3000的流量
//采用ipsec 提议 bj
//配置隧道本地地址 200.1.1.1
//配置隧道远端地址 100.1.1.1
//配置入方向SA编号12345
//配置入方向SA的认证密钥为qianyios
//配置出方向SA编号54321
//配置出方向SA的认证密钥为qianyios
在接口上应用
R1
sysint g0/0/1ipsec policy chengdu
R2
sysint g0/0/1ipsec policy beijing
隧道已经建立了，去测试主机是否互通

发现不行
问题分析:R1上配置了ACL2000和ACL3000，其中ACL2000用于匹配内部需要NAT的地址，ACL3000用于匹配需要通过VPN隧道加密的流量，两条ACL重合，即ACL2000 会把需要进行 VPN 传递的流量匹配出来，进行 NAT。


我们不可能说所有电脑都要建立隧道，只有部分电脑是需要建立隧道的，acl2000在前期本身就是给所有内部流量去做nat转换的这其中就包括要建立隧道的pc1流量，那我们就应该重新做acl，换成高级acl 3001来做nat转换，就是在acl 3001，把需要建立隧道的流量设置成deny，其他流量运行通过，应用在nat转换就好了，这样子就不会匹配他了，就可以匹配acl3000做隧道转换了，这个理论在R2也要重新推翻acl 2000去重做acl 3001
R1
sysint g0/0/1undo nat outbound 2000qundo acl 2000acl 3001rule 10 deny ip source 192.168.10.0 0.0.0.255 destination 192.168.20.0 0.0.0.255rule 20 permit ip int g0/0/1nat outbound 3001
acl 3001就是禁止192.168.10.0到192.168.20.0到流量，接着匹配其他ip允许通过，然后应用在出口路由器的nat转换
在R2也是一样的道理，删掉acl 2000，重做acl 3001
R2
sysint g0/0/1undo nat outbound 2000qundo acl 2000acl 3001rule 10 deny ip source 192.168.20.0 0.0.0.255 destination 192.168.10.0 0.0.0.255rule 20 permit ipint g0/0/1nat outbound 3001
已经可以互通了


手动配置ipsec成功
防火墙配置

这里的防火墙用的是USG6000V，首次运行需要防火墙导入包，要下载一个vdi下载地址，下载完导入即可
这里首先要输入的默认用户名：admin
然后回车，再输入默认密码：Admin@123
再回车，会询问你是否要修改密码，输入“y”表示yes
再回车，先输入一遍旧密码
再回车，输入新密码 qianyios@123
再回车，再输入一次新密码

配置要求
(1)防火墙接口的 IP地址如拓扑所示，将接口划入相应的安全区域。
(2)内网主机PC1可以主动访问Internet，但Internet无法主动访问PC1。
(3)出口防火墙进行NAT，NAT 公网地址池 100.1.1.10-100.1.1.20。
(4)Internet 可以通过公网地址100.1.1.100/24访问目的地址为 192.168.2.100/24的内部 Web 服务。
配置IP地址
本次实验就只用pc1做实验，都一样的，pc2就配置ip地址了



Internet
syssysname Internetun in enint g0/0/0ip add 100.1.1.2 24
Firewalld
前面的配置防火墙密码登入之后，再进行下面的操作
syssysname Firewallun in enint g1/0/1ip add 192.168.1.254 24int g1/0/2ip add 192.168.2.254 24int g1/0/3ip add 100.1.1.1 24
防火墙安全域
firewalld
把接口加入到相应的区域去
sysfirewall zone trustadd int g1/0/1firewall zone dmzadd int g1/0/2firewall zone untrustadd int g1/0/3
安全策略的配置
firewalld
syssecurity-policyrule name trust_to_untrustsource-zone trustdestination-zone untrustsource-address 192.168.1.0 24destination-address anyaction permitqq
做完之后，pc1是无法访问互联网的，因为出口没有做NAT,开启端口转换
nat address-group natgroup1mode patsection 0 100.1.1.10 100.1.1.20qnat-policyrule name policy_nat1source-zone trustdestination-zone untrustsource-address 192.168.1.0 24destination-address anyaction source-nat address-group natgroup1qq
pc1成功访问互联网


配置pc访问dmz
firewall
配置trust流量运行访问dmz区域，也就是服务器区域
syssecurity-policyrule name trust_to_dmzsource-zone trustdestination-zone dmzaction permit

配置端口映射
firewall
把内网web服务映射到公网地址
sysnat server policy_web protocol tcp global 100.1.1.100 80 inside 192.168.2.100 80
这时候测试internet通过公网100.1.1.100:80去访问服务器的80端口

不行，因为untrust没有放行流量到dmz区域
syssecurity-policyrule name untrust_to_dmzsource-zone untrustdestination-zone dmzdestination-address 192.168.2.100 32action permit

路由策略


配置ip地址
R1
syssysname R1un in enint g0/0/0ip address 12.1.1.1 30int LoopBack0ip add 10.1.1.1 24int LoopBack1ip add 10.1.2.1 24int LoopBack2ip add 10.1.3.1 24
R2
syssysname R2un in enint g0/0/0ip add 12.1.1.2 30int g0/0/1ip add 23.1.1.1 30
R3
syssysname R3un in enint g0/0/1ip add 23.1.1.2 30int LoopBack 0ip add 30.1.1.1 24int LoopBack 1ip add 30.1.2.1 24int LoopBack 2ip add 30.1.3.1 24
配置RIP和OSPF
R1
rip 1version 2network 12.0.0.0network 10.0.0.0
R2
rip 1version 2network 12.0.0.0qospf 1area 0network 23.1.1.0 0.0.0.3
R3
ospf 1area 0network 23.1.1.0 0.0.0.3network 30.1.1.0 0.0.0.255network 30.1.2.0 0.0.0.255network 30.1.3.0 0.0.0.255
在R2上查看路由表，已经通过RIP和OSPF分别学到了3条路由
dis ip rou

配置路由引入和路由策略以及设置开销
R2
acl 2000rule 10 permit source 30.1.1.0 0.0.0.255rule 20 permit source 30.1.2.0 0.0.0.255rule 30 permit source 30.1.3.0 0.0.0.255route-policy 10 permit node 10if-match acl 2000apply cost 10rip 1version 2import-route ospf 1 route-policy 10
R1上查看从何R2学到的rip路由，开销是11，也就是前面在R2引入ospf的路由，本身开销是10，而R2本身算一跳，所以开销就是11

R2上配置路由引入和路由策略，进行路由过滤
acl 2001rule 10 permit source 10.1.2.0 0.0.0.255rule 20 permit source 10.1.3.0 0.0.0.255qroute-policy 20 permit node 20if-match acl 2001ospf 1import-route rip route-policy 20
R3上查看路由学习情况,通过路由策略过滤,R3只学到了10.1.2.0/24和10.1.3.0/24,没有学到 10.1.1.0/24，实验成功!


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>基于K8S的CICD系统实现</title>
    <url>/posts/40462/</url>
    <content><![CDATA[
基于K8S的CICD系统实现
前情提要
在此声明，这个项目需要有32G+以上的运行内存，不然继续不了，32G运行内存电脑勉强能（阉割版流畅）运行，而且也是关了一台harbor2才能勉强运行，关了没事的，因为harbor是高可用集群，有一台harbor1就行了。
系统拓扑图
运行内存严格按照我的以下的内存规格来填写，这是勉强能运行的配置参数，如果内存多的话自行加多即可
K8S集群



主机名
ip1（NAT）
系统
磁盘1
磁盘2
内存
cpu




master1
192.168.48.101
OpenEuler-22.04-LTS
100G
100G
6.2G
2v2c


master2
192.168.48.102
OpenEuler-22.04-LTS
100G
100G
6.2G
2v2c


master3
192.168.48.103
OpenEuler-22.04-LTS
100G
100G
6.2G
2v2c


node01
192.168.48.104
OpenEuler-22.04-LTS
100G

9.9G
2v2c


高可用ip
192.168.48.200








harbor集群



角色
主机名
ip
系统
资源最低要求




Harbor1nginxKeepalived1
harbor1
192.168.48.106
OpenEuler-22.04-LTS
CPU：1核 内存：1G 硬盘：40G


Harbor2nginxKeepalived2
harbor2
192.168.48.107
OpenEuler-22.04-LTS
CPU：1核 内存：1G 硬盘：40G


postgresqlRedisNFS共享
zujian
192.168.48.108
OpenEuler-22.04-LTS
CPU：1核 内存：1G 硬盘：40G


高可用ip
192.168.48.100






系统架构图

系统流程图

部署K8S高可用集群
OpenEuler-部署K8S高可用集群（内部etcd） - 严千屹博客 (qianyios.top)
部署ceph集群

注意：原文章是部署在master1，node01，node02，由于硬件原因，现在需要部署在三台master，所以原文章开头加硬盘，你也只需要加在三台master上，请自己注意在2.1-2位置修改集群名称和硬盘名称。包括要下载的镜像，就在三台master下载就行了，接着你就可以继续做了

基于K8S1.28.2实验rook部署ceph - 严千屹博客 (qianyios.top)
以下进行cephfs存储做存储声明即Storageclass，方便后续jenkins和gitlab调用
创建cephfs文件系统
操作节点[master1]
cdcd rook/deploy/examplescat &gt;rook-cephfs.yaml &lt;&lt; &quot;EOF&quot;apiVersion: ceph.rook.io/v1kind: CephFilesystemmetadata:  name: rook-cephfs      #修改名字  namespace: rook-ceph spec:  metadataPool:    replicated:      size: 3      requireSafeReplicaSize: true # 参数指示是否要求副本数量必须是偶数    parameters:      compression_mode:        none  dataPools:    - name: replicated      failureDomain: host      replicated:        size: 3        requireSafeReplicaSize: false      parameters:        compression_mode:          none  preserveFilesystemOnDelete: true #当删除CephFilesystem资源时，是否保留Ceph集群中的实际文件系统。若设为true则保留，方便后续恢复使用。  metadataServer:    activeCount: 3    activeStandby: true    placement:      podAntiAffinity:        preferredDuringSchedulingIgnoredDuringExecution:          - weight: 100            podAffinityTerm:              labelSelector:                matchExpressions:                  - key: app                    operator: In                    values:                      - rook-ceph-mds              topologyKey: topology.kubernetes.io/zone    priorityClassName: system-cluster-critical    livenessProbe:      disabled: false    startupProbe:      disabled: falseEOFkubectl apply -f rook-cephfs.yaml kubectl get pod -n rook-ceph | grep rook-cephfs
可能要一分钟才会创建好
为什么我能直接在k8s直接运行ceph指令，而不进入pod，自行去第二步开通看部署ceph集群的

快捷链接：在K8S中直接调用出ceph命令
[root@master1 examples]# ceph fs statusrook-cephfs - 0 clients===========RANK      STATE            MDS          ACTIVITY     DNS    INOS   DIRS   CAPS 0        active      rook-cephfs-f  Reqs:    0 /s    10     13     12      0 1        active      rook-cephfs-c  Reqs:    0 /s     0      0      0      0 2        active      rook-cephfs-d  Reqs:    0 /s    10     12     11      00-s   standby-replay  rook-cephfs-a  Evts:    0 /s     0      0      0      02-s   standby-replay  rook-cephfs-b  Evts:    0 /s     0      0      0      01-s   standby-replay  rook-cephfs-e  Evts:    0 /s     0      0      0      0         POOL             TYPE     USED  AVAIL rook-cephfs-metadata   metadata     0   94.9Grook-cephfs-replicated    data       0   94.9GMDS version: ceph version 17.2.6 (d7ff0d10654d2280e08f1ab989c7cdf3064446a5) quincy (stable)[root@master1 examples]# ceph fs lsname: rook-cephfs, metadata pool: rook-cephfs-metadata, data pools: [rook-cephfs-replicated ][root@master1 examples]#
rook-cephfs-replicated是下面存储声明需要用到的 pool
创建Storageclass
是k8s调用ceph的声明，通过这个Storageclass才能去调用ceph
cd cd rook/deploy/examplescat &gt; rook-cephfs-sc.yaml &lt;&lt;&quot;EOF&quot;apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: rook-cephfs-scprovisioner: rook-ceph.cephfs.csi.ceph.comparameters:  clusterID: rook-ceph #ceph集群命名空间  fsName: rook-cephfs  #cephfs，刚刚创建的文件系统  pool: rook-cephfs-replicated   #刚刚创建的fs所包含的pool  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node  csi.storage.k8s.io/node-stage-secret-namespace: rook-cephreclaimPolicy: DeleteallowVolumeExpansion: trueEOFkubectl apply -f rook-cephfs-sc.yaml kubectl get sc

部署harbor进群
Harbor共享存储高可用 - 严千屹博客 (qianyios.top)
配置仓库地址
部署好后，需要将K8s各个节点，和harbor各个节点都要进行配置仓库地址
操作节点：[所有节点]
vim /etc/docker/daemon.json
# 客户端默认使用的是https协议，所以需要对docker做以下修改,在文件末尾添加insecure-registries[root@qianyios ~]# vim /etc/docker/daemon.json&#123;   ................  &quot;registry-mirrors&quot;: [],#无关紧要，不用看,  &quot;insecure-registries&quot;: [ &quot;192.168.48.100&quot; ],#重要加这行，别忘了如果他不是最后一行一定要在末尾加逗号 ................&#125;# 修改后，重启docker使其生效systemctl daemon-reloadsystemctl restart docker# 利用docker info查看是否添加上[root@qianyios ~]# docker infoContainers: 10 Running: 1 Paused: 0 Stopped: 9Images: 37... Experimental: false Insecure Registries:  192.168.48.100   ###要确保有这个才行  127.0.0.0/8 Registry Mirrors:

测试免密登入
接下来测试免密登入，第一次登入要密码，第二次登入就不用了，第二次之后就是免密登入
docker login 192.168.48.100

免密登入成功！
部署jenkins
操作节点:[masetr1]
创建rabc验证
cd cd jenkinscat &gt; jenkins-rabc.yaml &lt;&lt; &quot;EOF&quot;apiVersion: v1kind: ServiceAccountmetadata:  name: jenkins-admin  namespace: jenkins---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: jenkins-adminroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-adminsubjects:- kind: ServiceAccount  name: jenkins-admin  namespace: jenkinsEOFkubectl apply -f jenkins-rabc.yaml
创建jenkins用户的secret
cd cd jenkinscat &gt;jenkins-admin-secret.yaml &lt;&lt; &quot;EOF&quot;apiVersion: v1kind: Secretmetadata:  name: jenkins-admin-secret  namespace: jenkins  annotations:    kubernetes.io/service-account.name: jenkins-admintype: kubernetes.io/service-account-tokenEOFkubectl apply -f jenkins-admin-secret.yamlkubectl get secret -n jenkins[root@master1 jenkins]# kubectl get secret -n jenkinsNAME                   TYPE                                  DATA   AGEjenkins-admin-secret   kubernetes.io/service-account-token   3      15s
创建pvc
#创建命名空间cdkubectl create namespace jenkinsmkdir jenkinscd jenkinscat &gt; pvc.yaml &lt;&lt; EOFkind: PersistentVolumeClaimapiVersion: v1metadata:  name: jenkins-pvc  namespace: jenkinsspec:  storageClassName: rook-cephfs-sc  #这个是第4步部署完ceph集群创建的Storageclass  resources:    requests:      storage: 3Gi  accessModes:  - ReadWriteManyEOFkubectl apply -f pvc.yamlkubectl get pvc -n jenkins

部署jenkins
自行在harbor仓库创建好cicd仓库

构建jenkins镜像
docker pull jenkins/jenkins
可能会出现拉取缓慢现象，也可以用以下方法进行构建
基于阿里云容器服务构建私人docker镜像 - 严千屹博客 (qianyios.top)
构建之后就要进行拉取下载打标签，以下是我自己构建的，你可以用这个
#下载镜像docker pull registry.cn-hangzhou.aliyuncs.com/qianyios/jenkins:latest#打标签docker tag registry.cn-hangzhou.aliyuncs.com/qianyios/jenkins:latest 192.168.48.100/cicd/jenkins:latest#查看镜像[root@master1 jenkins]# docker images | grep jenk192.168.48.100/cicd/jenkins                                            latest    786c9e8a0cb8   6 days ago      472MBregistry.cn-hangzhou.aliyuncs.com/qianyios/jenkins                     latest    786c9e8a0cb8   6 days ago      472MB[root@master1 jenkins]##推送镜像到harbor仓库docker push 192.168.48.100/cicd/jenkins:latest

#给所有master节点打上master标签kubectl label node master1 k8s-type=masterkubectl label node master2 k8s-type=masterkubectl label node master3 k8s-type=master
cd cd jenkinscat &gt;jenkins-deploy.yaml &lt;&lt; &quot;EOF&quot;apiVersion: apps/v1kind: Deploymentmetadata:  name: jenkins  namespace: jenkinsspec:  replicas: 1  selector:    matchLabels:      app: jenkins-server  template:    metadata:      labels:        app: jenkins-server    spec:      securityContext:            fsGroup: 995            runAsUser: 1000      serviceAccountName: jenkins-admin      nodeSelector:        k8s-type: master      containers:        - name: jenkins          image: 192.168.48.100/cicd/jenkins:latest          imagePullPolicy: IfNotPresent          resources:            limits:              memory: &quot;2Gi&quot;              cpu: &quot;1000m&quot;            requests:              memory: &quot;500Mi&quot;              cpu: &quot;500m&quot;          ports:            - name: httpport              containerPort: 8080            - name: jnlpport              containerPort: 50000          livenessProbe:            httpGet:              path: &quot;/login&quot;              port: 8080            initialDelaySeconds: 90            periodSeconds: 10            timeoutSeconds: 5            failureThreshold: 5          readinessProbe:            httpGet:              path: &quot;/login&quot;              port: 8080            initialDelaySeconds: 60            periodSeconds: 10            timeoutSeconds: 5            failureThreshold: 3          volumeMounts:            - name: jenkins-data              mountPath: /var/jenkins_home            - name: kubectl              mountPath: /usr/bin/kubectl            - name: kube-config              mountPath: /root/.kube            - name: docker              mountPath: /usr/bin/docker            - name: docker-sock              mountPath: /var/run/docker.sock      volumes:        - name: jenkins-data          persistentVolumeClaim:              claimName: jenkins-pvc        - name: kubectl          hostPath:            path: /usr/bin/kubectl        - name: kube-config          hostPath:            path: /root/.kube        - name: docker          hostPath:            path: /usr/bin/docker        - name: docker-sock          hostPath:            path: /var/run/docker.sockEOFkubectl apply -f jenkins-deploy.yaml kubectl get pods -n jenkins
创建svc
cd cd jenkinscat &gt;jenkins-svc.yaml &lt;&lt; &quot;EOF&quot;apiVersion: v1kind: Servicemetadata:  name: jenkins-service  namespace: jenkins  annotations:      prometheus.io/scrape: &#x27;true&#x27;      prometheus.io/path:   /      prometheus.io/port:   &#x27;8080&#x27;spec:  selector:    app: jenkins-server  type: NodePort  ports:  - port: 8080    targetPort: 8080    nodePort: 32000    name: httpport  - name: jnlpport    port: 50000    targetPort: 50000EOFkubectl apply -f jenkins-svc.yamlkubectl get svc -n jenkins
访问页面
http://192.168.48.200:32000/

kubectl get pods -n jenkins#查看你的pod名字替换下面的名字即可(jenkins-799dc7cd88-d2n4k)#进入容器kubectl exec -it -n jenkins jenkins-799dc7cd88-d2n4k -- bash#修改Update Center源sed -i &#x27;s#https://updates.jenkins.io/update-center.json#http://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json#g&#x27; /var/jenkins_home/hudson.model.UpdateCenter.xml #替换插件源地址： sed -i &#x27;s#https://updates.jenkins.io/download#http://mirrors.aliyun.com/jenkins#g&#x27; /var/jenkins_home/updates/default.json#替换谷歌地址： sed -i &#x27;s#http://www.google.com#http://www.baidu.com#g&#x27; /var/jenkins_home/updates/default.json#查看网页密码jenkins@jenkins-546cf958bd-kq5f2:~$ cat /var/jenkins_home/secrets/initialAdminPasswordebfc1b12835f43c8a8c2677728e4aa55

先点无，这里先不安装，后面再安装

使用admin账户继续

默认下一步

自行去设置修改admin密码

配置k8s等插件和实现功能
安装插件
安装插件：Git / Git Parameter/Pipeline/Config File Provider/Gitlab/Generic Webhook Trigger/Blue Ocean/Localization: Chinese /Kubernetes
在首页点击系统管理然后点击插件管理-----安装Kubernetes插件

安装之后需要等待他重启


配置K8s代理节点
配置jenkins连接kubernetes
点击系统管理-cloud

点击Clouds-添加New cloud

点击create

查看Kubernetes 服务证书 key
[root@master1 jenkins]# cat /etc/kubernetes/pki/ca.crt
-----BEGIN CERTIFICATE-----
MIIDBTCCAe2gAwIBAgIIcJ+z+Wx0m20wDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE
AxMKa3ViZXJuZXRlczAeFw0yNDA5MjgxMTI5NDJaFw0zNDA5MjYxMTM0NDJaMBUx
EzARBgNVBAMTCmt1YmVybmV0ZXMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEK
AoIBAQDYeRyEbAJRMqWyAmQf0LfK9wecWoQSyKYX+gHDUvZMaQK/nkAfrEme5Wot
J8lcODlNAj0OImRRKkCzfBwlZl5WMYH3Roonn8z9j4hkUAX/EgTwMun1q0G/D1yV
zxcRSUxxiFDKlRCVsPxIsuHIUvTrAHkU0qpz1S4cITisF9o9hCvqZZZ/5fCudn7I
sLlDhxzL1TAI5R2hqZKFdondpoGxYF5oc2wuk+0g/3GJZeaGEO/9p5ySX/glamil
e5npU/EvLsG4er2UQqB7dc9wfxOT2p0Qlj7UjHcqgY8E6ufhd7GqYVKNDXSmKXiP
BZI5ba6/kZJj0nsSz3GnVdhFxUD1AgMBAAGjWTBXMA4GA1UdDwEB/wQEAwICpDAP
BgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBT5qTcFwRrY/VSJtfAZK85ZCp2ViTAV
BgNVHREEDjAMggprdWJlcm5ldGVzMA0GCSqGSIb3DQEBCwUAA4IBAQDX+1q8NXKb
HWnfR75MORrQJ898B9M1FBoHfLRdsmmCJVrSQXbBbKn1zVoJL6YLdjDOx2NfWa8o
f1Y7KSme9Z6B4j57tGFQ/4LST4Cwk4PPh1v6nrwVesW6xE6ClHVZ0N1S4ggZi8ll
Wcv3ZCgdGrjSQm15xsRr4oN7XGY/B1kAZWU4defpcxhtIMtLQ7/m74hfYo/P5L/b
Wu1knzRSs1/Cna2GFkWx3BYfbG78ZPBIh17mN4vFAt/x8ZOZCJ+bb0Ey6upYnhjP
H2jiNrHcyJVnnDO4N6kMII9sh2n+gYvK45u+/Vw8nPElf72LkHFHOyQc8QD9opXF
LC9bmhoJuU9v
-----END CERTIFICATE-----


获取凭证

不是低版本，请跳到下一步！！！！
低版本获取凭证方法，低版本会自动生成一个secret

[root@master1 ~]# SECRET_NAME=$(kubectl get serviceaccount jenkins-admin  -o=jsonpath='&#123;.secrets[0].name&#125;' -n jenkins)
[root@master1 ~]# kubectl get secrets $SECRET_NAME  -o=jsonpath='&#123;.data.token&#125;' -n jenkins | base64 -d


k8s1.28获取凭证方法，需要手动创建secret
使用kubectl create token获取的token会过期
这里手动创建secret，并查看相关token值

编写jenkins-admin用户secret清单,并创建
cd /root/jenkins
cat &gt;jenkins-admin-secret.yaml&lt;&lt; &quot;EOF&quot;
apiVersion: v1
kind: Secret
metadata:
  name: jenkins-admin-secret
  namespace: jenkins
  annotations:
    kubernetes.io/service-account.name: jenkins-admin
type: kubernetes.io/service-account-token
EOF
 kubectl apply -f jenkins-admin-secret.yaml

查看secret
kubectl get secret -n jenkins


获取token
[root@master1 jenkins]# kubectl get secrets jenkins-admin-secret  -o=jsonpath='&#123;.data.token&#125;' -n jenkins | base64 -d
eyJhbGciOiJSUzI1NiIsImtpZCI6ImpaNkJTN3d0M0pFblBCSVBaaGFoNzdUTVNLMHZiTERxY09Ibmg4WEtFNTQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJqZW5raW5zIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImplbmtpbnMtYWRtaW4tc2VjcmV0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImplbmtpbnMtYWRtaW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI3MTg2Mzg1Mi1jNDkwLTQyMDEtYTA2OS1lM2ZhOTcyNTgwODYiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6amVua2luczpqZW5raW5zLWFkbWluIn0.sfofmctQos9fcEAf0hZEfEKN2WiG7GklOW6NMEdXbiJHkg5SNe8-MuJK4c4JtubyIA9IFhHo6PfQoFl8-2smuFaojjSKZKn2esJJ5enT_WNsbIdXmu4tg8a2571EikotiGKLpGHuLEZOLdTyswTsQW-FSQH7K16VmfUSzHCkKjV5eXNK3gpRj9p3voGWlBRQ3Jqfm6NWSKp_1_XZqFyi_KM8DOaMHhffN24ejwC2lvIzKNcZp80yY2rW4BkCI0dLN2GqUNlzB9QXYUslLtsigObukCdjY8EefCc34Rh68kq2UQVpXb9eTkyK50J9HEFJtLiuameD3Yx8gNE9xWJ4Vw


配置kubernetes相关信息



Name: 这个自定义， 默认的是kubernetes




Kubernetes URL: https://kubernetes.default- 这个一般是从你的 service account 自动配置的




Kubernetes 服务证书 key： 如果您有 Kubernetes 集群 CA 证书，您可以添加它以实现安全连接。您可以从 pod 位置获取证书/var/run/secrets/kubernetes.io/serviceaccount/ca.crt。如果您没有证书，您可以启用“ disable https certificate check”(禁用HTTPS证书检查)选项




Kubernetes Namespace: 一般是 default 除非你要在一个特殊的命名空间 ，否则不要动他.因为我的jenkins部署在了jenkin命名空间，就用了jenkins




Credentials（凭据）: 为了让 Jenkins 与 Kubernetes 集群通信，我们需要一个服务帐户令牌，该令牌具有在设置的命名空间中部署 pod 的权限




Jenkins URL: http://&lt;your_jenkins_hostname&gt;:port




Jenkins tunnel: &lt;your_jenkins_hostname&gt;:50000 - 这就是用来和 Jenkins 启动的 agent 进行交互的端口



修改名称和kubernetes地址及服务证书key

此Kubernetes由于Jenkins 服务器在同一个 Kubernetes 集群中运行，这里直接通过service进行通讯
此服务证书key就是前几步获取的key

添加凭证
点击添加-&gt;jenkins


选择Domain为全局凭据
类型选择为secret txt
范围选择为全局
secret就是刚才获取的凭据
描述 就是凭据的名称


点击添加
选择凭据为刚才添加的,点击测试链接，验证 Kubernetes 集群的连接性

配置 Jenkins URL 详细信息

对于在k8s集群内部运行的 Jenkins master，您可以使用 Kubernetes 集群的 Service 端点作为 Jenkins URL，因为代理 pod 可以通过内部服务 DNS 连接到集群
url语法：http://.:8080
jenkins-service.jenkins:8080
注意：Jenkins 通道（Jenkins tunnel）链接不需要加http://，否则无法正常通讯


创建成功

创建Jenkins代理镜像
拉取jenkins代理镜像并上传到harbor仓库
docker pull jenkins/inbound-agent
#如果下不到可以用我的
docker pull registry.cn-hangzhou.aliyuncs.com/qianyios/inbound-agent:latest
docker tag registry.cn-hangzhou.aliyuncs.com/qianyios/inbound-agent:latest 192.168.48.100/library/inbound-agent:latest
docker push 192.168.48.100/library/inbound-agent:latest
docker images | grep inbound

部署gitlab
创建gitlab的secret
#长度为8个字符[root@master1 ~]# echo -n &#x27;qianyios&#x27; | base64cWlhbnlpb3M=#qianyios将会成为gitlab页面的登入密码
mkdir /root/gitlabcd /root/gitlab#创建namespacekubectl create namespace gitlab
cat &gt;gitlab-secret-pwd.yaml &lt;&lt; &quot;EOF&quot;apiVersion: v1data:  password: cWlhbnlpb3M=  #换成刚刚前面生成的密码kind: Secretmetadata:  creationTimestamp: null  name: gitlab-pwd  namespace: gitlabEOFkubectl apply -f gitlab-secret-pwd.yamlkubectl get secret -n gitlab

创建pvc
cat &gt; gitlab-pvc.yaml &lt;&lt;&quot;EOF&quot;kind: PersistentVolumeClaimapiVersion: v1metadata:  name: gitlab-pvc-logs  namespace: gitlabspec:  storageClassName: rook-cephfs-sc  resources:    requests:      storage: 5Gi  accessModes:  - ReadWriteOnce---kind: PersistentVolumeClaimapiVersion: v1metadata:  name: gitlab-pvc-config  namespace: gitlabspec:  storageClassName: rook-cephfs-sc  resources:    requests:      storage: 1Gi  accessModes:  - ReadWriteOnce---kind: PersistentVolumeClaimapiVersion: v1metadata:  name: gitlab-pvc-data  namespace: gitlabspec:  storageClassName: rook-cephfs-sc  resources:    requests:      storage: 10Gi  accessModes:  - ReadWriteOnceEOF  kubectl apply -f gitlab-pvc.yaml kubectl get pvc -n gitlab

部署gitlab
#下载gitlab-ce镜像，镜像很大要1个G多 docker pull gitlab/gitlab-ce# 如果下不了用我构建的镜像docker pull registry.cn-hangzhou.aliyuncs.com/qianyios/gitlab:latest#镜像打标签#这个里我用了自己构建的镜像，你要是用这个gitlab/gitlab-ce自行替换docker tag registry.cn-hangzhou.aliyuncs.com/qianyios/gitlab:latest 192.168.48.100/cicd/gitlab:latest#推送镜像docker push 192.168.48.100/cicd/gitlab:latest
给node节点打上k8s-type=node标签kubectl label nodes node01 k8s-type=node
cat &gt; gitlab-deploy.yaml &lt;&lt; &quot;EOF&quot;apiVersion: apps/v1kind: Deploymentmetadata:  creationTimestamp: null  labels:    app: gitlab  name: gitlab  namespace: gitlabspec:  replicas: 1  selector:    matchLabels:      app: gitlab  strategy: &#123;&#125;  template:    metadata:      creationTimestamp: null      labels:        app: gitlab    spec:      nodeSelector:        k8s-type: node      containers:      - image: 192.168.48.100/cicd/gitlab:latest  #更换镜像        imagePullPolicy: IfNotPresent        name: gitlab-ce        env:        - name: GITLAB_ROOT_PASSWORD          valueFrom:            secretKeyRef:              name: gitlab-pwd              key: password        - name: GITLAB_ROOT_MAIL          value: xiaoohu2002@126.com        ports:        - name: gitlab80          containerPort: 80        - name: gitlab22          containerPort: 22        - name: gitlab443          containerPort: 443        volumeMounts:        - name: gitlab-logs          mountPath: /var/log/gitlab        - name: gitlab-config          mountPath: /etc/gitlab        - name: gitlab-data          mountPath: /var/opt/gitlab      volumes:      - name: gitlab-logs        persistentVolumeClaim:          claimName: gitlab-pvc-logs      - name: gitlab-config        persistentVolumeClaim:          claimName: gitlab-pvc-config      - name: gitlab-data        persistentVolumeClaim:          claimName: gitlab-pvc-datastatus: &#123;&#125;EOFkubectl apply -f  gitlab-deploy.yamlkubectl get pod -n gitlab
创建svc
cat &gt; gitlab-svc.yaml &lt;&lt; &quot;EOF&quot;apiVersion: v1kind: Servicemetadata:  creationTimestamp: null  labels:    app: gitlab  name: gitlab  namespace: gitlabspec:  ports:  - name: 80-80    port: 80    protocol: TCP    targetPort: 80    nodePort: 30880  - name: 443-443    port: 443    protocol: TCP    targetPort: 443  - name: 22-22    port: 22    protocol: TCP    targetPort: 22  selector:    app: gitlab  type: NodePortstatus:  loadBalancer: &#123;&#125;EOFkubectl apply -f  gitlab-svc.yamlkubectl get svc -n gitlab
访问页面
http://192.168.48.200:30880/

页面测试
设置中文

创建项目


项目推送
操作节点:[master1]
yum install -y gitmkdir /root/cicd/cd /root/cicd/mkdir chatgpt &amp;&amp; cd chatgpt/
上传Chatgpt镜像站源码
项目地址：ChatGPTNextWeb
cd /root/cicd/chatgpt/wget https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/archive/refs/heads/main.zip
[root@master1 chatgpt]# ls main.zip
解压源代码压缩包
unzip /root/cicd/chatgpt/main.zipmv ChatGPT-Next-Web-main/* ./rm -rf ChatGPT-Next-Web-main main.zip
Git全局设置
git config --global user.name &quot;Administrator&quot;git config --global user.email &quot;XiaooHu2002@163.com&quot;
添加版本库
cd /root/cicd/chatgpt/git init
添加远程仓库
要注意是哪个主分支哦！这里的是main

git remote add origin http://192.168.48.200:30880/root/chatgpt.git#跟踪所有改动过的文件git add . #提交所有更新过的文件git commit -m &quot;first commit&quot;#将代码推送到远程仓库（主分支main）#如果主分支是main则直接执行下一步#首先我本地的主分支是master，我需要切换，可以通过git branch查看git branch -m master main#推送main分支git push -u origin main#会提示你让你输入gitlab的账号密码Username for &#x27;http://192.168.48.200:30880&#x27;: rootPassword for &#x27;http://root@192.168.48.200:30880&#x27;:#密码是qianyios


Jenkins对接gitlab
创建gitlab Secret令牌
进入gitlab创建Secret令牌


创建成功Secret令牌，并复制令牌

此令牌需要保存好，后面jenkins还需要用到
glpat-vLZgV6Z1nbC_96yyZdyp


Jenkins创建流水线
点击新建任务，输入名称并选择为流水线，并点击确定

在general中找到Generic Webhook Trigger 并勾选且复制webhook链接

http://JENKINS_URL/generic-webhook-trigger/invoke


添加gitlab凭据



配置Webhook
配置允许Webhook和服务对本地网络的请求
进入管理员-设置-网络

勾选允许来自 webhooks 和集成对本地网络的请求并保存更改即可

接下来配置webhooks
进入到项目中

填写相关信息

网址：就是刚才jenkins创建项目的时候复制的链接，这里只是对其进行修改了，因为jenkins和gitlab都是部署在同一个k8s集群中我这里将域名修改成了service的地址
原地址：http://JENKINS_URL/generic-webhook-trigger/invoke
修改后的地址:http://jenkins-service.jenkins:8080/generic-webhook-trigger/invoke

jenkins-service：这是服务的名称，它在 Kubernetes 集群内部用于标识 Jenkins 服务。
.jenkins：这是 Jenkins 服务所在的命名空间（namespace）。在 Kubernetes 中，服务的 DNS 名称会包含其命名空间。
:8080：这是服务暴露的端口号，Jenkins 默认使用 8080 端口提供 Web 服务。

自己可以根据自己的deployment去修改
Secret 令牌：就是刚创建的个人令牌
推送事件：指定只有推送到某个仓库时才触发，留空则全部分支

先看更改地址，这一步不需要操作什么，看清楚，替换了啥JENKINS_URL➡️jenkins-service.jenkins:8080

部署webhook


然后添加webhook即可，然后测试推送事件

配置jenkins流水线
添加harbor用户


输入相关信息并点击create创建

类型选择Username with password
范围选择全局
用户名为Harbor仓库用户名
密码为Harbor仓库用户密码
描述为此凭据的名称



创建好后，接下来配置流水线
先获取项目仓库地址

但是我们要对其中的地址改一下，改成

http://gitlab.gitlab/root/chatgpt.git

回到jenkins任务
点击Dashboard-&gt;ChatGPT-&gt;设置-&gt;并点击流水线


点击添加凭据

Domain 选择全局凭据
类型选择Username with password
范围选择全局
用户名即是gitlab账号
密码即是gitlab账号密码
描述为此凭据的名称


往下滑

保存即可
harbor新建项目
输入相关信息并点击确定

用于存放业务镜像
访问级别设置成公开
存储容量为-1即代表不限制存储容量


项目镜像准备
docker pull node:18-alpine#如果下载不了可以用我的docker pull registry.cn-hangzhou.aliyuncs.com/qianyios/node:18-alpinedocker tag registry.cn-hangzhou.aliyuncs.com/qianyios/node:18-alpine 192.168.48.100/library/node:18-alpinedocker push 192.168.48.100/library/node:18-alpine
编写业务部署清单
[root@master1 ~]# echo -n &quot;https://xiaoai.plus&quot; | base64aHR0cHM6Ly94aWFvYWkucGx1cw==[root@master1 ~]# echo -n &quot;sk-K19c1kSqZZ92sXp13d042aD4E2B74a60B749E717Ed69449e&quot; | base64c2stSzE5YzFrU3FaWjkyc1hwMTNkMDQyYUQ0RTJCNzRhNjBCNzQ5RTcxN0VkNjk0NDll
key是ChatGPTkey，这里已经对其base64加密
baseurl是chatgpt第三方服务商提供的代理地址，这里已经对其base64加密
cd /root/cicd/chatgptkubectl create deploy chatgpt --image=chatgpt-next-web --dry-run=client -oyamlkubectl create svc nodeport  chatgpt --tcp=3000:3000 --dry-run=client -oyaml &gt;&gt; deploy.yamlvim deploy.yaml
apiVersion: v1data:  key: c2stSzE5YzFrU3FaWjkyc1hwMTNkMDQyYUQ0RTJCNzRhNjBCNzQ5RTcxN0VkNjk0NDll  baseurl: aHR0cHM6Ly94aWFvYWkucGx1cw==kind: Secretmetadata:  creationTimestamp: null  name: chat-config---apiVersion: apps/v1kind: Deploymentmetadata:  creationTimestamp: null  labels:    app: chatgpt  name: chatgptspec:  replicas: 1  selector:    matchLabels:      app: chatgpt  strategy: &#123;&#125;  template:    metadata:      creationTimestamp: null      labels:        app: chatgpt    spec:      containers:      - image: chatgpt-next-web        name: chatgpt        ports:        - containerPort: 3000        imagePullPolicy: IfNotPresent        env:        - name: OPENAI_API_KEY          valueFrom:            secretKeyRef:              name: chat-config              key: key        - name: BASE_URL          valueFrom:            secretKeyRef:              name: chat-config              key: baseurl---apiVersion: v1kind: Servicemetadata:  creationTimestamp: null  labels:    app: chatgpt  name: chatgptspec:  ports:  - name: 3000-3000    port: 3000    protocol: TCP    targetPort: 3000  selector:    app: chatgpt  type: NodePortstatus:  loadBalancer: &#123;&#125;
编写镜像构建文件
cd /root/cicd/chatgptsed -i &#x27;s|https://registry.yarnpkg.com/|https://registry.npmmirror.com/|g&#x27; yarn.lockcat &gt; Dockerfile &lt;&lt;&quot;EOF&quot;# 基础镜像FROM 192.168.48.100/library/node:18-alpine AS base# 设置国内镜像源RUN sed -i &#x27;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g&#x27; /etc/apk/repositoriesRUN apk add --no-cache libc6-compat proxychains-ngWORKDIR /app# 依赖阶段COPY package.json yarn.lock ./RUN npm config set registry https://registry.npmmirror.comRUN yarn config set registry https://registry.npmmirror.com --global# 替换 yarn.lock 文件中的源地址RUN sed -i &#x27;s|https://registry.yarnpkg.com/|https://registry.npmmirror.com/|g&#x27; yarn.lockRUN sed -i &#x27;s|https://registry.npmjs.org/|https://registry.npmmirror.com/|g&#x27; yarn.lock# 清除缓存并安装依赖RUN yarn config list  # 查看当前的 Yarn 配置RUN yarn install# 更新 caniuse-liteRUN npx update-browserslist-db@latest# 构建阶段FROM base AS builderRUN apk update &amp;&amp; apk add --no-cache gitENV OPENAI_API_KEY=&quot;&quot;ENV BASE_URL=&quot;&quot;WORKDIR /appCOPY . .RUN yarn add @svgr/webpack@latest --frozen-lockfileRUN yarn add sharp  # 添加 sharp 包RUN yarn build# 运行阶段FROM base AS runnerWORKDIR /appRUN apk add proxychains-ngENV PROXY_URL=&quot;&quot;ENV OPENAI_API_KEY=&quot;&quot;ENV BASE_URL=&quot;&quot;COPY --from=builder /app/public ./publicCOPY --from=builder /app/.next/standalone ./COPY --from=builder /app/.next/static ./.next/staticCOPY --from=builder /app/.next/server ./.next/serverEXPOSE 3000CMD if [ -n &quot;$PROXY_URL&quot; ]; then \        export HOSTNAME=&quot;127.0.0.1&quot;; \        protocol=$(echo $PROXY_URL | cut -d: -f1); \        host=$(echo $PROXY_URL | cut -d/ -f3 | cut -d: -f1); \        port=$(echo $PROXY_URL | cut -d: -f3); \        conf=/etc/proxychains.conf; \        echo &quot;strict_chain&quot; &gt; $conf; \        echo &quot;proxy_dns&quot; &gt;&gt; $conf; \        echo &quot;remote_dns_subnet 224&quot; &gt;&gt; $conf; \        echo &quot;tcp_read_time_out 15000&quot; &gt;&gt; $conf; \        echo &quot;tcp_connect_time_out 8000&quot; &gt;&gt; $conf; \        echo &quot;localnet 127.0.0.0/255.0.0.0&quot; &gt;&gt; $conf; \        echo &quot;localnet ::1/128&quot; &gt;&gt; $conf; \        echo &quot;[ProxyList]&quot; &gt;&gt; $conf; \        echo &quot;$protocol $host $port&quot; &gt;&gt; $conf; \        cat /etc/proxychains.conf; \        proxychains -f $conf node server.js; \    else \        node server.js; \    fiEOF
编写流水线脚本

withCredentials 块来引用一个名为 my-credentials 的凭据，该凭据的类型是用户名密码。usernameVariable 和 passwordVariable 参数分别指定了在代码块中使用凭据时的变量名。
在 withCredentials 块内部，你可以执行需要使用凭据的操作，比如在 Shell 脚本中使用用户名和密码进行身份验证。
请注意，credentialsId 参数需要指定你在 Jenkins 中创建的凭据的 ID。确保凭据 ID 正确，并且具有访问该凭据的权限。
使用 withCredentials 块可以确保凭据的安全性，因为凭据的值不会明文显示在日志中，而是以变量的形式传递给代码块中的操作。这样可以避免凭据泄露的风险。
以下内容，自行修改，对照，每一行都去看
Harbor-passwd是jenkins创建的harbor的用户凭据

cd /root/cicd/chatgptcat &gt; Jenkinsfile&lt;&lt;&quot;EOF&quot;pipeline&#123;    agent &#123;        kubernetes &#123;            inheritFrom &quot;jenkins-slave&quot;            yaml &#x27;&#x27;&#x27;apiVersion: v1kind: Podmetadata:  name: jenkins-slavespec:  securityContext:    fsGroup: 0    runAsUser: 0  nodeSelector:    k8s-type: master  containers:  - name: jnlp    image: &quot;192.168.48.100/library/inbound-agent:latest&quot;    imagePullPolicy: IfNotPresent    volumeMounts:    - name: kubectl      mountPath: /usr/bin/kubectl    - name: kube-config      mountPath: /root/.kube    - name: docker      mountPath: /usr/bin/docker    - name: docker-sock      mountPath: /var/run/docker.sock  volumes:  - name: kubectl    hostPath:      path: /usr/bin/kubectl  - name: kube-config    hostPath:      path: /root/.kube  - name: docker    hostPath:      path: /usr/bin/docker  - name: docker-sock    hostPath:      path: /var/run/docker.sock&#x27;&#x27;&#x27;        &#125;    &#125;    environment &#123;        NPM_REGISTRY = &quot;https://registry.npmmirror.com/&quot;        YARN_REGISTRY = &quot;https://registry.npmmirror.com/&quot;    &#125;    stages&#123;        stage(&#x27;git clone&#x27;) &#123;            steps &#123;                sh &#x27;git version&#x27;            &#125;        &#125;        stage(&#x27;image-build&#x27;)&#123;            steps&#123;                withCredentials([usernamePassword(credentialsId: &#x27;Harbor-passwd&#x27;, usernameVariable: &#x27;USERNAME&#x27;, passwordVariable: &#x27;PASSWORD&#x27;)] &#123;                    sh &#x27;docker build -t 192.168.48.100/chatgpt/chatgpt:v1-$BUILD_NUMBER -f Dockerfile .&#x27;                    sh &#x27;docker login 192.168.48.100 -u $USERNAME -p $PASSWORD&#x27;                    sh &#x27;docker push 192.168.48.100/chatgpt/chatgpt:v1-$BUILD_NUMBER&#x27;                &#125;            &#125;        &#125;        stage(&#x27;cloud-deploy&#x27;)&#123;            steps&#123;                sh &#x27;sed -i &quot;s#chatgpt-next-web#192.168.48.100/chatgpt/chatgpt:v1-$BUILD_NUMBER#g&quot; deploy.yaml&#x27;                sh &#x27;cat deploy.yaml&#x27;                sh &#x27;kubectl get pod -A&#x27;                sh &#x27;kubectl get secret,deploy,svc -A&#x27;                sh &#x27;kubectl apply -f deploy.yaml&#x27;                sh &#x27;kubectl get -f deploy.yaml&#x27;            &#125;        &#125;    &#125;&#125;EOF
通过git上传代码
cd /root/cicd/chatgptgit add .git commit -m &quot;add Jenkinsfile Dockerfile and deploy.yaml&quot;git push -u origin mainUsername for &#x27;http://192.168.48.200:30880&#x27;: rootPassword for &#x27;http://root@192.168.48.200:30880&#x27;:


测试
通过刚刚的上传deploy.yaml，流水线已经开始执行
进入jenkins页面可以看到jenkins建的项目正在执行，并且可以看到右下角有一个jenkins代理，说明新建了一个jenkins代理执行


Harbor仓库也有构建好的镜像

测试项目部署成功

jenkins部署日志也显示成功


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>Ceph</tag>
        <tag>OpenEuler</tag>
        <tag>Harbor</tag>
        <tag>K8s</tag>
        <tag>Jenkins</tag>
        <tag>Gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title>基于OpenEuler部署K3S</title>
    <url>/posts/998bbc2f/</url>
    <content><![CDATA[
基于OpenEuler部署K3S
介绍
什么是K3s
K3s 是一个轻量级的 Kubernetes 发行版，它针对边缘计算、物联网等场景进行了高度优化。K3s 有以下增强功能：

打包为单个二进制文件。
使用基于 sqlite3 的轻量级存储后端作为默认存储机制。同时支持使用 etcd3、MySQL 和 PostgreSQL 作为存储机制。
封装在简单的启动程序中，通过该启动程序处理很多复杂的 TLS 和选项。
默认情况下是安全的，对轻量级环境有合理的默认值。
添加了简单但功能强大的batteries-included功能，例如：本地存储提供程序，服务负载均衡器，Helm controller 和 Traefik Ingress controller。
所有 Kubernetes control-plane 组件的操作都封装在单个二进制文件和进程中，使 K3s 具有自动化和管理包括证书分发在内的复杂集群操作的能力。
最大程度减轻了外部依赖性，K3s 仅需要 kernel 和 cgroup 挂载。

openeuler社区教程：K3s部署指南 | openEuler社区 | v24.03_LTS
K3s的更多用法可以参考K3s官网
https://rancher.com/docs/k3s/latest/en/
https://docs.rancher.cn/k3s/
K3s官网采用下载对应架构二进制可执行文件的格式，通过install.sh脚本进行离线安装，openEuler社区将该二进制文件的编译过程移植到社区中，并编译出RPM包。此处可通过yum命令直接进行下载安装。
主机拓扑



主机名
ip
CPU
内存




Server1
192.168.48.101
≧2
2G


Server2
192.168.48.102
≧2
2G


Server3
192.168.48.103
≧2
2G


Agent1
192.168.49.104
≧1
512MB



如果你是只用单serve1只需要创建server1和若干台agent，部署教程请跳转4.基础k3s
如果你是高可用一定需要≥3台的server节点，部署教程请跳转5.高可用K3S（内部etcd）
前提是要完成3.基础配置
基础配置
系统初始化
确保server节点及agent节点主机名不一致
vi system_init.sh
#!/bin/bashif [ $# -eq 2 ];then  echo &quot;设置主机名为：$1&quot;  echo &quot;ens33设置IP地址为：192.168.48.$2&quot;else  echo  &quot;使用方法：sh $0 主机名 主机位&quot;  exit 2fiecho &quot;--------------------------------------&quot;echo &quot;1.正在设置主机名：$1&quot;hostnamectl set-hostname $1echo &quot;2.正在关闭firewalld、selinux&quot;systemctl disable firewalld &amp;&gt; /dev/nullsystemctl stop firewalldsed -i &quot;s#SELINUX=enforcing#SELINUX=disabled#g&quot; /etc/selinux/configsetenforce 0echo &quot;3.正在设置ens33：192.168.48.$2&quot;cat &gt; /etc/sysconfig/network-scripts/ifcfg-ens33 &lt;&lt;EOFTYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noNAME=ens33UUID=53b402ff-5865-47dd-a853-7afcd6521738DEVICE=ens33ONBOOT=yesIPADDR=192.168.48.$2GATEWAY=192.168.48.2PREFIX=24DNS1=192.168.48.2DNS2=114.114.114.114EOFnmcli c reloadnmcli c up ens33echo &quot;4.更新yum源软件包缓存&quot;yum clean all &amp;&amp; yum makecacheecho &quot;5.添加hosts解析&quot;cat &gt; /etc/hosts &lt;&lt;EOF127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.48.101 Server1192.168.48.102 Agent1EOFecho &quot;6.必备工具安装&quot;yum install wget psmisc vim net-tools telnet socat device-mapper-persistent-data lvm2 git gcc -yecho &quot;7.重启系统&quot;reboot
sh system_init.sh 主机名  主机位[Server1] sh system_init.sh Server1 101[Agent1] sh system_init.sh Agent1 102
安装容器工具
请你考虑好，你的集群要以什么为运行时，下面提供了，docker和containerd，自行选择
只能二选一！！！
只能二选一！！！
只能二选一！！！
安装docker
操作节点:[所有节点]
sudo curl -L &quot;https://qygit.qianyisky.cn/https://github.com/docker/compose/releases/download/v2.35.1/docker-compose-linux-x86_64&quot; -o /usr/local/bin/docker-compose#卸载旧版本sudo yum -y remove docker \                  docker-client \                  docker-client-latest \                  docker-common \                  docker-latest \                  docker-latest-logrotate \                  docker-logrotate \                  docker-selinux \                  docker-engine-selinux \                  docker-enginesudo yum remove docker-ce docker-ce-cli containerd.io docker-compose docker-machine docker-swarm -ysudo rm /etc/yum.repos.d/docker-ce.reposudo rm -rf /var/lib/dockersudo yum install -y device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.reposudo sed -i &#x27;s/\$releasever/8/g&#x27; /etc/yum.repos.d/docker-ce.reposudo yum install docker-ce docker-ce-cli containerd.io -ysudo systemctl enable --now dockersudo chmod +x /usr/local/bin/docker-composesudo tee /etc/docker/daemon.json &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;&#123;  &quot;registry-mirrors&quot;: [    &quot;https://docker.xuanyuan.me&quot;,    &quot;https://docker.m.daocloud.io&quot;,    &quot;https://docker.1ms.run&quot;,    &quot;https://docker.1panel.live&quot;,    &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,    &quot;https://docker.qianyios.top&quot;  ],  &quot;max-concurrent-downloads&quot;: 10,  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-level&quot;: &quot;warn&quot;,  &quot;log-opts&quot;: &#123;    &quot;max-size&quot;: &quot;10m&quot;,    &quot;max-file&quot;: &quot;3&quot;  &#125;,  &quot;data-root&quot;: &quot;/var/lib/docker&quot;&#125;EOFsystemctl daemon-reloadsystemctl restart dockerdocker-compose --versiondocker version
安装containerd
操作节点:[所有节点]
sudo yum install -y yum-utilssudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposudo yum clean all &amp;&amp; yum makecachesudo yum install -y containerd.iosudo mkdir -p /etc/containerd/certs.d/docker.iosudo mkdir -p /etc/containerd/certs.d/registry.k8s.iosudo mkdir -p /etc/containerd/certs.d/k8s.gcr.iosudo mkdir -p /etc/containerd/certs.d/ghcr.iosudo mkdir -p /etc/containerd/certs.d/gcr.iosudo mkdir -p /etc/containerd/certs.d/quay.iosudo mkdir -p /etc/containerd/certs.d/registry-1.docker.iosudo tee /etc/containerd/certs.d/docker.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://docker.io&quot; [host.&quot;https://registry.cn-hangzhou.aliyuncs.com/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.xuanyuan.me&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.m.daocloud.io&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.1ms.run&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.1panel.live&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.qianyios.top/&quot;]   capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://reg-mirror.giniu.com&quot;]   capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/registry-1.docker.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://registry-1.docker.io&quot;[host.&quot;https://registry.cn-hangzhou.aliyuncs.com/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.xuanyuan.me&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.m.daocloud.io&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.1ms.run&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.1panel.live&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.qianyios.top/&quot;]   capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://reg-mirror.giniu.com&quot;]   capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/k8s.gcr.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://k8s.gcr.io&quot;[host.&quot;https://registry.aliyuncs.com/google_containers&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/ghcr.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://ghcr.io&quot;[host.&quot;https://ghcr.m.daocloud.io/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/gcr.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://gcr.io&quot;[host.&quot;https://gcr.m.daocloud.io/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;registry.k8s.io&quot;[host.&quot;k8s.m.daocloud.io&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;, &quot;push&quot;][host.&quot;https://registry.aliyuncs.com/v2/google_containers&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/quay.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://quay.io&quot;[host.&quot;https://quay.tencentcloudcr.com/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo sh -c &#x27;containerd config default &gt; /etc/containerd/config.toml&#x27;sudo sed -i &#x27;s#sandbox_image = &quot;registry.k8s.io/pause:.*&quot;#sandbox_image = &quot;registry.aliyuncs.com/google_containers/pause:3.10&quot;#&#x27; /etc/containerd/config.tomlsudo sed -i &#x27;s/SystemdCgroup = false/SystemdCgroup = true/g&#x27; /etc/containerd/config.tomlsed -i &#x27;/\[plugins\.&quot;io\.containerd\.grpc\.v1\.cri&quot;\.registry\]/!b;n;s/config_path = &quot;&quot;/config_path = &quot;\/etc\/containerd\/certs.d&quot;/&#x27; /etc/containerd/config.toml# 重启 containerd 服务sudo systemctl daemon-reloadsudo systemctl restart containerd.servicesudo ctr image ls
添加镜像源
操作节点:[所有节点]
sudo mkdir -p /etc/rancher/k3ssudo tee /etc/rancher/k3s/registries.yaml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;mirrors:  docker.io:    endpoint:      - &quot;https://registry.cn-hangzhou.aliyuncs.com/&quot;       - &quot;https://docker.xuanyuan.me&quot;       - &quot;https://docker.m.daocloud.io&quot;       - &quot;https://docker.1ms.run&quot;       - &quot;https://docker.1panel.live&quot;       - &quot;https://hub.rat.dev&quot;       - &quot;https://docker-mirror.aigc2d.com&quot;       - &quot;https://docker.qianyios.top/&quot;  quay.io:    endpoint:      - &quot;https://quay.tencentcloudcr.com/&quot;     registry.k8s.io:    endpoint:      - &quot;https://registry.aliyuncs.com/v2/google_containers&quot;     gcr.io:    endpoint:      - &quot;https://gcr.m.daocloud.io/&quot;     k8s.gcr.io:    endpoint:      - &quot;https://registry.aliyuncs.com/google_containers&quot;     ghcr.io:    endpoint:      - &quot;https://ghcr.m.daocloud.io/&quot;   EOF
建议在这里打个快照
基础K3S
部署K3s
由于OpenEuler已经编译好RPM的包了，可以直接安装
操作节点：[所有节点]
yum install -y k3s
部署server节点
操作节点：[Server1]
如需在单个服务器上安装 K3s，可以在 server 节点上执行如下操作：
要注意你选的是哪个容器工具哈
要注意你选的是哪个容器工具哈
要注意你选的是哪个容器工具哈
基于docker
如果有参数的值数错了，可以改一下，然后重新运行命令即可
--docke一定要放在所有参数的最前面
INSTALL_K3S_SKIP_DOWNLOAD=true \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \k3s-install.sh --docker \--system-default-registry &quot;registry.cn-hangzhou.aliyuncs.com&quot;

自行完成 9.解决非root用户使用kubectl等命令显示无命令的办法
基于containerd
如果有参数的值数错了，可以改一下，然后重新运行命令即可
INSTALL_K3S_SKIP_DOWNLOAD=true \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \k3s-install.sh \--system-default-registry &quot;registry.cn-hangzhou.aliyuncs.com&quot;

自行完成 9.解决非root用户使用kubectl等命令显示无命令的办法
查看镜像列表和pod情况
crictl imageskubectl get pod -A

部署Agent节点
操作节点：[Server1]
查看token
cat /var/lib/rancher/k3s/server/node-token

K10ed18fcd528981577fe508d419bd28fefeef1c372ccc246a79fff1fa4b371e5e1::server:a02d22a5169cdc2465bd989360029283

基于docker
操作节点：[Agent1]
如果有参数的值数错了，可以改一下，然后重新运行命令即可
--docke一定要放在所有参数的最前面
INSTALL_K3S_SKIP_DOWNLOAD=true \K3S_URL=https://192.168.48.101:6443  \K3S_TOKEN=a02d22a5169cdc2465bd989360029283 \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \k3s-install.sh --docker
· a02d22a5169cdc2465bd989360029283 是前面server1获取的token
· 192.168.48.101是server1的ip

这时候在server1查看是否成功加入集群
sudo kubectl get nodes

如果需要部署dashboard请跳转6.安装dashboard
基于containerd
如果有参数的值数错了，可以改一下，然后重新运行命令即可
操作节点：[Agent1]
INSTALL_K3S_SKIP_DOWNLOAD=true \K3S_URL=https://192.168.48.101:6443  \K3S_TOKEN=a02d22a5169cdc2465bd989360029283 \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \k3s-install.sh 
· a02d22a5169cdc2465bd989360029283 是前面server1获取的token
· 192.168.48.101是server1的ip

如果需要部署dashboard请跳转6.安装dashboard
高可用K3S（内部etcd）
官方教程：高可用嵌入式 etcd
具有嵌入式 etcd 的 HA K3s 集群由以下部分组成：

三个或多个 Server 节点为 Kubernetes API 提供服务并运行其他 control plane 服务，以及托管嵌入式 etcd 数据存储。
可选：零个或多个 Agent 节点，用于运行你的应用和服务
可选：固定注册地址，供 Agent 节点注册到集群

主机拓扑



主机名
ip
CPU
内存




Server1
192.168.48.101
≧2
2G


Server2
192.168.48.102
≧2
2G


Server3
192.168.48.103
≧2
2G


Agent1
192.168.49.104
≧1
512MB



现在所有的机子都从5.1部署K3s克隆这个部署好K3s的快照，也就是说现在所有机子的起点都在5.1部署K3s
记得给agent改ip哈
配置集群负载均衡器
官方教程：集群负载均衡器
按理来说我们需要两台额外的节点来做负载均衡和高可用vip节点，但是为了测试方便，我们直接部署在
server节点，也就是图中的第二种方法

操作节点：[所有的server]
yum install -y haproxy keepalivedcat &gt; /etc/haproxy/haproxy.cfg &lt;&lt;&quot;EOF&quot;frontend k3s-frontend    bind *:16443    mode tcp    option tcplog    default_backend k3s-backendbackend k3s-backend    mode tcp    option tcp-check    balance roundrobin    timeout connect 5s    timeout server 30s    timeout client 30s    default-server inter 10s downinter 5s    server server-1 192.168.48.101:6443 check    server server-2 192.168.48.102:6443 check    server server-3 192.168.48.103:6443 checkEOF
操作节点:[Server1]
cat &gt; /etc/keepalived/keepalived.conf &lt;&lt;&quot;EOF&quot;global_defs &#123;  enable_script_security  script_user root&#125;vrrp_script chk_haproxy &#123;    script &#x27;killall -0 haproxy&#x27;    interval 2&#125;vrrp_instance haproxy-vip &#123;    interface ens33  #这里要改，是你的网卡    state MASTER    #这里要改 server1是Master 其他都是Backup    priority 200    virtual_router_id 51    virtual_ipaddress &#123;        192.168.48.200/24      #高可用ip    &#125;    track_script &#123;        chk_haproxy    &#125;&#125;EOF
操作节点:[Server2]
cat &gt; /etc/keepalived/keepalived.conf &lt;&lt;&quot;EOF&quot;global_defs &#123;  enable_script_security  script_user root&#125;vrrp_script chk_haproxy &#123;    script &#x27;killall -0 haproxy&#x27;    interval 2&#125;vrrp_instance haproxy-vip &#123;    interface ens33  #这里要改，是你的网卡    state BACKUP    #这里要改 server1是Master 其他都是Backup    priority 150    virtual_router_id 51    virtual_ipaddress &#123;        192.168.48.200/24      #高可用ip    &#125;    track_script &#123;        chk_haproxy    &#125;&#125;EOF
操作节点:[Server3]
cat &gt; /etc/keepalived/keepalived.conf &lt;&lt;&quot;EOF&quot;global_defs &#123;  enable_script_security  script_user root&#125;vrrp_script chk_haproxy &#123;    script &#x27;killall -0 haproxy&#x27;    interval 2&#125;vrrp_instance haproxy-vip &#123;    interface ens33  #这里要改，是你的网卡    state BACKUP    #这里要改 server1是Master 其他都是Backup    priority 100    virtual_router_id 51    virtual_ipaddress &#123;        192.168.48.200/24      #高可用ip    &#125;    track_script &#123;        chk_haproxy    &#125;&#125;EOF
操作节点：[所有的Server]
sudo systemctl restart haproxy keepalivedsudo systemctl enable --now haproxy keepalived
现在来查看vip是否生成
操作节点：[Server1]
ip a

初始化第一个Server1
要注意你选的是哪个容器工具哈
要注意你选的是哪个容器工具哈
要注意你选的是哪个容器工具哈
基于docker
如果有参数的值数错了，可以改一下，然后重新运行命令即可

--docke一定要放在所有参数的最前面

K3S_TOKEN=qianyiosQianyios12345 \INSTALL_K3S_SKIP_DOWNLOAD=true \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \k3s-install.sh --docker --cluster-init - server \--system-default-registry &quot;registry.cn-hangzhou.aliyuncs.com&quot; \--tls-san= 192.168.48.200
qianyiosQianyios12345是作为集群间的共享密钥，可自定义

自行完成 9.解决非root用户使用kubectl等命令显示无命令的办法
基于containerd
如果有参数的值数错了，可以改一下，然后重新运行命令即可
操作节点：[Server1]
K3S_TOKEN=qianyiosQianyios12345 \INSTALL_K3S_SKIP_DOWNLOAD=true \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \k3s-install.sh --cluster-init - server \--system-default-registry &quot;registry.cn-hangzhou.aliyuncs.com&quot; \--tls-san= 192.168.48.200
qianyiosQianyios12345是作为集群间的共享密钥，可自定义
自行完成 9.解决非root用户使用kubectl等命令显示无命令的办法
更多参数：K3s Server 配置参考
其他server加入集群
基于docker
如果有参数的值数错了，可以改一下，然后重新运行命令即可
--docke一定要放在所有参数的最前面
K3S_TOKEN=qianyiosQianyios12345 \INSTALL_K3S_SKIP_DOWNLOAD=true \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \k3s-install.sh --docker - server \--system-default-registry &quot;registry.cn-hangzhou.aliyuncs.com&quot; \--server https://192.168.48.101:6443  \--tls-san= 192.168.48.200
qianyiosQianyios12345是作为第一个server1共享出来的密钥
–server https://192.168.48.101:6443 改成serve1的ip地址即可
自行完成 9.解决非root用户使用kubectl等命令显示无命令的办法
基于containerd
如果有参数的值数错了，可以改一下，然后重新运行命令即可
K3S_TOKEN=qianyiosQianyios12345 \INSTALL_K3S_SKIP_DOWNLOAD=true \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \k3s-install.sh - server \--system-default-registry &quot;registry.cn-hangzhou.aliyuncs.com&quot; \--server https://192.168.48.101:6443  \--tls-san= 192.168.48.200
qianyiosQianyios12345是作为第一个server1共享出来的密钥
–server https://192.168.48.101:6443 改成serve1的ip地址即可
自行完成 9.解决非root用户使用kubectl等命令显示无命令的办法
其他Agent加入集群
基于docker
如果有参数的值数错了，可以改一下，然后重新运行命令即可
--docke一定要放在所有参数的最前面
INSTALL_K3S_SKIP_DOWNLOAD=true \K3S_TOKEN=qianyiosQianyios12345 \K3S_URL=https://192.168.48.101:6443  \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \k3s-install.sh --docker - agent
基于containerd
如果有参数的值数错了，可以改一下，然后重新运行命令即可
INSTALL_K3S_SKIP_DOWNLOAD=true \K3S_TOKEN=qianyiosQianyios12345 \K3S_URL=https://192.168.48.101:6443  \INSTALL_K3S_REGISTRIES=&quot;https://registry.cn-hangzhou.aliyuncs.com,https://registry.aliyuncs.com/google_containers&quot; \k3s-install.sh - agent
这时候在Server可以查看node情况
kubectl get nodes

自行部署dashboard之后,查看他在哪个节点上，部署教程6.安装dashboard
https://192.168.48.200:30001/
现在不是有高可用的vip吗。那么正好可以用vip访问，端口不变

高可用模拟宕机测试
查看dashboard部署在哪个节点
kubectl get pods -A -l k8s-app=kubernetes-dashboard -o wide
我这里显示的是dashboard部署在Server2
那么我们就对Server2进行powerof关机，来模拟宕机看看dashboard能否被k3s自动调度到其他节点
但是我发现pod还在running的状态
[root@Server3 ~]# kubectl get pods -ANAMESPACE              NAME                                        READY   STATUS      RESTARTS   AGEkubernetes-dashboard   kubernetes-dashboard-668679b698-nlpqc       1/1     Running     0          15m[root@Server3 ~]# kubectl get nodesNAME      STATUS     ROLES                       AGE   VERSIONagent1    Ready      &lt;none&gt;                      35m   v1.24.2+k3s-server1   Ready      control-plane,etcd,master   36m   v1.24.2+k3s-server2   NotReady   control-plane,etcd,master   36m   v1.24.2+k3s-server3   Ready      control-plane,etcd,master   36m   v1.24.2+k3s-[root@Server3 ~]#

server2 已经被标记为 NotReady
说明 Kubernetes 已感知到它不可用（可能是关机、网络不通或 kubelet 崩溃等），但：

如果 Pod 的副本数是 1，Kubernetes 不会自动创建新的 Pod 。
默认的节点失联容忍时间较长（5分钟），所以即使节点 NotReady，也不会立刻触发 Pod 驱逐。


方案一 等待五分钟
经过漫长等待，dashboard的pod进行了重新分配

kubectl get pods -A -l k8s-app=kubernetes-dashboard -o wide
经过查看已经被调度到了Server3节点
结论：高可用实验，实验成功，且页面可以正常访问
方案二 手动删除 Pod 强制重建（推荐测试）
由于刚刚经过方案一的测试，被调度到了server3，所以这次对server3进行模拟宕机，然后手动删除pod
kubectl delete pod -n kubernetes-dashboard pod名字

经过手动删除，立马触发自动调度，已经被调度到了Server2节点
结论：高可用实验，实验成功，且页面可以正常访问
方案三 缩短节点失联容忍时间（适用于生产环境）
如果你希望 Kubernetes 更快地响应节点故障，可以在 K3s 启动参数中添加以下内容：
--node-monitor-grace-period=20s \--pod-eviction-timeout=30s
⚠️ 注意：这会影响整个集群的行为，适用于生产环境或需要快速故障恢复的场景。
安装dashboard
操作节点:[Server1]
sudo wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml &amp;&amp; \sudo sed -i &#x27;s/kubernetesui\/dashboard:v2.7.0/registry.cn-hangzhou.aliyuncs.com\/qianyios\/dashboard:v2.7.0/g&#x27; recommended.yaml sleep 3sudo sed -i &#x27;s/kubernetesui\/metrics-scraper:v1.0.8/registry.cn-hangzhou.aliyuncs.com\/qianyios\/metrics-scraper:v1.0.8/g&#x27; recommended.yaml sudo sed -i &#x27;/targetPort: 8443/a\      nodePort: 30001&#x27; recommended.yaml sudo sed -i &#x27;/nodePort: 30001/a\  type: NodePort&#x27; recommended.yaml
运行pod
kubectl apply -f recommended.yaml
创建token
#创建service account并绑定默认cluster-admin管理员群角色#创建用户kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard#用户授权kubectl create clusterrolebinding dashboard-admin \--clusterrole=cluster-admin \--serviceaccount=kubernetes-dashboard:dashboard-admin#临时获取用户Token（默认只有 30 分钟 ）kubectl create token dashboard-admin -n kubernetes-dashboard#永久获取用户Tokencat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Secretmetadata:  name: dashboard-admin-token  namespace: kubernetes-dashboard  annotations:    kubernetes.io/service-account.name: dashboard-admintype: kubernetes.io/service-account-tokenEOFKUBECONFIG_FILE=&quot;dashboard-kubeconfig.yaml&quot;# 自动获取 API Server 地址APISERVER=$(kubectl config view --minify -o jsonpath=&#x27;&#123;.clusters[0].cluster.server&#125;&#x27;)# 自动获取 CA 证书CA_CERT=$(kubectl config view --raw -o jsonpath=&#x27;&#123;.clusters[0].cluster.certificate-authority-data&#125;&#x27;)# 自动从 Secret 获取 Token（你提到的正确方式）TOKEN=$(kubectl get secret dashboard-admin-token -n kubernetes-dashboard -o jsonpath=&#x27;&#123;.data.token&#125;&#x27; | base64 --decode)# 生成 kubeconfig 文件cat &lt;&lt;EOF &gt; $&#123;KUBECONFIG_FILE&#125;apiVersion: v1kind: Configclusters:  - name: kubernetes    cluster:      server: $&#123;APISERVER&#125;      certificate-authority-data: $&#123;CA_CERT&#125;users:  - name: dashboard-admin    user:      token: $&#123;TOKEN&#125;contexts:  - name: dashboard-context    context:      cluster: kubernetes      user: dashboard-admincurrent-context: dashboard-contextEOFecho &quot;✅ kubeconfig 文件已生成：$&#123;KUBECONFIG_FILE&#125;&quot;
这时候就会提示你
✅ kubeconfig 文件已生成：dashboard-kubeconfig.yaml
你就把这个文件上传到dashboard的kubeconfig就可以免密登入了

修改启动参数
如果你在安装的时候有些参数输入错了，或者想改，可以在这里改
首先，停止 K3s 服务以避免在更新过程中出现冲突：
sudo systemctl stop k3s
修改k3s启动参数
sudo vi /etc/systemd/system/k3s.service

假设你的–tls-san的高可用地址输入错了，要改成别的，你就改完，记得保存
然后删除旧证书
sudo rm -f /var/lib/rancher/k3s/server/tls/serving-kube-apiserver*sudo rm -f /var/lib/rancher/k3s/server/tls/server*
重启服务
sudo systemctl daemon-reloadsudo systemctl start k3s
卸载K3S
官方教程：Uninstalling K3s | K3s
卸载Server
要从服务器节点卸载 K3s，请运行：
/usr/local/bin/k3s-uninstall.sh
卸载Agent
要从代理节点卸载 K3s，请运行：
/usr/local/bin/k3s-agent-uninstall.sh
解决非root用户使用kubectl等命令显示无命令的办法
这时候运行查看节点命令，提示找不到命令
sudo kubectl get nodes
一看发现只有具体到指定路径才可以正常运行，并且用户和权限组都是root

这时候在普通用户查看visudo
sudo visudo

一看地址，他并没有/usr/local/bin的路径，所以普通用户是没办法继承root的路径的，所以你要设置普通用户默认的环境变量（生成环境，建议仔细斟酌要不要添加，不然就只能用绝对路径）
/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

这时候再次运行k3s命令
sudo kubectl get nodessudo crictl images


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>OpenEuler</tag>
        <tag>K3s</tag>
      </tags>
  </entry>
  <entry>
    <title>广州商学院课程作业导航</title>
    <url>/posts/548777d4/</url>
    <content><![CDATA[
广州商学院课程作业导航




 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
  </entry>
  <entry>
    <title>实现私有Yum仓库</title>
    <url>/posts/f49cc7ae/</url>
    <content><![CDATA[
实现私有Yum仓库
架构图

每个系统的最后的路径都不一样，所以要看清，尽量跟大厂路径对齐，起始最重要的就是Packages和repodata，你看阿里云的路径，我们只要走到有repedata的文件夹下，就说明走对了，不用往下走了
主机拓扑



主机名
ip
os




yum server
192.168.48.128
Rocker8.9


client1
192.168.48.11
Centos Stream 8


client2
192.168.48.10
Centos 7



前情提要：所有要进行上传yum源的机子都要挂载光盘，也可以选择网络备份
自动挂载光盘
#自动挂载yum install autofs -ysystemctl enable --now autofssed -i &#x27;/^[[:space:]]*#.*misc.*\/etc\/auto.misc/ s/^#//&#x27; /etc/auto.mastersystemctl restart autofsls /misc/cd
部署yum server
操作节点：【yum server】
Rocky8
yum install -y httpdsystemctl disable --now firewalld.servicesystemctl enable --now httpd.service#自己看前面的教程实现rocky8的光盘自动挂载#将本地光盘中的内容CP到web目录中，给客户端使用mkdir -p /var/www/html/rockylinux/8/&#123;BaseOS,AppStream&#125;cp -r /misc/cd/BaseOS/* /var/www/html/rockylinux/8/BaseOScp -r /mnt/BaseOS/repodata /var/www/html/rockylinux/8/BaseOS/cp -r /misc/cd/AppStream/* /var/www/html/rockylinux/8/AppStreamcp -r /mnt/AppStream/repodata /var/www/html/rockylinux/8/AppStream/#这里可能有点慢，因为要拷贝的文件有点多
现在一些基本的文件都拷过去了访问网页就可以看见了
http://192.168.48.128/rockylinux/8/

假设我还有个extras源想拷过去呢？前面那个是走本地，这个可以走网络，这时候就可以用到yum仓同步工具
https://mirrors.aliyun.com/rockylinux/8/extras/x86_64/os/

#备份自己的源mkdir -p repo.bakmv /etc/yum.repos.d/* repo.bak/#这是私网yum server自己用的repocat &gt; /etc/yum.repos.d/qianyios.repo&lt;&lt;&quot;EOF2&quot;[AppStream]name=AppStreambaseurl=file:///var/www/html/rockylinux/8/AppStream/x86_64/os/gpgkey=/etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficialgpgcheck=1enabled=1[BaseOS]name=BaseOSbaseurl=file:///var/www/html/rockylinux/8/BaseOS/x86_64/os/gpgkey=/etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficialgpgcheck=1enabled=1[extras]name=extrasbaseurl=file:///var/www/html/rockylinux/8/extras/x86_64/os/gpgkey=/etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficialgpgcheck=1enabled=1EOF2#这是公网用来同步的的repocat &gt; /etc/yum.repos.d/qianyios-external-rocky8.repo&lt;&lt;&quot;EOF2&quot;[AppStream-external-rocky8]name=AppStream-external-rocky8baseurl=https://mirrors.aliyun.com/rockylinux/8/AppStream/x86_64/os/gpgkey=/etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficialgpgcheck=1enabled=1[BaseOS-external-rocky8]name=BaseOS-external-rocky8baseurl=https://mirrors.aliyun.com/rockylinux/8/BaseOS/x86_64/os/gpgkey=/etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficialgpgcheck=1enabled=1[extras-external-rocky8]name=extras-external-rocky8baseurl=https://mirrors.aliyun.com/rockylinux/8/extras/x86_64/os/gpgkey=/etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficialgpgcheck=1enabled=1EOF2
#CentOS 8 dnf 工具集成 REPOID是那个repo里的[]里的名字dnf reposync --repoid=REPOID --download-metadata -p /path dnf reposync --help #查看帮助#CentOS 7 以前版本，reposync工具来自于yum-utils包reposync --repoid=REPOID --download-metadata -p /path
#创建好对应的路径，由于AppStream和BaseOS已经手动拉过去了，但是路径不对，我们就手动移一下cd /var/www/html/rockylinux/8mkdir -p AppStream/x86_64/os/mkdir -p BaseOS/x86_64/os/mv AppStream/&#123;Packages,repodata&#125; AppStream/x86_64/os/mv BaseOS/&#123;Packages,repodata&#125; BaseOS/x86_64/os/#开始同步，这里最后-p 路径的就不用加extrasmkdir -p /var/www/html/rockylinux/8/yum reposync --repoid=extras-external-rocky8 --download-metadata -p /var/www/html/rockylinux/8/#但是这个时候他的名字是extras-external-rocky8，所以你要把他下面的内容移动到extras去，还要注意有没有x86_64/os/mv /var/www/html/rockylinux/8/&#123;extras-external-rocky8,extras&#125;mkdir -p extras/x86_64/os/mv extras/&#123;Packages,repodata&#125; extras/x86_64/os/

这就是成果
测试自己是否能创建缓存成功
#用的是私有repo，记得把其他的repo移走[root@localhost 8]# yum clean all &amp;&amp; yum makecache75 个文件已删除AppStream                                                   410 MB/s | 8.7 MB     00:00BaseOS                                                      237 MB/s | 2.6 MB     00:00extras                                                      6.4 MB/s |  15 kB     00:00元数据缓存已建立。
Centos Strem 8
远程同步
操作节点：【Centos Strem 8】
从Centos Strem 8复制到yumserver，如果你没有centos stream 8的机子，那就直接选网络同步
#自己看前面的教程实现rocky8的光盘自动挂载#将本地光盘中的内容CP到yum server 的web目录中scp -r /misc/cd/BaseOS 192.168.48.128:/var/www/html/centos/8-stream/scp -r /misc/cd/AppStream 192.168.48.128:/var/www/html/centos/8-stream/
网络同步
操作节点：【yum server】
#公网cat &gt; /etc/yum.repos.d/qianyios-external--Centos-Strem-8.repo&lt;&lt;&quot;EOF2&quot;[AppStream-external-Centos-Strem-8]name=AppStream-external-Centos-Strem-8baseurl=https://mirrors.aliyun.com/centos/8-stream/AppStream/x86_64/os/gpgkey=https://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-Officialgpgcheck=1enabled=1[BaseOS-external-Centos-Strem-8]name=BaseOS-external-Centos-Strem-8baseurl=https://mirrors.aliyun.com/centos/8-stream/BaseOS/x86_64/os/gpgkey=https://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-Officialgpgcheck=1enabled=1[extras-external-Centos-Strem-8]name=extras-external-Centos-Strem-8baseurl=https://mirrors.aliyun.com/centos/8-stream/extras/x86_64/os/gpgkey=https://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-Officialgpgcheck=1enabled=1EOF2
#前面已经复制过了AppStream和BaseOS，这里就不做演示了,如果你手动之后还想网络同步，我的建议是删掉手动的文件夹再来拉去取网络的#AppStreamyum reposync --repoid=AppStream-external-Centos-Strem-8 --download-metadata -p /var/www/html/centos/8-stream/#BaseOSyum reposync --repoid=BaseOS-external-Centos-Strem-8 --download-metadata -p /var/www/html/centos/8-stream/#extrasyum reposync --repoid=extras-external-Centos-Strem-8 --download-metadata -p /var/www/html/centos/8-stream/#注意修改名字哦mv /var/www/html/centos/8-stream/&#123;AppStream-external-Centos-Strem-8,AppStream&#125;mv /var/www/html/centos/8-stream/&#123;BaseOS-external-Centos-Strem-8,BaseOS&#125;mv /var/www/html/centos/8-stream/&#123;extras-external-Centos-Strem-8,extras&#125;cd /var/www/html/centos/8-stream/extrasmv Packages x86_64/os/mv repodata x86_64/os/

centos7
手动的我就不写了一个意思，这里我就写网络的
#公网cat &gt; /etc/yum.repos.d/qianyios-external-centos7.repo&lt;&lt;&quot;EOF2&quot;[base-external-centos7]name=BaseOS-external-centos7baseurl=https://mirrors.aliyun.com/centos/7/os/x86_64gpgkey=https://mirrors.aliyun.com/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7gpgcheck=1enabled=1[extras-external-centos7]name=extras-external-centos7baseurl=https://mirrors.aliyun.com/centos/7/extras/x86_64/gpgkey=https://mirrors.aliyun.com/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7gpgcheck=1enabled=1EOF2
mkdir -p /var/www/html/centos/7/os/x86_64/#bashyum reposync --repoid=base-external-centos7 --download-metadata -p /var/www/html/centos/7/#extrasyum reposync --repoid=extras-external-centos7 --download-metadata -p /var/www/html/centos/7/#注意修改名字哦mv /var/www/html/centos/7/&#123;base-external-centos7,base&#125;cd /var/www/html/centos/7mv base os/x86_64/mv /var/www/html/centos/7/&#123;extras-external-centos7,extras&#125;cd /var/www/html/centos/7/extrasmv Packages x86_64/mv repodata x86_64/

客户端使用
操作节点：【Centos Stream 8】
#备份原有的repomkdir repo.bakmv /etc/yum.repos.d/* repo.bak/#公网cat &gt; /etc/yum.repos.d/qianyios-external-Centos-Strem-8.repo&lt;&lt;&quot;EOF2&quot;[AppStream-external-Centos-Strem-8]name=AppStream-external-Centos-Strem-8baseurl=http://192.168.48.128/centos/8-stream/AppStream/x86_64/os/gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficialgpgcheck=1enabled=1[BaseOS-external-Centos-Strem-8]name=BaseOS-external-Centos-Strem-8baseurl=http://192.168.48.128/centos/8-stream/BaseOS/x86_64/os/gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficialgpgcheck=1enabled=1[extras-external-Centos-Strem-8]name=extras-external-Centos-Strem-8baseurl=http://192.168.48.128/centos/8-stream/extras/x86_64/os/gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficialgpgcheck=1enabled=1EOF2yum clean all &amp;&amp; yum makecache#成功[root@qianyi ~]# yum clean all &amp;&amp; yum makecache                                             13 files removedAppStream-external-Centos-Strem-8                           205 MB/s | 7.9 MB     00:00BaseOS-external-Centos-Strem-8                              158 MB/s | 2.7 MB     00:00extras-external-Centos-Strem-8                              6.2 MB/s |  18 kB     00:00Metadata cache created.[root@qianyi ~]#
如果他报这个错，你要确保你的extras下的那个路径有没有repodata这个文件夹，或者看看路径对不对

操作节点：【centos7】
#备份原有的repomkdir repo.bakmv /etc/yum.repos.d/* repo.bak/cat &gt; /etc/yum.repos.d/qianyios-external-centos7.repo&lt;&lt;&quot;EOF2&quot;[base-external-centos7]name=BaseOS-external-centos7baseurl=http://192.168.48.128/centos/7/os/x86_64/gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7gpgcheck=1enabled=1[extras-external-centos7]name=extras-external-centos7baseurl=http://192.168.48.128/centos/7/extras/x86_64/gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7gpgcheck=1enabled=1EOF2yum clean all &amp;&amp; yum makecache#成功[root@localhost ~]# yum clean all &amp;&amp; yum makecacheLoaded plugins: fastestmirror, langpacksCleaning repos: base-external-centos7 extras-external-centos7Cleaning up list of fastest mirrorsOther repos take up 102 M of disk space (use --verbose for details)Loaded plugins: fastestmirror, langpacksDetermining fastest mirrorsbase-external-centos7                                                | 3.6 kB  00:00:00extras-external-centos7                                              | 2.9 kB  00:00:00(1/7): base-external-centos7/group_gz                                | 153 kB  00:00:00(2/7): base-external-centos7/filelists_db                            | 3.3 MB  00:00:00(3/7): base-external-centos7/other_db                                | 1.3 MB  00:00:00(4/7): base-external-centos7/primary_db                              | 3.3 MB  00:00:00(5/7): extras-external-centos7/primary_db                            | 253 kB  00:00:00(6/7): extras-external-centos7/filelists_db                          | 305 kB  00:00:00(7/7): extras-external-centos7/other_db                              | 154 kB  00:00:00Metadata Cache Created[root@localhost ~]#
总结
1.你要下那个源仓库，你就走到对应的路径，如果看见repodata文件夹，那就是这个路径了

2.如果你是本地测试，拉的文件太多很大，你终止了你可能拉不到repodata，所以你可以通过光盘里的scp过去yum server
3.万事都要确定好路径喔，如果repo写错了，你就看看他那个报错的路径下有没有repodata

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>Yum</tag>
      </tags>
  </entry>
  <entry>
    <title>实现单点Mycat读写分离</title>
    <url>/posts/c89451f8/</url>
    <content><![CDATA[
实现单点Mycat读写分离
下载链接1：Mycat-server-1.6.7.6-release-20210303094759-linux.tar.gz
下载链接2：Mycat-server-1.6.7.6-release-20210303094759-linux.tar.gz
如有下载不了请及时在评论区留言
架构图




主机名
ip
内存
硬盘




master
192.168.48.10
2G
100G


slave
192.168.48.11
2G
100G


mycat
192.168.48.128
&gt;=2G
100G


client
192.168.48.101
2G
100G



部署主从复制
安装mysql
操作节点：[master，slave]
这是我的二进制mysql安装脚本，适用于大部分的linux通用安装，如果你有自己的安排可以直接略过这个，自己安装mysql
wget https://blog.qianyios.top/file/mysql_install.shbash mysql_install.sh
配置master
1.修改配置文件
vim /etc/my.cnf##其他安装mysql的方法，你只需要确保有如下选项就行了[mysqld]server-id=10log-bin=/data/logbin/qylog
sever-id ：必须是整个集群里面唯一的，不能重复
log-bin:后续会在/data/logbin生成qylog.000001的二进制文件，你可以自定义路径和文件名不用加后缀
mkdir -p /data/logbin;chown -R mysql:mysql /data/logbinsystemctl restart mysqld
2.查看二进制文件的位置
mysql -uroot -p123456show master logs;(root@localhost) [(none)]&gt; show master logs;+--------------+-----------+-----------+| Log_name     | File_size | Encrypted |+--------------+-----------+-----------+| qylog.000001 |       157 | No        |+--------------+-----------+-----------+1 row in set (0.00 sec)#记住这个qylog.000001和157，主从复制要用到
3.创建传输账号
create user repluser@&#x27;192.168.48.%&#x27; identified by &#x27;123456&#x27;;grant replication slave on *.* to repluser@&#x27;192.168.48.%&#x27;;
配置从节点
1.修改配置文件
#其他安装mysql的方法，你只需要确保有如下选项就行了[mysqld]server-id=11log-bin=/data/logbin/qylogread_only=ON #设置数据库只读，针对supper user无效#启动中继日志relay_log=/data/relaylog/relay-log #relay log的文件路径，默认值hostname-relay-binrelay_log_index=/data/relaylog/relay-log.index  #默认值hostname-relay-bin.index  
sever-id ：必须是整个集群里面唯一的，不能重复
log-bin:后续会在/data/logbin生成qylog.000001的二进制文件，你可以自定义路径和文件名不用加后缀
mkdir -p /data/logbin /data/relaylogchown -R mysql:mysql /data/logbin /data/relaylogsystemctl restart mysqld
启动复制线程
操作节点：[slave]
mysql -u root -p123456CHANGE MASTER TO MASTER_HOST=&#x27;192.168.48.10&#x27;, MASTER_USER=&#x27;repluser&#x27;, MASTER_PASSWORD=&#x27;123456&#x27;, MASTER_LOG_FILE=&#x27;qylog.000001&#x27;, MASTER_LOG_POS=157,get_master_public_key=1; start slave;show slave status\G;
确保有两个yes就行了

如果你想重置线程可以用以下命令
stop slave;reset slave;
有一个重要的点，因为二进制日志是记录你的操作的嘛，我们在开启二进制日志之后，不是在主节点创建了一个repluser用户吗，那这个操作肯定也被记录，然后这不是主从复制了吗，这里肯定，也会同步，也会运行，二进制日志本身就是一个sql文件，普通cat是看不了，你得用这个命令
[root@master ~]# mysqlbinlog -uroot -p123456 /data/logbin/qylog.000001 -v

所有既然主从复制了，那在从节点，应该也运行了这个二进制文件，就说明从节点也有这个账号
select host,user from mysql.user;

测试
接下来在主节点导入测试数据
全选复制粘贴退出mysql运行
cat &gt; hellodb.sql &lt;&lt;&quot;EOF&quot;CREATE DATABASE /*!32312 IF NOT EXISTS*/ `hellodb` /*!40100 DEFAULT CHARACTER SET utf8 */;USE `hellodb`;DROP TABLE IF EXISTS `classes`;CREATE TABLE `classes` (  `ClassID` tinyint(3) unsigned NOT NULL AUTO_INCREMENT,  `Class` varchar(100) DEFAULT NULL,  `NumOfStu` smallint(5) unsigned DEFAULT NULL,  PRIMARY KEY (`ClassID`)) ENGINE=InnoDB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8;LOCK TABLES `classes` WRITE;INSERT INTO `classes` VALUES(1,&#x27;Shaolin Pai&#x27;,10),(2,&#x27;Emei Pai&#x27;,7),(3,&#x27;QingCheng Pai&#x27;,11),(4,&#x27;Wudang Pai&#x27;,12),(5,&#x27;Riyue Shenjiao&#x27;,31),(6,&#x27;Lianshan Pai&#x27;,27),(7,&#x27;Ming Jiao&#x27;,27),(8,&#x27;Xiaoyao Pai&#x27;,15);UNLOCK TABLES;DROP TABLE IF EXISTS `coc`;CREATE TABLE `coc` (  `ID` int(10) unsigned NOT NULL AUTO_INCREMENT,  `ClassID` tinyint(3) unsigned NOT NULL,  `CourseID` smallint(5) unsigned DEFAULT NULL,  PRIMARY KEY (`ID`)) ENGINE=InnoDB AUTO_INCREMENT=15 DEFAULT CHARSET=utf8;LOCK TABLES `coc` WRITE;INSERT INTO `coc` VALUES (1,1,2),(2,1,5),(3,2,2),(4,2,6),(5,3,1),(6,3,7),(7,4,5),(8,4,2),(9,5,1),(10,5,9),(11,6,3),(12,6,4),(13,7,4),(14,7,3);UNLOCK TABLES;DROP TABLE IF EXISTS `courses`;CREATE TABLE `courses` (  `CourseID` smallint(5) unsigned NOT NULL AUTO_INCREMENT,  `Course` varchar(100) NOT NULL,  PRIMARY KEY (`CourseID`)) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8;LOCK TABLES `courses` WRITE;/*!40000 ALTER TABLE `courses` DISABLE KEYS */;INSERT INTO `courses` VALUES (1,&#x27;Hamo Gong&#x27;),(2,&#x27;Kuihua Baodian&#x27;),(3,&#x27;Jinshe Jianfa&#x27;),(4,&#x27;Taiji Quan&#x27;),(5,&#x27;Daiyu Zanghua&#x27;),(6,&#x27;Weituo Zhang&#x27;),(7,&#x27;Dagou Bangfa&#x27;);/*!40000 ALTER TABLE `courses` ENABLE KEYS */;UNLOCK TABLES;DROP TABLE IF EXISTS `scores`;CREATE TABLE `scores` (  `ID` int(10) unsigned NOT NULL AUTO_INCREMENT,  `StuID` int(10) unsigned NOT NULL,  `CourseID` smallint(5) unsigned NOT NULL,  `Score` tinyint(3) unsigned DEFAULT NULL,  PRIMARY KEY (`ID`)) ENGINE=InnoDB AUTO_INCREMENT=16 DEFAULT CHARSET=utf8;LOCK TABLES `scores` WRITE;INSERT INTO `scores` VALUES (1,1,2,77),(2,1,6,93),(3,2,2,47),(4,2,5,97),(5,3,2,88),(6,3,6,75),(7,4,5,71),(8,4,2,89),(9,5,1,39),(10,5,7,63),(11,6,1,96),(12,7,1,86),(13,7,7,83),(14,8,4,57),(15,8,3,93);UNLOCK TABLES;DROP TABLE IF EXISTS `students`;CREATE TABLE `students` (  `StuID` int(10) unsigned NOT NULL AUTO_INCREMENT,  `Name` varchar(50) NOT NULL,  `Age` tinyint(3) unsigned NOT NULL,  `Gender` enum(&#x27;F&#x27;,&#x27;M&#x27;) NOT NULL,  `ClassID` tinyint(3) unsigned DEFAULT NULL,  `TeacherID` int(10) unsigned DEFAULT NULL,  PRIMARY KEY (`StuID`)) ENGINE=InnoDB AUTO_INCREMENT=26 DEFAULT CHARSET=utf8;LOCK TABLES `students` WRITE;INSERT INTO `students` VALUES (1,&#x27;Shi Zhongyu&#x27;,22,&#x27;M&#x27;,2,3),(2,&#x27;Shi Potian&#x27;,22,&#x27;M&#x27;,1,7),(3,&#x27;Xie Yanke&#x27;,53,&#x27;M&#x27;,2,16),(4,&#x27;Ding Dian&#x27;,32,&#x27;M&#x27;,4,4),(5,&#x27;Yu Yutong&#x27;,26,&#x27;M&#x27;,3,1),(6,&#x27;Shi Qing&#x27;,46,&#x27;M&#x27;,5,NULL),(7,&#x27;Xi Ren&#x27;,19,&#x27;F&#x27;,3,NULL),(8,&#x27;Lin Daiyu&#x27;,17,&#x27;F&#x27;,7,NULL),(9,&#x27;Ren Yingying&#x27;,20,&#x27;F&#x27;,6,NULL),(10,&#x27;Yue Lingshan&#x27;,19,&#x27;F&#x27;,3,NULL),(11,&#x27;Yuan Chengzhi&#x27;,23,&#x27;M&#x27;,6,NULL),(12,&#x27;Wen Qingqing&#x27;,19,&#x27;F&#x27;,1,NULL),(13,&#x27;Tian Boguang&#x27;,33,&#x27;M&#x27;,2,NULL),(14,&#x27;Lu Wushuang&#x27;,17,&#x27;F&#x27;,3,NULL),(15,&#x27;Duan Yu&#x27;,19,&#x27;M&#x27;,4,NULL),(16,&#x27;Xu Zhu&#x27;,21,&#x27;M&#x27;,1,NULL),(17,&#x27;Lin Chong&#x27;,25,&#x27;M&#x27;,4,NULL),(18,&#x27;Hua Rong&#x27;,23,&#x27;M&#x27;,7,NULL),(19,&#x27;Xue Baochai&#x27;,18,&#x27;F&#x27;,6,NULL),(20,&#x27;Diao Chan&#x27;,19,&#x27;F&#x27;,7,NULL),(21,&#x27;Huang Yueying&#x27;,22,&#x27;F&#x27;,6,NULL),(22,&#x27;Xiao Qiao&#x27;,20,&#x27;F&#x27;,1,NULL),(23,&#x27;Ma Chao&#x27;,23,&#x27;M&#x27;,4,NULL),(24,&#x27;Xu Xian&#x27;,27,&#x27;M&#x27;,NULL,NULL),(25,&#x27;Sun Dasheng&#x27;,100,&#x27;M&#x27;,NULL,NULL);UNLOCK TABLES;DROP TABLE IF EXISTS `teachers`;CREATE TABLE `teachers` (  `TID` smallint(5) unsigned NOT NULL AUTO_INCREMENT,  `Name` varchar(100) NOT NULL,  `Age` tinyint(3) unsigned NOT NULL,  `Gender` enum(&#x27;F&#x27;,&#x27;M&#x27;) DEFAULT NULL,  PRIMARY KEY (`TID`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;LOCK TABLES `teachers` WRITE;INSERT INTO `teachers` VALUES (1,&#x27;Song Jiang&#x27;,45,&#x27;M&#x27;),(2,&#x27;Zhang Sanfeng&#x27;,94,&#x27;M&#x27;),(3,&#x27;Miejue Shitai&#x27;,77,&#x27;F&#x27;),(4,&#x27;Lin Chaoying&#x27;,93,&#x27;F&#x27;);UNLOCK TABLES;DROP TABLE IF EXISTS `toc`;CREATE TABLE `toc` (  `ID` int(10) unsigned NOT NULL AUTO_INCREMENT,  `CourseID` smallint(5) unsigned DEFAULT NULL,  `TID` smallint(5) unsigned DEFAULT NULL,  PRIMARY KEY (`ID`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;LOCK TABLES `toc` WRITE;UNLOCK TABLES;EOF
在主节点导入
mysql -uroot -p123456 &lt; hellodb.sqlmysql -uroot -p123456 hellodb -e &quot;show tables;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+-------------------+| Tables_in_hellodb |+-------------------+| classes           || coc               || courses           || scores            || students          || teachers          || toc               |+-------------------+mysql -uroot -p123456use hellodb;insert into teachers (name,age,gender)values(&quot;XIAOHU&quot;,18,&#x27;M&#x27;);select * from teachers;(root@localhost) [hellodb]&gt; select * from teachers;+-----+---------------+-----+--------+| TID | Name          | Age | Gender |+-----+---------------+-----+--------+|   1 | Song Jiang    |  45 | M      ||   2 | Zhang Sanfeng |  94 | M      ||   3 | Miejue Shitai |  77 | F      ||   4 | Lin Chaoying  |  93 | F      ||   5 | XIAOHU        |  18 | M      |#这就是新加的数据+-----+---------------+-----+--------+
在从节点查看是否同步
mysql -uroot -p123456 hellodb -e &quot;use hellodb;select * from teachers;&quot;[root@slave ~]# mysql -uroot -p123456 hellodb -e &quot;use hellodb;select * from teachers;&quot;mysql: [Warning] Using a password on the command line interface can be insecure.+-----+---------------+-----+--------+| TID | Name          | Age | Gender |+-----+---------------+-----+--------+|   1 | Song Jiang    |  45 | M      ||   2 | Zhang Sanfeng |  94 | M      ||   3 | Miejue Shitai |  77 | F      ||   4 | Lin Chaoying  |  93 | F      ||   5 | XIAOHU        |  18 | M      |+-----+---------------+-----+--------+
至此主从复制成功同步
部署mycat
注意：这台机上不能用mysql等其他数据库
操作节点：[mycat]
安装jdk
wget https://alist.qianyios.top/d/%E6%B8%B8%E5%AE%A2/%E8%B1%86%E5%8C%85haha/%E8%BF%90%E7%BB%B4/mysql/%E5%AE%89%E8%A3%85%E5%8C%85/jdk-8u202-linux-x64.rpm?sign=1F6ZiF7m6XPKTiV2PiNEH2xIInIVrb2uHIm3TvyFXG0=:0mv &#x27;jdk-8u202-linux-x64.rpm?sign=1F6ZiF7m6XPKTiV2PiNEH2xIInIVrb2uHIm3TvyFXG0=:0&#x27; jdk-8u202-linux-x64.rpmrpm -ivh jdk-8u202-linux-x64.rpm
[root@mycat ~]# java -versionjava version &quot;1.8.0_202&quot;Java(TM) SE Runtime Environment (build 1.8.0_202-b08)Java HotSpot(TM) 64-Bit Server VM (build 25.202-b08, mixed mode)
下载安装mycat
wget https://alist.qianyios.top/d/%E6%B8%B8%E5%AE%A2/%E8%B1%86%E5%8C%85haha/%E8%BF%90%E7%BB%B4/mysql/%E5%AE%89%E8%A3%85%E5%8C%85/Mycat-server-1.6.7.6-release-20210303094759-linux.tar.gz?sign=T83V18vz0xSMy6INAGV9eAzETOL7c0gxQuZA5YYWlhw=:0mv &#x27;Mycat-server-1.6.7.6-release-20210303094759-linux.tar.gz?sign=T83V18vz0xSMy6INAGV9eAzETOL7c0gxQuZA5YYWlhw=:0&#x27; Mycat-server-1.6.7.6-release-20210303094759-linux.tar.gzmkdir -p /appstar xf Mycat-server-1.6.7.6-release-20210303094759-linux.tar.gz -C /apps
mycat安装目录结构：

bin  mycat命令，启动、重启、停止等
catlet  catlet为Mycat的一个扩展功能
conf  Mycat 配置信息,重点关注
lib  Mycat引用的jar包，Mycat是java开发的
logs  日志文件，包括Mycat启动的日志和运行的日志
version.txt  mycat版本说明

logs目录:

wrapper.log mycat启动日志
mycat.log mycat详细工作日志

Mycat的配置文件都在conf目录里面，这里介绍几个常用的文件：

server.xml Mycat软件本身相关的配置文件，设置账号、参数等
schema.xml Mycat对应的物理数据库和数据库表的配置,读写分离、高可用、分布式策略定制、节点控制
rule.xml Mycat分片（分库分表）规则配置文件,记录分片规则列表、使用方法等

启动和连接
echo &#x27;PATH=/apps/mycat/bin:$PATH&#x27; &gt; /etc/profile.d/mycat.shsource /etc/profile.d/mycat.shmycat start#连接mycat：mysql -uroot -p123456 -h 127.0.0.1 -P8066exit
Mycat 主要配置文件说明
server.xml
存放Mycat软件本身相关的配置文件，比如：连接Mycat的用户，密码，数据库名称等
server.xml文件中配置的参数解释说明：



参数
说明




user
用户配置节点


name
客户端登录 MyCAT 的用户名，也就是客户端用来连接 Mycat 的用户名。


password
客户端登录 MyCAT 的密码


schemas
数据库名，这里会和 schema.xml 中的配置关联，多个用逗号分开，例如：db1,db2


privileges
配置用户针对表的增删改查的权限


readOnly
mycat 逻辑库所具有的权限。true 为只读，false 为读写都有，默认为 false



注意：

server.xml文件里登录mycat的用户名和密码可以任意定义，这个账号和密码是为客户机登录mycat时使用的账号信息
逻辑库名(如上面的TESTDB，也就是登录mycat后显示的库名，切换这个库之后，显示的就是代理的真实mysql数据库的表)要在schema.xml里面也定义，否则会导致mycat服务启动失败！
这里只定义了一个标签，所以把多余的都注释了。如果定义多个标签，即设置多个连接mycat的用户名和密码，那么就需要在schema.xml文件中定义多个对应的库！

schema.xml
是最主要的配置项，此文件关联mysql读写分离策略，读写分离、分库分表策略、分片节点都是在此文件中配置的.MyCat作为中间件，它只是一个代理，本身并不进行数据存储，需要连接后端的MySQL物理服务器，此文件就是用来连接MySQL服务器的
schema.xml文件中配置的参数解释说明：



参数
说明




schema
数据库设置，此数据库为逻辑数据库，name 与 server.xml 中的 schema 对应


dataNode
分片信息，也就是分库相关配置


dataHost
物理数据库，真正存储数据的数据库



配置说明
name属性唯一标识dataHost标签，供上层的标签使用。
maxCon属性指定每个读写实例连接池的最大连接。也就是说，标签内嵌套的writeHost、readHost标签都会使用这个属性的值来实例化出连接池的最大连接数
minCon属性指定每个读写实例连接池的最小连接，初始化连接池的大小
每个节点的属性逐一说明
schema:



属性
说明




name
逻辑数据库名，与 server.xml 中的 schema 对应


checkSQLschema
数据库前缀相关设置，这里为 false


sqlMaxLimit
SELECT 时默认的 LIMIT，避免查询全表



table



属性
说明




name
表名，物理数据库中表名


dataNode
表存储到哪些节点，多个节点用逗号分隔。节点为下文 dataNode 设置的 name


primaryKey
主键字段名，自动生成主键时需要设置


autoIncrement
是否自增


rule
分片规则名，具体规则下文 rule 详细介绍



dataNode



属性
说明




name
节点名，与 table 中的 dataNode 对应


datahost
物理数据库名，与 datahost 中的 name 对应


database
物理数据库中数据库名



dataHost



属性
说明




name
物理数据库名，与 dataNode 中的 dataHost 对应


balance
均衡负载的方式


writeType
写入方式


dbType
数据库类型


heartbeat
心跳检测语句，注意语句结尾的分号要加



schema.xml文件中有三点需要注意：balance=“1”，writeType=“0” ,switchType=“1”
schema.xml中的balance的取值决定了负载均衡对非事务内的读操作的处理。balance 属性负载均衡类型，目前的取值有 4 种：
balance=&quot;0&quot;：不开启读写分离机制，所有读操作都发送到当前可用的writeHost上,即读请求仅发送到writeHost上
balance=&quot;1&quot;：一般用此模式，读请求随机分发到当前writeHost对应的readHost和standby的writeHost上。即全部的readHost与stand by writeHost 参与 select 语句的负载均衡，简单的说，当双主双从模式(M1 -&gt;S1 ， M2-&gt;S2，并且 M1 与 M2 互为主备)，正常情况下， M2,S1, S2 都参与 select 语句的负载均衡
balance=&quot;2&quot;：读请求随机分发到当前dataHost内所有的writeHost和readHost上。即所有读操作都随机的在writeHost、 readhost 上分发
balance=&quot;3&quot;：读请求随机分发到当前writeHost对应的readHost上。即所有读请求随机的分发到wiriterHost 对应的 readhost 执行, writerHost 不负担读压力，注意 balance=3 只在 1.4 及其以后版本有，1.3 没有
writeHost和readHost 标签
这两个标签都指定后端数据库的相关配置给mycat，用于实例化后端连接池。
唯一不同的是：writeHost指定写实例、readHost指定读实例，组合这些读写实例来满足系统的要求。
在一个dataHost内可以定义多个writeHost和readHost。但是，如果writeHost指定的后端数据库宕机，那么这个writeHost绑定的所有readHost都将不可用。另一方面，由于这个writeHost宕机系统会自动的检测到，并切换到备用的writeHost上去
注意：Mycat主从分离只是在读的时候做了处理，写入数据的时候，只会写入到writehost，需要通过mycat的主从复制将数据复制到readhost
修改server.xml文件配置Mycat的连接信息
vim /apps/mycat/conf/server.xml
        &lt;property name=&quot;processorBufferPoolType&quot;&gt;0&lt;/property&gt;#在上行下面添加下面的信息，端口3306        &lt;property name=&quot;serverPort&quot;&gt;3306&lt;/property&gt;        &lt;property name=&quot;managerPort&quot;&gt;9066&lt;/property&gt;        &lt;property name=&quot;idleTimeout&quot;&gt;300000&lt;/property&gt;        &lt;property name=&quot;authTimeout&quot;&gt;15000&lt;/property&gt;        &lt;property name=&quot;bindIp&quot;&gt;0.0.0.0&lt;/property&gt;        &lt;property name=&quot;dataNodeIdleCheckPeriod&quot;&gt;300000&lt;/property&gt;        &lt;property name=&quot;frontWriteQueueSize&quot;&gt;4096&lt;/property&gt;        &lt;property name=&quot;processors&quot;&gt;32&lt;/property&gt;                #再往下面翻翻，找到下面这行，用户名root，密码123456        &lt;user name=&quot;root&quot; defaultAccount=&quot;true&quot;&gt;        &lt;property name=&quot;password&quot;&gt;123456&lt;/property&gt;        &lt;property name=&quot;schemas&quot;&gt;TESTDB&lt;/property&gt;        #schemas对应的TESTDB 数据库要和后面要改的schema.xml相对应
这里填的是用户去连接的mysql，也就是说用户用mysql -uroot -p123465连的是mycat1的这个虚拟的TESTDB数据库，但是他映射的是后端的数据库，这个在下一步会讲到
修改schema.xml
#直接复制我的文件就行了，全选一键复制，看一下有什么信息要修改，修改后就可以复制了cat &gt; /apps/mycat/conf/schema.xml &lt;&lt;&quot;EOF&quot;&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;schema name=&quot;TESTDB&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot; dataNode=&quot;dn1&quot;&gt; &lt;/schema&gt; &lt;dataNode name=&quot;dn1&quot; dataHost=&quot;localhost1&quot; database=&quot;hellodb&quot; /&gt; &lt;dataHost name=&quot;localhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;1&quot; 			writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;!-- 写的节点--&gt; &lt;writeHost host=&quot;host1&quot; url=&quot;192.168.48.10:3306&quot; user=&quot;qianyios&quot; password=&quot;123456&quot;&gt;      &lt;!-- 读的节点--&gt;     &lt;readHost host=&quot;host2&quot; url=&quot;192.168.48.11:3306&quot; user=&quot;qianyios&quot; password=&quot;123456&quot; /&gt; &lt;/writeHost&gt;  &lt;/dataHost&gt;&lt;/mycat:schema&gt;EOF

&lt;schema name=&quot;TESTDB&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot; dataNode=&quot;dn1&quot;&gt; &lt;/schema&gt;这一行的dn1对应下面的这一行
&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;localhost1&quot; database=&quot;hellodb&quot; /&gt;
TESTDB是前面上一步修改server.xml提到的要和前面一一对应的虚拟数据库
hellodb是后端对应的数据库名，就是说，我后端有哪个数据库是想映射到mycat的，就写，我总不能说所有数据库都写都映射
&lt;dataHost name=&quot;localhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;1&quot;  			writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt;
其中balance=&quot;1&quot;设置为1表示读写分离
 
&lt;writeHost host=&quot;host1&quot; url=&quot;192.168.48.10:3306&quot; user=&quot;qianyios&quot; password=&quot;123456&quot;&gt;

&lt;readHost host=&quot;host2&quot; url=&quot;192.168.48.11:3306&quot; user=&quot;qianyios&quot; password=&quot;123456&quot; /&gt;
这个qianyios用户等等后面会创建


#重启mycatmycat stopmycat start#查看端口状态[root@mycat ~]# ss -ntlpState      Recv-Q     Send-Q          Local Address:Port            Peer Address:Port     ProcessLISTEN     0          1                   127.0.0.1:32000                0.0.0.0:*         users:((&quot;java&quot;,pid=60546,fd=4))LISTEN     0          128                   0.0.0.0:22                   0.0.0.0:*         users:((&quot;sshd&quot;,pid=991,fd=7))LISTEN     0          2048                        *:9066#这个                *:*         users:((&quot;java&quot;,pid=60546,fd=179))LISTEN     0          50                          *:35183                      *:*         users:((&quot;java&quot;,pid=60546,fd=70))LISTEN     0          128                      [::]:22                      [::]:*         users:((&quot;sshd&quot;,pid=991,fd=8))LISTEN     0          50                          *:36743                      *:*         users:((&quot;java&quot;,pid=60546,fd=68))LISTEN     0          50                          *:1984                       *:*         users:((&quot;java&quot;,pid=60546,fd=69))LISTEN     0          2048                        *:3306#这个               *:*         users:((&quot;java&quot;,pid=60546,fd=183))[root@mycat ~]#
在master主节点导入测试的数据库
#全新复制运行即可，要退出mysql，在shell终端运行cat &gt; hellodb.sql &lt;&lt;&quot;EOF&quot;CREATE DATABASE /*!32312 IF NOT EXISTS*/ `hellodb` /*!40100 DEFAULT CHARACTER SET utf8 */;USE `hellodb`;DROP TABLE IF EXISTS `classes`;CREATE TABLE `classes` (  `ClassID` tinyint(3) unsigned NOT NULL AUTO_INCREMENT,  `Class` varchar(100) DEFAULT NULL,  `NumOfStu` smallint(5) unsigned DEFAULT NULL,  PRIMARY KEY (`ClassID`)) ENGINE=InnoDB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8;LOCK TABLES `classes` WRITE;INSERT INTO `classes` VALUES(1,&#x27;Shaolin Pai&#x27;,10),(2,&#x27;Emei Pai&#x27;,7),(3,&#x27;QingCheng Pai&#x27;,11),(4,&#x27;Wudang Pai&#x27;,12),(5,&#x27;Riyue Shenjiao&#x27;,31),(6,&#x27;Lianshan Pai&#x27;,27),(7,&#x27;Ming Jiao&#x27;,27),(8,&#x27;Xiaoyao Pai&#x27;,15);UNLOCK TABLES;DROP TABLE IF EXISTS `coc`;CREATE TABLE `coc` (  `ID` int(10) unsigned NOT NULL AUTO_INCREMENT,  `ClassID` tinyint(3) unsigned NOT NULL,  `CourseID` smallint(5) unsigned DEFAULT NULL,  PRIMARY KEY (`ID`)) ENGINE=InnoDB AUTO_INCREMENT=15 DEFAULT CHARSET=utf8;LOCK TABLES `coc` WRITE;INSERT INTO `coc` VALUES (1,1,2),(2,1,5),(3,2,2),(4,2,6),(5,3,1),(6,3,7),(7,4,5),(8,4,2),(9,5,1),(10,5,9),(11,6,3),(12,6,4),(13,7,4),(14,7,3);UNLOCK TABLES;DROP TABLE IF EXISTS `courses`;CREATE TABLE `courses` (  `CourseID` smallint(5) unsigned NOT NULL AUTO_INCREMENT,  `Course` varchar(100) NOT NULL,  PRIMARY KEY (`CourseID`)) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8;LOCK TABLES `courses` WRITE;/*!40000 ALTER TABLE `courses` DISABLE KEYS */;INSERT INTO `courses` VALUES (1,&#x27;Hamo Gong&#x27;),(2,&#x27;Kuihua Baodian&#x27;),(3,&#x27;Jinshe Jianfa&#x27;),(4,&#x27;Taiji Quan&#x27;),(5,&#x27;Daiyu Zanghua&#x27;),(6,&#x27;Weituo Zhang&#x27;),(7,&#x27;Dagou Bangfa&#x27;);/*!40000 ALTER TABLE `courses` ENABLE KEYS */;UNLOCK TABLES;DROP TABLE IF EXISTS `scores`;CREATE TABLE `scores` (  `ID` int(10) unsigned NOT NULL AUTO_INCREMENT,  `StuID` int(10) unsigned NOT NULL,  `CourseID` smallint(5) unsigned NOT NULL,  `Score` tinyint(3) unsigned DEFAULT NULL,  PRIMARY KEY (`ID`)) ENGINE=InnoDB AUTO_INCREMENT=16 DEFAULT CHARSET=utf8;LOCK TABLES `scores` WRITE;INSERT INTO `scores` VALUES (1,1,2,77),(2,1,6,93),(3,2,2,47),(4,2,5,97),(5,3,2,88),(6,3,6,75),(7,4,5,71),(8,4,2,89),(9,5,1,39),(10,5,7,63),(11,6,1,96),(12,7,1,86),(13,7,7,83),(14,8,4,57),(15,8,3,93);UNLOCK TABLES;DROP TABLE IF EXISTS `students`;CREATE TABLE `students` (  `StuID` int(10) unsigned NOT NULL AUTO_INCREMENT,  `Name` varchar(50) NOT NULL,  `Age` tinyint(3) unsigned NOT NULL,  `Gender` enum(&#x27;F&#x27;,&#x27;M&#x27;) NOT NULL,  `ClassID` tinyint(3) unsigned DEFAULT NULL,  `TeacherID` int(10) unsigned DEFAULT NULL,  PRIMARY KEY (`StuID`)) ENGINE=InnoDB AUTO_INCREMENT=26 DEFAULT CHARSET=utf8;LOCK TABLES `students` WRITE;INSERT INTO `students` VALUES (1,&#x27;Shi Zhongyu&#x27;,22,&#x27;M&#x27;,2,3),(2,&#x27;Shi Potian&#x27;,22,&#x27;M&#x27;,1,7),(3,&#x27;Xie Yanke&#x27;,53,&#x27;M&#x27;,2,16),(4,&#x27;Ding Dian&#x27;,32,&#x27;M&#x27;,4,4),(5,&#x27;Yu Yutong&#x27;,26,&#x27;M&#x27;,3,1),(6,&#x27;Shi Qing&#x27;,46,&#x27;M&#x27;,5,NULL),(7,&#x27;Xi Ren&#x27;,19,&#x27;F&#x27;,3,NULL),(8,&#x27;Lin Daiyu&#x27;,17,&#x27;F&#x27;,7,NULL),(9,&#x27;Ren Yingying&#x27;,20,&#x27;F&#x27;,6,NULL),(10,&#x27;Yue Lingshan&#x27;,19,&#x27;F&#x27;,3,NULL),(11,&#x27;Yuan Chengzhi&#x27;,23,&#x27;M&#x27;,6,NULL),(12,&#x27;Wen Qingqing&#x27;,19,&#x27;F&#x27;,1,NULL),(13,&#x27;Tian Boguang&#x27;,33,&#x27;M&#x27;,2,NULL),(14,&#x27;Lu Wushuang&#x27;,17,&#x27;F&#x27;,3,NULL),(15,&#x27;Duan Yu&#x27;,19,&#x27;M&#x27;,4,NULL),(16,&#x27;Xu Zhu&#x27;,21,&#x27;M&#x27;,1,NULL),(17,&#x27;Lin Chong&#x27;,25,&#x27;M&#x27;,4,NULL),(18,&#x27;Hua Rong&#x27;,23,&#x27;M&#x27;,7,NULL),(19,&#x27;Xue Baochai&#x27;,18,&#x27;F&#x27;,6,NULL),(20,&#x27;Diao Chan&#x27;,19,&#x27;F&#x27;,7,NULL),(21,&#x27;Huang Yueying&#x27;,22,&#x27;F&#x27;,6,NULL),(22,&#x27;Xiao Qiao&#x27;,20,&#x27;F&#x27;,1,NULL),(23,&#x27;Ma Chao&#x27;,23,&#x27;M&#x27;,4,NULL),(24,&#x27;Xu Xian&#x27;,27,&#x27;M&#x27;,NULL,NULL),(25,&#x27;Sun Dasheng&#x27;,100,&#x27;M&#x27;,NULL,NULL);UNLOCK TABLES;DROP TABLE IF EXISTS `teachers`;CREATE TABLE `teachers` (  `TID` smallint(5) unsigned NOT NULL AUTO_INCREMENT,  `Name` varchar(100) NOT NULL,  `Age` tinyint(3) unsigned NOT NULL,  `Gender` enum(&#x27;F&#x27;,&#x27;M&#x27;) DEFAULT NULL,  PRIMARY KEY (`TID`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;LOCK TABLES `teachers` WRITE;INSERT INTO `teachers` VALUES (1,&#x27;Song Jiang&#x27;,45,&#x27;M&#x27;),(2,&#x27;Zhang Sanfeng&#x27;,94,&#x27;M&#x27;),(3,&#x27;Miejue Shitai&#x27;,77,&#x27;F&#x27;),(4,&#x27;Lin Chaoying&#x27;,93,&#x27;F&#x27;);UNLOCK TABLES;DROP TABLE IF EXISTS `toc`;CREATE TABLE `toc` (  `ID` int(10) unsigned NOT NULL AUTO_INCREMENT,  `CourseID` smallint(5) unsigned DEFAULT NULL,  `TID` smallint(5) unsigned DEFAULT NULL,  PRIMARY KEY (`ID`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;LOCK TABLES `toc` WRITE;UNLOCK TABLES;EOF
#在主节点导入mysql -uroot -p123456 &lt; hellodb.sqlmysql -uroot -p123456 hellodb -e &quot;show tables;&quot;

在master节点创建用户并对mycat授权
mysql -uroot -p123456 -e &quot;create user &#x27;qianyios&#x27;@&#x27;192.168.48.%&#x27; identified by &#x27;123456&#x27; ;&quot;mysql -uroot -p123456 -e &quot;GRANT ALL PRIVILEGES ON *.* TO &#x27;qianyios&#x27;@&#x27;192.168.48.%&#x27;;&quot;mysql -uroot -p123456 -e&quot;flush privileges;&quot;
qianyios用来给mycat连接本数据库的的账号，映射到前端
在主节点创建好后，由于主从复制，在从节点是可以看得见的
select host,user from mysql.user

测试读写分离

客户端登入mycat
操作节点:[client]
mysql -u root -p123456 -h192.168.48.128
-h192.168.48.128要链接的是mycat的地址
登上了，但是看不了表

一查看报错
[root@mycat ~]# tail -f /apps/mycat/logs/*

can&#x27;t connect to mysql server ,errmsg:Client does not support authentication protocol requested by server; consider upgrading MySQL client
这是由于 MySQL 8 默认使用了新的认证协议 caching_sha2_password，而 MyCat 客户端可能不支持这种新的认证协议。以下是解决方法：
#在master主节点运行mysql -uroot -p123456 -e &quot;ALTER USER &#x27;qianyios&#x27;@&#x27;192.168.48.%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;123456&#x27;;&quot;mysql -uroot -p123456 -e &quot;FLUSH PRIVILEGES;&quot;
再次重新登入就可以连接了并查看表里
操作节点:[client]
mysql -u root -p123456 -h192.168.48.128use TESTDB;show tables;mysql&gt; show tables;+-------------------+| Tables_in_hellodb |+-------------------+| classes           || coc               || courses           || scores            || students          || teachers          || toc               |+-------------------+7 rows in set (0.00 sec)
验证读写分离
为了确保数据可观，这里要重启一下mycat
操作节点:[mycat]
9066是mycat的管理端口
mycat stop;mycat start;mysql -uroot -p123456 -P9066 -h127.0.0.1#查看读写分离状态信息show @@datasource;

目的读和写的信息数是0
开始测试
操作节点:[client]
mysql -uroot -p123456 -h 192.168.48.128 -P 3306#进行一次查询use TESTDB;select * from teachers;

回到mycat的状态栏

是不是读的操作就出来了，就是他走的是从节点
接下来验证写操作
操作节点:[client]
mysql&gt; insert into teachers(name,age,gender)values(&quot;xiaohu&quot;,25,&#x27;M&#x27;);Query OK, 1 row affected (0.05 sec)
回到mycat的状态栏

是不是一目了然，主节点有了写入的数据，从节点没有
至此读写分离部署成功
模拟从节点宕机
[root@slave ~]# systemctl stop mysqld
然后里面去客户端进行一次查询
use TESTDB;select * from teachers;

然后去mycat节点查看状态

这时候从节点宕机之后，客户端会出现几秒钟的查询不到的状态，然后mycat会把请求转向主节点
面试题
mycat是如何检查主从节点存活的
看我的操作，答案在最后
在主从两个节点的my.cnt添加这个
[mysqld]general_log#重启systemctl restart mysqld
general_log 是一个日志系统，用于记录所有由 MySQL 服务器接收到的连接和执行的 SQL 语句。这个日志对于监控数据库操作、分析性能问题、审计和故障排除非常有用。
[root@master ~]# tail -f /data/mysql/master.log

[root@slave ~]# tail -f /data/mysql/slave.log

解答：在mycat的/apps/mycat/conf/schema.xml的心跳机制里

mycat就会每10s发送一次select user()去监测主从节点状态

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>Mycat</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据hadoop实验</title>
    <url>/posts/d721e715/</url>
    <content><![CDATA[
大数据hadoop实验
镜像下载：ubuntu-18.04.6-desktop-amd64.iso
大作业
下面两个文档，自己选一个，那个你会用你就用，替换逻辑是一样的
word文档：大作业文档
markdown文档：markdown文档
下载之后，打开按Ctrl+H进行替换内容
查找内容：student/202206150540/yanjiaxi
替换为：student/你的学号/你的名字
注意：前后没有斜杠
确定之后你就可以自己复制代码运行了
先给自己的虚拟机打快照，在做大作业

安装ubuntu系统



一路确定就行了

设置dhcp模式否则无法联网安装
#dhcp服务

然后开机，然后选择中文,然后按提示安装


然后就开始安装就行了,到后面重启之后，可能会遇到这个界面

解决办法

然后你再开机就行了
切换阿里云镜像源


等待更新缓存
到桌面后右键桌面空白处打开终端进行输入下面指令
一键安装vm-tools可以实现跨端复制粘贴
sudo apt-get install -y wgetsudo wget https://resource.qianyios.top/init.shsudo chmod +x init.shbash init.sh 
接下来重启等待软件生效之后，你就关机，这时候你要打个快照，以便后面做项目出错可以恢复，然后开机

创建hadoop用户
创建hadoop用户并且设置密码
sudo useradd -m hadoop -s /bin/bashsudo passwd hadoop

给hadoop用户添加sudo权限
sudo adduser hadoop sudo

这时候桌面右上角注销账号切换成hadoop

设置ssh免密
一键全部复制，然后粘贴回车就会自动进行免密

代码中有password=“123456”,记得改成你的hadoop用户的密码

sudo cat &gt;ssh.sh&lt;&lt;&quot;EOF&quot;#!/bin/bashsudo apt-get install openssh-server -ysudo systemctl disable ufw --now# 确保 PasswordAuthentication 设置为 yesecho &quot;正在更新 SSH 配置...&quot;sudo sed -i &#x27;s/^#*PasswordAuthentication.*/PasswordAuthentication yes/&#x27; /etc/ssh/sshd_configsudo systemctl restart ssh# 安装 sshpassecho &quot;正在安装 sshpass...&quot;sudo apt updatesudo apt install -y sshpass || &#123; echo &quot;安装 sshpass 失败&quot;; exit 1; &#125;echo &quot;sshpass 安装完成。&quot;# 创建 .ssh 目录并设置权限echo &quot;正在检查 .ssh 目录...&quot;if [ ! -d ~/.ssh ]; then    sudo mkdir -p ~/.sshfisudo chmod 700 ~/.sshsudo chown -R hadoop:hadoop ~/.ssh# 目标主机列表hosts=(&quot;localhost&quot;)# 密码password=&quot;123456&quot;# 生成 SSH 密钥对echo &quot;正在生成 SSH 密钥对...&quot;if [ ! -f ~/.ssh/id_rsa ]; then    ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa || &#123; echo &quot;生成 SSH 密钥对失败&quot;; exit 1; &#125;fichmod 600 ~/.ssh/id_rsachmod 644 ~/.ssh/id_rsa.pubecho &quot;SSH 密钥对已生成。&quot;# 循环遍历目标主机for host in &quot;$&#123;hosts[@]&#125;&quot;do    echo &quot;正在为 $host 配置免密登录...&quot;        # 确保目标主机的 .ssh 目录存在    sshpass -p &quot;$password&quot; ssh -o StrictHostKeyChecking=no &quot;$host&quot; &quot;mkdir -p ~/.ssh &amp;&amp; chmod 700 ~/.ssh&quot;        # 将公钥复制到目标主机    sshpass -p &quot;$password&quot; ssh-copy-id -i ~/.ssh/id_rsa.pub -o StrictHostKeyChecking=no &quot;$host&quot; || &#123; echo &quot;复制公钥到 $host 失败&quot;; exit 1; &#125;        # 验证免密登录是否成功    sshpass -p &quot;$password&quot; ssh -o StrictHostKeyChecking=no &quot;$host&quot; &quot;echo &#x27;免密登录成功&#x27;&quot; || &#123; echo &quot;验证免密登录失败&quot;; exit 1; &#125;doneecho &quot;所有配置已完成。&quot;EOF
运行脚本
bash ssh.sh
测试登入localhost是否可以实现无密码登入
ssh localhost

成功
安装java和hadoop
将两个文件复制到下载的目录去

然后在这个文件夹下，空白处右键，打开终端

确认一下当前文件夹是不是有这两个文件
ls

以下的全部复制运行
sudo mkdir /usr/lib/jvm#安装java8sudo tar -xf jdk-8u162-linux-x64.tar.gz  -C /usr/lib/jvmecho &quot;export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162&quot; &gt;&gt; ~/.bashrcecho &quot;export PATH=\$JAVA_HOME/bin:\$PATH&quot; &gt;&gt; ~/.bashrcsource ~/.bashrcjava -version#安装hadoop-3.1.3sudo tar -zxf hadoop-3.1.3.tar.gz -C /usr/localsudo mv /usr/local/hadoop-3.1.3/ /usr/local/hadoopecho &quot;export HADOOP_HOME=/usr/local/hadoop&quot; &gt;&gt; ~/.bashrcecho &quot;export PATH=\$HADOOP_HOME/bin/:\$HADOOP_HOME/sbin/:\$PATH&quot; &gt;&gt; ~/.bashrcsource ~/.bashrcsudo chown -R hadoop /usr/local/hadoophadoop version

这里是作业要截图的地方



这时候关机打个快照，命名为基础
伪分布安装
编写cort-site.yaml文件
以下的全部复制运行
cat &gt; /usr/local/hadoop/etc/hadoop/core-site.xml&lt;&lt; &quot;EOF&quot;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;EOF
编写hdfs-site.xml
以下的全部复制运行
cat &gt;/usr/local/hadoop/etc/hadoop/hdfs-site.xml&lt;&lt;&quot;EOF&quot;&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;dfs.replication&lt;/name&gt;        &lt;value&gt;1&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;EOF
启动hhdfs服务
hdfs初始化
这条命令只需要运行一次，以后都不要再运行了！！！！！！
这条命令只需要运行一次，以后都不要再运行了！！！！！！
这条命令只需要运行一次，以后都不要再运行了！！！！！！
hdfs namenode -format

出现这个说明初始化成功
添加hdfs yarn的环境变量
以下的全部复制运行
echo &quot;export HDFS_NAMENODE_USER=hadoop&quot; &gt;&gt; ~/.bashrcecho &quot;export HDFS_DATANODE_USER=hadoop&quot; &gt;&gt; ~/.bashrcecho &quot;export HDFS_SECONDARYNAMENODE_USER=hadoop&quot; &gt;&gt; ~/.bashrcecho &quot;export YARN_RESOURCEMANAGER_USER=hadoop&quot; &gt;&gt; ~/.bashrcecho &quot;export YARN_NODEMANAGER_USER=hadoop&quot; &gt;&gt; ~/.bashrcsource ~/.bashrcecho &quot;export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162&quot; &gt;&gt; /usr/local/hadoop/etc/hadoop/hadoop-env.sh
#开启hadoop的命令start-all.sh#当你要关机的时候先运行下面的命令关掉hadoop先，再关机stop-all.sh

这里是作业要截图的地方


jps命令用来查看进程是否启动，以上是hadoop正常启动的进程，总共有6个
访问hadoop网页
看看你的ip
ip a

如果你这里没有ip说明你没有开启dhcp服务，自行回到最开始，找开启dhcp的方法，关机开启dhcp，然后开机就会有ip了

这里是作业要截图的地方

http://ip:9870
http://192.168.48.132:9870/

http://ip:8088

关机步骤
这时候关闭hadoop集群
stop-all.sh
然后关机打快照，命名伪分布
sudo poweroff
然后在这里打个快照，命名为伪分布安装成功，等你哪天机子坏了，你就可以恢复快照


严肃告知，别说我没提醒你，不要直接关机，也不要挂起虚拟机，否则你的虚拟机和hadoop坏了，你就重装吧


第一次实验
熟悉常用的Linux操作
1）cd命令：切换目录
（1） 切换到目录“/usr/local”
cd /usr/local
（2） 切换到当前目录的上一级目录
cd ..
（3） 切换到当前登录Linux系统的用户的自己的主文件夹
cd ~

2）ls命令：查看文件与目录
查看目录“/usr”下的所有文件和目录
cd /usrls -al

3）mkdir命令：新建目录
（1）进入“/tmp”目录，创建一个名为“a”的目录，并查看“/tmp”目录下已经存在哪些目录
cd /tmpmkdir als -al

（2）进入“/tmp”目录，创建目录“a1/a2/a3/a4”
cd /tmpmkdir -p a1/a2/a3/a4

4）rmdir命令：删除空的目录
（1）将上面创建的目录a（在“/tmp”目录下面）删除
（2）删除上面创建的目录“a1/a2/a3/a4” （在“/tmp”目录下面），然后查看“/tmp”目录下面存在哪些目录
cd /tmprmdir acd /tmprmdir -p a1/a2/a3/a4ls -al

5）cp命令：复制文件或目录
（1）将当前用户的主文件夹下的文件.bashrc复制到目录“/usr”下，并重命名为bashrc1
sudo cp ~/.bashrc /usr/bashrc1

（2）在目录“/tmp”下新建目录test，再把这个目录复制到“/usr”目录下
cd /tmpmkdir testsudo cp -r /tmp/test /usr

6）mv命令：移动文件与目录，或更名
（1）将“/usr”目录下的文件bashrc1移动到“/usr/test”目录下
sudo mv /usr/bashrc1 /usr/test
（2）将“/usr”目录下的test目录重命名为test2
sudo mv /usr/test /usr/test2

7）rm命令：移除文件或目录
（1）将“/usr/test2”目录下的bashrc1文件删除
sudo rm /usr/test2/bashrc1
（2）将“/usr”目录下的test2目录删除
sudo rm -r /usr/test2

8）cat命令：查看文件内容
查看当前用户主文件夹下的.bashrc文件内容
cat ~/.bashrc

9）tac命令：反向查看文件内容
反向查看当前用户主文件夹下的.bashrc文件的内容
tac ~/.bashrc

10）more命令：一页一页翻动查看
翻页查看当前用户主文件夹下的.bashrc文件的内容
more ~/.bashrc

11）head命令：取出前面几行
（1）查看当前用户主文件夹下.bashrc文件内容前20行
head -n 20 ~/.bashrc
（2）查看当前用户主文件夹下.bashrc文件内容，后面50行不显示，只显示前面几行
head -n -50 ~/.bashrc

12）tail命令：取出后面几行
（1）查看当前用户主文件夹下.bashrc文件内容最后20行
tail -n 20 ~/.bashrc
（2）查看当前用户主文件夹下.bashrc文件内容，并且只列出50行以后的数据
tail -n +50 ~/.bashrc

13）touch命令：修改文件时间或创建新文件
（1）在“/tmp”目录下创建一个空文件hello，并查看文件时间
cd /tmptouch hellols -l hello

（2）修改hello文件，将文件时间整为5天前
touch -d &quot;5 days ago&quot; hello

14）chown命令：修改文件所有者权限
将hello文件所有者改为root帐号，并查看属性
sudo chown root /tmp/hellols -l /tmp/hello

15）find命令：文件查找
找出主文件夹下文件名为.bashrc的文件
find ~ -name .bashrc

16）tar命令：压缩命令
（1）在根目录“/”下新建文件夹test，然后在根目录“/”下打包成test.tar.gz
sudo mkdir /testsudo tar -zcv -f /test.tar.gz test
（2）把上面的test.tar.gz压缩包，解压缩到“/tmp”目录
sudo tar -zxv -f /test.tar.gz -C /tmp

17）grep命令：查找字符串
从“～/.bashrc”文件中查找字符串’examples’
grep -n &#x27;examples&#x27; ~/.bashrc

18）配置环境变量
（1）请在“～/.bashrc”中设置，配置Java环境变量
echo &quot;export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162&quot; &gt;&gt; ~/.bashrcecho &quot;export PATH=\$JAVA_HOME/bin:\$PATH&quot; &gt;&gt; ~/.bashrcsource ~/.bashrcjava -version
（2）查看JAVA_HOME变量的值
echo $JAVA_HOME

熟悉常用的Hadoop操作
（1）使用hadoop用户登录Linux系统，启动Hadoop（Hadoop的安装目录为“/usr/local/hadoop”），为hadoop用户在HDFS中创建用户目录“/user/hadoop”
start-dfs.shhdfs dfs -mkdir -p /user/hadoop 

（2）接着在HDFS的目录“/user/hadoop”下，创建test文件夹，并查看文件列表
hdfs dfs -mkdir testhdfs dfs -ls .

（3）将Linux系统本地的“～/.bashrc”文件上传到HDFS的test文件夹中，并查看test
hdfs dfs -put ~/.bashrc testhdfs dfs -ls test

（4）将HDFS文件夹test复制到Linux系统本地文件系统的“/usr/local/hadoop”目录下
hdfs dfs -get test ./

（3.7.3）实验
安装eclipse
为了提高程序编写和调试效率，本教程采用Eclipse工具编写Java程序。
现在要执行的任务是：假设在目录hdfs://localhost:9000/user/hadoop下面有几个文件，分别是file1.txt、file2.txt、file3.txt、file4.abc和file5.abc，这里需要从该目录中过滤出所有后缀名不为.abc的文件，对过滤之后的文件进行读取，并将这些文件的内容合并到文件hdfs://localhost:9000/user/hadoop/merge.txt中。
要确保HDFS的/user/hadoop目录下已经存在file1.txt、file2.txt、file3.txt、file4.abc和file5.abc，每个文件里面有内容。这里，假设文件内容如下：
file1.txt的内容是： this is file1.txt
file2.txt的内容是： this is file2.txt
file3.txt的内容是： this is file3.txt
file4.abc的内容是： this is file4.abc
file5.abc的内容是： this is file5.abc

后面我会给命令，上面的内容就先看看

登入hadoop用户不多说了，启动hadoop集群
start-all.sh
下载eclipse安装包到ubuntu的下载目录,然后在空白处右键打开终端

sudo ls
sudo tar -zxvf eclipse-4.7.0-linux.gtk.x86_64.tar.gz -C /usr/local sudo chown -R hadoop /usr/local/eclipseecho &quot;export ECLIPSE_HOME=/usr/local/eclipse&quot; &gt;&gt; ~/.bashrcecho &quot;export PATH=\$ECLIPSE_HOME/:\$PATH&quot; &gt;&gt; ~/.bashrcsource ~/.bashrc
启动eclipse
eclipse
在Eclipse中创建项目
启动Eclipse。当Eclipse启动以后，会弹出如下图所示界面，提示设置工作空间（workspace）。


选择File--&gt;New--&gt;Java Project菜单，开始创建一个Java工程，会弹出如下图所示界面。在Project name后面输入工程名称HDFSExample，选中Use default location，让这个Java工程的所有文件都保存到/home/hadoop/workspace/HDFSExample目录下。在JRE这个选项卡中，可以选择当前的Linux系统中已经安装好的JDK，比如jdk1.8.0_162。然后，点击界面底部的Next&gt;按钮，进入下一步的设置。

为项目添加需要用到的JAR包
为了能够运行程序，我们有四个目录的jar包要添加到工程去

(1)/usr/local/hadoop/share/hadoop/common目录下的所有JAR包，包括
hadoop-common-3.1.3.jar、hadoop-kms-3.1.3.jar
hadoop-common-3.1.3-tests.jar、hadoop-nfs-3.1.3.jar
注意，不包括目录jdiff、lib、sources和webapps；

(2)/usr/local/hadoop/share/hadoop/common/lib目录下的所有JAR包；
(3)/usr/local/hadoop/share/hadoop/hdfs目录下的所有JAR包，注意，不包括目录jdiff、lib、sources和webapps；
(4)/usr/local/hadoop/share/hadoop/hdfs/lib目录下的所有JAR包。


以下我只演示第一种和第二种!!!!!!!!!
以下我只演示第一种和第二种!!!!!!!!!
以下我只演示第一种和第二种!!!!!!!!!
以下我只演示第一种和第二种!!!!!!!!!

第一种
/usr/local/hadoop/share/hadoop/common目录下的所有JAR包
点击Add External JARs…按钮，点击其他位置，自己看这个路径定位到这/usr/local/hadoop/share/hadoop/common,选择下面的四个包，然后点击ok

第二种
/usr/local/hadoop/share/hadoop/common/lib目录下的所有JAR包；


以下两个目录，我就不演示了，如果有文件夹被全选中，你就按住ctrl然后点击文件夹，就可以取消选中了，我们只添加所有后缀名为.jar的包
(3)/usr/local/hadoop/share/hadoop/hdfs目录下的所有JAR包，注意，不包括目录jdiff、lib、sources和webapps；
(4)/usr/local/hadoop/share/hadoop/hdfs/lib目录下的所有JAR包。

最后是这样的


编写Java应用程序

在该界面中，只需要在Name后面输入新建的Java类文件的名称，这里采用称MergeFile，其他都可以采用默认设置，然后，点击界面右下角Finish按钮。


把下面的代码直接写到MergeFile.java,全选复制粘贴，这就不多说了，然后记得Ctrl+S保存
import java.io.IOException;import java.io.PrintStream;import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.*;/** * 过滤掉文件名满足特定条件的文件  */class MyPathFilter implements PathFilter &#123;     String reg = null;      MyPathFilter(String reg) &#123;          this.reg = reg;     &#125;     public boolean accept(Path path) &#123;        if (!(path.toString().matches(reg)))            return true;        return false;    &#125;&#125;/*** * 利用FSDataOutputStream和FSDataInputStream合并HDFS中的文件 */public class MergeFile &#123;    Path inputPath = null; //待合并的文件所在的目录的路径    Path outputPath = null; //输出文件的路径    public MergeFile(String input, String output) &#123;        this.inputPath = new Path(input);        this.outputPath = new Path(output);    &#125;    public void doMerge() throws IOException &#123;        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://localhost:9000&quot;);          conf.set(&quot;fs.hdfs.impl&quot;,&quot;org.apache.hadoop.hdfs.DistributedFileSystem&quot;);        FileSystem fsSource = FileSystem.get(URI.create(inputPath.toString()), conf);        FileSystem fsDst = FileSystem.get(URI.create(outputPath.toString()), conf);                //下面过滤掉输入目录中后缀为.abc的文件        FileStatus[] sourceStatus = fsSource.listStatus(inputPath,                new MyPathFilter(&quot;.*\\.abc&quot;));         FSDataOutputStream fsdos = fsDst.create(outputPath);        PrintStream ps = new PrintStream(System.out);        //下面分别读取过滤之后的每个文件的内容，并输出到同一个文件中        for (FileStatus sta : sourceStatus) &#123;            //下面打印后缀不为.abc的文件的路径、文件大小            System.out.print(&quot;路径：&quot; + sta.getPath() + &quot;    文件大小：&quot; + sta.getLen()                    + &quot;   权限：&quot; + sta.getPermission() + &quot;   内容：&quot;);            FSDataInputStream fsdis = fsSource.open(sta.getPath());            byte[] data = new byte[1024];            int read = -1;            while ((read = fsdis.read(data)) &gt; 0) &#123;                ps.write(data, 0, read);                fsdos.write(data, 0, read);            &#125;            fsdis.close();        &#125;        ps.close();        fsdos.close();    &#125;    public static void main(String[] args) throws IOException &#123;        MergeFile merge = new MergeFile(                &quot;hdfs://localhost:9000/user/hadoop/&quot;,                &quot;hdfs://localhost:9000/user/hadoop/merge.txt&quot;);        merge.doMerge();    &#125;&#125;
编译运行程序
在这里强调一下，如果你没启动hadoop自行启动，我早已在7.1告知启动了
编写测试文件
echo &quot;this is file1.txt&quot; &gt; file1.txtecho &quot;this is file2.txt&quot; &gt; file2.txtecho &quot;this is file3.txt&quot; &gt; file3.txtecho &quot;this is file4.abc&quot; &gt; file4.abcecho &quot;this is file5.abc&quot; &gt; file5.abchdfs dfs -mkdir -p /user/hadoophdfs dfs -put file1.txt /user/hadoop/hdfs dfs -put file2.txt /user/hadoop/hdfs dfs -put file3.txt /user/hadoop/hdfs dfs -put file4.abc /user/hadoop/hdfs dfs -put file5.abc /user/hadoop/hdfs dfs -ls /user/hadoop




最后验证是否成功
hdfs dfs -cat /user/hadoop/merge.txt

应用程序的部署
因为前面只是在eclipse运行java项目才会生成merge.txt，我们的目的是通过hadoop去执行这个java项目，所以我们要对工程打包
创建myapp目录
目的：用来存放hadoop应用程序目录
mkdir /usr/local/hadoop/myapp
开始打包程序

Launch configuration下拉选择MergeFile-HDFSExample
Export destination填写 /usr/local/hadoop/myapp/HDFSExample.jar



查看是否生成
ls /usr/local/hadoop/myapp

重新验证项目的运行
由于我们在eclipse测试过了项目，之前就在hdfs目录生成了/user/hadoop/merge.txt，为了验证刚刚打包的项目，我们要删掉这个/user/hadoop/merge.txt，等等重新运行项目
hdfs dfs -rm /user/hadoop/merge.txthadoop jar /usr/local/hadoop/myapp/HDFSExample.jarhdfs dfs -cat /user/hadoop/merge.txt


如果你没事了，要关机了就回到这里5.6 关机步骤,去执行关机
顺便把eclipse的窗口关掉

严肃告知，别说我没提醒你，不要直接关机，也不要挂起虚拟机，否则你的虚拟机和你的hadoop坏了，你就重装，如果你坏了你也可以恢复快照到伪分布安装成功，但是你只是要重新做这周的实验


练习文件
写入文件
import org.apache.hadoop.conf.Configuration;  import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.Path; public class write &#123;            public static void main(String[] args) &#123;                 try &#123;                        Configuration conf = new Configuration();                          conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://localhost:9000&quot;);                        conf.set(&quot;fs.hdfs.impl&quot;,&quot;org.apache.hadoop.hdfs.DistributedFileSystem&quot;);                        FileSystem fs = FileSystem.get(conf);                        byte[] buff = &quot;Hello world&quot;.getBytes(); // 要写入的内容                        String filename = &quot;gcc-test&quot;; //要写入的文件名                        FSDataOutputStream os = fs.create(new Path(filename));                        os.write(buff,0,buff.length);                        System.out.println(&quot;Create:&quot;+ filename);                        os.close();                        fs.close();                &#125; catch (Exception e) &#123;                          e.printStackTrace();                  &#125;          &#125;  &#125;

hdfs dfs -ls /user/hadoophdfs dfs -cat /user/hadoop/gcc-test

判断文件是否存在
import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path; public class panduan &#123;        public static void main(String[] args) &#123;                    try &#123;                            String filename = &quot;gcc-test&quot;;                             Configuration conf = new Configuration();                            conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://localhost:9000&quot;);                            conf.set(&quot;fs.hdfs.impl&quot;,&quot;org.apache.hadoop.hdfs.DistributedFileSystem&quot;);                            FileSystem fs = FileSystem.get(conf);                            if(fs.exists(new Path(filename)))&#123;                                    System.out.println(&quot;文件存在&quot;);                            &#125;else&#123;                                    System.out.println(&quot;文件不存在&quot;);                            &#125;                            fs.close();                &#125; catch (Exception e) &#123;                        e.printStackTrace();                &#125;        &#125;&#125; 

读取文件
import java.io.BufferedReader;import java.io.InputStreamReader; import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.fs.FSDataInputStream; public class read &#123;        public static void main(String[] args) &#123;                try &#123;                        Configuration conf = new Configuration();                        conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://localhost:9000&quot;);                        conf.set(&quot;fs.hdfs.impl&quot;,&quot;org.apache.hadoop.hdfs.DistributedFileSystem&quot;);                        FileSystem fs = FileSystem.get(conf);                        Path file = new Path(&quot;gcc-test&quot;);                         FSDataInputStream getIt = fs.open(file);                        BufferedReader d = new BufferedReader(new InputStreamReader(getIt));                        String content = d.readLine(); //读取文件一行                        System.out.println(content);                        d.close(); //关闭文件                        fs.close(); //关闭hdfs                &#125; catch (Exception e) &#123;                        e.printStackTrace();                &#125;        &#125;&#125;

第二次实验
编程实现以下指定功能，并利用Hadoop提供的Shell命令完成相同的任务。
① 向HDFS中上传任意文本文件，如果指定的文件在HDFS中已经存在，由用户指定是追加到原有文件末尾还是覆盖原有的文件。
shell
检查文件是否存在，可以使用如下命令:
echo &quot;gcc-text&quot; &gt; /home/hadoop/text.txthdfs dfs -put /home/hadoop/text.txt /user/hadoop/text.txthdfs dfs -test -e text.txtecho $?

返回 0 表示文件存在。
返回 1 表示文件不存在。
如果结果显示文件已经存在，则用户可以选择追加到原来文件末尾或者覆盖原来文件，具体命令如下：
echo &quot;gcc-local&quot; &gt; /home/hadoop/local.txt
local.txt 是本地文件的路径。
/text.txt 是 HDFS 中的文件路径。
#追加到原文件末尾hdfs dfs -appendToFile local.txt text.txthdfs dfs -cat text.txt#覆盖原来文件，第一种命令形式hdfs dfs -copyFromLocal -f  local.txt text.txthdfs dfs -cat text.txt#覆盖原来文件，第二种命令形式hdfs dfs -cp -f  file:///home/hadoop/local.txt text.txthdfs dfs -cat text.txt



实际上，也可以不用上述方式，而是采用如下命令来实现：
hdfs dfs -rm text.txthdfs dfs -put text.txt hdfs dfs -cat text.txtif $(hdfs dfs -test -e text.txt);then $(hdfs dfs -appendToFile local.txt text.txt);else $(hdfs dfs -copyFromLocal -f local.txt text.txt);fihdfs dfs -cat text.txt

Java

我这里只说一次，自己创建好HDFSApi.java后面的每个实验，都会覆盖前面一个实验的代码
你就不要手欠，去创建别的，你要是自己会也行
后面就不会再说了


import  org.apache.hadoop.conf.Configuration;import  org.apache.hadoop.fs.*;import java.io.*;public class HDFSApi &#123;    /**     * 判断路径是否存在     */    public static boolean test(Configuration  conf, String path) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        return fs.exists(new Path(path));    &#125;    /**     * 复制文件到指定路径     * 若路径已存在，则进行覆盖     */    public static void  copyFromLocalFile(Configuration conf, String localFilePath, String  remoteFilePath) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path localPath = new  Path(localFilePath);        Path remotePath = new  Path(remoteFilePath);        /* fs.copyFromLocalFile 第一个参数表示是否删除源文件，第二个参数表示是否覆盖 */        fs.copyFromLocalFile(false, true,  localPath, remotePath);        fs.close();    &#125;    /**     * 追加文件内容     */    public static void  appendToFile(Configuration conf, String localFilePath, String remoteFilePath)  throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path remotePath = new  Path(remoteFilePath);        /* 创建一个文件读入流 */        FileInputStream in = new  FileInputStream(localFilePath);        /* 创建一个文件输出流，输出的内容将追加到文件末尾 */        FSDataOutputStream out =  fs.append(remotePath);        /* 读写文件内容 */        byte[] data = new byte[1024];        int read = -1;        while ( (read = in.read(data)) &gt; 0  ) &#123;            out.write(data,  0, read);        &#125;        out.close();        in.close();        fs.close();    &#125;    /**     * 主函数     */    public static void main(String[] args)  &#123;        Configuration conf = new  Configuration();        conf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000&quot;);        String localFilePath =  &quot;/home/hadoop/text.txt&quot;;     // 本地路径        String remoteFilePath =  &quot;/user/hadoop/text.txt&quot;;     // HDFS路径        String choice =  &quot;append&quot;;    // 若文件存在则追加到文件末尾//            String choice =  &quot;overwrite&quot;;    // 若文件存在则覆盖        try &#123;            /* 判断文件是否存在 */            Boolean fileExists =  false;            if (HDFSApi.test(conf,  remoteFilePath)) &#123;                fileExists = true;                System.out.println(remoteFilePath  + &quot; 已存在.&quot;);            &#125; else &#123;                System.out.println(remoteFilePath  + &quot; 不存在.&quot;);            &#125;            /* 进行处理 */            if ( !fileExists) &#123; // 文件不存在，则上传                HDFSApi.copyFromLocalFile(conf,  localFilePath, remoteFilePath);                System.out.println(localFilePath  + &quot; 已上传至 &quot; + remoteFilePath);            &#125; else if (  choice.equals(&quot;overwrite&quot;) ) &#123;     // 选择覆盖                HDFSApi.copyFromLocalFile(conf,  localFilePath, remoteFilePath);                System.out.println(localFilePath  + &quot; 已覆盖 &quot; + remoteFilePath);            &#125; else if (  choice.equals(&quot;append&quot;) ) &#123;    // 选择追加                HDFSApi.appendToFile(conf,  localFilePath, remoteFilePath);                System.out.println(localFilePath  + &quot; 已追加至 &quot; + remoteFilePath);            &#125;        &#125; catch (Exception e) &#123;            e.printStackTrace();        &#125;    &#125;&#125;

验证
hdfs dfs -cat text.txt

② 从HDFS中下载指定文件，如果本地文件与要下载的文件名称相同，则自动对下载的文件重命名。
shell
ls | grep textif $(hdfs dfs -test -e  file:///home/hadoop/text.txt);then $(hdfs dfs -copyToLocal text.txt ./text2.txt); else $(hdfs dfs -copyToLocal text.txt ./text.txt); fils | grep text

Java
import  org.apache.hadoop.conf.Configuration;import  org.apache.hadoop.fs.*;import java.io.*;public class HDFSApi &#123;    /**     * 下载文件到本地     * 判断本地路径是否已存在，若已存在，则自动进行重命名     */    public static void  copyToLocal(Configuration conf, String remoteFilePath, String localFilePath)  throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path remotePath = new  Path(remoteFilePath);        File f = new File(localFilePath);        /* 如果文件名存在，自动重命名(在文件名后面加上 _0, _1  ...) */        if (f.exists()) &#123;               System.out.println(localFilePath  + &quot; 已存在.&quot;);               Integer  i = 0;               while  (true) &#123;                      f  = new File(localFilePath + &quot;_&quot; + i.toString());                      if  (!f.exists()) &#123;                             localFilePath  = localFilePath + &quot;_&quot; + i.toString();                             break;                      &#125;               &#125;               System.out.println(&quot;将重新命名为: &quot; +  localFilePath);        &#125;        // 下载文件到本地       Path localPath = new  Path(localFilePath);       fs.copyToLocalFile(remotePath,  localPath);       fs.close();    &#125;       /**        * 主函数        */       public static void main(String[] args)  &#123;              Configuration conf = new  Configuration();    conf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000&quot;);              String localFilePath =  &quot;/home/hadoop/text.txt&quot;;     // 本地路径              String remoteFilePath =  &quot;/user/hadoop/text.txt&quot;;     // HDFS路径              try &#123;                     HDFSApi.copyToLocal(conf,  remoteFilePath, localFilePath);                     System.out.println(&quot;下载完成&quot;);              &#125; catch (Exception e) &#123;                     e.printStackTrace();              &#125;       &#125;&#125;

验证：
ls | grep text

③ 将HDFS中指定文件的内容输出到终端。
shell
hdfs dfs -cat text.txt

Java
import  org.apache.hadoop.conf.Configuration;import  org.apache.hadoop.fs.*;import java.io.*;public class HDFSApi &#123;    /**     * 读取文件内容     */    public static void cat(Configuration  conf, String remoteFilePath) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path remotePath = new  Path(remoteFilePath);        FSDataInputStream in =  fs.open(remotePath);        BufferedReader d = new  BufferedReader(new InputStreamReader(in));        String line = null;        while ( (line = d.readLine()) != null  ) &#123;              System.out.println(line);        &#125;       d.close();       in.close();       fs.close();    &#125;       /**        * 主函数        */       public static void main(String[] args)  &#123;              Configuration conf = new  Configuration();    conf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000&quot;);              String remoteFilePath =  &quot;/user/hadoop/text.txt&quot;;     // HDFS路径               try &#123;                     System.out.println(&quot;读取文件: &quot; +  remoteFilePath);                     HDFSApi.cat(conf,  remoteFilePath);                     System.out.println(&quot;\n读取完成&quot;);              &#125; catch (Exception e) &#123;                     e.printStackTrace();              &#125;       &#125;&#125;

④ 显示HDFS中指定的文件读写权限、大小、创建时间、路径等信息。
shell
hdfs dfs -ls -h text.txt

Java
import  org.apache.hadoop.conf.Configuration;import  org.apache.hadoop.fs.*;import java.io.*;import  java.text.SimpleDateFormat;public class HDFSApi &#123;    /**     * 显示指定文件的信息     */    public static void ls(Configuration conf,  String remoteFilePath) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path remotePath = new  Path(remoteFilePath);        FileStatus[] fileStatuses =  fs.listStatus(remotePath);        for (FileStatus s : fileStatuses) &#123;               System.out.println(&quot;路径: &quot; +  s.getPath().toString());               System.out.println(&quot;权限: &quot; +  s.getPermission().toString());               System.out.println(&quot;大小: &quot; +  s.getLen());               /*  返回的是时间戳,转化为时间日期格式 */               Long  timeStamp = s.getModificationTime();               SimpleDateFormat  format =  new  SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);               String  date = format.format(timeStamp);                 System.out.println(&quot;时间: &quot; +  date);        &#125;        fs.close();    &#125;       /**        * 主函数        */       public static void main(String[] args)  &#123;              Configuration conf = new  Configuration();    conf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000&quot;);              String remoteFilePath =  &quot;/user/hadoop/text.txt&quot;;     // HDFS路径              try &#123;                     System.out.println(&quot;读取文件信息: &quot; +  remoteFilePath);                     HDFSApi.ls(conf,  remoteFilePath);                     System.out.println(&quot;\n读取完成&quot;);              &#125; catch (Exception e) &#123;                     e.printStackTrace();              &#125;       &#125;&#125;

⑤ 给定HDFS中某一个目录，输出该目录下的所有文件的读写权限、大小、创建时间、路径等信息，如果该文件是目录，则递归输出该目录下所有文件相关信息。
shell
hdfs dfs -ls -R -h /user/hadoop

别管我这里有什么文件，你能显示出来就行
Java
import  org.apache.hadoop.conf.Configuration;import  org.apache.hadoop.fs.*;import java.io.*;import  java.text.SimpleDateFormat;public class HDFSApi &#123;    /**     * 显示指定文件夹下所有文件的信息（递归）     */    public static void lsDir(Configuration  conf, String remoteDir) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path dirPath = new Path(remoteDir);        /* 递归获取目录下的所有文件 */         RemoteIterator&lt;LocatedFileStatus&gt; remoteIterator =  fs.listFiles(dirPath, true);        /* 输出每个文件的信息 */        while (remoteIterator.hasNext()) &#123;               FileStatus  s = remoteIterator.next();            System.out.println(&quot;路径: &quot; +  s.getPath().toString());            System.out.println(&quot;权限: &quot; +  s.getPermission().toString());            System.out.println(&quot;大小: &quot; +  s.getLen());               /*  返回的是时间戳,转化为时间日期格式 */               Long  timeStamp = s.getModificationTime();               SimpleDateFormat  format =  new  SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);               String  date = format.format(timeStamp);                 System.out.println(&quot;时间: &quot; +  date);               System.out.println();        &#125;        fs.close();    &#125;            /**        * 主函数        */       public static void main(String[] args)  &#123;              Configuration conf = new  Configuration();    conf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000&quot;);              String remoteDir =  &quot;/user/hadoop&quot;;    // HDFS路径              try &#123;                     System.out.println(&quot;(递归)读取目录下所有文件的信息: &quot; +  remoteDir);                     HDFSApi.lsDir(conf,  remoteDir);                     System.out.println(&quot;读取完成&quot;);              &#125; catch (Exception e) &#123;                     e.printStackTrace();              &#125;       &#125;&#125;

⑥ 提供一个HDFS中的文件的路径，对该文件进行创建和删除操作。如果文件所在目录不存在，则自动创建目录。
shell
if $(hdfs dfs -test -d dir1/dir2);then $(hdfs dfs -touchz  dir1/dir2/filename);else $(hdfs dfs -mkdir -p dir1/dir2  &amp;&amp; hdfs dfs -touchz dir1/dir2/filename); fihdfs dfs -rm dir1/dir2/filename

Java
import  org.apache.hadoop.conf.Configuration;import  org.apache.hadoop.fs.*;import java.io.*;public class HDFSApi &#123;    /**     * 判断路径是否存在     */    public static boolean test(Configuration  conf, String path) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        return fs.exists(new Path(path));    &#125;    /**     * 创建目录     */    public static boolean mkdir(Configuration  conf, String remoteDir) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path dirPath = new Path(remoteDir);        boolean result = fs.mkdirs(dirPath);        fs.close();        return result;    &#125;    /**     * 创建文件     */    public static void touchz(Configuration  conf, String remoteFilePath) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path remotePath = new  Path(remoteFilePath);        FSDataOutputStream outputStream =  fs.create(remotePath);        outputStream.close();        fs.close();    &#125;    /**     * 删除文件     */    public static boolean rm(Configuration  conf, String remoteFilePath) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path remotePath = new  Path(remoteFilePath);        boolean result =  fs.delete(remotePath, false);        fs.close();        return result;    &#125;       /**        * 主函数        */       public static void main(String[] args)  &#123;              Configuration conf = new  Configuration();    conf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000&quot;);              String remoteFilePath =  &quot;/user/hadoop/input/text.txt&quot;;     // HDFS路径              String remoteDir =  &quot;/user/hadoop/input&quot;;    //  HDFS路径对应的目录                         try &#123;                     /* 判断路径是否存在，存在则删除，否则进行创建 */                     if ( HDFSApi.test(conf,  remoteFilePath) ) &#123;                            HDFSApi.rm(conf,  remoteFilePath); // 删除                            System.out.println(&quot;删除路径: &quot; +  remoteFilePath);                     &#125; else &#123;                            if (  !HDFSApi.test(conf, remoteDir) ) &#123; // 若目录不存在，则进行创建                                   HDFSApi.mkdir(conf,  remoteDir);                                   System.out.println(&quot;创建文件夹: &quot; +  remoteDir);                            &#125;                            HDFSApi.touchz(conf,  remoteFilePath);                            System.out.println(&quot;创建路径: &quot; +  remoteFilePath);                     &#125;              &#125; catch (Exception e) &#123;                     e.printStackTrace();              &#125;       &#125;&#125;

⑦ 提供一个HDFS的目录的路径，对该目录进行创建和删除操作。创建目录时，如果目录文件所在目录不存在则自动创建相应目录；删除目录时，由用户指定当该目录不为空时是否还删除该目录。
shell
hdfs dfs -mkdir -p dir1/dir2hdfs dfs -rmdir dir1/dir2#上述命令执行以后，如果目录非空，则会提示not empty，删除操作不会执行。如果要强制删除目录，可以使用如下命令：hdfs dfs -rm -R dir1/dir2

Java
import  org.apache.hadoop.conf.Configuration;import  org.apache.hadoop.fs.*;import java.io.*;public class HDFSApi &#123;    /**     * 判断路径是否存在     */    public static boolean test(Configuration  conf, String path) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        return fs.exists(new Path(path));    &#125;    /**     * 判断目录是否为空     * true: 空，false: 非空     */    public static boolean  isDirEmpty(Configuration conf, String remoteDir) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path dirPath = new Path(remoteDir);         RemoteIterator&lt;LocatedFileStatus&gt; remoteIterator =  fs.listFiles(dirPath, true);        return !remoteIterator.hasNext();    &#125;          /**     * 创建目录     */    public static boolean mkdir(Configuration  conf, String remoteDir) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path dirPath = new Path(remoteDir);        boolean result = fs.mkdirs(dirPath);        fs.close();        return result;    &#125;    /**     * 删除目录     */    public static boolean rmDir(Configuration  conf, String remoteDir) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path dirPath = new Path(remoteDir);        /* 第二个参数表示是否递归删除所有文件 */        boolean result = fs.delete(dirPath,  true);        fs.close();        return result;    &#125;       /**        * 主函数        */       public static void main(String[] args)  &#123;              Configuration conf = new  Configuration();    conf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000&quot;);              String remoteDir =  &quot;/user/hadoop/input&quot;;    //  HDFS目录              Boolean forceDelete =  false;  // 是否强制删除              try &#123;                     /* 判断目录是否存在，不存在则创建，存在则删除 */                     if ( !HDFSApi.test(conf,  remoteDir) ) &#123;                            HDFSApi.mkdir(conf,  remoteDir); // 创建目录                            System.out.println(&quot;创建目录: &quot; +  remoteDir);                     &#125; else &#123;                            if (  HDFSApi.isDirEmpty(conf, remoteDir) || forceDelete ) &#123; // 目录为空或强制删除                                   HDFSApi.rmDir(conf,  remoteDir);                                   System.out.println(&quot;删除目录: &quot; +  remoteDir);                            &#125; else  &#123; // 目录不为空                                   System.out.println(&quot;目录不为空，不删除: &quot; +  remoteDir);                            &#125;                     &#125;              &#125; catch (Exception e) &#123;                     e.printStackTrace();              &#125;       &#125;&#125;

⑧ 向HDFS中指定的文件追加内容，由用户指定将内容追加到原有文件的开头或结尾。
shell
rm -rf text.txthdfs dfs -appendToFile local.txt text.txthdfs dfs -get text.txtcat text.txt &gt;&gt; local.txthdfs dfs -copyFromLocal -f text.txt text.txthdfs dfs -cat text.txt

Java
import  org.apache.hadoop.conf.Configuration;import  org.apache.hadoop.fs.*;import java.io.*;public class HDFSApi &#123;    /**     * 判断路径是否存在     */    public static boolean test(Configuration  conf, String path) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        return fs.exists(new Path(path));    &#125;    /**     * 追加文本内容     */    public static void  appendContentToFile(Configuration conf, String content, String  remoteFilePath) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path remotePath = new  Path(remoteFilePath);        /* 创建一个文件输出流，输出的内容将追加到文件末尾 */        FSDataOutputStream out =  fs.append(remotePath);        out.write(content.getBytes());        out.close();        fs.close();&#125;    /**     * 追加文件内容     */    public static void  appendToFile(Configuration conf, String localFilePath, String remoteFilePath)  throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path remotePath = new  Path(remoteFilePath);        /* 创建一个文件读入流 */        FileInputStream in = new  FileInputStream(localFilePath);        /* 创建一个文件输出流，输出的内容将追加到文件末尾 */        FSDataOutputStream out =  fs.append(remotePath);        /* 读写文件内容 */        byte[] data = new byte[1024];        int read = -1;        while ( (read = in.read(data)) &gt; 0  ) &#123;               out.write(data,  0, read);        &#125;        out.close();        in.close();        fs.close();    &#125;    /**     * 移动文件到本地     * 移动后，删除源文件     */    public static void  moveToLocalFile(Configuration conf, String remoteFilePath, String  localFilePath) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path remotePath = new  Path(remoteFilePath);        Path localPath = new  Path(localFilePath);        fs.moveToLocalFile(remotePath,  localPath);    &#125;    /**     * 创建文件     */    public static void touchz(Configuration  conf, String remoteFilePath) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path remotePath = new  Path(remoteFilePath);        FSDataOutputStream outputStream =  fs.create(remotePath);        outputStream.close();        fs.close();    &#125;       /**        * 主函数        */       public static void main(String[] args)  &#123;              Configuration conf = new  Configuration();    conf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000&quot;);              String remoteFilePath =  &quot;/user/hadoop/text.txt&quot;;     // HDFS文件              String content = &quot;新追加的内容\n&quot;;              String choice =  &quot;after&quot;;         //追加到文件末尾//            String choice =  &quot;before&quot;;    // 追加到文件开头              try &#123;                     /* 判断文件是否存在 */                     if ( !HDFSApi.test(conf,  remoteFilePath) ) &#123;                            System.out.println(&quot;文件不存在: &quot; +  remoteFilePath);                     &#125; else &#123;                            if (  choice.equals(&quot;after&quot;) ) &#123; // 追加在文件末尾                                   HDFSApi.appendContentToFile(conf,  content, remoteFilePath);                                   System.out.println(&quot;已追加内容到文件末尾&quot; +  remoteFilePath);                            &#125; else if (  choice.equals(&quot;before&quot;) )  &#123;  // 追加到文件开头                                   /* 没有相应的api可以直接操作，因此先把文件移动到本地*//*创建一个新的HDFS，再按顺序追加内容 */                                   String  localTmpPath = &quot;/user/hadoop/tmp.txt&quot;;                                   // 移动到本地HDFSApi.moveToLocalFile(conf, remoteFilePath,  localTmpPath);   // 创建一个新文件                          HDFSApi.touchz(conf,  remoteFilePath);                     // 先写入新内容                          HDFSApi.appendContentToFile(conf,  content, remoteFilePath);                    // 再写入原来内容                           HDFSApi.appendToFile(conf,  localTmpPath, remoteFilePath);                            System.out.println(&quot;已追加内容到文件开头: &quot; +  remoteFilePath);                            &#125;                     &#125;              &#125; catch (Exception e) &#123;                     e.printStackTrace();              &#125;       &#125;&#125;

hdfs dfs -cat text.txt

⑨ 删除HDFS中指定的文件。
shell
rm text.txthdfs dfs -get text.txthdfs dfs -rm text.txt

hdfs dfs -put text.txt
Java
import  org.apache.hadoop.conf.Configuration;import  org.apache.hadoop.fs.*;import java.io.*;public class HDFSApi &#123;    /**     * 删除文件     */    public static boolean rm(Configuration  conf, String remoteFilePath) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path remotePath = new  Path(remoteFilePath);        boolean result =  fs.delete(remotePath, false);        fs.close();        return result;    &#125;       /**        * 主函数        */       public static void main(String[] args)  &#123;              Configuration conf = new  Configuration();    conf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000&quot;);              String remoteFilePath =  &quot;/user/hadoop/text.txt&quot;;     // HDFS文件                       try &#123;                     if ( HDFSApi.rm(conf,  remoteFilePath) ) &#123;                            System.out.println(&quot;文件删除: &quot; +  remoteFilePath);                     &#125; else &#123;                            System.out.println(&quot;操作失败（文件不存在或删除失败）&quot;);                     &#125;              &#125; catch (Exception e) &#123;                     e.printStackTrace();              &#125;       &#125;&#125;

⑩ 在HDFS中将文件从源路径移动到目的路径。
shell
hdfs dfs -put text.txthdfs dfs -mv text.txt text2.txthdfs dfs -ls

hdfs dfs -put text.txt
Java
import  org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.*;import java.io.*;public class HDFSApi &#123;    /**     * 移动文件     */    public static boolean mv(Configuration  conf, String remoteFilePath, String remoteToFilePath) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path srcPath = new  Path(remoteFilePath);        Path dstPath = new  Path(remoteToFilePath);        boolean result = fs.rename(srcPath,  dstPath);        fs.close();        return result;    &#125;       /**        * 主函数        */       public static void main(String[] args) &#123;              Configuration conf = new Configuration();    conf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000&quot;);              String remoteFilePath =  &quot;hdfs:///user/hadoop/text.txt&quot;;     // 源文件HDFS路径              String remoteToFilePath =  &quot;hdfs:///user/hadoop/new.txt&quot;;     // 目的HDFS路径              try &#123;                     if ( HDFSApi.mv(conf, remoteFilePath,  remoteToFilePath) ) &#123;                            System.out.println(&quot;将文件 &quot; +  remoteFilePath + &quot; 移动到 &quot; + remoteToFilePath);                     &#125; else &#123;                                   System.out.println(&quot;操作失败(源文件不存在或移动失败)&quot;);                     &#125;              &#125; catch (Exception e) &#123;                     e.printStackTrace();              &#125;       &#125;&#125;

hdfs dfs -ls | grep new

（2）编程实现一个类“MyFSDataInputStream”，该类继承“org.apache.hadoop.fs.FSDataInput Stream”，要求如下： 实现按行读取HDFS中指定文件的方法“readLine()”，如果读到文件末尾，则返回空，否则返回文件一行的文本。
shell
hdfs dfs -put text.txt
Java

自己创建好MyFSDataInputStream.java

import  org.apache.hadoop.conf.Configuration;import  org.apache.hadoop.fs.FSDataInputStream;import  org.apache.hadoop.fs.FileSystem;import  org.apache.hadoop.fs.Path;import  java.io.*;public  class MyFSDataInputStream extends FSDataInputStream &#123;       public MyFSDataInputStream(InputStream  in) &#123;              super(in);       &#125;       /**     * 实现按行读取     * 每次读入一个字符，遇到&quot;\n&quot;结束，返回一行内容     */       public static String  readline(BufferedReader br) throws IOException &#123;              char[] data = new char[1024];              int read = -1;              int off = 0; // 循环执行时，br 每次会从上一次读取结束的位置继续读取//因此该函数里，off 每次都从0开始              while ( (read = br.read(data,  off, 1)) != -1 ) &#123;                     if  (String.valueOf(data[off].equals(&quot;\n&quot;) ) &#123;                            off += 1;                            break;                     &#125;                     off += 1;              &#125;                        if (off &gt; 0) &#123;                     return  String.valueOf(data);              &#125; else &#123;                     return null;              &#125;       &#125;       /**     * 读取文件内容     */    public static void cat(Configuration  conf, String remoteFilePath) throws IOException &#123;        FileSystem fs = FileSystem.get(conf);        Path remotePath = new  Path(remoteFilePath);        FSDataInputStream in =  fs.open(remotePath);        BufferedReader br = new  BufferedReader(new InputStreamReader(in));        String line = null;        while ( (line =  MyFSDataInputStream.readline(br)) != null ) &#123;               System.out.println(line);        &#125;        br.close();        in.close();        fs.close();    &#125;          /**        * 主函数        */       public static void main(String[] args)  &#123;              Configuration conf = new  Configuration();    conf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000&quot;);              String remoteFilePath =  &quot;/user/hadoop/text.txt&quot;;     // HDFS路径              try &#123;                     MyFSDataInputStream.cat(conf,  remoteFilePath);              &#125; catch (Exception e) &#123;                     e.printStackTrace();              &#125;       &#125;&#125;

（3）查看Java帮助手册或其他资料，用“java.net.URL”和“org.apache.hadoop.fs.FsURLStream HandlerFactory”编程来输出HDFS中指定文件的文本到终端中。
Java
用回HDFSApi
import org.apache.hadoop.fs.*;import org.apache.hadoop.io.IOUtils;import java.io.*;import java.net.URL;public class HDFSApi &#123;    static &#123;        URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());    &#125;    /**     * 主函数     */    public static void main(String[] args) throws Exception &#123;        // HDFS文件路径，需包含主机名和端口号        String remoteFilePath = &quot;hdfs://localhost:9000/user/hadoop/text.txt&quot;; // 修改为正确的HDFS URI        InputStream in = null;        try &#123;            /* 通过URL对象打开数据流，从中读取数据 */            in = new URL(remoteFilePath).openStream();            IOUtils.copyBytes(in, System.out, 4096, false); // 将数据输出到控制台        &#125; finally &#123;            IOUtils.closeStream(in); // 关闭输入流        &#125;    &#125;&#125;

Hbase安装(4.6.1)
这里有两个题，就是要交两个截图，我会注明
进hadoop用户

自行启动hadoop
安装hbase
sudo ls

sudo tar -xf hbase-2.5.4-bin.tar.gz -C /usr/local/sudo mv /usr/local/hbase-2.5.4 /usr/local/hbasesudo chown -R hadoop:hadoop /usr/local/hbaseecho &quot;export HBASE_HOME=/usr/local/hbase&quot; &gt;&gt; ~/.bashrcecho &quot;export PATH=\$PATH:\$HBASE_HOME/bin&quot; &gt;&gt; ~/.bashrcsource ~/.bashrcsudo sed -i &quot;s/CLASSPATH=\$&#123;CLASSPATH&#125;:\$JAVA_HOME\/lib\/tools.jar/CLASSPATH=\$&#123;CLASSPATH&#125;:\$JAVA_HOME\/lib\/tools.jar:\/usr\/local\/hbase\/lib\/*/g&quot; /usr/local/hbase/bin/hbaseecho &quot;export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162&quot; &gt;&gt; $HBASE_HOME/conf/hbase-env.shecho &quot;export HBASE_CLASSPATH=/usr/local/hbase/conf&quot; &gt;&gt; $HBASE_HOME/conf/hbase-env.shecho &quot;export HBASE_MANAGES_ZK=true&quot; &gt;&gt; $HBASE_HOME/conf/hbase-env.shecho &quot;export HBASE_DISABLE_HADOOP_CLASSPATH_LOOKUP=true&quot; &gt;&gt; $HBASE_HOME/conf/hbase-env.shcat &gt;$HBASE_HOME/conf/hbase-site.xml&lt;&lt;&quot;EOF&quot;&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;hbase.rootdir&lt;/name&gt;                &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;                &lt;value&gt;true&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt;                &lt;value&gt;false&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;EOFhbase version

这个截图交到4.6.1开头第一个作业


启动hbase
开机顺序：一定是先启动hadoop（大）在启动hbase（小）
开机顺序：一定是先启动hadoop（大）在启动hbase（小）
开机顺序：一定是先启动hadoop（大）在启动hbase（小）
start-all.shstart-hbase.sh
然后输入jps,有以下三个个就安装成功

这是4.6.1里最下面的第二个作业截图


测试hbase
hbase shelllist

能运行没报错就行
退出hbase数据库用exit

访问hbase网页
http://ip:16010/

关闭hbase
关机顺序：先关habse(小)再关hadoop（大）
关机顺序：先关habse(小)再关hadoop（大）
关机顺序：先关habse(小)再关hadoop（大）
关机顺序：先关habse(小)再关hadoop（大）
不按操作来，机器坏了，自己重装吧
stop-hbase.shstop-all.shsudo poweroff
关机后自己打个habse的快照
（4.6.2）实验
启动eclipse
eclipse


新建项目
名为HBaseExample


现在有几个目录要添加注意了！！！
现在有几个目录要添加注意了！！！
现在有几个目录要添加注意了！！！
现在有几个目录要添加注意了！！！

/usr/local/hbase/lib下所有的jar包
/usr/local/hbase/lib/client-facing-thirdparty下所有的jar包




最后直接点击finish完成创建
新建class

然后在你创建的这个java文件输入，别运行
import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.*;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.util.Bytes; import java.io.IOException;public class ExampleForHBase &#123;    public static Configuration configuration;    public static Connection connection;    public static Admin admin;    public static void main(String[] args)throws IOException&#123;        init();        createTable(&quot;student&quot;,new String[]&#123;&quot;score&quot;&#125;);        insertData(&quot;student&quot;,&quot;zhangsan&quot;,&quot;score&quot;,&quot;English&quot;,&quot;69&quot;);        insertData(&quot;student&quot;,&quot;zhangsan&quot;,&quot;score&quot;,&quot;Math&quot;,&quot;86&quot;);        insertData(&quot;student&quot;,&quot;zhangsan&quot;,&quot;score&quot;,&quot;Computer&quot;,&quot;77&quot;);        getData(&quot;student&quot;, &quot;zhangsan&quot;, &quot;score&quot;,&quot;English&quot;);        close();    &#125;     public static void init()&#123;        configuration  = HBaseConfiguration.create();        configuration.set(&quot;hbase.rootdir&quot;,&quot;hdfs://localhost:9000/hbase&quot;);        try&#123;            connection = ConnectionFactory.createConnection(configuration);            admin = connection.getAdmin();        &#125;catch (IOException e)&#123;            e.printStackTrace();        &#125;    &#125;     public static void close()&#123;        try&#123;            if(admin != null)&#123;                admin.close();            &#125;            if(null != connection)&#123;                connection.close();            &#125;        &#125;catch (IOException e)&#123;            e.printStackTrace();        &#125;    &#125;     public static void createTable(String myTableName,String[] colFamily) throws IOException &#123;        TableName tableName = TableName.valueOf(myTableName);        if(admin.tableExists(tableName))&#123;            System.out.println(&quot;talbe is exists!&quot;);        &#125;else &#123;            TableDescriptorBuilder tableDescriptor = TableDescriptorBuilder.newBuilder(tableName);            for(String str:colFamily)&#123;                ColumnFamilyDescriptor family = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(str)).build();                tableDescriptor.setColumnFamily(family);            &#125;            admin.createTable(tableDescriptor.build());        &#125;     &#125;     public static void insertData(String tableName,String rowKey,String colFamily,String col,String val) throws IOException &#123;         Table table = connection.getTable(TableName.valueOf(tableName));        Put put = new Put(rowKey.getBytes());        put.addColumn(colFamily.getBytes(),col.getBytes(), val.getBytes());        table.put(put);        table.close();     &#125;     public static void getData(String tableName,String rowKey,String colFamily, String col)throws  IOException&#123;         Table table = connection.getTable(TableName.valueOf(tableName));        Get get = new Get(rowKey.getBytes());        get.addColumn(colFamily.getBytes(),col.getBytes());        Result result = table.get(get);        System.out.println(new String(result.getValue(colFamily.getBytes(),col==null?null:col.getBytes())));        table.close();     &#125;&#125;
自行启动hadoop和hbase，不记得了回去翻记录，我有写启动顺序，别搞错了，搞错了就恢复快照吧，下面是关闭和启动的顺序
9.2 启动hbase
9.3 关闭hbase
没启动就不要做下面的内容！！！
没启动就不要做下面的内容！！！
没启动就不要做下面的内容！！！
没启动就不要做下面的内容！！！
运行代码

就会出现这样的结果

4.6.2实验要交的截图1


这时候进入hbase数据库查看有没有student表
hbase shell
这是进入hbase数据库的命令，我前面也有写后面不会再说了，记不住就自己找办法
listscan &#x27;student&#x27;

4.6.2实验要交的截图2


(4.8实验3)
如果这里你输入第一条和第二条命令就报错，自己找找原因，我不想说了

第一题
（1）编程实现以下指定功能，并用Hadoop提供的HBaseShell命令完成相同的任务。
①列出HBase所有表的相关信息，如表名、创建时间等。
shell
hbase shelllist

java
自己创建一个test.java,要在HBaseExample的项目下,后面一直都会用这个java
import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.*;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.util.Bytes;import java.util.List;import java.io.IOException;public class test &#123;    public static Configuration configuration;    public static Connection connection;    public static Admin admin;    public static void main(String[] args)throws IOException&#123;        init();        List&lt;TableDescriptor&gt;  tableDescriptors = admin.listTableDescriptors();        for(TableDescriptor tableDescriptor :  tableDescriptors)&#123;         TableName tableName =  tableDescriptor.getTableName();         System.out.println(&quot;Table:&quot; + tableName);        &#125;        close();    &#125;    public static void init() &#123;        configuration = HBaseConfiguration.create();        configuration.set(&quot;hbase.rootdir&quot;, &quot;hbase://localhost:9000/hbase&quot;);        try &#123;            connection = ConnectionFactory.createConnection(configuration);            if (connection == null) &#123;                System.err.println(&quot;Failed to create HBase connection.&quot;);            &#125; else &#123;                System.out.println(&quot;HBase connection created successfully.&quot;);            &#125;            admin = connection.getAdmin();            if (admin == null) &#123;                System.err.println(&quot;Failed to get HBase Admin.&quot;);            &#125; else &#123;                System.out.println(&quot;HBase Admin initialized successfully.&quot;);            &#125;        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125;    &#125;     public static void close()&#123;        try&#123;            if(admin != null)&#123;                admin.close();            &#125;            if(null != connection)&#123;                connection.close();            &#125;        &#125;catch (IOException e)&#123;            e.printStackTrace();        &#125;    &#125;     public static void createTable(String myTableName,String[] colFamily) throws IOException &#123;        TableName tableName = TableName.valueOf(myTableName);        if(admin.tableExists(tableName))&#123;            System.out.println(&quot;talbe is exists!&quot;);        &#125;else &#123;            TableDescriptorBuilder tableDescriptor = TableDescriptorBuilder.newBuilder(tableName);            for(String str:colFamily)&#123;                ColumnFamilyDescriptor family = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(str)).build();                tableDescriptor.setColumnFamily(family);            &#125;            admin.createTable(tableDescriptor.build());        &#125;     &#125;     public static void insertData(String tableName,String rowKey,String colFamily,String col,String val) throws IOException &#123;         Table table = connection.getTable(TableName.valueOf(tableName));        Put put = new Put(rowKey.getBytes());        put.addColumn(colFamily.getBytes(),col.getBytes(), val.getBytes());        table.put(put);        table.close();     &#125;     public static void getData(String tableName,String rowKey,String colFamily, String col)throws  IOException&#123;         Table table = connection.getTable(TableName.valueOf(tableName));        Get get = new Get(rowKey.getBytes());        get.addColumn(colFamily.getBytes(),col.getBytes());        Result result = table.get(get);        System.out.println(new String(result.getValue(colFamily.getBytes(),col==null?null:col.getBytes())));        table.close();     &#125;&#125;

②在终端输出指定表的所有记录数据。
shell
scan &#x27;student&#x27;

java
import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.*;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.util.Bytes;import java.util.List;import java.io.IOException;public class test &#123;    public static Configuration configuration;    public static Connection connection;    public static Admin admin;    public static void main(String[] args) throws IOException &#123;    	  // 指定表名 &quot;student&quot; 并获取所有记录    	  String tableName = &quot;student&quot;;    	  getData(tableName);    	 &#125;    	 // 在终端打印出指定表的所有记录数据    	 public static void getData(String tableName) throws IOException &#123;    	  init(); // 初始化连接    	  Table table = connection.getTable(TableName.valueOf(tableName)); // 获取表对象    	  Scan scan = new Scan(); // 创建扫描器    	  ResultScanner scanner = table.getScanner(scan); // 获取扫描结果    	  System.out.println(&quot;表 &quot; + tableName + &quot; 的所有记录如下：&quot;);    	  for (Result result : scanner) &#123; // 遍历每一行数据    	printRecoder(result); // 打印每条记录的详情    	  &#125;    	  close(); // 关闭连接    	 &#125;    	 // 打印一条记录的详情    	 public static void printRecoder(Result result) throws IOException &#123;    	  for (Cell cell : result.rawCells()) &#123; // 遍历每个单元格    	System.out.print(&quot;行键: &quot; + new String(Bytes.toString(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength())));    	System.out.print(&quot; 列簇: &quot; + new String(Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(), cell.getFamilyLength())));    	System.out.print(&quot; 列: &quot; + new String(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength())));    	System.out.print(&quot; 值: &quot; + new String(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength())));    	System.out.println(&quot; 时间戳: &quot; + cell.getTimestamp());    	  &#125;    	 &#125;    public static void init() &#123;        configuration = HBaseConfiguration.create();        configuration.set(&quot;hbase.rootdir&quot;, &quot;hbase://localhost:9000/hbase&quot;);        try &#123;            connection = ConnectionFactory.createConnection(configuration);            if (connection == null) &#123;                System.err.println(&quot;Failed to create HBase connection.&quot;);            &#125; else &#123;                System.out.println(&quot;HBase connection created successfully.&quot;);            &#125;            admin = connection.getAdmin();            if (admin == null) &#123;                System.err.println(&quot;Failed to get HBase Admin.&quot;);            &#125; else &#123;                System.out.println(&quot;HBase Admin initialized successfully.&quot;);            &#125;        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125;    &#125;     public static void close()&#123;        try&#123;            if(admin != null)&#123;                admin.close();            &#125;            if(null != connection)&#123;                connection.close();            &#125;        &#125;catch (IOException e)&#123;            e.printStackTrace();        &#125;    &#125;     public static void createTable(String myTableName,String[] colFamily) throws IOException &#123;        TableName tableName = TableName.valueOf(myTableName);        if(admin.tableExists(tableName))&#123;            System.out.println(&quot;talbe is exists!&quot;);        &#125;else &#123;            TableDescriptorBuilder tableDescriptor = TableDescriptorBuilder.newBuilder(tableName);            for(String str:colFamily)&#123;                ColumnFamilyDescriptor family = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(str)).build();                tableDescriptor.setColumnFamily(family);            &#125;            admin.createTable(tableDescriptor.build());        &#125;     &#125;     public static void insertData(String tableName,String rowKey,String colFamily,String col,String val) throws IOException &#123;         Table table = connection.getTable(TableName.valueOf(tableName));        Put put = new Put(rowKey.getBytes());        put.addColumn(colFamily.getBytes(),col.getBytes(), val.getBytes());        table.put(put);        table.close();     &#125;     public static void getData(String tableName,String rowKey,String colFamily, String col)throws  IOException&#123;         Table table = connection.getTable(TableName.valueOf(tableName));        Get get = new Get(rowKey.getBytes());        get.addColumn(colFamily.getBytes(),col.getBytes());        Result result = table.get(get);        System.out.println(new String(result.getValue(colFamily.getBytes(),col==null?null:col.getBytes())));        table.close();     &#125;&#125;
运行结果如下

③向已经创建好的表添加和删除指定的列族或列。
shell
create &#x27;s1&#x27;,&#x27;score&#x27;put &#x27;s1&#x27;,&#x27;zhangsan&#x27;,&#x27;score:Math&#x27;,&#x27;69&#x27;delete &#x27;s1&#x27;,&#x27;zhangsan&#x27;,&#x27;score:Math&#x27;

JAVA
import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.*;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;public class test &#123;    public static Configuration configuration;    public static Connection connection;    public static Admin admin;    public static void main(String[] args) throws IOException &#123;        init(); // 初始化连接        String tableName = &quot;s1&quot;; // 表名        String[] columnFamilies = &#123;&quot;score&quot;&#125;; // 列簇        String rowKey = &quot;zhangsan&quot;; // 行键        String colFamily = &quot;score&quot;; // 列簇        String col = &quot;Math&quot;; // 列名        String val = &quot;69&quot;; // 值        // 创建表        System.out.println(&quot;开始创建表...&quot;);        createTable(tableName, columnFamilies);        // 插入数据        System.out.println(&quot;开始插入数据...&quot;);        insertRow(tableName, rowKey, colFamily, col, val);        // 查询数据        System.out.println(&quot;验证插入的数据...&quot;);        getData(tableName, rowKey, colFamily, col);        // 删除数据        System.out.println(&quot;开始删除数据...&quot;);        deleteRow(tableName, rowKey, colFamily, col);        // 验证删除        System.out.println(&quot;验证删除后的数据...&quot;);        getData(tableName, rowKey, colFamily, col);        close(); // 关闭连接    &#125;    public static void init() &#123;        configuration = HBaseConfiguration.create();        configuration.set(&quot;hbase.rootdir&quot;, &quot;hdfs://localhost:9000/hbase&quot;); // 注意这里用的是 hdfs        configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;localhost&quot;); // 指定 zookeeper 地址        try &#123;            connection = ConnectionFactory.createConnection(configuration);            admin = connection.getAdmin();            System.out.println(&quot;HBase connection and Admin initialized successfully.&quot;);        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125;    &#125;    public static void close() &#123;        try &#123;            if (admin != null) &#123;                admin.close();            &#125;            if (connection != null) &#123;                connection.close();            &#125;        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125;    &#125;    public static void createTable(String myTableName, String[] colFamilies) throws IOException &#123;        TableName tableName = TableName.valueOf(myTableName);        if (admin.tableExists(tableName)) &#123;            System.out.println(&quot;表已存在！&quot;);        &#125; else &#123;            TableDescriptorBuilder tableDescriptor = TableDescriptorBuilder.newBuilder(tableName);            for (String cf : colFamilies) &#123;                ColumnFamilyDescriptor family =                        ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(cf)).build();                tableDescriptor.setColumnFamily(family);            &#125;            admin.createTable(tableDescriptor.build());            System.out.println(&quot;表 &quot; + myTableName + &quot; 创建成功！&quot;);        &#125;    &#125;    public static void insertRow(String tableName, String rowKey, String colFamily, String col, String val) throws IOException &#123;        Table table = connection.getTable(TableName.valueOf(tableName));        Put put = new Put(Bytes.toBytes(rowKey));        put.addColumn(Bytes.toBytes(colFamily), Bytes.toBytes(col), Bytes.toBytes(val));        table.put(put);        table.close();        System.out.println(&quot;数据插入成功！&quot;);    &#125;    public static void getData(String tableName, String rowKey, String colFamily, String col) throws IOException &#123;        Table table = connection.getTable(TableName.valueOf(tableName));        Get get = new Get(Bytes.toBytes(rowKey));        get.addColumn(Bytes.toBytes(colFamily), Bytes.toBytes(col));        Result result = table.get(get);        byte[] value = result.getValue(Bytes.toBytes(colFamily), Bytes.toBytes(col));        if (value != null) &#123;            System.out.println(&quot;获取到数据: &quot; + new String(value));        &#125; else &#123;            System.out.println(&quot;未找到数据。&quot;);        &#125;        table.close();    &#125;    public static void deleteRow(String tableName, String rowKey, String colFamily, String col) throws IOException &#123;        Table table = connection.getTable(TableName.valueOf(tableName));        Delete delete = new Delete(Bytes.toBytes(rowKey));        delete.addColumn(Bytes.toBytes(colFamily), Bytes.toBytes(col));        table.delete(delete);        table.close();        System.out.println(&quot;数据删除成功！&quot;);    &#125;&#125;


④清空指定表的所有记录数据。
shell
create &#x27;s1&#x27;,&#x27;score&#x27;put &#x27;s1&#x27;,&#x27;zhangsan&#x27;,&#x27;score:Math&#x27;,&#x27;69&#x27;truncate &#x27;s1&#x27;scan &#x27;s1&#x27;

put &#x27;s1&#x27;,&#x27;zhangsan&#x27;,&#x27;score:Math&#x27;,&#x27;69&#x27;
java
教材中的代码
clearRows() 方法缺了一个 关键点：

在删除表之后重新创建时，需要重新添加原来的列簇（否则建出来的表是空结构）。

所以我用新的
import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;public class test &#123;    public static Configuration configuration;    public static Connection connection;    public static Admin admin;    public static void main(String[] args) throws IOException &#123;        String tableName = &quot;s1&quot;; // 你要清空的表名        clearRows(tableName);    &#125;    // 初始化 HBase 连接    public static void init() throws IOException &#123;        configuration = HBaseConfiguration.create();        configuration.set(&quot;hbase.rootdir&quot;, &quot;hdfs://localhost:9000/hbase&quot;);        configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;localhost&quot;);        connection = ConnectionFactory.createConnection(configuration);        admin = connection.getAdmin();    &#125;    // 关闭 HBase 连接    public static void close() throws IOException &#123;        if (admin != null) admin.close();        if (connection != null) connection.close();    &#125;    // 清空指定表的所有数据，保留列簇结构    public static void clearRows(String tableNameStr) throws IOException &#123;        init(); // 初始化连接        TableName tableName = TableName.valueOf(tableNameStr);        if (!admin.tableExists(tableName)) &#123;            System.out.println(&quot;表不存在，无法清空！&quot;);            close();            return;        &#125;        // 获取原始表结构        TableDescriptor descriptor = admin.getDescriptor(tableName);        // 禁用表        if (!admin.isTableDisabled(tableName)) &#123;            admin.disableTable(tableName);        &#125;        // 删除表        admin.deleteTable(tableName);        // 重新创建表（使用原结构）        admin.createTable(descriptor);        System.out.println(&quot;表 [&quot; + tableNameStr + &quot;] 已清空（保留列簇结构）&quot;);        close(); // 关闭连接    &#125;&#125;

这时候再去查表


⑤统计表的行数。
shell
put &#x27;s1&#x27;,&#x27;zhangsan&#x27;,&#x27;score:Math&#x27;,&#x27;69&#x27;count  &#x27;s1&#x27;

java
import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.*;import java.io.IOException;public class test &#123;    public static Configuration configuration;    public static Connection connection;    public static Admin admin;    public static void main(String[] args) throws IOException &#123;        String tableName = &quot;s1&quot;; // 你要统计的表名        countRows(tableName);    &#125;    // 初始化 HBase 连接    public static void init() throws IOException &#123;        configuration = HBaseConfiguration.create();        configuration.set(&quot;hbase.rootdir&quot;, &quot;hdfs://localhost:9000/hbase&quot;);        configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;localhost&quot;);        connection = ConnectionFactory.createConnection(configuration);        admin = connection.getAdmin();    &#125;    // 关闭 HBase 连接    public static void close() throws IOException &#123;        if (admin != null) admin.close();        if (connection != null) connection.close();    &#125;    // 统计表的行数    public static void countRows(String tableName) throws IOException &#123;        init(); // 初始化连接        Table table = connection.getTable(TableName.valueOf(tableName));        Scan scan = new Scan();        scan.setCaching(500); // 可选优化，加快扫描速度        ResultScanner scanner = table.getScanner(scan);        int num = 0;        for (Result result = scanner.next(); result != null; result = scanner.next()) &#123;            num++;        &#125;        System.out.println(&quot;表 [&quot; + tableName + &quot;] 的总行数为: &quot; + num);        scanner.close();        table.close(); // 关闭 Table 对象        close();       // 关闭连接    &#125;&#125;

第二题
（2）现有以下关系数据库中的表（见表4-21、表4-22和表4-23），要求将其转换为适合HBase存储的表并插入数据。
表4-21 学生（Student）表



学号（S_No）
姓名（S_Name）
性别（S_Sex）
年龄（S_Age）




2015001
Zhangsan
male
23


2015002
Mary
female
22


2015003
Lisi
male
24



表4-22 课程（Course）表



课程号（C_No）
课程名（C_Name）
学分（C_Credit）




123001
Math
2.0


123002
Computer Science
5.0


123003
English
3.0



表4-23  选课（SC）表



学号（SC_Sno）
课程号（SC_Cno）
成绩（SC_Score）




2015001
123001
86


2015001
123003
69


2015002
123002
77


2015002
123003
99


2015003
123001
98


2015003
123002
95



shell
disable &#x27;student&#x27;drop &#x27;student&#x27;
创建学生 student表
create &#x27;Student&#x27;,&#x27;S_No&#x27;,&#x27;S_Name&#x27;,&#x27;S_Sex&#x27;,&#x27;S_Age&#x27;put &#x27;Student&#x27;,&#x27;s001&#x27;,&#x27;S_No&#x27;,&#x27;2015001&#x27;put &#x27;Student&#x27;,&#x27;s001&#x27;,&#x27;S_Name&#x27;,&#x27;Zhangsan&#x27;put &#x27;Student&#x27;,&#x27;s001&#x27;,&#x27;S_Sex&#x27;,&#x27;male&#x27;put &#x27;Student&#x27;,&#x27;s001&#x27;,&#x27;S_Age&#x27;,&#x27;23&#x27;put &#x27;Student&#x27;,&#x27;s002&#x27;,&#x27;S_No&#x27;,&#x27;2015002&#x27;put &#x27;Student&#x27;,&#x27;s002&#x27;,&#x27;S_Name&#x27;,&#x27;Mary&#x27;put &#x27;Student&#x27;,&#x27;s002&#x27;,&#x27;S_Sex&#x27;,&#x27;female&#x27;put &#x27;Student&#x27;,&#x27;s002&#x27;,&#x27;S_Age&#x27;,&#x27;22&#x27;put &#x27;Student&#x27;,&#x27;s003&#x27;,&#x27;S_No&#x27;,&#x27;2015003&#x27;put &#x27;Student&#x27;,&#x27;s003&#x27;,&#x27;S_Name&#x27;,&#x27;Lisi&#x27;put &#x27;Student&#x27;,&#x27;s003&#x27;,&#x27;S_Sex&#x27;,&#x27;male&#x27;put &#x27;Student&#x27;,&#x27;s003&#x27;,&#x27;S_Age&#x27;,&#x27;24&#x27;

创建课程 Course 表
create &#x27;Course&#x27;,&#x27;C_No&#x27;,&#x27;C_Name&#x27;,&#x27;C_Credit&#x27;put &#x27;Course&#x27;,&#x27;c001&#x27;,&#x27;C_No&#x27;,&#x27;123001&#x27;put &#x27;Course&#x27;,&#x27;c001&#x27;,&#x27;C_Name&#x27;,&#x27;Math&#x27;put &#x27;Course&#x27;,&#x27;c001&#x27;,&#x27;C_Credit&#x27;,&#x27;2.0&#x27;put &#x27;Course&#x27;,&#x27;c002&#x27;,&#x27;C_No&#x27;,&#x27;123002&#x27;put &#x27;Course&#x27;,&#x27;c002&#x27;,&#x27;C_Name&#x27;,&#x27;Computer&#x27;put &#x27;Course&#x27;,&#x27;c002&#x27;,&#x27;C_Credit&#x27;,&#x27;5.0&#x27;put &#x27;Course&#x27;,&#x27;c003&#x27;,&#x27;C_No&#x27;,&#x27;123003&#x27;put &#x27;Course&#x27;,&#x27;c003&#x27;,&#x27;C_Name&#x27;,&#x27;English&#x27;put &#x27;Course&#x27;,&#x27;c003&#x27;,&#x27;C_Credit&#x27;,&#x27;3.0&#x27;

创建选课 SC 表
create &#x27;SC&#x27;,&#x27;SC_Sno&#x27;,&#x27;SC_Cno&#x27;,&#x27;SC_Score&#x27;put &#x27;SC&#x27;,&#x27;sc001&#x27;,&#x27;SC_Sno&#x27;,&#x27;2015001&#x27;put &#x27;SC&#x27;,&#x27;sc001&#x27;,&#x27;SC_Cno&#x27;,&#x27;123001&#x27;put &#x27;SC&#x27;,&#x27;sc001&#x27;,&#x27;SC_Score&#x27;,&#x27;86&#x27;put &#x27;SC&#x27;,&#x27;sc002&#x27;,&#x27;SC_Sno&#x27;,&#x27;2015001&#x27;put &#x27;SC&#x27;,&#x27;sc002&#x27;,&#x27;SC_Cno&#x27;,&#x27;123003&#x27;put &#x27;SC&#x27;,&#x27;sc002&#x27;,&#x27;SC_Score&#x27;,&#x27;69&#x27;put &#x27;SC&#x27;,&#x27;sc003&#x27;,&#x27;SC_Sno&#x27;,&#x27;2015002&#x27;put &#x27;SC&#x27;,&#x27;sc003&#x27;,&#x27;SC_Cno&#x27;,&#x27;123002&#x27;put &#x27;SC&#x27;,&#x27;sc003&#x27;,&#x27;SC_Score&#x27;,&#x27;77&#x27;put &#x27;SC&#x27;,&#x27;sc004&#x27;,&#x27;SC_Sno&#x27;,&#x27;2015002&#x27;put &#x27;SC&#x27;,&#x27;sc004&#x27;,&#x27;SC_Cno&#x27;,&#x27;123003&#x27;put &#x27;SC&#x27;,&#x27;sc004&#x27;,&#x27;SC_Score&#x27;,&#x27;99&#x27;put &#x27;SC&#x27;,&#x27;sc005&#x27;,&#x27;SC_Sno&#x27;,&#x27;2015003&#x27;put &#x27;SC&#x27;,&#x27;sc005&#x27;,&#x27;SC_Cno&#x27;,&#x27;123001&#x27;put &#x27;SC&#x27;,&#x27;sc005&#x27;,&#x27;SC_Score&#x27;,&#x27;98&#x27;put &#x27;SC&#x27;,&#x27;sc006&#x27;,&#x27;SC_Sno&#x27;,&#x27;2015003&#x27;put &#x27;SC&#x27;,&#x27;sc006&#x27;,&#x27;SC_Cno&#x27;,&#x27;123002&#x27;put &#x27;SC&#x27;,&#x27;sc006&#x27;,&#x27;SC_Score&#x27;,&#x27;95&#x27;

验证
scan &#x27;Student&#x27;scan &#x27;Course&#x27;scan &#x27;SC&#x27;

disable &#x27;Student&#x27;drop &#x27;Student&#x27;disable &#x27;Course&#x27;drop &#x27;Course&#x27;disable &#x27;SC&#x27;drop &#x27;SC&#x27;
java
① createTable(String tableName, String[] fields)。
② addRecord(String tableName, String row, String[] fields, String[]values)。
③ scanColumn(String tableName, String column)。
④ modifyData(String tableName, String row, String column)。
⑤ deleteRow(String tableName, String row)。
import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.*;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.util.Bytes;public class test &#123;    static Connection connection;    static Admin admin;    public static void init() throws IOException &#123;        Configuration conf = HBaseConfiguration.create();        conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;localhost&quot;);        connection = ConnectionFactory.createConnection(conf);        admin = connection.getAdmin();    &#125;    public static void close() throws IOException &#123;        if (admin != null) admin.close();        if (connection != null) connection.close();    &#125;    public static void createTable(String tableName, String[] fields) throws IOException &#123;        init();        TableName tablename = TableName.valueOf(tableName);        if (admin.tableExists(tablename)) &#123;            System.out.println(&quot;表 &quot; + tableName + &quot; 已存在，正在删除...&quot;);            admin.disableTable(tablename);            admin.deleteTable(tablename);        &#125;        TableDescriptorBuilder tableDescriptor = TableDescriptorBuilder.newBuilder(tablename);        for (String str : fields) &#123;            tableDescriptor.setColumnFamily(ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(str)).build());        &#125;        admin.createTable(tableDescriptor.build());        System.out.println(&quot;表 &quot; + tableName + &quot; 创建成功&quot;);        close();    &#125;    public static void addRecord(String tableName, String row, String[] fields, String[] values) throws IOException &#123;        init();        Table table = connection.getTable(TableName.valueOf(tableName));        Put put = new Put(Bytes.toBytes(row));        for (int i = 0; i &lt; fields.length; i++) &#123;            String[] parts = fields[i].split(&quot;:&quot;);            put.addColumn(Bytes.toBytes(parts[0], Bytes.toBytes(parts[1], Bytes.toBytes(values[i]);        &#125;        table.put(put);        table.close();        close();    &#125;    public static void scanColumn(String tableName, String column) throws IOException &#123;        init();        Table table = connection.getTable(TableName.valueOf(tableName));        Scan scan = new Scan();        if (column.contains(&quot;:&quot;)) &#123;            String[] parts = column.split(&quot;:&quot;);            scan.addColumn(Bytes.toBytes(parts[0], Bytes.toBytes(parts[1]);        &#125; else &#123;            scan.addFamily(Bytes.toBytes(column));        &#125;        ResultScanner scanner = table.getScanner(scan);        for (Result result = scanner.next(); result != null; result = scanner.next()) &#123;            showCell(result);        &#125;        scanner.close();        table.close();        close();    &#125;    public static void showCell(Result result) &#123;        for (Cell cell : result.rawCells()) &#123;            System.out.println(&quot;RowName: &quot; + Bytes.toString(CellUtil.cloneRow(cell)));            System.out.println(&quot;Timestamp: &quot; + cell.getTimestamp());            System.out.println(&quot;ColumnFamily: &quot; + Bytes.toString(CellUtil.cloneFamily(cell)));            System.out.println(&quot;Column: &quot; + Bytes.toString(CellUtil.cloneQualifier(cell)));            System.out.println(&quot;Value: &quot; + Bytes.toString(CellUtil.cloneValue(cell)));            System.out.println(&quot;----------------------------------------&quot;);        &#125;    &#125;    public static void modifyData(String tableName, String row, String column, String val) throws IOException &#123;        init();        Table table = connection.getTable(TableName.valueOf(tableName));        String[] parts = column.split(&quot;:&quot;);        Put put = new Put(Bytes.toBytes(row));        put.addColumn(Bytes.toBytes(parts[0], Bytes.toBytes(parts[1], Bytes.toBytes(val));        table.put(put);        System.out.println(&quot;修改表 &quot; + tableName + &quot; 中 &quot; + row + &quot; 行的列 &quot; + column + &quot; 为 &quot; + val);        table.close();        close();    &#125;    public static void deleteRow(String tableName, String row) throws IOException &#123;        init();        Table table = connection.getTable(TableName.valueOf(tableName));        Delete delete = new Delete(Bytes.toBytes(row));        table.delete(delete);        System.out.println(&quot;删除表 &quot; + tableName + &quot; 中的行 &quot; + row);        table.close();        close();    &#125;    public static void main(String[] args) throws IOException &#123;        // Student 表        createTable(&quot;Student&quot;, new String[]&#123;&quot;S_No&quot;, &quot;S_Name&quot;, &quot;S_Sex&quot;, &quot;S_Age&quot;&#125;);        addRecord(&quot;Student&quot;, &quot;s001&quot;, new String[]&#123;&quot;S_No:S_No&quot;, &quot;S_Name:S_Name&quot;, &quot;S_Sex:S_Sex&quot;, &quot;S_Age:S_Age&quot;&#125;,                new String[]&#123;&quot;2015001&quot;, &quot;Zhangsan&quot;, &quot;male&quot;, &quot;23&quot;&#125;);        addRecord(&quot;Student&quot;, &quot;s002&quot;, new String[]&#123;&quot;S_No:S_No&quot;, &quot;S_Name:S_Name&quot;, &quot;S_Sex:S_Sex&quot;, &quot;S_Age:S_Age&quot;&#125;,                new String[]&#123;&quot;2015002&quot;, &quot;Mary&quot;, &quot;female&quot;, &quot;22&quot;&#125;);        addRecord(&quot;Student&quot;, &quot;s003&quot;, new String[]&#123;&quot;S_No:S_No&quot;, &quot;S_Name:S_Name&quot;, &quot;S_Sex:S_Sex&quot;, &quot;S_Age:S_Age&quot;&#125;,                new String[]&#123;&quot;2015003&quot;, &quot;Lisi&quot;, &quot;male&quot;, &quot;24&quot;&#125;);        System.out.println(&quot;Student 表插入数据成功&quot;);        // Course 表        createTable(&quot;Course&quot;, new String[]&#123;&quot;C_No&quot;, &quot;C_Name&quot;, &quot;C_Credit&quot;&#125;);        addRecord(&quot;Course&quot;, &quot;c001&quot;, new String[]&#123;&quot;C_No:C_No&quot;, &quot;C_Name:C_Name&quot;, &quot;C_Credit:C_Credit&quot;&#125;,                new String[]&#123;&quot;123001&quot;, &quot;Math&quot;, &quot;2.0&quot;&#125;);        addRecord(&quot;Course&quot;, &quot;c002&quot;, new String[]&#123;&quot;C_No:C_No&quot;, &quot;C_Name:C_Name&quot;, &quot;C_Credit:C_Credit&quot;&#125;,                new String[]&#123;&quot;123002&quot;, &quot;Computer&quot;, &quot;5.0&quot;&#125;);        addRecord(&quot;Course&quot;, &quot;c003&quot;, new String[]&#123;&quot;C_No:C_No&quot;, &quot;C_Name:C_Name&quot;, &quot;C_Credit:C_Credit&quot;&#125;,                new String[]&#123;&quot;123003&quot;, &quot;English&quot;, &quot;3.0&quot;&#125;);        System.out.println(&quot;Course 表插入数据成功&quot;);        // SC 表        createTable(&quot;SC&quot;, new String[]&#123;&quot;SC_Sno&quot;, &quot;SC_Cno&quot;, &quot;SC_Score&quot;&#125;);        addRecord(&quot;SC&quot;, &quot;sc001&quot;, new String[]&#123;&quot;SC_Sno:SC_Sno&quot;, &quot;SC_Cno:SC_Cno&quot;, &quot;SC_Score:SC_Score&quot;&#125;,                new String[]&#123;&quot;2015001&quot;, &quot;123001&quot;, &quot;86&quot;&#125;);        addRecord(&quot;SC&quot;, &quot;sc002&quot;, new String[]&#123;&quot;SC_Sno:SC_Sno&quot;, &quot;SC_Cno:SC_Cno&quot;, &quot;SC_Score:SC_Score&quot;&#125;,                new String[]&#123;&quot;2015001&quot;, &quot;123003&quot;, &quot;69&quot;&#125;);        addRecord(&quot;SC&quot;, &quot;sc003&quot;, new String[]&#123;&quot;SC_Sno:SC_Sno&quot;, &quot;SC_Cno:SC_Cno&quot;, &quot;SC_Score:SC_Score&quot;&#125;,                new String[]&#123;&quot;2015002&quot;, &quot;123002&quot;, &quot;77&quot;&#125;);        addRecord(&quot;SC&quot;, &quot;sc004&quot;, new String[]&#123;&quot;SC_Sno:SC_Sno&quot;, &quot;SC_Cno:SC_Cno&quot;, &quot;SC_Score:SC_Score&quot;&#125;,                new String[]&#123;&quot;2015002&quot;, &quot;123003&quot;, &quot;99&quot;&#125;);        addRecord(&quot;SC&quot;, &quot;sc005&quot;, new String[]&#123;&quot;SC_Sno:SC_Sno&quot;, &quot;SC_Cno:SC_Cno&quot;, &quot;SC_Score:SC_Score&quot;&#125;,                new String[]&#123;&quot;2015003&quot;, &quot;123001&quot;, &quot;98&quot;&#125;);        addRecord(&quot;SC&quot;, &quot;sc006&quot;, new String[]&#123;&quot;SC_Sno:SC_Sno&quot;, &quot;SC_Cno:SC_Cno&quot;, &quot;SC_Score:SC_Score&quot;&#125;,                new String[]&#123;&quot;2015003&quot;, &quot;123002&quot;, &quot;95&quot;&#125;);        System.out.println(&quot;SC 表插入数据成功&quot;);        // 示例输出        System.out.println(&quot;===== 浏览 Student 表的全部 S_Name 列族 =====&quot;);        scanColumn(&quot;Student&quot;, &quot;S_Name&quot;);        System.out.println(&quot;===== 修改 Student 表中 s002 的 S_Age 为 25 =====&quot;);        modifyData(&quot;Student&quot;, &quot;s002&quot;, &quot;S_Age:S_Age&quot;, &quot;25&quot;);        System.out.println(&quot;===== 删除 Student 表中的 s003 =====&quot;);        deleteRow(&quot;Student&quot;, &quot;s003&quot;);    &#125;&#125;

（7.5.5）
基础
自行开启hadoop集群和hbase
cdcat &gt; wordfile1.txt&lt;&lt;&quot;EOF&quot;I love SparkI love HadoopEOFcat &gt; wordfile2.txt&lt;&lt;&quot;EOF&quot;Hadoop is goodSpark is fastEOFhdfs dfs -rm -r inputhdfs dfs -rm -r outputhdfs dfs -mkdir inputhdfs dfs -put wordfile1.txt inputhdfs dfs -put wordfile2.txt input

在eclipse创建项目
启动eclipse
eclipse
创建项目
File-new-project
名字：WordCount

为了编写一个MapReduce程序，一般需要向Java工程中添加以下JAR包：
（1)/usr/local/hadoop/share/hadoop/common目录下的hadoop-common-3.1.3.jar和haoop-nfs-3.1.3.jar；
（2)/usr/local/hadoop/share/hadoop/common/lib目录下的所有JAR包；
（3)/usr/local/hadoop/share/hadoop/mapreduce目录下的所有JAR包，但是，不包括jdiff、lib-examples和sources目录.

添加完成就finish就行
编写Java应用程序
创建WordCount.java,自己看好，以后不会再说了

添加代码
import java.io.IOException;import java.util.Iterator;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;public class WordCount &#123;    public WordCount() &#123;    &#125;     public static void main(String[] args) throws Exception &#123;        Configuration conf = new Configuration();        String[] otherArgs = (new GenericOptionsParser(conf, args)).getRemainingArgs();        if(otherArgs.length &lt; 2) &#123;            System.err.println(&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;);            System.exit(2);        &#125;        Job job = Job.getInstance(conf, &quot;word count&quot;);        job.setJarByClass(WordCount.class);        job.setMapperClass(WordCount.TokenizerMapper.class);        job.setCombinerClass(WordCount.IntSumReducer.class);        job.setReducerClass(WordCount.IntSumReducer.class);        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(IntWritable.class);         for(int i = 0; i &lt; otherArgs.length - 1; ++i) &#123;            FileInputFormat.addInputPath(job, new Path(otherArgs[i]);        &#125;        FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1]);        System.exit(job.waitForCompletion(true)?0:1);    &#125;    public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123;        private static final IntWritable one = new IntWritable(1);        private Text word = new Text();        public TokenizerMapper() &#123;        &#125;        public void map(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123;            StringTokenizer itr = new StringTokenizer(value.toString());             while(itr.hasMoreTokens()) &#123;                this.word.set(itr.nextToken());                context.write(this.word, one);            &#125;        &#125;    &#125;public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;        private IntWritable result = new IntWritable();        public IntSumReducer() &#123;        &#125;        public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123;            int sum = 0;            IntWritable val;            for(Iterator i$ = values.iterator(); i$.hasNext(); sum += val.get()) &#123;                val = (IntWritable)i$.next();            &#125;            this.result.set(sum);            context.write(key, this.result);        &#125;    &#125;&#125;
运行一下


7.5.5截图1

编译打包程序
如果你没有myapp文件夹你就自己创建，我的是有的



选择WordCount项目
填写路径/usr/local/hadoop/myapp/WordCount.jar
然后finish
这时候会有弹窗，一路ok下来就行了
ll  /usr/local/hadoop/myapp

运行程序
hadoop jar /usr/local/hadoop/myapp/WordCount.jar input outputhdfs dfs -cat output/*



7.5.5 截图2和3

（7.3.4）
第四题
shell
cat &gt; avg.txt &lt;&lt;&quot;EOF&quot;math 80math 90english 70english 100EOFhdfs dfs -rm -r inputhdfs dfs -rm -r outputhdfs dfs -mkdir inputhdfs dfs -put avg.txt input
创建AverageCalculator.java
import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class AverageCalculator &#123;    public static class AvgMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123;        private Text keyOut = new Text();        private IntWritable valOut = new IntWritable();        @Override        protected void map(LongWritable key, Text value, Context context)                throws IOException, InterruptedException &#123;            String[] parts = value.toString().split(&quot;\\s+&quot;);            if (parts.length == 2) &#123;                keyOut.set(parts[0];                valOut.set(Integer.parseInt(parts[1]);                context.write(keyOut, valOut);            &#125;        &#125;    &#125;    public static class AvgReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;        private IntWritable result = new IntWritable();        @Override        protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)                throws IOException, InterruptedException &#123;            int sum = 0;            int count = 0;            for (IntWritable val : values) &#123;                sum += val.get();                count++;            &#125;            if (count != 0) &#123;                result.set(sum / count);                context.write(key, result);            &#125;        &#125;    &#125;    public static void main(String[] args) throws Exception &#123;        if (args.length != 2) &#123;            System.err.println(&quot;Usage: AverageCalculator &lt;input path&gt; &lt;output path&gt;&quot;);            System.exit(-1);        &#125;        Configuration conf = new Configuration();        Job job = Job.getInstance(conf, &quot;Average Calculator&quot;);        job.setJarByClass(AverageCalculator.class);        job.setMapperClass(AvgMapper.class);        job.setReducerClass(AvgReducer.class);        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(IntWritable.class);        FileInputFormat.setInputPaths(job, new Path(args[0]);        FileOutputFormat.setOutputPath(job, new Path(args[1]);        System.exit(job.waitForCompletion(true) ? 0 : 1);    &#125;&#125;
自己运行一下，不然下面打包程序出现不了，运行报错别管，运行就对了，别问
自己运行一下，不然下面打包程序出现不了，运行报错别管，运行就对了，别问
自己运行一下，不然下面打包程序出现不了，运行报错别管，运行就对了，别问
自己运行一下，不然下面打包程序出现不了，运行报错别管，运行就对了，别问
打包项目
这是打包java项目的路径教程，自己看，后面还会有各种java要打包，后面不会再说了


一路ok
hadoop jar /usr/local/hadoop/myapp/AverageCalculator.jar input outputhadoop dfs -cat output/*


这个是第四题的图片

第八题和第九题
cat &gt; maxmin.txt &lt;&lt;&quot;EOF&quot;23451267893456EOFhdfs dfs -rm -r inputhdfs dfs -rm -r outputhdfs dfs -mkdir inputhdfs dfs -put maxmin.txt input
创建MaxMinValue.java
import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class MaxMinValue &#123;    public static class MaxMinMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123;        private IntWritable valueOut = new IntWritable();        @Override        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;            int num = Integer.parseInt(value.toString());            valueOut.set(num);            context.write(new Text(&quot;max&quot;), valueOut);            context.write(new Text(&quot;min&quot;), valueOut);        &#125;    &#125;    public static class MaxMinReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;        private IntWritable result = new IntWritable();        @Override        protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)                throws IOException, InterruptedException &#123;            int finalValue = key.toString().equals(&quot;max&quot;) ? Integer.MIN_VALUE : Integer.MAX_VALUE;            for (IntWritable val : values) &#123;                if (key.toString().equals(&quot;max&quot;)) &#123;                    finalValue = Math.max(finalValue, val.get());                &#125; else if (key.toString().equals(&quot;min&quot;)) &#123;                    finalValue = Math.min(finalValue, val.get());                &#125;            &#125;            result.set(finalValue);            context.write(key, result);        &#125;    &#125;    public static void main(String[] args) throws Exception &#123;        Configuration conf = new Configuration();        Job job = Job.getInstance(conf, &quot;Max and Min Values&quot;);        job.setJarByClass(MaxMinValue.class);        job.setMapperClass(MaxMinMapper.class);        job.setReducerClass(MaxMinReducer.class);        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(IntWritable.class);        FileInputFormat.setInputPaths(job, new Path(args[0]);        FileOutputFormat.setOutputPath(job, new Path(args[1]);        System.exit(job.waitForCompletion(true) ? 0 : 1);    &#125;&#125;
然后自己打包程序，不会就去翻前面，前面有说

hadoop jar /usr/local/hadoop/myapp/MaxMinValue.jar input outputhadoop dfs -cat output/*


这个是第八题和第九题的图片

接下来的创建java打包程序不说了，自己翻前面的
第十题
cat &gt; SalesVolume.txt &lt;&lt;&quot;EOF&quot;2025-01-01 2002025-01-15 1502025-02-10 3002025-02-25 2502025-03-05 4002025-03-15 350EOFhdfs dfs -rm -r inputhdfs dfs -rm -r outputhdfs dfs -mkdir inputhdfs dfs -put SalesVolume.txt input
创建SalesByMonth.java
import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class SalesByMonth &#123;    public static class SalesMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123;        private Text month = new Text();        private IntWritable salesAmount = new IntWritable();        @Override        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;            String[] parts = value.toString().split(&quot;\\s+&quot;);            String date = parts[0];  // 日期格式为 YYYY-MM-DD            int amount = Integer.parseInt(parts[1];  // 销售金额            // 提取月份（格式为 YYYY-MM）            String yearMonth = date.substring(0, 7);            month.set(yearMonth);            salesAmount.set(amount);            context.write(month, salesAmount);        &#125;    &#125;    public static class SalesReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;        private IntWritable result = new IntWritable();        @Override        protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)                throws IOException, InterruptedException &#123;            int totalSales = 0;            for (IntWritable val : values) &#123;                totalSales += val.get();            &#125;            result.set(totalSales);            context.write(key, result);        &#125;    &#125;    public static void main(String[] args) throws Exception &#123;        Configuration conf = new Configuration();        Job job = Job.getInstance(conf, &quot;Sales by Month&quot;);        job.setJarByClass(SalesByMonth.class);        job.setMapperClass(SalesMapper.class);        job.setReducerClass(SalesReducer.class);        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(IntWritable.class);        FileInputFormat.setInputPaths(job, new Path(args[0]);        FileOutputFormat.setOutputPath(job, new Path(args[1]);        System.exit(job.waitForCompletion(true) ? 0 : 1);    &#125;&#125;
自己打包程序，然后运行程序
hadoop jar /usr/local/hadoop/myapp/SalesByMonth.jar input outputhadoop dfs -cat output/*


这是第十题的图片

实验四
第一题
shell
1．编程实现文件合并和去重操作
shell
对于两个输入文件，即文件A和文件B，请编写MapReduce程序，对两个文件进行合并，并剔除其中重复的内容，得到一个新的输出文件C。下面是输入文件和输出文件的一个样例，以供参考。
cat &gt;a.txt&lt;&lt;&quot;EOF&quot;20150101    x20150102    y20150103    x20150104    y20150105    z20150106    xEOFcat &gt;b.txt&lt;&lt;&quot;EOF&quot;20150101     y20150102     y20150103     x20150104     z20150105     yEOFhdfs dfs -rm -r inputhdfs dfs -rm -r outputhdfs dfs -mkdir inputhdfs dfs -put a.txt inputhdfs dfs -put b.txt input

创建Merge.java
import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class Merge &#123;    /**     * 对A,B两个文件进行合并，并剔除其中重复的内容，得到一个新的输出文件C     */    // Mapper: 清理每行多余的空白，并作为 key 输出    public static class Map extends Mapper&lt;Object, Text, Text, Text&gt; &#123;        private Text outKey = new Text();        public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123;            String line = value.toString().trim(); // 去除首尾空格            StringTokenizer tokenizer = new StringTokenizer(line);            StringBuilder sb = new StringBuilder();            while (tokenizer.hasMoreTokens()) &#123;                sb.append(tokenizer.nextToken()).append(&quot; &quot;); // 用单空格连接            &#125;            if (sb.length() &gt; 0) sb.setLength(sb.length() - 1); // 去掉最后一个空格            outKey.set(sb.toString());            context.write(outKey, new Text(&quot;&quot;));        &#125;    &#125;    // Reducer: 每个 key 写出一次，达到去重目的    public static class Reduce extends Reducer&lt;Text, Text, Text, Text&gt; &#123;        public void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123;            context.write(key, new Text(&quot;&quot;));        &#125;    &#125;    public static void main(String[] args) throws Exception &#123;        Configuration conf = new Configuration();        conf.set(&quot;fs.default.name&quot;, &quot;hdfs://localhost:9000&quot;);        String[] otherArgs = new String[]&#123;&quot;input&quot;, &quot;output&quot;&#125;;        if (otherArgs.length != 2) &#123;            System.err.println(&quot;Usage: Merge &lt;in&gt; &lt;out&gt;&quot;);            System.exit(2);        &#125;        Job job = Job.getInstance(conf, &quot;Merge and Deduplicate&quot;);        job.setJarByClass(Merge.class);        job.setMapperClass(Map.class);        job.setCombinerClass(Reduce.class);        job.setReducerClass(Reduce.class);        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(Text.class);        FileInputFormat.addInputPath(job, new Path(otherArgs[0]);        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]);        // 强制使用一个 reducer，确保全局去重        job.setNumReduceTasks(1);        System.exit(job.waitForCompletion(true) ? 0 : 1);    &#125;&#125;
打包程序，然后运行
hadoop jar /usr/local/hadoop/myapp/Merge.jar input outputhadoop dfs -cat output/*

java
java
导入hdfs包


/usr/local/hadoop/share/hadoop/yarn/lib下的所有jar包
/usr/local/hadoop/share/hadoop/yarn下的所有jar包
/usr/local/hadoop/share/hadoop/hdfs下的这两个包也要添加

测试文件
cat &gt;a.txt&lt;&lt;&quot;EOF&quot;20150101    x20150102    y20150103    x20150104    y20150105    z20150106    xEOFcat &gt;b.txt&lt;&lt;&quot;EOF&quot;20150101     y20150102     y20150103     x20150104     z20150105     yEOFhdfs dfs -rm -r inputhdfs dfs -rm -r outputhdfs dfs -mkdir inputhdfs dfs -put a.txt inputhdfs dfs -put b.txt input
更新Merge.java代码
import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class Merge &#123;    /**     * 对A,B两个文件进行合并，并剔除其中重复的内容，得到一个新的输出文件C     */    // Mapper: 清理每行多余的空白，并作为 key 输出    public static class Map extends Mapper&lt;Object, Text, Text, Text&gt; &#123;        private Text outKey = new Text();        public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123;            String line = value.toString().trim(); // 去除首尾空格            StringTokenizer tokenizer = new StringTokenizer(line);            StringBuilder sb = new StringBuilder();            while (tokenizer.hasMoreTokens()) &#123;                sb.append(tokenizer.nextToken()).append(&quot; &quot;); // 用单空格连接            &#125;            if (sb.length() &gt; 0) sb.setLength(sb.length() - 1); // 去掉最后一个空格            outKey.set(sb.toString());            context.write(outKey, new Text(&quot;&quot;));        &#125;    &#125;    // Reducer: 每个 key 写出一次，达到去重目的    public static class Reduce extends Reducer&lt;Text, Text, Text, Text&gt; &#123;        public void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123;            context.write(key, new Text(&quot;&quot;));        &#125;    &#125;    public static void main(String[] args) throws Exception &#123;        Configuration conf = new Configuration();        conf.set(&quot;fs.default.name&quot;, &quot;hdfs://localhost:9000&quot;);        // 写死输入输出路径        String inputPath = &quot;hdfs://localhost:9000/user/hadoop/input&quot;;        String outputPath = &quot;hdfs://localhost:9000/user/hadoop/output&quot;;        Job job = Job.getInstance(conf, &quot;Merge and Deduplicate&quot;);        job.setJarByClass(Merge.class);        job.setMapperClass(Map.class);        job.setCombinerClass(Reduce.class);        job.setReducerClass(Reduce.class);        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(Text.class);        FileInputFormat.addInputPath(job, new Path(inputPath));        FileOutputFormat.setOutputPath(job, new Path(outputPath));        // 强制使用一个 reducer，确保全局去重        job.setNumReduceTasks(1);        System.exit(job.waitForCompletion(true) ? 0 : 1);    &#125;&#125;

查看结果
hadoop dfs -cat output/*

第二题
shell
2．编程实现对输入文件的排序
现在有多个输入文件，每个文件中的每行内容均为一个整数。要求读取所有文件中的整数，进行升序排列后，将其输出到一个新的文件中，输出的数据格式为每行两个整数，第一个整数为第二个整数的排序位次，第二个整数为原待排列的整数。下面是输入文件和输出文件的一个样例，以供参考。
shell
cat &gt;a.txt&lt;&lt;&quot;EOF&quot;33371240EOFcat &gt;b.txt&lt;&lt;&quot;EOF&quot;416395EOFcat &gt;c.txt&lt;&lt;&quot;EOF&quot;14525EOFhdfs dfs -rm -r inputhdfs dfs -rm -r outputhdfs dfs -mkdir inputhdfs dfs -put a.txt inputhdfs dfs -put b.txt inputhdfs dfs -put c.txt input
创建MergeSort.java
import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;public class MergeSort &#123;    /**     * @param args     * 输入多个文件，每个文件中的每行内容均为一个整数     * 输出到一个新的文件中，输出的数据格式为每行两个整数，第一个数字为第二个整数的排序位次，第二个整数为原待排列的整数     */    //map函数读取输入中的value，将其转化成IntWritable类型，最后作为输出key    public static class Map extends Mapper&lt;Object, Text, IntWritable, IntWritable&gt;&#123;        private static IntWritable data = new IntWritable();        public void map(Object key, Text value, Context context) throws IOException,InterruptedException&#123;            String text = value.toString();            data.set(Integer.parseInt(text));            context.write(data, new IntWritable(1));        &#125;    &#125;    //reduce函数将map输入的key复制到输出的value上，然后根据输入的value-list中元素的个数决定key的输出次数,定义一个全局变量line_num来代表key的位次    public static class Reduce extends Reducer&lt;IntWritable, IntWritable, IntWritable,  IntWritable&gt;&#123;        private static IntWritable line_num = new IntWritable(1);               public void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context  context) throws IOException,InterruptedException&#123;            for(IntWritable val : values)&#123;                 context.write(line_num, key);                line_num = new IntWritable(line_num.get() + 1);            &#125;        &#125;    &#125;    //自定义Partition函数，此函数根据输入数据的最大值和MapReduce框架中Partition的数量获取将输入数据按照大小分块的边界，然后根据输入数值和边界的关系返回对应的Partiton ID    public static class Partition extends Partitioner&lt;IntWritable, IntWritable&gt;&#123;        public int getPartition(IntWritable key, IntWritable value, int num_Partition)&#123;            int Maxnumber = 65223;//int型的最大数值            int bound = Maxnumber/num_Partition+1;            int keynumber = key.get();            for (int i = 0; i&lt;num_Partition; i++)&#123;                if(keynumber&lt;bound * (i+1) &amp;&amp; keynumber&gt;=bound * i)&#123;                    return i;                &#125;            &#125;            return -1;        &#125;    &#125;    public static  void main(String[] args) throws Exception&#123;         // TODO  Auto-generated method stub        Configuration  conf = new Configuration();               conf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000&quot;);        String[] otherArgs = new String[]&#123;&quot;input&quot;,&quot;output&quot;&#125;; /* 直接设置输入参数 */        if (otherArgs.length != 2) &#123;            System.err.println(&quot;Usage: wordcount &lt;in&gt;&lt;out&gt;&quot;);            System.exit(2);            &#125;               Job job = Job.getInstance(conf,&quot;Merge and sort&quot;);        job.setJarByClass(MergeSort.class);        job.setMapperClass(Map.class);         job.setReducerClass(Reduce.class);        job.setPartitionerClass(Partition.class);        job.setOutputKeyClass(IntWritable.class);         job.setOutputValueClass(IntWritable.class);         FileInputFormat.addInputPath(job, new Path(otherArgs[0]);        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]);        System.exit(job.waitForCompletion(true) ? 0 : 1);    &#125;&#125;
自己打包项目
hadoop jar /usr/local/hadoop/myapp/MergeSort.jar input outputhadoop dfs -cat output/*

java
java
写入测试文件
cat &gt;a.txt&lt;&lt;&quot;EOF&quot;33371240EOFcat &gt;b.txt&lt;&lt;&quot;EOF&quot;416395EOFcat &gt;c.txt&lt;&lt;&quot;EOF&quot;14525EOFhdfs dfs -rm -r inputhdfs dfs -rm -r outputhdfs dfs -mkdir inputhdfs dfs -put a.txt inputhdfs dfs -put b.txt inputhdfs dfs -put c.txt input
更新代码
MergeSort.java
import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MergeSort &#123;    /**     * @param args     * 输入多个文件，每个文件中的每行内容均为一个整数     * 输出到一个新的文件中，输出的数据格式为每行两个整数，     * 第一个数字为第二个整数的排序位次（从1开始），第二个整数为原待排列的整数     */    // Mapper：读取每行整数，作为 key 输出    public static class Map extends Mapper&lt;Object, Text, IntWritable, IntWritable&gt; &#123;        private static final IntWritable data = new IntWritable();        public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123;            String text = value.toString().trim();            if (!text.isEmpty()) &#123;                data.set(Integer.parseInt(text));                context.write(data, new IntWritable(1));            &#125;        &#125;    &#125;    // Reducer：根据 key 的顺序输出排名和对应的整数    public static class Reduce extends Reducer&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt; &#123;        private static IntWritable line_num = new IntWritable(1);        public void reduce(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context)                throws IOException, InterruptedException &#123;            for (IntWritable val : values) &#123;                context.write(line_num, key);                line_num = new IntWritable(line_num.get() + 1);            &#125;        &#125;    &#125;    // 自定义 Partitioner：按数值大小分块，提升并行效率    public static class Partition extends Partitioner&lt;IntWritable, IntWritable&gt; &#123;        public int getPartition(IntWritable key, IntWritable value, int num_Partition) &#123;            int Maxnumber = 65223; // 可以根据数据范围调整            int bound = Maxnumber / num_Partition + 1;            int keynumber = key.get();            for (int i = 0; i &lt; num_Partition; i++) &#123;                if (keynumber &lt; bound * (i + 1) &amp;&amp; keynumber &gt;= bound * i) &#123;                    return i;                &#125;            &#125;            return -1;        &#125;    &#125;    public static void main(String[] args) throws Exception &#123;        Configuration conf = new Configuration();        // 设置新的 fs.defaultFS，并设置本地执行模式        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);        conf.set(&quot;mapreduce.framework.name&quot;, &quot;local&quot;);        conf.set(&quot;mapreduce.jobtracker.address&quot;, &quot;local&quot;);        // 写死输入输出路径        String inputPath = &quot;hdfs://localhost:9000/user/hadoop/input&quot;;        String outputPath = &quot;hdfs://localhost:9000/user/hadoop/output&quot;;        Job job = Job.getInstance(conf, &quot;Merge and Sort&quot;);        job.setJarByClass(MergeSort.class);        job.setMapperClass(Map.class);        job.setReducerClass(Reduce.class);        job.setPartitionerClass(Partition.class);        job.setOutputKeyClass(IntWritable.class);        job.setOutputValueClass(IntWritable.class);        FileInputFormat.addInputPath(job, new Path(inputPath));        FileOutputFormat.setOutputPath(job, new Path(outputPath));        System.exit(job.waitForCompletion(true) ? 0 : 1);    &#125;&#125;

hadoop dfs -cat output/*

第三题
shell
3．对给定的表格进行信息挖掘
下面给出一个child-parent的表格，要求挖掘其中的父子关系，给出祖孙关系的表格。
cat &gt;a.txt&lt;&lt;&quot;EOF&quot;child parentSteven LucySteven JackJone LucyJone JackLucy MaryLucy FrankJack AliceJack JesseDavid AliceDavid JessePhilip DavidPhilip AlmaMark DavidMark AlmaEOFhdfs dfs -rm -r inputhdfs dfs -rm -r outputhdfs dfs -mkdir inputhdfs dfs -put a.txt input
创建simple_data_mining.java
import java.io.IOException;import java.util.*;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;public class simple_data_mining &#123;     public static int time = 0;    /**     * @param args      * 输入一个child-parent的表格     * 输出一个体现grandchild-grandparent关系的表格     */    //Map将输入文件按照空格分割成child和parent，然后正序输出一次作为右表，反序输出一次作为左表，需要注意的是在输出的value中必须加上左右表区别标志    public static class Map extends Mapper&lt;Object, Text, Text, Text&gt;&#123;        public void map(Object key, Text value, Context context) throws IOException,InterruptedException&#123;            String child_name = new String();            String parent_name = new String();            String relation_type = new String();            String line = value.toString();            int i = 0;            while(line.charAt(i)!= &#x27; &#x27;)&#123;                i++;            &#125;            String[] values = &#123;line.substring(0,i),line.substring(i+1)&#125;;             if(values[0].compareTo(&quot;child&quot;)!= 0)&#123;                child_name = values[0];                parent_name = values[1];                relation_type = &quot;1&quot;;//左右表区分标志                context.write(new Text(values[1], new Text(relation_type+&quot;+&quot;+child_name+&quot;+&quot;+parent_name));                //左表                relation_type = &quot;2&quot;;                context.write(new Text(values[0], new Text(relation_type+&quot;+&quot;+child_name+&quot;+&quot;+parent_name));                //右表            &#125;        &#125;    &#125;    public static class Reduce extends Reducer&lt;Text, Text, Text, Text&gt;&#123;        public void reduce(Text key, Iterable&lt;Text&gt; values,Context context) throws IOException,InterruptedException&#123;            if(time == 0)&#123;   //输出表头                context.write(new Text(&quot;grand_child&quot;), new Text(&quot;grand_parent&quot;));                time++;            &#125;            int grand_child_num = 0;             String grand_child[] = new String[10];            int grand_parent_num = 0;            String grand_parent[]= new String[10];            Iterator ite = values.iterator();            while(ite.hasNext())&#123;                String record = ite.next().toString();                int len = record.length();                int i = 2;                if(len == 0) continue;                char relation_type = record.charAt(0);                String child_name = new String();                String parent_name = new String();                //获取value-list中value的child                while(record.charAt(i)!= &#x27;+&#x27;)&#123;                   child_name = child_name + record.charAt(i);                   i++;                &#125;                i=i+1;                //获取value-list中value的parent                while(i&lt;len)&#123;                   parent_name = parent_name+record.charAt(i);                   i++;                &#125;                 //左表，取出child放入grand_child                if(relation_type  == &#x27;1&#x27;)&#123;                   grand_child[grand_child_num]  = child_name;                    grand_child_num++;                &#125;                else&#123;//右表，取出parent放入grand_parent                   grand_parent[grand_parent_num]  = parent_name;                   grand_parent_num++;                &#125;            &#125;             if(grand_parent_num != 0 &amp;&amp; grand_child_num != 0 )&#123;                for(int m = 0;m&lt;grand_child_num;m++)&#123;                  for(int n=0;n&lt;grand_parent_num;n++)&#123;                                                                        context.write(new  Text(grand_child[m], new Text(grand_parent[n]);                       //输出结果                   &#125;                &#125;             &#125;        &#125;     &#125;    public static void main(String[] args) throws Exception&#123;        //  TODO Auto-generated method stub         Configuration conf = new Configuration();               conf.set(&quot;fs.default.name&quot;,&quot;hdfs://localhost:9000&quot;);        String[] otherArgs = new String[]&#123;&quot;input&quot;,&quot;output&quot;&#125;; /* 直接设置输入参数 */        if (otherArgs.length != 2) &#123;            System.err.println(&quot;Usage: wordcount &lt;in&gt;&lt;out&gt;&quot;);            System.exit(2);            &#125;               Job job =  Job.getInstance(conf,&quot;Single table join&quot;);        job.setJarByClass(simple_data_mining.class);        job.setMapperClass(Map.class);         job.setReducerClass(Reduce.class);         job.setOutputKeyClass(Text.class);        job.setOutputValueClass(Text.class);        FileInputFormat.addInputPath(job, new Path(otherArgs[0]);        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]);        System.exit(job.waitForCompletion(true) ? 0 : 1);    &#125;&#125;
自己打包文件
hadoop jar /usr/local/hadoop/myapp/simple_data_mining.jar input outputhadoop dfs -cat output/*

java
java
更新测试文件
cat &gt;a.txt&lt;&lt;&quot;EOF&quot;child parentSteven LucySteven JackJone LucyJone JackLucy MaryLucy FrankJack AliceJack JesseDavid AliceDavid JessePhilip DavidPhilip AlmaMark DavidMark AlmaEOFhdfs dfs -rm -r inputhdfs dfs -rm -r outputhdfs dfs -mkdir inputhdfs dfs -put a.txt input
更新simple_data_mining.java
import java.io.IOException;import java.util.*;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class simple_data_mining &#123;    public static int time = 0; // 控制表头只输出一次    /**     * 输入：每行两个字段 child parent     * 输出：grand_child grand_parent     */    // Mapper：将每条记录正序和逆序分别输出，标识左表和右表    public static class Map extends Mapper&lt;Object, Text, Text, Text&gt; &#123;        public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123;            String line = value.toString().trim();            if (line.isEmpty() || line.startsWith(&quot;child&quot;)) return;            String[] parts = line.split(&quot;\\s+&quot;);            if (parts.length &lt; 2) return;            String child = parts[0];            String parent = parts[1];            // 左表输出：parent -&gt; left+child+parent            context.write(new Text(parent), new Text(&quot;1+&quot; + child + &quot;+&quot; + parent));            // 右表输出：child -&gt; right+child+parent            context.write(new Text(child), new Text(&quot;2+&quot; + child + &quot;+&quot; + parent));        &#125;    &#125;    // Reducer：将同一key下的左右表数据组合，找出所有祖孙关系    public static class Reduce extends Reducer&lt;Text, Text, Text, Text&gt; &#123;        public void reduce(Text key, Iterable&lt;Text&gt; values, Context context)                throws IOException, InterruptedException &#123;            List&lt;String&gt; leftList = new ArrayList&lt;&gt;();            List&lt;String&gt; rightList = new ArrayList&lt;&gt;();            for (Text val : values) &#123;                String record = val.toString();                if (record.isEmpty()) continue;                char relationType = record.charAt(0);                String[] fields = record.substring(2).split(&quot;\\+&quot;);                if (fields.length &lt; 2) continue;                String child = fields[0];                String parent = fields[1];                if (relationType == &#x27;1&#x27;) &#123;                    leftList.add(child); // 左表：child 是孙子                &#125; else &#123;                    rightList.add(parent); // 右表：parent 是祖父                &#125;            &#125;            // 输出所有可能的祖孙组合            if (time == 0) &#123;                context.write(new Text(&quot;grand_child&quot;), new Text(&quot;grand_parent&quot;));                time++;            &#125;            for (String lc : leftList) &#123;                for (String rp : rightList) &#123;                    context.write(new Text(lc), new Text(rp));                &#125;            &#125;        &#125;    &#125;    public static void main(String[] args) throws Exception &#123;        Configuration conf = new Configuration();        // 设置新的 fs.defaultFS，并设置本地执行模式        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);        conf.set(&quot;mapreduce.framework.name&quot;, &quot;local&quot;);        conf.set(&quot;mapreduce.jobtracker.address&quot;, &quot;local&quot;);        // 写死输入输出路径        String inputPath = &quot;hdfs://localhost:9000/user/hadoop/input&quot;;        String outputPath = &quot;hdfs://localhost:9000/user/hadoop/output&quot;;        Job job = Job.getInstance(conf, &quot;Simple Data Mining - Grandchild to Grandparent&quot;);        job.setJarByClass(simple_data_mining.class);        job.setMapperClass(Map.class);        job.setReducerClass(Reduce.class);        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(Text.class);        FileInputFormat.addInputPath(job, new Path(inputPath));        FileOutputFormat.setOutputPath(job, new Path(outputPath));        System.exit(job.waitForCompletion(true) ? 0 : 1);    &#125;&#125;

hadoop dfs -cat output/*

Hive

就在下载页面空白处，右键打开终端
sudo ls

sudo tar -xf apache-hive-3.1.3-bin.tar.gzsudo mv apache-hive-3.1.3-bin /usr/local/hiveecho &quot;export HIVE_HOME=/usr/local/hive&quot; &gt;&gt; ~/.bashrcecho &quot;export PATH=\$HIVE_HOME/bin:\$PATH&quot; &gt;&gt; ~/.bashrcsource ~/.bashrcsudo tee /usr/local/hive/conf/hive-site.xml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;    &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;    &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;    &lt;value&gt;hive&lt;/value&gt;    &lt;description&gt;username to use against metastore database&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;    &lt;value&gt;hive&lt;/value&gt;    &lt;description&gt;password to use against metastore database&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.exec.mode.local.auto&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;EOFsudo mv mysql-connector-java-5.1.46.tar&#123;\(2\).gz,.gz&#125;sudo tar -xf mysql-connector-java-5.1.46*.tar.gzsudo mv mysql-connector-java-5.1.46/mysql-connector-java-5.1.46.jar /usr/local/hive/lib/sudo chown hadoop:hadoop /usr/local/hive/lib/mysql-connector-java-5.1.46.jarsudo chmod 644 /usr/local/hive/lib/mysql-connector-java-5.1.46.jar
安装mysql
sudo apt remove mariadb* -y#上面报错不用管sudo apt install -y net-tools wget mysql-serversudo systemctl enable --now mysql
进入mysql初始化
sudo mysql_secure_installation
下面开始要手动输入了，输入yes或no
下面开始要手动输入了，输入yes或no
下面开始要手动输入了，输入yes或no
下面开始要手动输入了，输入yes或no
1. Press y|Y for Yes, any other key for No:   noPlease set the password for root here.New password:                 输入密码123456Re-enter new password:        输入密码123456....2. Remove anonymous users?  :   yes....3.Disallow root login remotely?  : no....4.Remove test database and access to it?: yes ....5.Reload privilege tables now? : yes
登入数据库床创建用户
cdsudo mysql -uroot -p123456UNINSTALL PLUGIN validate_password;create database hive;grant all on *.* to hive@localhost identified by &#x27;hive&#x27;;flush privileges; exitsudo sed -i &#x27;s/^bind-address\s*=\s*127.0.0.1/bind-address = 0.0.0.0/&#x27; /etc/mysql/mysql.conf.d/mysqld.cnfsudo sed -i &#x27;s/^mysqlx-bind-address\s*=\s*127.0.0.1/mysqlx-bind-address = 0.0.0.0/&#x27; /etc/mysql/mysql.conf.d/mysqld.cnfsudo systemctl restart mysql

升级元数据
sudo rm -rf /usr/local/hive/lib/guava*.jarsudo cp /usr/local/hadoop/share/hadoop/common/lib/guava-*.jar /usr/local/hive/lib/sudo cp /home/hadoop/下载/mysql-connector-java-5.1.46.tar* /usr/local/hive/lib/sudo cp /home/hadoop/Downloads/mysql-connector-java-5.1.46.tar* /usr/local/hive/lib/start-all.sh#hive初始化，这个命令只需要运行一次schematool -dbType mysql -initSchema

这个是成功的意思
启动hive，要启动hadoop
hiveshow databases;exit;

这个图片交到9.9去
实验五

自己启动hadoop，在下载文件夹空白处右键
sudo mv data ../cdls | grep data

启动hive
hive
（1）创建内部表 stocks
表A-6 stocks表结构



col_name
data_type




exchange
string


symbol
string


ymd
string


price_open
float


price_high
float


price_low
float


price_close
float


volume
int


price_adj_close
float



create table if not exists stocks(`exchange` string,`symbol` string,`ymd` string,`price_open` float,`price_high` float,`price_low` float,`price_close` float,`volume` int,`price_adj_close` float)row format delimited fields terminated by &#x27;,&#x27;;

（2）创建一个外部分区表dividends（分区字段为exchange和symbol）
表A-7 dividends表结构



col_name
data_type




ymd
string


dividend
float


exchange
string


symbol
string



create external table if not exists dividends(`ymd` string,`dividend` float)partitioned by(`exchange` string ,`symbol` string)row format delimited fields terminated by &#x27;,&#x27;;

（3）从stocks.csv文件向stocks表中导入数据。
load data local inpath &#x27;/home/hadoop/data/stocks/stocks.csv&#x27; overwrite into table stocks;

（4） 创建一个未分区的外部表dividends_unpartitioned，并从dividends.csv向其中导入数据，表结构如表A-8所示。
​    表A-8 dividends_unpartitioned表结构



col_name
data_type




ymd
string


dividend
float


exchange
string


symbol
string



create external table if not exists dividends_unpartitioned(`exchange` string ,`symbol` string,`ymd` string,`dividend` float)row format delimited fields terminated by &#x27;,&#x27;;load data local inpath &#x27;/home/hadoop/data/dividends/dividends.csv&#x27; overwrite into table dividends_unpartitioned;

（5）通过对dividends_unpartitioned的查询语句，利用Hive自动分区特性向分区表dividends各个分区中插入对应数据。
set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nonstrict;set hive.exec.max.dynamic.partitions.pernode=1000;insert overwrite table dividends partition(`exchange`,`symbol`) select `ymd`,`dividend`,`exchange`,`symbol` from dividends_unpartitioned;
运行一分钟，有点慢

（6）查询IBM公司(symbol=IBM)从2000年起所有支付股息的交易日(dividends表中有对应记录)的收盘价(price_close)。
select s.ymd,s.symbol,s.price_closefrom stocks s LEFT SEMI JOIN dividends dON s.ymd=d.ymd and s.symbol=d.symbolwhere s.symbol=&#x27;IBM&#x27; and year(ymd)&gt;=2000;

（7）查询苹果公司(symbol=AAPL)2008年10月每个交易日的涨跌情况，涨显示rise，跌显示fall,不变显示unchange。
select ymd,case    when price_close-price_open&gt;0 then &#x27;rise&#x27;    when price_close-price_open&lt;0 then &#x27;fall&#x27;    else &#x27;unchanged&#x27;end as situationfrom stockswhere symbol=&#x27;AAPL&#x27; and substring(ymd,0,7)=&#x27;2008-10&#x27;;

（8）查询stocks表中收盘价(price_close)比开盘价(price_open)高得最多的那条记录的交易所(exchange)、股票代码(symbol)、日期(ymd)、收盘价、开盘价及二者差价。
select `exchange`,symbol,ymd,price_close-price_open as `diff`from(    select *    from stocks    order by price_close-price_open desc    limit 1)t;

（9）从stocks表中查询苹果公司（symbol=AAPL）年平均调整后收盘价(price_adj_close) 大于50美元的年份及年平均调整后收盘价。
select    year(ymd) as `year`,    avg(price_adj_close) as avg_price from stockswhere `exchange`=&#x27;NASDAQ&#x27; and symbol=&#x27;AAPL&#x27;group by year(ymd)having avg_price &gt; 50;

（10）查询每年年平均调整后收盘价(price_adj_close)前三名的公司的股票代码及年平均调整后收盘价。
SET mapreduce.job.reduces=2;select t2.year, t2.symbol, t2.avg_pricefrom (    select *,           row_number() over (partition by year order by avg_price desc) as rn    from (        select            year(ymd) as year,            symbol,            avg(price_adj_close) as avg_price        from stocks        group by year(ymd), symbol    ) t1) t2where t2.rn &lt;= 3;

(9.9.3)
1.利用mapreduce实现词频统计
sudo ls &amp;&amp; start-all.sh
cdhdfs dfs -rm -r inputhdfs dfs -rm -r outputhdfs dfs -mkdir inputrm -rf inputmkdir inputecho &quot;hello world&quot; &gt; input/file1.txtecho &quot;hello hadoop&quot; &gt; input/file2.txthdfs dfs -put input/* input/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount input outputhdfs dfs -cat output/*

2.利用hive实现词频统计
hive
create table docs(line string);load data inpath &#x27;file:///home/hadoop/input&#x27; overwrite into table docs;create table word_count as select word, count(1) as count from(select explode(split(line,&#x27; &#x27;))as word from docs) wgroup by wordorder by word;
对文本进行“分词 + 炸裂”，得到所有单词列表
SELECT explode(split(line, &#x27; &#x27;)) AS word FROM docs;

对单词进行分组并计数
SELECT word, COUNT(*) AS countFROM (  SELECT explode(split(line, &#x27; &#x27;)) AS word FROM docs) wGROUP BY word;


三张图片都交到9.9.3

]]></content>
  </entry>
  <entry>
    <title>工学云自动签到</title>
    <url>/posts/53959/</url>
    <content><![CDATA[
工学云自动签到
前情提要
原作者项目：Rockytkg/AutoMoGuDingCheckIn: 工学云自动打卡，支持多用户、自定义地区、周报、日报、月报，支持免服务器运行
帮作者点一下右上角的star（星星）

仅供学习交流测试！您必须在下载或Fork此源码的24小时内删除所有内容！！
1、请务必认真阅读此文档后继续！
2、本项目开源&amp;免费，所有开发均仅限于学习交流，禁止用于任何商业用途。
3、如基于或参考此项目进行二次开发，请注明原作者并使用GPL2.0许可证开源
4、使用本项目对自己账号有一定的风险，在这里本站不承担任何责任和后果，所有危险后果自负，一切由使用者本人负责（到这里如果介意了可以退出本站，不用继续看了）

  效果呈现


使用教程
环境

Python 3.10+
pip（Python 包管理器）

服务器部署（可以先用虚拟机测试）
测试机centos，openeuler
#设置pip国内镜像源mkdir ~/.pipcat &gt;~/.pip/pip.conf &lt;&lt; &quot;EOF&quot;[global]index-url = http://mirrors.aliyun.com/pypi/simple/ [install]trusted-host=mirrors.aliyun.comEOF#安装python3yum install -y python3.11 python3.11-pipsudo rm /usr/bin/pythonsudo ln -s /usr/bin/python3.11 /usr/bin/pythonpython3 -m pip install --upgrade pip
拉取代码
#安装git yum install -y git#在root目录下执行cd /root#拉取代码(这个拉取不了就用下面国内镜像拉取)git clone https://github.com/Rockytkg/AutoMoGuDingCheckIn.git AutoMoGuDingCheckIn#国内镜像拉取git clone https://github.site/Rockytkg/AutoMoGuDingCheckIn.git AutoMoGuDingCheckIn
配置个人信息
vim AutoMoGuDingCheckIn/user/example.json
设置个人信息

设置邮箱发送


收件邮箱一定要是QQ邮箱

例子：我用的发件邮箱是网易的，收件邮箱一定要是QQ邮箱不然最后的微信邮件提醒，你操作不了&#123;    &quot;type&quot;: &quot;SMTP&quot;,    &quot;enabled&quot;: true,    &quot;host&quot;: &quot;Smtp.163.com&quot;,    &quot;port&quot;: 465,    &quot;username&quot;: &quot;xxxxxx@163.com&quot;,      &quot;password&quot;: &quot;网易授权码，按照下面教程获取&quot;,    &quot;from&quot;: &quot;严千屹&quot;,    &quot;to&quot;: &quot;xxxxx@qq.com&quot;   #一定要是qq邮箱&#125;
那个smtp密码是授权码，不是邮箱密码，我用的是网易的
网易邮箱（126/163）：授权码获取攻略_网易邮箱授权码-CSDN博客
自行获取就行了
发件邮箱要是想用其他邮箱类似操作获取授权码就行，但是收件邮箱一定要是QQ邮箱
填好之后，这就是一个人的信息了
多用户配置,有多个人，你就看下面，没有就略过
下载这个配置文件：example.json
修改信息之后，更改文件名（随意），任何运行脚本即可。

最后再运行代码，会帮你自动设置计划任务，自动打卡
cd /root/AutoMoGuDingCheckIn/ &amp;&amp; chmod +x setup.shbash setup.sh
按照他的指示操作，进行设置打卡时间就行了，这样他会按照你的时间，每天进行打卡，比如说你设置的事实8,17  那么他就会每天早上8点（上班），下午17点（下班）进行打卡

无服务器部署
Github 工作流（免服务器部署）
参见 Wiki
切记不要将配置文件上传到公开仓库，否则会造成信息泄露。请使用环境变量！！！，已经泄露请立刻修改工学云密码！！！
微信邮件推送
微信搜索QQ邮件提醒

绑定好邮箱

这样，等每一次打卡之后，就会邮件推送，然后就是下面这样的


 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <tags>
        <tag>工学云</tag>
      </tags>
  </entry>
  <entry>
    <title>运维必备工具安装</title>
    <url>/posts/1b1a13ed/</url>
    <content><![CDATA[
运维必备工具安装
系统测试：OpenEuler24.03LTS，Centos 8 stream
理论上不限制系统，其他系统可对一些命令自行做修改即可
Nginx（RPM安装）
资料来源：Nginx入门笔记 | 严千屹博客
官方文档：nginx：Linux 软件包
截止2025年4月20日目前：nginx最新版本：1.26.3

centos7或centos8
sudo yum install -y tar gcc make pcre pcre-devel zlib zlib-devel openssl openssl-develsudo yum install yum-utils -y#添加nginx源sudo cat &gt;/etc/yum.repos.d/nginx.repo &lt;&lt; &quot;EOF&quot;[nginx-stable]name=nginx stable repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=1enabled=1gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=true[nginx-mainline]name=nginx mainline repobaseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/gpgcheck=1enabled=0gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=trueEOFsudo yum install nginx -ysudo systemctl enable nginx --nownginx -v
因为没有适配openeuler的版本，这里直接用centos8的来代替，兼容的没关系，你可以根据你的系统去自行替换
原本是$releasever的，但是没有openeuler的版本直接用8来代替也就是centos8，openeuler兼容centos
sudo yum install -y tar gcc make pcre pcre-devel zlib zlib-devel openssl openssl-develsudo cat &gt;/etc/yum.repos.d/nginx.repo &lt;&lt; &quot;EOF&quot;[nginx-stable]name=nginx stable repo#baseurl=http://nginx.org/packages/centos/$releasever/$basearch/baseurl=http://nginx.org/packages/centos/8/$basearch/gpgcheck=1enabled=1gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=true[nginx-mainline]name=nginx mainline repo#baseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/baseurl=http://nginx.org/packages/mainline/centos/8/$basearch/gpgcheck=1enabled=0gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=trueEOFsudo yum install nginx -ysudo systemctl enable nginx --nownginx -v
Docker
截止2025年4月20日目前：
docker-compose最新版2.35.1 版
docker-ce最新版：Docker 文档 26.1.3
centos7或centos8
sudo curl -L &quot;https://qygit.qianyisky.cn/https://github.com/docker/compose/releases/download/v2.35.1/docker-compose-linux-x86_64&quot; -o /usr/local/bin/docker-compose#卸载旧版本sudo yum -y remove docker \                  docker-client \                  docker-client-latest \                  docker-common \                  docker-latest \                  docker-latest-logrotate \                  docker-logrotate \                  docker-selinux \                  docker-engine-selinux \                  docker-enginesudo yum remove docker-ce docker-ce-cli containerd.io docker-compose docker-machine docker-swarm -ysudo rm /etc/yum.repos.d/docker-ce.reposudo rm -rf /var/lib/dockersudo yum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposudo yum install docker-ce -ysudo systemctl enable --now dockersudo chmod +x /usr/local/bin/docker-composesudo tee /etc/docker/daemon.json &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;&#123;  &quot;registry-mirrors&quot;: [    &quot;https://docker.xuanyuan.me&quot;,    &quot;https://docker.m.daocloud.io&quot;,    &quot;https://docker.1ms.run&quot;,    &quot;https://docker.1panel.live&quot;,    &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,    &quot;https://docker.qianyios.top&quot;,    &quot;https://ghcr.nju.edu.cn&quot;,    &quot;https://k8s.nju.edu.cn&quot;  ],  &quot;max-concurrent-downloads&quot;: 10,  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-level&quot;: &quot;warn&quot;,  &quot;log-opts&quot;: &#123;    &quot;max-size&quot;: &quot;10m&quot;,    &quot;max-file&quot;: &quot;3&quot;  &#125;,  &quot;data-root&quot;: &quot;/var/lib/docker&quot;&#125;EOFsystemctl daemon-reloadsystemctl restart dockerdocker-compose --versiondocker version
openeuler
sudo curl -L &quot;https://qygit.qianyisky.cn/https://github.com/docker/compose/releases/download/v2.35.1/docker-compose-linux-x86_64&quot; -o /usr/local/bin/docker-compose#卸载旧版本sudo yum -y remove docker \                  docker-client \                  docker-client-latest \                  docker-common \                  docker-latest \                  docker-latest-logrotate \                  docker-logrotate \                  docker-selinux \                  docker-engine-selinux \                  docker-enginesudo yum remove docker-ce docker-ce-cli containerd.io docker-compose docker-machine docker-swarm -ysudo rm /etc/yum.repos.d/docker-ce.reposudo rm -rf /var/lib/dockersudo yum install -y device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.reposudo sed -i &#x27;s/\$releasever/8/g&#x27; /etc/yum.repos.d/docker-ce.reposudo yum install docker-ce docker-ce-cli containerd.io -ysudo systemctl enable --now dockersudo chmod +x /usr/local/bin/docker-composesudo tee /etc/docker/daemon.json &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;&#123;  &quot;registry-mirrors&quot;: [    &quot;https://docker.xuanyuan.me&quot;,    &quot;https://docker.m.daocloud.io&quot;,    &quot;https://docker.1ms.run&quot;,    &quot;https://docker.1panel.live&quot;,    &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,    &quot;https://docker.qianyios.top&quot;,    &quot;https://ghcr.nju.edu.cn&quot;,    &quot;https://k8s.nju.edu.cn&quot;  ],  &quot;max-concurrent-downloads&quot;: 10,  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-level&quot;: &quot;warn&quot;,  &quot;log-opts&quot;: &#123;    &quot;max-size&quot;: &quot;10m&quot;,    &quot;max-file&quot;: &quot;3&quot;  &#125;,  &quot;data-root&quot;: &quot;/var/lib/docker&quot;&#125;EOFsystemctl daemon-reloadsystemctl restart dockerdocker-compose --versiondocker version
PHP(RPM)
截止2025年4月20日目前：PHP最新版8.4.6
centos 8 stream
默认安装最新版8.3.19
yum update -yyum install yum-utils -yrpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpmyum install -y php php-fpm php-cli php-common php-devel php-gd php-pdo php-mbstring php-bcmath php-xml php-process php-intl php-soap php-ldap php-opcache 
openeuler24.03lts
欧拉系统有自己的仓库,默认安装最新版8.3.19
yum update -yyum install -y php php-fpm php-cli php-common php-devel php-gd php-pdo php-mbstring php-bcmath php-xml php-process php-intl php-soap php-ldap php-opcache php -v
Mysql LTS
截止2025年4月20日目前：Mysql LTS最新版8.4.5
资料来源：Linux安装Mysql8.4.2 LTS | 严千屹博客


这是在root目录下,自行去到root目录，你也可以自己定义路径
注意：如果需要搭载php使用，需要安装php7.2，因为rpm -ivh mysql-community-libs-compat是php的依赖。；如果不安装php，则无需安装php7.2的依赖。
这里就演示需要php，版本7.2以上，安装教程
centos 8 stream,openeuler
mkdir mysql-installtar -xvf mysql-8.4*.rpm-bundle.tar -C mysql-installcd mysql-install# 卸载 mariadb 相关的包yum remove mariadb mariadb-config mariadb-libs -y# 如果之前安装过 MySQL 社区版，也需要一并移除yum remove mysql-community-common mysql-community-icu-data-files mysql-community-client-plugins mysql-community-libs mysql-community-client mysql-community-server mysql-community-libs-compat -y# 最好按照以下顺序按照，不然会报错#全局的依赖（common）rpm -ivh mysql-community-common-8.4.5-1.el8.x86_64.rpmrpm -ivh mysql-community-icu-data-files-8.4.5-1.el8.x86_64.rpmrpm -ivh mysql-community-client-plugins-8.4.5-1.el8.x86_64.rpmrpm -ivh mysql-community-libs-8.4.5-1.el8.x86_64.rpmrpm -ivh mysql-community-client-8.4.5-1.el8.x86_64.rpmrpm -ivh mysql-community-server-8.4.5-1.el8.x86_64.rpm#php依赖文件rpm -ivh mysql-community-libs-compat-8.4.5-1.el8.x86_64.rpmsystemctl enable mysqld --nowmysqlpasswd=$(awk &#x27;/temporary password/ &#123;print $NF&#125;&#x27; /var/log/mysqld.log)mysql -u root -p&quot;$mysqlpasswd&quot;#Qianyios@007是修改后的密码ALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;Qianyios@007&#x27;;FLUSH PRIVILEGES;exitmysql -uroot -pQianyios@007
例子1：可以设置123456为密码
mysql -uroot -pQianyios@007ALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;Qianyios@007&#x27;;SHOW VARIABLES LIKE &#x27;validate_password%&#x27;;SET GLOBAL validate_password.policy = LOW;SET GLOBAL validate_password.length = 6;ALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;123456&#x27;;FLUSH PRIVILEGES;exitmysql -uroot -p123456
containerd
sudo yum install -y yum-utilssudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposudo yum clean all &amp;&amp; yum makecachesudo yum install -y containerd.iosudo mkdir -p /etc/containerd/certs.d/docker.iosudo mkdir -p /etc/containerd/certs.d/registry.k8s.iosudo mkdir -p /etc/containerd/certs.d/k8s.gcr.iosudo mkdir -p /etc/containerd/certs.d/ghcr.iosudo mkdir -p /etc/containerd/certs.d/gcr.iosudo mkdir -p /etc/containerd/certs.d/quay.iosudo mkdir -p /etc/containerd/certs.d/registry-1.docker.iosudo tee /etc/containerd/certs.d/docker.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://docker.io&quot; [host.&quot;https://registry.cn-hangzhou.aliyuncs.com/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.xuanyuan.me&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.m.daocloud.io&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.1ms.run&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.1panel.live&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.qianyios.top/&quot;]   capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://reg-mirror.giniu.com&quot;]   capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/registry-1.docker.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://registry-1.docker.io&quot;[host.&quot;https://registry.cn-hangzhou.aliyuncs.com/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.xuanyuan.me&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.m.daocloud.io&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.1ms.run&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.1panel.live&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.qianyios.top/&quot;]   capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://reg-mirror.giniu.com&quot;]   capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/k8s.gcr.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://k8s.gcr.io&quot;[host.&quot;https://registry.aliyuncs.com/google_containers&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/ghcr.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://ghcr.io&quot;[host.&quot;https://ghcr.m.daocloud.io/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/gcr.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://gcr.io&quot;[host.&quot;https://gcr.m.daocloud.io/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;registry.k8s.io&quot;[host.&quot;k8s.m.daocloud.io&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;, &quot;push&quot;][host.&quot;https://registry.aliyuncs.com/v2/google_containers&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo tee /etc/containerd/certs.d/quay.io/hosts.toml &gt; /dev/null &lt;&lt;&#x27;EOF&#x27;server = &quot;https://quay.io&quot;[host.&quot;https://quay.tencentcloudcr.com/&quot;]  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFsudo sh -c &#x27;containerd config default &gt; /etc/containerd/config.toml&#x27;sudo sed -i &#x27;s#sandbox_image = &quot;registry.k8s.io/pause:.*&quot;#sandbox_image = &quot;registry.aliyuncs.com/google_containers/pause:3.10&quot;#&#x27; /etc/containerd/config.tomlsudo sed -i &#x27;s/SystemdCgroup = false/SystemdCgroup = true/g&#x27; /etc/containerd/config.tomlsed -i &#x27;/\[plugins\.&quot;io\.containerd\.grpc\.v1\.cri&quot;\.registry\]/!b;n;s/config_path = &quot;&quot;/config_path = &quot;\/etc\/containerd\/certs.d&quot;/&#x27; /etc/containerd/config.toml# 重启 containerd 服务sudo systemctl daemon-reloadsudo systemctl restart containerd.servicesudo ctr image ls
系统更换Yum源办法
阿里网站：repo安装包下载_开源镜像站-阿里云 (aliyun.com)
Centos Stream 8
mkdir repo.bakmv /etc/yum.repos.d/* repo.bak/wget -O /etc/yum.repos.d/CentOS-Stream-AppStream.repo https://mirrors.aliyun.com/repo/centos-stream/8/CentOS-Stream-AppStream.repowget -O /etc/yum.repos.d/CentOS-Stream-BaseOS.repo https://mirrors.aliyun.com/repo/centos-stream/8/CentOS-Stream-BaseOS.repodnf clean all &amp;&amp; dnf makecache
Centos 7
mkdir repo.bakmv /etc/yum.repos.d/* repo.bak/wget -O /etc/yum.repos.d/Centos-7.repo https://mirrors.aliyun.com/repo/Centos-7.repoyum clean all &amp;&amp; yum makecache
Centos 8
mkdir repo.bakmv /etc/yum.repos.d/* repo.bak/curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repoyum clean all &amp;&amp; yum makecache
Rocky 系列
sed -e &#x27;s|^mirrorlist=|#mirrorlist=|g&#x27; \    -e &#x27;s|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g&#x27; \    -e&#x27;s|^#baseurl=http://mirrors.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g&#x27; \    -i.bak \    /etc/yum.repos.d/Rocky-*.repoyum clean all &amp;&amp; yum makecache
运维脚本（更新ing）
wget https://blog.qianyios.top/file/qy-yunwei.shbash qy-yunwei.sh

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>Centos 8</tag>
        <tag>Docker</tag>
        <tag>Centos 7</tag>
        <tag>OpenEuler</tag>
        <tag>Mysql</tag>
        <tag>Nginx</tag>
        <tag>Php</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机网络技术课程综合实验</title>
    <url>/posts/f5e08620/</url>
    <content><![CDATA[
计算机网络技术课程综合实验
空拓扑下载：空拓扑
前情提要
自行安装ensp软件，结合教程来安装
华为ensp模拟器安装教程 | 严千屹博客
打开显示端口

简单说一下一些常识
1.用户视图&lt;Huawei&gt;： - 这是登录后的初始界面，在这里可以执行一些基本信息查看和简单的命令。
2.系统视图：[Huawei] - 输入system-view或者sys进入此模式，用于全局性的设备配置。
3.接口视图：[Huawei-GigabitEthernet0/0/1] - 使用interface或int命令进入特定接口进行配置。
4.协议视图：[Huawei-ospf-1] - 对应于路由协议等高级功能的配置。

以后的每一步实验都会从&lt;Huawei&gt;开始，如果你是系统视图，自行输入q退回用户试图

基础配置
要求：
1、给6个路由器AR1到AR6，6个交换机LSW1到LSW6全部改好设备名字，设备名称修改为自己名字首字母缩写+设备名。比如最左边的AR1路由器，学生名字为张三，则设备名称改为 yjxAR1

2、给第一行的AR1、AR2、AR3、AR4、AR5共5个路由器的接口，配置好IP地址。在网络拓扑图明确给出来的，按图示的配置。没有明确给出来的，则统一默认规则：左右并排连接的，左边接口最后1个字节配1，右边接口配2； 上下连接的，下边接口配1，上边接口配2.

如果刚开始是&lt;Huawei&gt;也叫用户视图就可以直接输入下面的指令，如果是[Huawei]也叫系统视图，要输入一个quit或q进行退出。这样是为了方便统一大家的输入，我们全部退回到用户视图，这样就可以全部输入指令了

以下简写的命令，回到前情提要自己看
路由器配置

主机位没有明确给出来的，则统一默认规则：左右并排连接的，左边接口最后1个字节配1，右边接口配2； 上下连接的，下边接口配1，上边接口配2.

yjxAR1
syssysname yjxAR1int g0/0/2ip address 10.1.3.2 30qint g0/0/1ip address 10.1.1.2 30qint g0/0/0ip address 10.1.2.2 30qint g4/0/0ip address 10.12.100.10 24qq
复制全部，粘贴全部一步到位

yjxAR2
syssysname yjxAR2int g0/0/0ip address 10.12.100.20 24qint s4/0/0ip address 23.1.1.1 24qq

yjxAR3
syssysname yjxAR3int s4/0/0ip address 23.1.1.2 24qint g0/0/0ip address 34.1.1.1 30qq

yjxAR4
syssysname yjxAR4int g0/0/0ip address 34.1.1.2 30qint g0/0/1ip address 45.1.1.1 30qq

yjxAR5
un te mosyssysname yjxAR5int g0/0/0ip address 45.1.1.2 30q

3、重要提醒：所有网络设备的配置，记得最后用save命令保存。否则软件关闭后将不保留。

pc配置静态ip
4、给网络拓扑图最底下的PC，按图配置好静态IP地址。










AR路由表展示





第二周
配置vlan
开启LSW1、LSW3、LSW4，关闭LSW2避免形成环路，配置VLAN库，配置交换机端口属性，配置交换机端口归属哪些VLAN
yjxLSW3
syssysname yjxLSW3#配置VLAN库vlan batch 10 20#配置交换机端口属性int g0/0/1port link-type accessint g0/0/2port link-type accessint g0/0/5port link-type accessint g0/0/3port link-type trunk#配置交换机端口归属哪些VLAN或哪些VLAN能通过int g0/0/1port default vlan 10int g0/0/2port default vlan 20int g0/0/5port default vlan 10int g0/0/3port trunk allow-pass vlan 10 20 #查看配置信息display vlan

yjxLSW4
un te mosyssysname yjxLSW4vlan batch 10 20int g0/0/1port link-type accessint g0/0/2port link-type accessint g0/0/3port link-type trunkint g0/0/1port default vlan 10int g0/0/2port default vlan 20int g0/0/3port trunk allow-pass vlan 10 20display vlan

配置网关
在LSW1交换机上，配置VLAN的虚拟接口SVI，配置SVI的IP地址，从而实现各VLAN的连通。一般配置VLAN接口，是在核心交换机上，本拓扑图即为LSW1。PC1到PC4补充上对应的网关，即SVI的IP地址
在yjxLSW1配置网关
yjxLSW1
un te mosyssysname yjxLSW1vlan batch 10 20int vlan 10ip address 192.168.1.254 24int vlan 20ip address 192.168.2.254 24dis ip int br

最后LSW1,3,4全部退回到用户视图，保存
save
给pc1-pc4配置网关

第三周
开启这两台机

配置链路聚合
yjxLSW1
un te mosyssysname yjxLSW1int Eth-Trunk 1port link-type trunkport trunk allow-pass vlan 10 20int Eth-Trunk 1mode lacp-staticmax active-linknumber 2trunkport g0/0/3trunkport g0/0/4trunkport g0/0/6
yjxLSW2
un te mosyssysname yjxLSW2int Eth-Trunk 1port link-type trunkport trunk allow-pass vlan 10 20int Eth-Trunk 1mode lacp-staticmax active-linknumber 2trunkport g0/0/3trunkport g0/0/4trunkport g0/0/6
两个交换机一起打
dis eth-trunk

当连接GE0/0/3的线路出现问题时，会自动切换到使用GE0/0/4和GE0/0/6端口的线路。
在此实验中，手工将GE0/0/3的线路删除，再显示链路聚合的状态信息
右键线路删除，注意如果3口还在说明你删错了，连回来


你测试完记得加回来

然后保存关机

第四周
本实验通过配置动态路由RIP协议，实现内网全网互通。
删除三条链路聚合的线，并且开起圈内的机子

开启内网的所有设备：R1、LSW1、LSW2、LSW3、LSW4，Serve1及内网PC。为避免环路影响，本次实验将连接LSW1与LSW2的聚合链路（即心跳线）先删除断开。
任务1
配置LSW1、LSW2、LSW3、LSW4的VLAN库，端口属性，端口归属。其中
yjxLSW1的VLAN库有VLAN 10，VLAN 20，VLAN 110
un te mosysvlan 110dis vlan

yjxLSW2的VLAN库有VLAN 10，VLAN 20，VLAN 120
un te mosysvlan 120dis vlan

yjxLSW3的VLAN库有VLAN 10，VLAN 20 （之前已配置）

yjxLSW4的VLAN库有VLAN 10，VLAN 20 （之前已配置）

yjxLSW1中，GE0/0/1口和GE0/0/2口为Trunk口，允许VLAN 10和VLAN 20通过
GE0/0/5口为Access口，归属VLAN 110
Eth-Trunk1为Trunk口，允许VLAN 10和VLAN 20通过（之前已配置）
un te mosysvlan batch 10 20int g0/0/1port link-type trunkport trunk allow-pass vlan 10 20int g0/0/2port link-type trunkport trunk allow-pass vlan 10 20int g0/0/5port link-type accessport default vlan 110dis vlan

yjxLSW2中，GE0/0/1口和GE0/0/2口为Trunk口，允许VLAN 10和VLAN 20通过
GE0/0/5口为Access口，归属VLAN 120
Eth-Trunk1为Trunk口，允许VLAN 10和VLAN 20通过
un te mosysvlan batch 10 20int g0/0/1port link-type trunkport trunk allow-pass vlan 10 20int g0/0/2port link-type trunkport trunk allow-pass vlan 10 20int g0/0/5port link-type accessport default vlan 120dis vlan

yjxLSW3中，GE0/0/3口和GE0/0/4口为Trunk口，允许VLAN 10和VLAN 20通过（3口之前已配置）
GE0/0/1口和GE0/0/5口为Access口，归属VLAN 10（之前已配置）
GE0/0/2口为Access口，归属VLAN 20（之前已配置）
un te mosysint g0/0/4port link-type trunkport trunk allow-pass vlan 10 20dis vlan

yjxLSW4中，GE0/0/3口和GE0/0/4口为Trunk口，允许VLAN 10和VLAN 20通过（3口之前已配置）
GE0/0/1口为Access口，归属VLAN 10（之前已配置）
GE0/0/2口为Access口，归属VLAN 20（之前已配置）
un te mosysint g0/0/4port link-type trunkport trunk allow-pass vlan 10 20dis vlan

任务2
给AR1路由器的4个端口配置IP地址；给LSW1的VLAN10、VLAN20、VLAN110配置IP地址；给LSW2的VLAN10、VLAN、VLAN120配置IP地址。遵行未明确指示的情况下，IP地址最后一个字节：左边为1，右边为2；下面为1，上面为2。
yjxAR1
un te mosyssysname yjxAR1int g0/0/2ip address 10.1.3.2 30int g0/0/1ip address 10.1.1.2 30int g0/0/0ip address 10.1.2.2 30int g4/0/0ip address 10.12.100.10 24dis ip in br

yjxLSW1
un te mosysint vlan 10ip address 192.168.1.253 24int vlan 20ip address 192.168.2.253 24int vlan 110ip address 10.1.1.1 30dis ip in br

yjxLSW2
un te mosysint vlan 10ip address 192.168.1.252 24int vlan 20ip address 192.168.2.252 24int vlan 120ip address 10.1.2.1 30dis ip in br

任务3
给AR1、LSW1、LSW2配置使用动态路由协议RIPv2，
server1配置IP地址10.1.3.1  网关10.1.3.2
处于VLAN 10的PC手动切换网关为 1.253或者1.252
处于VLAN 20的PC手动切换网关为 2.253或者2.252
配置完成后，内网全网互相能Ping通，通过显示路由表信息确定结果。


要点保存哈

配置rip协议
yjxAR1
配置AR1路由器使用RIP协议，如果直接按照拓扑图给的网络地址配置，如此命令[yjxAR1-rip-1]network 10.1.1.0将会提示出错
原因是Error: The network address is invalid, and the specified address must be major-net address without any subnets.
错误：网络地址无效，指定的地址必须是主网地址，不能包含任何子网。
sysrip 1version 2network 10.0.0.0
yjxLSW1
sysrip 1version 2network 10.0.0.0network 192.168.1.0network 192.168.2.0
yjxLSW2
sysrip 1version 2network 10.0.0.0network 192.168.1.0network 192.168.2.0
yjxAR1
dis ip rou

yjxLSW1
dis ip rou

yjxLSW2
dis ip rou

测试互通
pc1 ping pc3和pc4

pc1 ping client1和pc2

pc1 ping yjxAR1和server

pc2 ping yjxAR1和server

pc3 ping yjxAR1和server

pc4 ping yjxAR1和server

配置保存，给你所有的交换机路由器输入save，这里就不多说了
第五周
开这几台

目的：给LSW1和LSW2配置VRRP，给VLAN 10和VLAN 20配置不同的虚拟路由备份组，既有主备备份功能，又有负载分担功能。

可以还很清晰的看出，两个区域的pc走的是各自的网关
yjxLSW1
sysun in enint vlan 10vrrp vrid 10 virtual-ip 192.168.1.254vrrp vrid 10 priority 120vrrp vrid 10 preempt-mode time delay 20int vlan 20vrrp vrid 20 virtual-ip 192.168.2.254vrrp vrid 20 preempt-mode time delay 20
yjxLSW2
sysun in enint vlan 10vrrp vrid 10 virtual-ip 192.168.1.254vrrp vrid 10 preempt-mode time delay 20int vlan 20vrrp vrid 20 virtual-ip 192.168.2.254vrrp vrid 20 priority 120vrrp vrid 20 preempt-mode time delay 20
yjxLSW1
display vrrp interface Vlanif 10display vrrp interface Vlanif 20

yjxLSW2
display vrrp interface Vlanif 10display vrrp interface Vlanif 20

所有PC修改网关，记得点击应用和保存

验证pc

实验可知通过配置vrrp实现yjxLSW1和yjxLSW2之间构建虚拟ip，也就是虚拟网关，然后pc设置这个虚拟网关，达到负载均衡，且有主备-备份功能。就是原本有两个网关的，192.168.1.253和192.168.2.252,如果yjxLSW1和yjxLSW2之间一台挂了，就没办法与服务器通信了，这时候通过vrrp构建虚拟ip192.168.1.254和192.168.2.254实现了主备容灾
然后给yjxLSW1和yjxLSW2进行save保存
第六周

开这几台
任务1：使用display stp brief 命令查看LSW1、LSW2、LSW3、LSW4交换机各端口在生成树中的角色。画出处于工作状态的生成树的结构图，提示：保留根端口和指定端口即可得到。
任务2：给LSW1与LSW2的3根连线（心跳线）接上，再使用display stp brief 命令查看LSW1、LSW2、LSW3、LSW4交换机各端口在生成树中的角色。画出处于工作状态的生成树的结构图，提示：保留根端口和指定端口即可得到。
dis stp br

任务3：配置多生成树协议(MSTP)。实例1的根桥为LSW1，备份根桥为LSW2；实例2的根桥为LSW2，备份根桥为LSW1。将VLAN 10映射到实例1；将VLAN 20映射到实例2。最后查看实例0、实例1、实例2各自工作状态中构成的生成树。方法：查看各交换机各实例各端口的角色，保留根端口和指定端口即可得到。

yjxLSW1
region-name yanjiaxi改成你的名字
sysun in enstp mode mstpstp region-configurationregion-name yanjiaxiinstance 1 vlan 10instance 2 vlan 20active region-configurationstp instance 1 root primarystp instance 2 root secondary
yjxLSW2
region-name yanjiaxi改成你的名字
sysun in enstp region-configurationregion-name yanjiaxiinstance 1 vlan 10instance 2 vlan 20active region-configurationstp instance 1 root secondarystp instance 2 root primary
yjxLSW3
region-name yanjiaxi改成你的名字
sysun in enstp region-configurationregion-name yanjiaxiinstance 1 vlan 10instance 2 vlan 20active region-configuration
yjxLSW4
region-name yanjiaxi改成你的名字
sysun in enstp region-configurationregion-name yanjiaxiinstance 1 vlan 10instance 2 vlan 20active region-configuration
yjxLSW1
display stp region-configuration

dis stp br

四个交换机保存配置
第七周
开这几台

任务1：网络拓扑图中，AR2为出口路由器，AR1与AR2之间的网络属于内网。配置AR2使用RIP协议公告与AR1相连的网段，注意不要公告外网。
yjxAR2
sysun in enrip 1version 2network 10.0.0.0
pc1  ping 10.12.100.20

任务2：AR2上配置SSH服务端，AR1上配置SSH客户端
说明：AAA为3A认证，即：认证(Authentication)；授权(Authorization)；计帐(Accounting)。
yjxAR2
#生成本地密钥对，SSH需要使用密钥对进行加密通信sysun in enrsa local-key-pair create

ssh040是我的学号，自己改
stelnet server enableaaalocal-user ssh040 password cipher 123456local-user ssh040 privilege level 15local-user ssh040 service-type ssh#这里用all和password可以，用其他两种不可以。因为如果使用密钥验证，需要在PC机上生成密钥对，并将公钥拷贝至网络设备。ssh user ssh040 authentication-type password   #配置VTY接口，VTY接口用于管理远程登录。配置VTY接口支持SSH协议：user-interface vty 0 4authentication-mode aaaprotocol inbound ssh
yjxAR1
sysun in enssh client first-time enable stelnet 10.12.100.20#输入用户名，然后按两次y，然后输入密码

在AR1登入之后变成AR2就行
最后两个路由器保存配置
第八周
开这几台

任务1： 在LSW1与LSW2分别配置默认路由指向AR1,AR1配置默认路由指向AR2,AR2配置默认路由指向AR3。
yjxAR1
sysun in enip route-static 0.0.0.0 0 10.12.100.20
yjxLSW1
sysun in enip route-static 0.0.0.0 0 10.1.1.2
yjxLSW2
sysun in enip route-static 0.0.0.0 0 10.1.2.2
yjxAR2
sysun in enip route-static 0.0.0.0 0 23.1.1.2
任务2：在AR2配置静态NAT,把服务器Server1 10.1.3.1映射为公网的23.1.1.5，用于公网能访问。
未配置前先测试，AR1路由器、Server1都无法Ping通外网

同理 Server1不能Ping通23.1.1.2
yjxAR2
配置静态NAT映射：将内网IP地址映射到外网IP地址
sysun in ennat static global 23.1.1.5 inside 10.1.3.1interface Serial 4/0/0nat static enable 
配置后再测试，AR1路由器与外网是不通的，但是Server1能Ping通外网
ping 23.1.1.2
ping 23.1.1.5

sysun in endisplay nat static 

任务3：配置PAT把内网IP映射到端口S4/0/0，实现内网能够访问外网。
未配置前，先测试
ping 23.1.1.2

同理 PC1不能Ping通23.1.1.2
配置ACL
yjxAR2
sysun in enacl number 2000rule 0 permit source 192.168.1.0 0.0.0.255rule 1 permit source 192.168.2.0 0.0.0.255
配置NAT Outbound
yjxAR2
在外网接口上应用ACL，并启用NAT Outbound功能。
interface Serial 4/0/0nat outbound 2000
配置后，AR1路由器是不能访问外网的，表明内网与外网是分隔开的。但是PC机已经能访问外网，表明NAT配置成功。
ping 23.1.1.2

但是： PC1能Ping通23.1.1.2

yjxAR2
display acl 2000display nat outbound

任务4：用Server1对AR3的Serial4/0/0口进行Ping测试，抓包査看NAT地址是否转换成功？
在AR2的Serial4/0/0口上抓取的Ping包如下：



AR3  ping 23.1.1.5
在AR1的GE4/0/0口或者AR2的GE0/0/0口，抓取的Ping包如下：
两个接口二选一，然后抓包，不会打开就去看前面…
AR3  ping 23.1.1.5

最后所有配置过的交换机路由器保存
第九周

任务0：掌握单臂路由的原理，配置方法以及应用场合。配置前：测试不同VLAN的连通情况，例如PC7与PC8的连通情况。

任务1：在交换机LSW6上配置VLAN，VLAN库有VLAN50与VLAN60。GE0/0/1口为Trunk口，GE0/0/2与GE0/0/3为Access口。
yjxLSW6
syssysname yjxLSW6un in envlan batch 50 60interface GigabitEthernet 0/0/1port link-type trunk port trunk allow-pass vlan 50 60interface GigabitEthernet 0/0/2port link-type access port default vlan 50interface GigabitEthernet 0/0/3port link-type access port default vlan 60
任务2：在路由器AR5上配置子接口，子接口通过使用802.1q协议与对应的VLAN进行识别绑定，给子接口配置IP地址。此地址作为对应VLAN网段的网关。
yjxAR5
sysun in enint g0/0/2.5dot1q termination vid 50ip add 192.168.8.1 24arp broadcast enableint g0/0/2.6dot1q termination vid 60ip add 192.168.9.1 24arp broadcast enable

自己保存配置
第十周

任务1：参考上一周的单臂路由实验，在交换机LSW5一侧的网络，配置单臂路由。
1）在交换机LSW5上配置VLAN，VLAN库有VLAN30与VLAN40。GE0/0/1口为Trunk口，GE0/0/2、GE0/0/3与GE0/0/5为Access口。同时配置端口加入到相应的VLAN中。
yjxLSW5
syssysname yjxLSW5un in envlan batch 30 40interface GigabitEthernet 0/0/1port link-type trunk port trunk allow-pass vlan 30 40interface GigabitEthernet 0/0/2	port link-type access port default vlan 30interface GigabitEthernet 0/0/3port link-type access port default vlan 40interface GigabitEthernet 0/0/5	port link-type access port default vlan 40
2）在AR5上配置单臂路由，在路由器AR5上配置子接口，子接口通过使用802.1q协议与对应的VLAN进行识别绑定，给子接口配置IP地址。此地址作为对应VLAN网段的网关。
yjxAR5
sysun in eninterface GigabitEthernet 0/0/1.30dot1q termination vid 30ip address 172.16.1.254 24arp broadcast enable interface GigabitEthernet 0/0/1.40dot1q termination vid 40ip address 172.16.2.254 24arp broadcast enable 
任务2：在AR5路由器上配置DHCP服务。
AR5

启用 DHCP 功能 ，全局启用 DHCP 服务：

sysdhcp enable 

配置 DHCP 地址池，为每个 VLAN 创建 DHCP 地址池，配置网关、DNS 和 IP 地址范围
配置 VLAN30 的 DHCP 地址池

ip pool vlan30network 172.16.1.0 mask 24 gateway-list 172.16.1.254dns-list 8.8.8.8 ip pool vlan40network 172.16.2.0 mask 255.255.255.0gateway-list 172.16.2.254dns-list 8.8.8.8excluded-ip-address 172.16.2.2
#excluded-ip-address 172.16.2.2   //此处排除的IP，是留给终端Client2

在子接口上启用 DHCP 服务，在每个子接口上启用 DHCP 服务，选择使用全局地址池

interface GigabitEthernet 0/0/1.30dhcp select globalinterface GigabitEthernet 0/0/1.40dhcp select global 
任务3： 在LSW5交换机上配置DHCP Snooping功能      // snoop v. 监听、监控
yjxLSW5
1）在LSW5交换机上开启全局DHCP 以及DHCP Snooping
sysdhcp enabledhcp snooping enable

2）LSW5交换机GE0/0/1、GE0/0/2、GE0/0/3接口开启 dhcp snooping，此时默认情况下接口为非信任接口，只能发送DHCP请求包，无法接收DHCP应答包
interface GigabitEthernet 0/0/1dhcp snooping enable interface GigabitEthernet 0/0/2dhcp snooping enable interface GigabitEthernet 0/0/3dhcp snooping enable
PC 先执行PC&gt;ipconfig /release，再执行PC&gt;ipconfig /renew发现无法获取到IP地址。




3）配置交换机GE0/0/1口为信任接口
interface GigabitEthernet 0/0/1dhcp snooping trusted 

PC5   执行PC&gt;ipconfig /renew发现现在可以获取到IP地址。


4）查看确认dhcp snooping的配置
yjxLSW5
display dhcp snooping configuration 

测试pc5  ping 其他机子

然后交换机，路由器保存配置
第十一周
开AR3和AR2就行
任务一
任务1：在AR3路由器上配置PPP认证，AR3为认证方，AR2为被认证方。
yjxAR3

在AR3路由器的Serial 4/0/0口配置PPP认证的方式为PAP

sysun in eninterface Serial 4/0/0ppp authentication-mode papq

在AR3路由器的aaa视图下，创建用户R2，服务类型为ppp。

aaalocal-user R2 password cipher 123456local-user R2 service-type pppq

检查AR2路由器的接口状态，特别留意Serial4/0/0口的物理状态和协议状态。

yjxAR2
display ip interface brief


在AR3路由器上，重启Serial4/0/0口。

yjxAR3
sysun in eninterface Serial 4/0/0shutdownundo shutdown

再次检查AR2路由器的接口状态，特别留意Serial4/0/0口的物理状态和协议状态。

yjxAR2
display ip interface brief

观察结果 ：AR2与AR3无法正常通信，链路物理状态正常，但链路层协议状态不正常，Ping命令无法通过。这是因为PPP链路上的PAP认证未通过，需要在被认证方AR2上配置相关PAP认证参数。

在AR2上的S4/0/0接口下，使用PPP pap local-user命令配置本端被对端以PAP方式验证时本地发送的PAP用户名和密码。

yjxAR2
sysun in eninterface Serial 4/0/0ppp pap local-user R2 password cipher 123456

配置完成后，等待20秒以上，再次查看链路状态并测试连通性。特别留意Serial4/0/0口的物理状态和协议状态。

yjxAR2
display ip interface brief
输出示例：

任务二
任务2：网络管理员发现网络频繁遭受攻击，因为PAP认证的密码为明文的，密码经常被盗用，遂将PPP认证的方式由PAP改为CHAP。
1）首先，删除原有的PAP认证配置
yjxAR3
interface Serial 4/0/0undo ppp authentication-mode
yjxAR2
interface Serial 4/0/0undo ppp pap local-user
2）再重新配置CHAP认证配置
yjxAR3
interface Serial 4/0/0ppp authentication-mode chap
保留继续使用任务1 在aaa视图下配置的用户名R1与密码。此时可不用重复配置。
3）在AR3路由器上，重启Serial4/0/0口。
yjxAR3
interface Serial 4/0/0shutdownundo shutdown
4）检查AR2路由器的接口状态，特别留意Serial4/0/0口的物理状态和协议状态。
yjxAR2
display ip interface brief
可以观察到，现在AR2与AR3无法正常通信，链路物理状态正常，但是链路层协议状态不正常，用Ping命令将无法Ping通。这是因为此时PPP链路上的CHAP认证未通过，现在仅仅配置了认证方设备AR3，还需要在被认证方AR2上配置相关CHAP认证参数。

5)在AR2上的S4/0/0接口下，使用PPP chap 命令配置本端被对端以CHAP方式验证时本地发送的用户名和密码。
yjxAR2
interface Serial 4/0/0ppp chap user R2ppp chap password cipher 123456
配置完成后，等待20秒以上
6)再次查看链路状态并测试连通性。特别留意Serial4/0/0口的物理状态和协议状态。
yjxAR2
display ip interface brief

两个路由器保存配置
第十二周
全开
任务1：对网段 AR2-AR3、AR3-AR4、AR4-AR5 配置 OSPF 路由协议实现互通
yjxAR2 配置
出口路由器需要配置默认路由指向互联网
sysun in enip route-static 0.0.0.0 0.0.0.0 23.1.1.2
yjxAR3 配置
sysun in enospf 1area 0network 23.1.1.0 0.0.0.255network 34.1.1.0 0.0.0.3
yjxAR4 配置
sysun in enospf 1area 0network 34.1.1.0 0.0.0.3network 45.1.1.0 0.0.0.3
yjxAR5 配置
sysun in enospf 1area 0network 45.1.1.0 0.0.0.3
检查各路由器配置
sysun in endis ip rou



任务2：对网段 AR3-AR6 配置 OSPF 路由协议进行通告网段，并划分到区域 1
yjxAR3 配置
sysun in eninterface GigabitEthernet 0/0/1ip address 36.1.1.2 30ospf 1area 1network 36.1.1.0 0.0.0.3
yjxAR6 配置
sysun in ensysname yjxAR6interface GigabitEthernet 0/0/0ip address 36.1.1.1 30interface GigabitEthernet 0/0/1ip address 10.10.2.2 30ospf 1area 1network 36.1.1.0 0.0.0.3
检查AR5能否学习到area1的路由
sysun in endis ip rou

任务3：AR3 与 AR6 之间配置 OSPF 认证
在 AR3、AR6 之间配置 OSPF的接口认证，认证模式为 MD5 认证。测试认证前后网络的连通性。AR3 和 AR6 的配置完全一样。同一网段的接口认证模式和口令必须相同，不同网段可以不同。缺省情况下，OSPF接口没有配置认证方式。建议配置认证方式，否则系统可能不安全。
yjxAR3 配置
sysun in eninterface GigabitEthernet 0/0/1ospf authentication-mode md5 1 cipher jwkshutdownundo shutdown
yjxAR6 配置
sysun in eninterface GigabitEthernet 0/0/0ospf authentication-mode md5 1 cipher jwk
测试与外网互通
AR6和pc1  ping   AR5
ping 45.1.1.2


保存配置，涉及啥路由器自己保存
第十三周
全开
任务 0: 理解 NAT 配置相关知识(不用做)

nat outbound —— 配置源 NAT (SNAT)，用于内网用户访问外网（通常结合 ACL 或地址池使用）。
nat server —— 配置目的 NAT（端口映射），允许外网访问内网服务器（如 Web、FTP 等）。
nat static —— 配置静态 NAT（一对一映射），将公网 IP 直接映射到内网 IP，双向可达。

[zsAR6-GigabitEthernet0/0/0]nat ?    outbound      Specify net address translation    server        Specify NAT server    static        Specify static NAT
注意事项
接口类型:nat outbound 和natserver 需配置在连接公网的接口。
ACL 控制:建议用 ACL限制 natserver 的访问来源，避免暴露内网服务。
ALG 兼容性:FTP、SIP 等协议依赖 ALG，若异常可尝试 nat alg ftp disable 关闭调试
优先级:nat static&gt;na tserver&gt;nat outbound，冲突时按优先级生效。
任务 1: 配置路由器端口与服务器的 IP 地址

AR6 路由器补齐全端口 IP 地址 GE0/0/1 的 IP 地址为 10.10.2.2

yjxAR6
int g0/0/1ip address 10.10.2.2 30

Server2 配上 IP 地址：10.10.2.1 255.255.255.252 关联对端路由器端口地址 10.10.2.2

Server2

任务 2: 在 AR5 和 AR6 配置 PAT 把内网 IP 映射到路由器的外网端口 GE0/0/0，实现内网能够访问外网。

配置访问控制列表（ACL）

yjxAR6
sysun in enacl 2000rule 0 permit source 10.10.20.0 0.0.0.3

启用 NAT Outbound 功能，并关联相应的 ACL

yjxAR6
int g0/0/0nat outbound 2000

配置模块 2 的出口路由器 AR5

yjxAR5
sysun in enacl 2000rule 0 permit source 192.168.8.0 0.0.0.255rule 2 permit source 192.168.9.0 0.0.0.255rule 3 permit source 172.16.1.0 0.0.0.255rule 4 permit source 172.16.2.0 0.0.0.255int g0/0/0nat outbound 2000
pc5  ping  （pc8 ，AR2，ar6）

任务 3: Server2 开启 HTTP、FTP 服务。模块二的客户端可以获取到 Server2 的共享文件。
1.配置端口映射
yjxAR6
int g0/0/0nat server protocol tcp global current-interface ftp inside 10.10.2.1 ftp输入ynat server protocol tcp global current-interface www inside 10.10.2.1 www输入y
2.启用 FTP 协议依赖的 NAT ALG
yjxAR6
qnat alg ftp enable
在服务器 Server2 上将 HttpServer和 FtpServer 启动，目录文件夹放入文件。 然后在模块 2中，用客户端进行 HTTP 和 FTP 的访问测试。
在你电脑找个位置创建一个文件夹和测试文件

在server2开启两个服务


在client2进行测试


ar5和ar6进行保存
第十四周
任务 1:配置访问控制列表ACL，只允许client2可以访问服务器server 2，拒绝其它网络访问
1)在路由器 AR6 配置 ACL,只允许来自网段 45.1.1.0,也就是 AR4-AR5 网段的数据流量通过应用在GE0/0/0口，在这里是作为流量人口inbound方向，在这里注意，应用在路由器不同的接口(端口)，需要根据数据流量的流动方向，来判断此接口的流量方向是流入inbound还是流出 outbound.
yjxAR6
sysun in enacl number 3000rule 5 permit tcp source 45.1.1.0 0.0.0.3 destination 36.1.1.0 0.0.0.3rule 10 deny tcprule 15 permit ipinterface GigabitEthernet 0/0/0traffic-filter inbound acl 3000
2)在路由器 AR5配置 ACL,只允许源IP为 172.16.2.2的终端访问目标IP为36.1.1.1的终端
yjxAR5
sysun in enacl number 3000rule 5 permit tcp source 172.16.2.2 0 destination 36.1.1.1 0rule 10 deny tcp destination 36.1.1.1 0rule 15 permit ipinterface GigabitEthernet 0/0/1traffic-filter inbound acl 3000interface GigabitEthernet 0/0/2traffic-filter inbound acl 3000
测试client2可以访问server2
自己开启server2的服务


测试client1访问


测试更改pc7为client3，看看是否可访问server2
把pc7删除更换为clien3，然后配置好ip地址，然后pc8进行ping  client3

测试clent3能否访问server2


ar6和ar5  save

 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！
]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>ensp</tag>
      </tags>
  </entry>
  <entry>
    <title>网络工程师备考笔记</title>
    <url>/posts/2c5e9e22/</url>
    <content><![CDATA[
网络工程师备考笔记
计算机网络概论
早期的计算机网络是面向终端的计算机网络，在这种网络中主要存在的是终端和中心计算机进行通信。计算机网络:计算机技术与通信技术的结合。
计算机网络的分类
1.通信子网和资源子网：
通信子网:通信节点(集线器、交换机、路由器等)和通信链路(电话线、同轴电缆、无线电线路,卫星线路、微波中继线路和光纤缆线)。
用户资源子网:PC、服务器等

3.LAN MAN WAN

按地域范围，可分为局域网(LAN)、城域网(MAN)和广域网(WAN)3 类。
按所采用网络协议的不同，可分为TCP/IP 网络、SPX/IPX网络等。
按交换方式，可分为电路交换网、分组交换网、报文交换等。



按通信介质，可分为有线网和无线网。
按网络控制方式，可分为集中式和分布式。


我国互联网的发展

计算机网络的应用



案例分析题
某仓储企业网络拓扑结构如图1-1所示，该企业占地500亩。有五层办公楼1栋，大型仓库10栋。每栋仓库内、外部配置视频监控16台，共计安装视频监控160台，SwitchA、服务器、防火墙、管理机、Router A等设备部署在企业办公楼一层的数据机房中，Switch B部署在办公楼一层配线间作为一层的接入设备，Switch C和Switch D分别部署在仓库1和仓库2，各仓库的交换机与Switch A相连。办公楼的其他楼层的交换机以及其他仓库的交换机的网络接入方式与图1-1中Switch B、SwitchC、SwitchD接入方式相同，不再单独在图1-1上标示。
若接入的IPC采用1080P的图像传输质量传输数据，SwitchC、Switch A选用百兆交换机是否满足带宽要求，请说明理由。


OSI和TCP/IP参考模型
前情提要
为什么要进行网络分层？

OSI参考模型:CPU/内存/硬盘/显卡/主板等标准化
某一层所做的改动不会影响到其他的层，利于设计、开发和故障排除。
通过定义在模型的每一层实现功能，鼓励产业的标准化。
通过网络组件的标准化，允许多个供应商协同进行开发。
允许各种类型的网络硬件和软件互相通信，无缝融合。
促进网络技术快速迭代，降低成本
OSI参考模型
七层的详细知识点可以跳转：2.6 OSI协议集
物理层(PI):确定物理设备接口，提供点一点的二进制位流传输的物理链路。
数据链路层(DLL):利用差错处理技术，提供高可靠的点到点传输的数据链路
网络层(NL):利用逻辑寻址;路由选择，实现用户数据的传输。
传输层(TL):提供可靠或者不可靠的端到端传输。
会话层(SL):为通信双方指定通信方式，并创建、注销会话
表示层(PL):数据和信息的语法转换内码，数据压缩解压、加密解密
应用层(AL):各种应用程序、协议

TCP/IP参考模型


TCP/IP参考模型对应协议

OSI与TCP/IP模型对&amp;协议层次

例题


在OSI参考模型中，实现端到端的应答、分组排序和流量控制功能的协议层是©。
A.数据链路层 B.网络层 C.传输层 D.会话层


在OSI参考模型中，(B)在物理线路上提供可靠的数据传输服务。


​    A.物理层 B.数据链路层 C.网络层 D.传输层
【解析】本题非常容易误选D，一定要学会区分
简单理解:物理链路之上是数据链路层，OSI模型的数据链路层有很多可靠性保障机制。
深入理解:数据链路层与传输层的区别,比如有3个节点是1-2-3,1到3是端到端，可靠性通过传输层协议保障，1到2或者2到3是物理链路，可靠性通过数据链路层保障，那么这题明显问的是物理链路上的可靠性。

数据封装与解封过程分析
数据封装（从上往下）

数据解封（从下往上）

例子：

例题

在ISO OSI/RM中，(1)实现数据压缩功能。在OSI参考模型中，数据链路层处理的数据单位是(2)。

(1)A.应用层 B.表示层 C.会话层 D.网络层
(2)A.比特 B.帧 C.分组  D.报文
答案：B;B


在OSI参考模型中，传输层上传输的数据单位是(D)。
A.比特  B.帧  C.分组  D.报文



数据通信基础
信道特性
概念

通信的目的就是传递信息。
通信中产生和发送信息的一端叫信源，接收信息的一端叫信宿，信源和信宿之间的通信线路称为信道。
信道带宽

码元和码元速率

码元（N或信号状态）:一个数字脉冲称为一个码元(可理解为时钟周期的信号)
码元速率:单位时间内信道传送的码元个数。如果码元宽度(脉冲周期)为T，则码元速率(波特率)为B=1/T，单位是波特 Baud。
一个码元携带信息量n(位)与码元种类数(N)的关系 n=log2N。
奈奎斯特定理
要理解，慢慢理解很重要！
奈奎斯特定理:在一个理想的(没有噪声环境)信道中，若信道带宽为W，最大码元速率为B=2W(Baud)。
极限数据速率为R=Blog2N=2Wlog2N。(N表示码元种类数)
方便理解示意：（以下码元用包裹表示，以下都用简单的语言描述）
码元速率（B）= 每秒发送包裹个数,概念要记住（码元速率:单位时间内信道传送的码元个数）
数据速率 = 每秒发送包裹重量
⬇️
每秒发送包裹重量® = 每秒发送包裹个数(B) * 每个包裹重量(n)
⬇️
数据速率® = 码元速率(B) * 每个码元携带信息量(n)
下面针对上面的图解：

香农定理
香农定理:在一个噪声信道的极限数据速率和带宽之间的关系。    C(也叫信道容量)
极限数据速率公式为:C=Wlog2(1+S/N)
分贝与信噪比关系:dB=10log10S/N
其中W为带宽，S为信号平均功率，N为噪声平均功率，S/N为信噪比。
考试一般不会直接给你信噪比，会给你dB
例题：
dB=10，S/N=10
dB=30，S/N=1000  （书上的例题）
带宽/码元速率/数据速率关系梳理

误码率

例题：





信道延迟
本节注意：这里做题不可套用前面提到的奈奎斯特定理和香浓定理等

信道延迟的计算

例题：







卫星信道延迟



传输介质
概念
要记住： 无线电波:500kHz-1000Mhz   微波:300MHZ-300GHz


例题：

同轴电缆

通常把表示数字信号的方波所固有的频带称为基带，这种电缆也叫基带同轴电缆，直接传输方波信号称为基带传输。
宽带同轴电缆:用于传输模拟信号
宽带系统的优点是传输距离远，可达几十干米，而且可同时提供多个信道。然而和基带系统相比，它的技术更复杂，需要专门的射频技术人员安装和维护，宽带系统的接口设备也更昂贵
模拟信号比数字脉冲受噪声和衰减的影响更小，可以传播更远的距离，甚至达到100km。
双绞线
缠绕目的：降低信号干扰的程度



光纤






跳线和尾纤

例题





数据编码
曼切斯特编码
曼彻斯特编码能从数据信号波形中提取同步信号。

差分曼切斯特编码

两种曼切斯特编号特点
曼码和差分曼码是典型的双相码，双相码要求每一位都有一个电平转换，一高一低，必须翻转。
曼码和差分曼码具有自定时和检测错误的功能。
两种曼彻斯特编码优点:将时钟和数据包含在信号数据流中，也称自同步码。
编码效率低:编码效率都是50%
两种曼码数据速率是码元速率的一半，当数据传输速率为100Mbps时，码元速率为200M baud。
这里的高亮句子很重要
其他编码
没特殊说明。只要看见以太网就是10M

例题






数字调制技术










脉冲编码调制PCM
本节注意：这里做题不可套用前面提到的奈奎斯特定理和香浓定理等

这里有个小知识点，为什么语音最高频率是4kHz，所以取样频率是8kHz,前面有提到:
根据奈奎斯特采样定理，为了无失真地重建一个连续时间信号，采样频率(8K)必须至少是信号最高频率(4K)的两倍。这个最低的采样频率被称为奈奎斯特频率（8K）。


为什么这里不用前面提到的奈奎斯特定理和香农定理,因为这两个定理算的是极限值,而这里不是算极限值的哇


通信和交换方式
数据通信方式

按通信方向分




通信方式
消息传递方式
应用实例




单工通信
单向一条信道
无线电广播、电视广播


半双工通信
双向一天信道
对讲机


全双工通信
两条信道
实时电话通信



单工通信:信息只能在一个方向传送，发送方不能接收，接收方不能发送(电视/广播)
半双工通信:通信的双方可以交替发送和接收信息，但不能同时接收或发送(对讲机/Wi-Fi/Hub)
全双工通信:通信双方可同时进行双向的信息传送(电话/交换机)

按同步方式分





数据交换方式

关键词：电路交换，报文交换，分组交换，数据报，虚电路





多路复用技术
多路复用技术
多路复用技术是把多个低速信道组合成一个高速信道的技术
光纤入户:上网、电视、电话。

这种技术要用到两个设备：

频分复用（FDM）
在一条传输介质上使用多个频率不同的模拟载波信号进行多路传输，通过不同的频率来区分不同的业务

时分复用（TDM）
以信道的传输时间作为分割对象(划分时间片)来实现的复用，并且可以根据时间片是否固定分配，分为同步时分复用技术和统计时分复用技术。
同步时分复用

统计时分复用

E1和T1
同步时分复用技术的应用（下面有考点）

波分复用（WDM）





同步数字系列
​             SONET/SDH多路复用的速率（时分复用）

考点
考法1：

考法2：



名称
技术原理
载波速率
子信道
传送一帧的时间
每秒钟传多少个帧
子信道数据速率




E1载波
同步时分复用
2.048Mbps
32个
125μs
8000b/s（1b/125μs=8000）
64kbps (8bit*8000)


T1载波
同步时分复用
1.544Mbps
24个
125μs
8000b/s（1b/125μs=8000）
56kbps (7bit*8000)



例题





海明码纠错码
差错控制

奇偶校验



海明码


海明码原理

海明码编码⭐️


例题





CRC循环冗余校验码
末尾加入CRC循环冗余校验码能检错不能纠错，广泛用于网络通信和磁盘存储。
以下我们采用例题来了解：
1.采用CRC进行差错校验，生成多项式为G(X)=X4+X+1，信息码字为10111，则计算出CRC校验码是()。
A.0000 B.0100 C.0010 D.1100
解：
1.判断校验位数:生成多项式的最高次方是几，校验位就是几位。4位检验位
因为G(X)=X4+X+1 中 x4有最高次方是4，那么到时候算出来的CRC校验码也会是四位
一般选项会出现不固定数字的校验码，这时候你就要看，最高次方，排除不符合的选项
2.组合被除数  信息码字+检验位   ==101110000
第一步看出有多少位校验位就在信息码字后补多少个0
3.寻找除数==提取多项式的系数

4.用第二步的结果，除以第三步的结果(异或运算) 余数就是CRC校验码，余数不够位，前面补0


例题



章节总结


局域网
局域网技术概论
结构

早期组网使用集线器组网，现在都是交换机，现在真正应用广泛的是树形和混合型

传统局域网是分组广播式网络，这是与分组交换式的广域网的主要区别
CSMA/CD
CSMA基本原理:发送数据之前，先监听信道上是否有人在发送。若有，说明信道正忙，否则说明信道是空闲的，然后根据预定的策略决定:
(1)若信道空闲，是否立即发送。
(2)若信道忙，是否继续监听
如果连续发生16次碰撞后，认为网络繁忙或故障，不再尝试发送。
CSMA三种监听算法

(1)非坚持型监听算法:后退随机时间由于随机时延后退，从而减少了冲突的概率。问题是因为后退而使信道闲置一段时间，这使信道的利用率降低，而且增加了发送时延
(21-坚持型监听算法:继续监听，不等待有利于抢占信道，减少信道空闲时间。但是，多个站同时都在监听信道时必然会发生冲突。冲突概率和利用率都高(双高)
(3)P-坚持型监听算法:若信道空闲，以概率P发送，以概率(1-P)延迟一个时间单位，P大小可调整。


冲突检测原理CD
载波监听只能减小冲突的概率,不能完全避免冲突。当两个帧发生冲突后，若继续发送，将会浪费网络带宽。为了改进带宽利用率，发送站应采取边发边听的冲突检测方法，即
(1)发送期间同时接收，并把接收的数据与站中存储的数据进行比较。
(2)若比较结果一致，说明没有冲突，重复(1)。
(3)若比较结果不一致，说明发生了冲突，立即停止发送,并发送一个简短的干扰信号(Jamming)使所有站都停止发送。
(4)发送Jamming信号后，等待一段随机长的时间，重新监听，再试着发送。



对总线型、星型和树型拓扑访问控制协议是CSMA/CD(Carrier Sense Multiple Access/ColisionDetection，载波侦听多路访问/冲突检测)


带冲突检测的监听算法把浪费带宽的时间减少到检测冲突的时间。







二进制指数退避算法

例题：






最小帧长计算





以太网帧
以太网硬件地址（MAC地址，不是帧哦）的位数是48位。例如：00:1A:2B:3C:4D:5E
前面7+1字节用于时钟同步，不算入帧长。
数据46-1500字节，不够至少填充到46字节。
校验位4字节，CRC循环冗余校验 32位。
最小帧长64字节:6+6+2+46+4=64    （从目的地址开始往后算）
最大帧长1518字节:6+6+2+1500+4=1518.   （从目的地址开始往后算）

考法：
1.简单直白的问你，最大帧长和最小帧长是多少
2.问你确认帧是多少，最小的64字节，最大1518字节
3.以太网最大利用率=1500/1518   最小利用率=46/64   就传输的数据除以最大的帧长
以太网报文封装

1.以太网报头是

2.在MTU里，最小字节是20+20+6字节，因为数据段（MTU段）是最小是46字节
扩展知识：
IP报头和TCP报头各自最大的字节都是60字节，标准是20字节
例题：






以太网物理层规范
以下所有的表格都要记住
802.3以太网(10M)

快速以太网802.3u(100M)



干兆以太网(1000M)

编码总结
关键字：8B/10B,4D-PAM5,64B,66B,MLT-3,8B/6T





万兆以太网802.3ae(10G)




VLAN
Vlan基础
虚拟局域网(Virtual Local Area Netwok，VLAN)
根据管理功能、组织机构或应用类型对交换局域网进行分段而形成的逻辑网络。
不同VLAN通信必须经过三层设备:路由器(考点最多)、三层交换机、防火墙等。

冲突域和广播域:
一个中继器和集线器是一个冲突域
网桥/交换机的一个接口为一个冲突域
一个VLAN为一个广播域，交换机默认所有接口都在VLAN 1
路由一个接口就是广播域

冲突域

广播域

例题：

交换机VLAN划分
静态划分VLAN:基于交换机端口。
动态划分VLAN:基于MAC地址、基于策略、基于网络层协议、基于网络层地址。

vlan划分配置
简单了解一下，后面还会讲

VLAN作用
(1)控制网络流量。一个VLAN 内部的通信(包括广播通信)不会转发到其他VLAN 中去，从而有助于控制广播风暴，减小冲突域，提高网络带宽的利用率。
(2)提高网络的安全性。可以通过配置VLAN 之间的路由来提供广播过滤、安全和流量控制等功能。
不同VLAN 之间的通信受到限制，提高了企业网络的安全性。
(3)灵活的网络管理。VLAN 机制使得工作组可以突破地理位置的限制而根据管理功能来划分。如果根据MAC 地址划分VLAN，用户可以在任何地方接入交换网络，实现移动办公。
802.1Q标签
802.1Q 标签字段，重点掌握PRI和VID。
PRI(3 位):Priority表示优先级，提供0~7共8个优先级，当有多个帧等待发送时，按优先级顺序发送数据包。
VID(12 位):即VLAN 标识符，最多可以表示212=4096个VLAN，其中VID0用于识别优先级,VID 4095 保留未用，所以最多可配置4094个VLAN（1-4094）。默认管理VLAN是1,不能删除。
交换机添加和删除VLAN标签的过程由专用硬件自动实现，处理速度很快，不会引入太大的延迟。
从用户角度看，数据源产生标准的以太帧，目标接收的也是标准的以太帧，VLAN标记对用户是透明的。
以下是在以太网帧的基础上加了一个标签段
最小帧长54字节

交换机端口类型
Access接口:只能传送单个VLAN数据，一般用于连接PC/摄像头等终端。
Trunk接口:能传送多个VLAN数据，一般用于交换机之间互联。
Hybrid接口:混合接口，包含access和trunk属性。
QinQ:双层标签，一般用于运营商城域网。

例题




生成树协议STP
生成树背景


二层环路问题-广播风暴

网络中若存在二层环路，一旦出现广播数据帧，这些数据帧将被交换机不断泛洪，造成广播风暴。
广播风暴对网络危害非常大，将严重消耗设备CPU资源及网络带宽，需要格外注意。
广播风暴现象:网络慢、接口指示灯高速闪烁、CPU使用率高、CLI卡顿、远程管理卡或登录不上
例题

二层环路问题-MAC表震荡


STP基本概念
采用生成树(Spanning-tree)技术，能够在网络中存在二层环路时，通过逻辑阻塞(Block)特定端口，从而打破环路，并且在网络出现拓扑变更时及时收敛，保障网络余性。

当出现网络故障时
在网络出现拓扑变更时及时收敛，保障网络冗余性

网桥ID(Bridge ID)
桥ID一共8个字节，由2个字节优先级和6个字节的MAC地址构成。(ip地址32位是4个字节，1字节=8bit)
桥优先级默认为32768，可以手工修改。
MAC地址为交换机背板MAC.

路径开销(Path Cost)
路径开销是一个端口量，是STP/RSTP协议用于选择链路的参考值。
端口路径开销的默认值及取值范围由选定的路径开销算法决定，路径开销与端口带宽成反比。
华为设备路径开销标准有:802.1d-1998、802.1t及私有的legacy，默认为802.1t标准。

STP选举操作⭐️
1.确定一个根桥(Root Bridge)【选优先级和MAC地址最小的网桥】
2.确定其他网桥的根端口(Root Port)【非根桥的端口到根桥最近的端口】
3.每个段选择一个指定端口(Designated Port)【先选指定桥，指定桥上为指定端口】
4.选出非指定端口(NonDesignated Port)

【选举第一步】
1.确定一个根桥(Root Bridge)【选优先级和MAC地址最小的网桥】
首先比较优先级，其次再是mac地址，两者都是越小越优先
图中优先级都一样，但是sw1的mac地址最小，所以他是根桥
【BPDU】:是运行STP的交换机之间交换的信息帧。 BPDU内包含了STP所需的路径和优先权信息，STP便利用这些信息来确定根桥以及到根桥的路径。

【选举第二步】
2.确定其他网桥的根端口(Root Port)【非根桥的端口到根桥最近的端口】

【选举第三步】
3.每个段选择一个指定端口(Designated Port)【先选指定桥，指定桥上为指定端口】

【选举第四步】
4.选出非指定端口(NonDesignated Port)

STP案例分析



几种生成树协议
生成树协议     802.1d   STP(慢，拓扑收敛需要30-50s)
快速生成树协议 802.1W   RSTP(快，6s内完成收敛)
多生成树协议   802.1    SMSTP(实现多个VLAN负载均衡)
例题









城域网基础
(1)E-LAN技术是802.1Q的VLAN帧标记，双层标记，打了两层VLAN标签，这种技术被定义为IEEE802.1ad，也称为QinQ技术。
QinQ实际是把用户VLAN嵌套在运营商城域以太网VLAN中传送
(2)IEEE802.1ah，也称为PBB，也叫MAC-IN-MAC技术。



章节总结

无线通信网
移动通讯与4G/5G
介绍

中国移动、中国联通、中国电信的移动通信制式
TD-LTE-Aavanced是我国自主知识产权的4G标准

移动通信技术标准

4G标准

4G关键技术
4G关键技术包括:OFDMA(Orthogonal Frequency Division Multiple Access)、MIMO (MultipleInput Multiple Output)、软件无线电(Software Defined Radio,SDR)技术、VoIP (Voice overInternet Protocol)技术等

OFDMA

MIMO

软件无线电SDR

Voip

5G
5G应用场景

5G两种组网模式（NSA/SA）

5G关键技术
5G关键技术包括:超密集异构无线网络、大规模输入输出(MIMO)、毫米波通信、软件定义网络和网络功能虚拟化。
超密集异构无线网络:相对于4G网络，5G使用较高的频谱，覆盖范围相对较小，需要密集部署宏基站、微基站和室分等不同架构的网络满足覆盖需求
大规模输入输出(MIMO):5G沿用了4G网络的多进多出技术(MultipleInput MultipleOutput,MIMO)，能有效提升网络带宽。
毫米波通信:毫米波小基站可以增强高速环境下用户的网络体验，提升网络的组网灵活性。
SDN和NFV:SDN技术实现控制层面和数据层面分离，提升网络灵活性、可管理性和扩展性。NFV技术可以实现软件和硬件解耦，比如传统网络需要购买防火墙、入侵检测、防病毒等硬件安全设备NFV实现网络功能虚拟化后，只需要购买标准服务器，然后虚拟出多台虚拟机，可以在虚拟机上运行软件的虚拟防火墙(vFW)、虚拟机入侵检测(VIPS)和虚拟防病毒(VAV)，从而大幅降低网络的建设和维护成本，
例题







CDMA计算
原理
CDMA系统为不同用户分配码片，根据计算正交值来判断是否接收数据和接收的数据是多少
正交结果为1，表示发送数据1
正交结果为-1，表示发送数据为0
正交结果为0，表示未向该终端发送数据。


WLAN通信技术
无线网主要使用三种通信技术:红外线、扩展频谱(扩频)和窄带微波技术。
扩展频谱通信:将信号散步到更宽的带宽上以减少发送阻塞和干扰的机会，有利于保密。
扩频技术就是用伪随机序列对代表数据的模拟信号进行调制
WLAN主要使用扩展频谱技术:频率跳动扩频FHSS(蓝牙)和直接序列扩展频谱DSSS(Wi-Fi)。

直接序列扩展频谱DSSS：信号源中的每一位用称为码片的N个位来传输。
频率跳动扩频FHSS是在传输过程中反复转换频率

例题



WLAN频谱与信道
wlan网络分类
WLAN网络可以分为三类:基础无线网络、Ad Hoc网络和分布式无线系统。
基础无线网络(Infrastructure Networking):用户通过无线接入点AP接入
特殊网络(Ad Hoc Networking):用于军用自组网或寝室局域网联机游戏。
分布式无线系统:通过AC控制大量AP组成的无线网络。

ISM频段

不重叠信道🌟

世界分布的2.4g和5g频道


信道重用与AP部署




802.11技术标准对比


🌟
1.Wi-Fi标准的工作频段。
2.4GHZ:802.11、802.11b和802.119
5GHz:802.11a和802.11ac
2.4GHZ+5GHz:802.11n和802.11ax
2.非重看信道数量。2.4GHz频段包含13个信道，有3个不重合信道，常用信道为1、6和11，不重合信道间隔5个信道。
3.不同802.11标准的最大速率。比如，802.11n最大支持600Mbps，802.11ax速率可达9600Mbps。
WIFI7(802.11be)


例题












802.11 MAC层
802.11 MAC子层定义了3种访问控制机制:CSMA/CA(支持竞争访问)，RTS/CTS和点协调功能(支持无竞争访问)。
CSMA/CA核心原理:发送数据前先检测信道是否使用，若信道空闲，则等待一段随机时间后，发送数据。所有终端都遵守这个规则，故这个算法对参与竞争的终端是公平的，按先来先服务的顺序获得发送机会
RTS/CTS信道预约:发送前先打报告，其他终端记录信道占用时间。
PCF点协调功能:由AP集中轮询所有终端，将发送权限轮流交给各个终端，类似令牌，拿到令牌的终端可以发送数据，没有令牌的终端则等待。点协调功能比DCF分布式协调优先级更高。(无争用)

三种帧间间隔
DIFS(分布式协调IFS):最长的IFS，优先级最低，用于异步帧竞争访问的时延
PIFS(点协调IFS):中等长度的IFS，优先级居中，在PCF操作中使用。
SIFS(短IFS):最短的IFS，优先级最高,用于需要立即响应的操作(确认ACK)
帧间间隔用途

例题




移动AD Hoc网络
802.11定义AD Hoc网络是由无线移动节点组成的对等网，无需网络基础设施的支持，每个节点既是主机，又是路由器，是一种MANNET(Mobile Ad Hoc Network)网络。
Ad Hoc是拉丁语，具有”即兴，临时”的意思。

MANET网络特点（选择题）
网络拓扑结构动态变化的，不能使用传统路由协议（RIP,OSPF,BGP）。
无线信道提供的带宽较小，信号衰落和噪声干扰的影响却很大。
无线终端携带的电源能量有限。
容易招致网络窃听、欺骗、拒绝服务等恶意攻击的威胁。
MANET路由协议

例题




WLAN安全技术
wlan安全机制

wep和wpa扩展

例题







无线个人网
蓝牙和zigbee技术

Zigbee设备

Zigbee底层技术

例题




网络互联

广域网技术
公共交换电话网

X.25

帧中继FR

例题

ISDN和ATM

例题




ipv4
ip报文格式
IHL字段是一个4位的字段，它的单位是4字节（或者说32位）。IHL字段的值表示的是IPv4头部的长度，以4字节为单位。这意味着：

最小值：IHL字段的最小值是5，这意味着IPv4头部的最小长度是5个4字节，即20字节。
最大值：IHL字段的最大值是15，这意味着IPv4头部的最大长度是15个4字节，即60字节。

版本号:0100
分片偏移是8字节的倍数，在下面讲ip分片的时候会用到
IP负载也就是除开报头的数据部分最大为65535-20=65515






ip分片与计算
关键词：偏移，MF，片偏移
IP报文最大65535字节，而以太网MTU为1500字节
相当于货轮能载重65535，而火车载重1500，那么必须把货轮上的货物分装给多个火车运输。

例题

类比解析
原题：一个IP数据报文长度为3000字节(包括首部长度)，要经过一个MTU为1500字节的网络传输。此时需将原始数据报切分为3片进行传输，请将每个数据报分片的总长度、数据长度、MF标志和片偏移填入答题纸对应表格中。
类比翻译:一个大型集装箱，货物+集装箱=3000斤,现在要用小型集装箱进行分装,每个小型集装箱自身重量+货物重量=1500斤,已知大型集装箱和小型集装箱重量都是20斤,问:可以用几个小型集装箱进行分装?每个小集装箱的货物分别是多少斤?

例题







IP分类与特殊ip地址
关键词：A类、B类、C类、D类、E类

特殊ipv4地址


0.0.0.0
主机端:DHCP分配过程中，用0.0.0.0表示本机，比如主机DHCPDiscover广播报文源目地址和端口是0.0.0.0:68-&gt;255.255.255.255:67.
服务器端:0.0.0.0本机所有IPv4地址，如果某主机有两个IP地址，该主机一个服务监听的地址是0.0.0.0，那么通过两个IP地址都能够访问该服务。
路由:0.0.0.0表示默认路由，即当路由表中没有找到完全匹配路由的时候所对应的路由。


255.255.255.255
受限广播地址，表示3层广播的目标地址，在同一个广播域范围内所有主机都会接收这个包，广播域的范围可变，跟子网划分相关。


169.254.0.0/16
使用DHCP自动获取IP地址，当DHCP服务器发生故障，或响应时间超时系统会为你分配这样一个地址，不能正常上网。


127.0.0.0/8(127.0.0.1-127.255.255.255)
本地环回地址，能ping通127地址，证明TCP/IP协议栈正常。


RFC1918私有IP地址IPv4地址空间中有一部分特殊的地址，成为私有IP地址，私有IP地址不能直接访问公网(Internet)的IP，只能在本地使用，
A类:10.0.0.0/8(10.0.0.1-10.255.255.255)1个A类网络
B类:172.16.0.0/12(172.16.0.1-172.31.255.255)16个B类网络
C类:192.168.0.0/16(192.168.0.1-192.168.255.255)256个C


常见组播


224.0.0.1 所有主机
224.0.0.2 所有路由器
224.0.0.5 所有运行OSPF的路由器
224.0.0.6 DR和BDR的组播接收地址
224.0.0.9 RIPv2组播更新地址
224.0.0.18 VRRP组播地址
例题






ipv6
IPV6改进

IPV6报文格式

版本（4位）：用0110表示IPv6。
通信类型/流量等级（8位）：用于区分不同的IP分组，相当于IPv4中服务类型字段。
流标签（20位）：标识某些需要特别处理的分组。
负载长度（16位）：表示除了IPv6固定头部40个字节之外的负载长度，扩展头包含在负载长度之中。
下一头部（8位）：指明下一个头部类型，可能是IPv6扩展头部或高层协议(tcp,udp)的头部。
跳数限制（8位）：用于检测路由循环，类似IPV4的TTL。
源地址（128位）：发送节点的地址。
目标地址（128位）：接收节点的地址。
IPV6最大负载长度=固定40字节加+65535=65575
IPV6扩展头部



下一头部编号
下一头部类型
解释




0
Hop-by-Hop Options Header
逐跳选项：这些信息由沿途各个路由器处理


6
TCP (Upper Layer)
该IPv6报文的上层封装是TCP


17
UDP (Upper Layer)
该IPv6报文的上层封装是UDP


43
Routing Header
路由选择头：给出一个路由器地址列表，类似于IPv4的松散源路由和路由记录


44
Fragment Header
分段：处理数据报的分段问题


50
Encapsulating Security Payload
ESP：封装安全载荷，跟IPSec类似


51
Authentication Header
AH：认证头，跟IPSec类似


60
Destination Options
目标选项：选项中的信息由目标节点检查处理



IPV6相关协议
关键词：RIPng、OSPFv3、BGP4+

例题






IPV6地址
IPV6地址基础
IPv6地址128位，采用冒号分隔的十六进制数(8组)表示。
例如:8000:0000:0000:0000:0123:4567:89AB:CDEF
​      ①   ②   ③   ④    ⑤   ⑥   ⑦   ⑧
书写规则：
每个字段前面的0可以省去，例如0123可以简写为123.
一个或多个全0字段，可以用一对冒号“::”代替
有效0位不可以简写，双冒号只能出现一次。
以上地址可简写为 8000::123:4567:89AB:CDEF
IPv4兼容地址可以写为 ∷192.168.10.1
ipv6地址书写
合法写法

12AB:0000:0000:CD30:0000:0000:0000:0000/60
12AB::CD30:0:0:0:0/60
12AB:0:0:CD30::/60

非法写法

12AB:0:0:CD3/60（在16位的字段中可以省掉前面的0，但不能省掉后面的0）（省略了后面的0，缺项了以及CD30的最后一个0）
12AB::CD3/60（这种表示可展开为12AB:0000:0000:0000:0000:0000:0000:0CD3）
12AB::CD30/60（这种表示可展开为12AB:0000:0000:0000:0000:0000:0000:CD30）

IPV6地址分类
单播地址

可聚合全球单播地址：这种地址在全球范围内有效，相当于IPv4公用地址（前缀为001）。
链路本地地址：用于同一链路的相邻节点间的通信（前缀为1111111010），结合MAC地址自动生成。
站点本地地址：相当于IPv4中的私网地址（前缀为1111111011）。
助记：1聚2链3站（1[01]聚2[10]恋3[11]占有）最后两位二进制

组播地址

IPv6中没有广播地址，广播功能被组播代替。
IPv6组播地址的格式前缀为1111 1111，即FF00开头。

任意播地址

表示一组接口的标识符，通常是路由距离最近的接口。
任意播地址不能用作源地址，而只能作为目标地址。
任意播地址不能指定给IPv6主机，只能指定给IPv6路由器。

特殊地址对比（IPV4  VS  IPV6）



IPv4地址
IPv6地址




点分十进制表示
带冒号的十六进制表示，0可以压缩


分为A、B、C、D、E  5类
不分类


组播地址224.0.0.0/4
组播地址FF00::/8


广播地址（主机部分全为1）
任意播（限于子网内部）


默认地址0.0.0.0
不确定地址::


回环地址127.0.0.1
回环地址::1


公共地址
可聚合全球单播地址 FP(前缀)=001


私有地址10.0.0.0/8127.16.0.0/12,192.168.0.0/16
站点本地地址 FEC0::/48


自动专用IP地址169.254.0.0/16
链路本地地址 FE80::/48


-
☆6to4隧道地址 2002::/16



例题：








过度技术
双栈技术:同时运行IPv4和IPv6。
隧道技术:解决IPv6节点之间通过IPv4网络进行通信。
翻译技术:解决纯IPv6节点与纯IPv4节点之间通进行通信。
隧道技术

地址翻译技术
NAT-PT(Network Address Trannslation-Protocol Translator )
实现纯IPv6节点与纯IPv4节点间的通信。
静态NAT-PT 1:1
动态NAT-PT M:N
基于端口NAPT-PT M:1



总结
IPv6报文格式:每个字段作用(跳数限制)
IPv6地址:128位，冒号十六进制法
简写规则:每段中无效零位可以省略，连续一段或多段零可以用::代替，但::只能出现一次。
地址分类:单播/组播/任意播，三类单播:1聚2恋3占有:
过渡技术
隧道技术:解决IPv6节点之间通过IPv4网络进行通信。
双栈技术:同时运行IPv4和IPv6。
翻译技术:解决纯IPv6节点与纯IPv4节点之间通进行通信
ARP与RARP协议
ARP（Address Resolution Protocol，地址解析协议）是将IP地址解析为以太网MAC地址（物理地址）的协议。
RARP分组的格式与ARP分组基本一致。RARP为逆地址解析协议，作用与ARP相反，用于将MAC地址转换为IP地址。










以下是详细解释
上图展示主机A进行arp请求主机B的过程
以下涉及图片来源：【ARP地址解析协议】CSDN博客

1）主机A首先会查询自身的ARP缓存表，是否存在目标ARP缓存条目。
有缓存条目则直接通信。无，则需发送ARP请求报文，来获取对方的mac地址。以广播的形式发送。

二层头部中，目标mac地址全F填充（FF-FF-FF-FF-FF），在该广播域范围内所有主机都会收到该ARP请求报文。（以便于正确的主机能收到该报文）
​    三层头部中，目标mac地址未知，为了保证报文的完整性，全0填充（00-00-00-00-00）。（告诉对方pc1，未知你的mac地址）


2）由于主机A已经将自己的源IP、源mac地址发送给主机B，主机B收到后，把主机A的IP、MAC对应信息填入自己的ARP缓存中(ARP学习)，会将自身ip、自身mac填充，以单播的形式响应（ARP REPLY）。


3）当主机A收到响应后，剥离二层头部，将三层头部中的源IP地址、源Mac地址的映射关系，记录在自己的ARP缓存表中。（此时，双方都有对方的ARP条目，可以正常通信了。）

为了防止ARP缓存表项过大，每个ARP条目隐藏运行一个1200s的定时器，如果在此时间内收到该条易的报产则刷新该表项，如果没收到，则清空该条目
ARP缓存表的条目不是一直缓存在里面的，会隔一段时间进行刷新，或者将长时间不进行通信的条目清除掉。
免费arp

ARP代理

总结：

例题







ICMP和ICMPv6
ICMP
ICMP(Internet Control Message Protocol,Internet控制报文协议),协议号为1,封装在IP报文中，用来传递差错、控制、查询等信息，典型应用ping/tracert依赖ICMP报文。
ICMP报文类型与代码

ICMP应用-ping

Echo Request和Echo Reply分别用来查询和响应某些信息，进行差错检测。
ICMP应用-tracert
tracert（traceroute）是一个常用的网络工具，用于追踪分析数据包在网络中传输时经过的路径，并输出到达目标地址的延迟情况和节点信息。

例题









ICMPv6
ICMPv6报文分为两类:差错报文和信息报文

ICMPv6差错报文应用-Path MTU发现

ICMPV6信息报文应用-Ping

NDP
IPv6邻居发现协议(Neighbor Discovery Protocol,简称NDP或ND)定义了5种类型的信息，包括路由器宣告、路由器请求、路由重定向、邻居请求和邻居宣告，具体如下功能:
路由器发现:发现链路上的路由器，获得路由器通告的信息。
无状态自动配置:通过路由器通告的地址前缀，终端自动生成IPv6地址。
重复地址检测:获得地址后，进行地址重复检测，确保地址不存在冲突。
地址解析:请求目的网络地址对应的数据链路层地址，类似IPv4的ARP
邻居状态跟踪:通过NDP发现链路上的邻居并跟踪邻居状态。
前缀重编址:路由器对所通告的地址前缀进行灵活设置，实现网络重编址。
重定向:告知其他设备，到达目标网络的更优下一跳。
NDP报文类型及功能

IPV6地址自动配置的分类
无状态地址自动配置、有状态地址自动配置


例题：

IP组播技术与MPLS
组播技术

组播网络架构

例题：

MPLS

例题：


TCP和UDP协议
TCP和UDP介绍
传输层主要两个传输协议，分别是TCP和UDP
(1)TCP是面向连接的，一般用于传输数据量比较少，且对可靠性要求高的应用。[文件]
(2)UDP是一种不可靠的、无连接的协议。用于传输数据量大，可靠性要求不高但要求速度快的场景【音视频】


例题：


TCP报文格式
TCP头部最大可以扩展到60字节



例题：



TCP伪首部
TCP伪首部本质是IP头的一部分，包含源目IP地址，协议号、TCP报头和用户数据，主要用于TCP校验 和计算。

UDP报文格式
UDP的最大负载也就是除开报头的数据（65527B），报文总长度16位

与TCP相比，做了很大精简，省略诸多控制字段。
例题





TCP三次握手和四次挥手
ACK=0表示第一次初始化报文


tcp四次挥手断开连接

例题







TCP序列号及确认号（控制报文）

TCP序列号及确认号（数据报文）

例题



流量控制和拥塞控制
流量控制:为了防止发送方发送速度过快,导致接收方处理不过来，造成丢包重传，浪费网络资源。
TCP流量控制机制:可变大小的滑动窗口

TCP滑动窗口机制

5分钟了解滑动窗口协议-[iiiiiF呀]
这个博主讲的很细，非常ok
TCP拥塞控制
有了流量控制，可以调节发送端和接收端的节奏，为什么还要有拥塞控制?
流量控制:在AB两个端点进行。
拥塞控制:在AB和所有网络节点中进行。


快重传快恢复


例题：









重点协议端口号总结

源端口随机分配，目标端口使用知名端口。
应用客户端使用的源端口一般为系统中未使用的且大于1024
目的端口号为服务器端应用服务器的进程，如telnet为23
主流网络协议魔力图


例题








路由协议
路由表
什么是路由
当路由器(或其他三层设备)收到一个IP数据包时，会查看数据包的IP头部中的目的IP地址，并在路由表中进行查找，在匹配到最优的路由后，将数据包扔给该路由所指出接口或者下一跳。

路由器工作原理
建立并维护路由表RIB.
直连路由:路由器本地接口所在网段，
静态路由:手工配置的路由条目，
动态路由:路由器之间通过动态路由协议学习到的路由
根据路由表进行数据转发
查看路由表




路由协议或路由种类
相应路由的优先级




DIRECT
0


OSPF
10


IS-IS
15


STATIC
60


RIP
100


OSPF AS E
150


OSPF NSSA
150


IBGP
255


EBGP
255



迭代路由

IP路由查找的最长匹配原则


例题：




静态路由和默认路由
众所周知：不同网段之间是无法通信的，10网段想去到R2或R3怎么办呢


静态路由配置
静态路由的配置(关联下一跳IP的方式 ) 90%都会用这个方式来配置
[Router]ip route-static 网络号 掩码 下一跳IP地址
静态路由的配置(关联出接口的方式)
[Router]ip route-static 网络号 掩码 出接囗
静态路由的配置(关联出接口和下一跳IP的方式 )
[Router] ip route-static 网络号 掩码 出接囗 下一跳IP地址
配置示例

子网掩码可以选择255.255.255.0也可以选择24,不仅限于24掩码，就是其他掩码也可以适用

#去往100网段[R1] ip route-static 192.168.100.0 255.255.255.0 192.168.12.2
#去往200网段[R1] ip route-static 192.168.200.0 24 192.168.13.2

默认路由


静态路由与默认路由特点

静态路由
配置简单
手工配置，可控性高
节省网络带宽
网络大，工作量大，比如配置1000条静态路由
网络故障，无法响应拓扑动态变化


默认路由
默认路由是一种特殊的静态路由，走投无路的选择
配置简单，简化管理
降低路由 CPU、内存资源
用处:网络出口路由器/防火墙/核心交换机

等价路由和浮动路由

例题：


路由协议分类
路由协议分类方式:按距离矢量和链路状态分类、按内部网关和外部网关协议
距离矢量路由协议一般基于Bellman-Ford算法，链路状态路由协议基于Dijkstra算法(也叫SPF最短路径优先算法)

例题：



RIP
RIP(Routing Information Protocol,路由信息协议)
内部网关协议，距离矢量路由协议。
华为设备上路由优先级为100。
计算跳数:最大15跳，16跳不可达，一般用于小型网络。
几个时钟:30s周期性更新路由表、180s无更新表示不存在、300s删除路由表。
支持等价负载均衡和链路冗余，使用UDP 520端口。
RIPv2支持自动路由汇总

RIPv1与RIPv2对比★非常重要要记忆

距离矢量路由协议

距离矢量路由协议特点

RIP路由的度量值
RIP以跳数作为度量值，虽然简单，但事实上不科学，如下图


路由的优先级,越小越优先

RIP防环机制 简单理解一下就行，不用背，大厂都删了
①最大跳数:当一个路由条目发送出去会自加1跳，跳数最大为16跳，意味着不可达。
②水平分割:一条路由信息不会发送给信息的来源。
③反向毒化的水平分割:把从邻居学习到的路由信息设为16跳，再发送给那个邻居
④抑制定时器和触发更新也可以防止环路。
例题：




OSPF
OSPF(Open Shortest Path First，开放式最短路径优先协议)是目前应用最广泛的路由协议。
OSPF是一种内部网关协议IGP，也是链路状态路由协议，支持VLSM，通过带宽计算最佳路径，采用Dijkstra算法(也叫SPF最短路径算法)
华为设备OSPF协议优先级Internal10，External150(import-route)
支持在ABR/ASBR手工路由汇总，不支持自动汇总。
OSPF特点

HELLO定时器


DR与BDR的作用




LSA

例题：


OSPF Cost
OSPF使用Cost“开销”作为路由度量值。
OSPF接口cost=100M/接口带宽，其中100M为OSPF参考带宽(reference-bandwidth)，可修改。
每一个激活OSPF的接口都有一个cost值。
一条OSPF路由的cost由该路由从起源 一路到达本地的所有入接口cost值的总和。

OSPF区域概念
所有非骨干区域必须与骨干区域直连。

OSPF路由器角色

区域内路由器IR     Internal Router
区域边界路由器ABR  Area Border Router
骨干路由器BR       Backbone Router
AS边界路由器ASBR   AS Boundary Router
总结
触发式更新、分层路由，支持大型网络。
Area 0.0.0.0或者Area 0来表示骨干区域，不是区域1
点对点网络上每10秒发送一次hello，在NBMA网络每30秒发送一次，Deadtime为hello时间4倍。
OSPF系统内几个特殊组播地址:
224.0.0.1-在本地子网的所有主机。
224.0.0.2-在本地子网的所有路由器,
224.0.0.5-运行OSPF协议的路由器。
224.0.0.6-OSPF指定/备用指定路由器DR/BDR.
224.0.0.9-RIPv2路由器
224.0.0.18 VRRP（虚拟路由器冗余协议）
目标地址224.0.0.5指所有路由器，用于发现建立邻居、还用于选出区域内的指定路由器DR和备份指定路由器BDR(DR/BDR组播地址是224.0.0.6)
例题：








OSPF和RIP的区别
(1)RIP使用距离矢量算法，通过学习其他路由器发送的路由表信息，生成路由表。OSPF首先获取全网的拓扑信息，然后利用SPF最短路径优先(也叫Diikstra)算法，生成路由表。
(2)RIP一般适用于小型网络，OSPF适用于中大型网络。
(3)RIP和OSPF都是动态路由协议，可以根据拓扑变化，更新路由表。RIP配置简单，功能也相对简单，收敛速度慢，容易形成环路。OSPF支持层次化组网、网络优化、等价负载均衡、报文加密等功能。
BGP
BGP(Border Gateway Protocol,边界网关协议)外部网关协议,用于不同自治系统AS之间，寻找最佳路由。

BGP四个报文

BGP选路规则
最重要的时候2,3,4,5,6

丢弃下一跳不可达的路由。
优选Preference_Value最高的路由（私有属性，仅本地有效）。
优选Local_Preference最高的路由。
优选手动聚合 &gt; 自动聚合 &gt; network &gt; import &gt; 从对等体学到的。
优选AS_Path最短的路由。
起源类型IGP &gt; EGP &gt; Incomplete。
对于来自同一AS的路由，优选MED最小的。
优选从EBGP学来的路由（EBGP &gt; IBGP）。
优选AS内部IGP的Metric最小的路由。
优选Cluster_List最短的路由。
优选Originator_ID最小的路由。
优选Router_ID最小的路由器发布的路由。
优选IP地址最小的邻居学来的路由。


ISIS


例题











要分两种情况AA是配ospf之后，DD是配OSPF之前，具体看下面的router-id选举规则





internet应用
远程登入

例题


文件传输协议
关键词：ftp，tftp

例题


电子邮件协议
关键词：pop3，smtp，imap

例题




超文本传输协议HTP

网页访问过程

例题：


P2P应用





网络互联设备
总结

中继器与集线器


集线器工作原理


就是因为会出现对所有接口进行泛洪，这样所有的终端都会收到数据，会造成数据泄露
网桥与交换机

交换机工作原理（交换机寻址）








例题

路由器与三层交换机
路由器

三层交换机


例题


路由器跟三层交换机有什么区别 ?
典型园区网拓扑结构

对比

多层交换机和网关设备
网关是互连网络中操作在OSI传输层之上的设施。
网关的主要功能:
·(1)连接网络层之上执行不同协议的子网，组成异构型的互联网。
·(2)网关能对互不兼容的高层协议进行转换。
·(3)为了实现异构型设备之间的通信，网关要对不同传输层、会话层、表示层、应用层协议进行翻译和转换。


SDN可编程交换机

网络演进总结

子网划分专题VLSM  CIDR
IP地址基础



网络掩码（子网掩码）

三类地址

子网划分VLSM


子网划分原理-网络位向主机位借位(分饼)

考点
考点1:已知子网数量，进行子网划分


考点2:已知子网主机数量，进行子网划分

考点3:掩码转换

例题：



考点4:掩码作用位置与地址块计算




考点5:应用型子网划分













无类域间路由CIDR






IPv6子网划分


要点：

2024年5月案例分析试题一







网络安全
网络安全基础
网络安全威胁类型
(1)窃听:例如搭线窃听、安装通信监视器和读取网上的信息等。
(2)假冒:当一个实体假扮成另一个实体进行网络活动时就发生了假冒。
(3)流量分析:对网上信息流观察和分析推断出网上传输的有用信息。
(4)重放攻击:重复发送一份报文或报文的一部分，以便产生一个被授权效果。
(解决办法：随机数、时间戳)

(5)数据完整性破坏:有意或无意地修改或破坏信息系统，或者在非授权和不能监测的方式下对数据进行
(6)分布式拒绝服务DDoS:当一个授权实体不能获得应有的对网络资源的访问或紧急操作被延迟时，就发生了拒绝服务。DDoS是对传统DoS攻击的发展，黑客控制海量肉鸡发起。SYN-Flooding、HTTP-Flood/CC
(7)恶意软件：恶意软件指任何故意设计会损害计算机或信息系统的文件或程序包括木马病毒、流氓软件、间谍软件、勒索病毒等，这些恶意软件将自己伪装成合法文件从而绕过检测。
(8)web攻击： 包括:跨站脚本(XSS)攻击、SQL注入攻击、跨站域请求伪造(CSRF)攻击、WebShell攻击以及利用软件漏洞进行的攻击。
(9)高级可持续（APT）攻击：APT攻击是多种常见网络攻击手段/技术的组合，通过间接迂回方式，渗透进组织内部系统潜伏起来，持续不断地收集攻击目标相关的各种信息，其潜伏和收集信息时间可能会长达数年，当条件成熟时，伺机而动，达到攻击目的。这类攻击一般是有组织有预谋的，攻击目标一般为国家和政府部门的核心信息系统，一旦对这些系统造成破坏，对国家安全、社会秩序、经济活动会造成非常大的影响。
网络攻击分类
被动攻击：嗅探、窃听、流量分析，最难被检测，重点是预防，主要手段是加密
主动攻击：假冒、重放、数据完整性破坏、分布式拒绝服务DDoS、恶意软件、web攻击、高级可持续（APT）攻击等，重点是检测而不是预防，手段有防火墙、IDS
网络安全防范技术
网络安全措施： 数据加密、数字签名、身份认证、防火墙、特征过滤等。
(1)数据加密。
(2)数字签名：用来验证数据或程序的完整性。
(3)身份认证：认证用户的合法性，例如密码技术、利用人体生理特征(如指纹)进行识别、智能 IC卡、数字证书等。
(4)防火墙：防火墙是位于两个网络之间的屏障，进行访问控制。
(5)入侵检测和阻断：对网络流量或应用访问进行攻击特征匹配和过滤，阻断非法攻击，常见设备有 入侵防护系统 (IPS)、Web  应用防火墙 (WAF)  等。
(6)访问控制：在骨干网络设备或者服务器配置访问控制策略，允许或者拒绝某些源对目标的访问， 实现网络安全防护。
(7)行为审计。对网络行为或者用户操作进行审计，阻断非法操作或者高危操作行为，常见设备：数据库审计系统、堡垒机、上网行为管理系统等。
等级保护




网络信息安全法律与政策

例题






CC攻击日志

DDoS攻击和CC攻击





DDoS攻击VS CC攻击







来源
危害
协议
变种


CC攻击
真实IP
业务故障、或者业务可用  但用户隐私等被窃取
HTTP
较多


DDoS攻击
伪造IP
流量打满，业务彻底不可 用
TCP/UDP/IP
较少






信息加密技术
密码学基本概念
信息安全的核心是密码技术，研究数据加密的科学叫作密码学(Cryptography)。
现代密码理论的一个根本性原则Kerckhoffs原则:密码体制的安全性不依赖于算法的保密，而仅仅依赖于密钥的保密。
不论窃听者获取了多少密文，如果密文中没有足够的信息可以确定出对应的明文，则这种密码体制是无条件安全的，或称为理论上不可破解的。
在无任何限制的条件下，目前几乎所有的密码体制都不是理论上不可破解的。能否破解给定的密码取决于使用的计算资源。
经典加密技术
经典加密技术主要有3种:
(1)替换加密(Substitutkm)。用一个字母替换另一个字母，例如Caesar密码(D替换a, E替换b等)。这种方法保留了明文的顺序，可根据自然语言的统计特性(例如字母出现的频率)破译。
(2)换位加密(Transposition).按照一定的规律重排字母的顺序。(3)一次性填充(One-TimePad)。把明文变为位串(例如用ASCII编码)，选择一个等长的随机位串作为密钥，对二者进行按位异或得到密文。
现代加密技术
现代密码体制使用的基本方法仍然是替换和换位，但是采用更加复杂的加密算法和简单的密钥，而且增加了对付主动攻击的手段。例如加入随机的冗余信息，以防止制造假消息;加入时间控制信息，以防止旧消息重放。
替换和换位可以用简单的电路来实现
私钥密码/对称密码体制
私钥密码又称对称密码，该体制的特点是加密和解密使用相同的密钥。消息的收发双方必须事先通过安全渠道交换密钥。
·优点:加解密速度快、密文紧凑、使用长密钥时的难破解。
·缺点:密钥分配问题、密钥管理问题、无法认证源。
常见的对称密钥加密算法如下:DES、3DES、AES、RC4/5、IDEA。

以下这个表格要背！



算法
解释
特点




DES
Data Encryption Standard，数据加密标准，分组加密算法，采用移位+替换，速度快，密钥易产生。
分组长度64位，密钥长度64位，有效密钥长度是56位


3DES
三重DES（TDEA），使用DES对明文进行“加密-解密-加密”操作。加密：K1加密→K2解密→K3加密解密：K3解密→K2加密→K1解密一般K1和K3是相同的密钥。
密钥长度112位


IDEA
International Data Encryption Algorithm，国际数据加密算法，分组加密算法。设计思想：混合使用来自不同代数群中的运算。
明文和密文分组都是64位，密钥长度为128位，用于PGP


AES
分组加密算法Advanced Encryption Standard，高级加密标准。可以通过硬件实现，速度快，像3DES一样安全。
分组长度128位，支持128，192和256位三种密钥长度


RC4/5
流加密算法，用于WIFI。加密速度快，可达到DES的10倍。
分组和密钥长度都可变



例题



公钥密码/非对称密码
公钥密码又称为非对称加密，就是对数据加密和解密的密钥是不同的。
·优点:密钥分发方便、密钥保管量少、支持数字签名
·缺点:加密速度慢(计算量大，不适合加密大数据)、数据膨胀率高
每个实体有两个密钥:公钥公开，私钥自己保存
·公钥加密，私钥解密，可实现保密通信
·私钥加密，公钥解密，可实现数字签名
常见的非对称加密算法如下:
·RSA:512位(或1024位)密钥，计算量极大，难破解。
·Elgamal、ECC(椭圆曲线算法)、背包算法、Rabin、DH等。

密码分类

例题

混合密码
混合密码:发送方用对称密钥加密需要发送的消息，再用接收方的公钥加密对称密钥，然后一起发送给接收方;接收方先用自己的私钥解密得到对称密钥，然后用对称密钥解密得到明文

国产加密算法（SM）系列
《中华人民共和国密码法》密码分为核心密码、普通密码和商用密码，实行分类管理
核心密码、普通密码用于保护国家秘密信息，属于国家秘密，由密码管理部门依法实行严格统一管理。
商用密码用于保护不属于国家秘密的信息，公民、法人可用。
国产密码算法:是指由国家密码研究相关机构自主研发，具有相关知识产权的商用密码算法，目前已经公布的国产密码算法如下:



算法名称
算法特征描述




SM1
对称加密，分组长度和密钥长度都为128比特


SM2
非对称加密，用于公钥加密算法、密钥交换协议、数字签名算法椭圆曲线问题ECC算法、摘要算法）


SM3
杂凑算法（哈希），分组512位，输出杂凑值长度为256位


SM4
对称加密，分组长度和密钥长度都为128比特


SM9
标识密码算法，支持公钥加密、密钥交换、数字签名等安全功能



2非3哈9标识，14成双对


例题






Hash哈希算法
HASH函数，又称为杂凑函数、散列函数，它能够将任意长度的信息转换成固定长度的哈希值（数字摘要），并且任意不同消息或文件所生成的哈希值是不一样的。
h表示hash函数，则h满足下列条件：
（1）h的输入可以是任意长度的消息或文件M。
（2）h的输出的长度是固定的。
（3）给定h和M，计算h（M）是容易的。
（4）给定h的描述，找两个不同的消息M1和M2，使得h（M1）=h（M2）是计算上不可行的。
哈希函数特性：不可逆性（单向）、无碰撞性、雪崩效应。
常见的Hash算法有：
（1）MD5算法：以512位数据块为单位来处理输入，产生128位的信息摘要。常用于文件校验。
（2）SHA算法：以512位数据块为单位来处理输入，产生160位的哈希值，具有比MD5更强的安全性。
（3）SM3国产算法：消息分组长度为512比特，输出256位摘要。
HASH应用-1.文件完整性校验
重要文件确保无损坏等之后生成hash值1，将文件发送给对方，他自己再生成hash值2两个值如果相等，这发送给对方的文件无损坏

HASH应用-2.账号密码存储

明文存储，无安全防护




用户名
密码




test
123456




哈希存储（Rainbow Table Attack，彩虹表攻击）




用户名
密码




test
e10adc3949ba59abbe56e057f20f883e




（盐+哈希）存储（彩虹表攻击失效）




用户名
盐
密码




test
2026-2-20 20:08:23
d8e423a9d5eb97da9e2d58cd57b92808



HASH应用-3.用户身份认证
增加一个随机数R做哈希 MAC=Hash(密码+R)
需要双方预先知道这个R
MAC:消除中间人攻击，源认证+完整性校验
张三发起认证，服务端检查有张三返回一个随机数R，客户端将自己输入的密码和随机数R合在一起生成hash值（MAC），发回给服务端，然后相同操作，进行对比，如果相同，则对比成功

例题




数字签名

签名方用自己的私钥进行签名，对方收到后，用签名方的公钥进行验证。
数字签名算法（公钥加密算法）：RSA、Rabin、ELGamal签名体制和DSS标准。
数据签名是用于确认发送者身份和消息完整性的一个加密消息摘要，具有如下特点：

（1）数字签名是可信的。
（2）数字签名不可伪造。
（3）数字签名不能重新使用。
（4）签名文件是不能改变的。
（5）数字签名不能抵赖。
（6）接收者能够核实发送者身份。



数字签名过程：常用的签名算法是RSA，采用发送者私钥签名，接收方收到数据后，采用发送者的公钥进行验证。可以直接对明文进行签名，由于明文文件可能很大，这种签名方案效率低。所以也可以先由明文生成Hash(比如MD5生成128位)，再对Hash值进行签名，效率更高。







数字证书与CA
为什么需要数字证书

数字证书类比

PKI体系结构
关键词：pki、ca、RA、证书发布系统、crl库

例题






证书链
如果用户数量很多，通常由多个CA，每个CA为一部分用户发行和签署证书。
如果有两个CA，X1和X2，假设用户A从CA机构X1获得了证书，用户B从X2获得证书，如果两个证书发放机构X1和X2彼此间安全交换了公钥，彼此信任，那么他们的证书可以形成证书链。

A通过一个证书链来获取B的公钥，证书链表示为:X1《X2》X2《B》
B也能通过相反的证书链来获取A的公开密钥:X2《X1》X1《A》





虚拟专用网
虚拟专用网基础
虚拟专用网(Virtual Private Network)

一种建立在公网上的，由某一组织或某一群用户专用的通信网络
二层:L2TP和PPTP(基于PPP)  （数据链路层）
三层:IPSec和GRE    （网络层）
四层:SSL/TLS       （传输层）

其中L2TP和GRE不是加密技术
实现虚拟专用网关键技术
隧道技术(Tuneling)
加解密技术(Encryption&amp;Decryption)
密钥管理技术(Key Management)
身份认证技术(Authentication)
VPN分类：根据应用场景

VPN分类：根据VPN实现层次

二层隧道协议

PPP协议可以在点对点链路上传输多种上层协议的数据包，有校验位。

二层隧道协议有PPTP和L2TP，都是把数据封装在PPP帧中在因特网上传输。
PPP认证方式：PAP和CHAP
PAP:两次握手验证协议，口令以明文传送，被验证方首先发起请求。
CHAP:三次握手，认证过程不传送认证口令，传送HMAC散列值。

例题



IPSec基础
IPSec(IP Security)是IETF定义的一组协议，用于增强IP网络的安全性。
IPSec协议集提供如下安全服务:
数据完整性(Data Integrity )
认证(Autentication )
保密性(Confidentiality)
应用透明安全性(Application-transparent Security)
IPSec原理
IPSec功能分为三类:认证头(AH)、封装安全负荷(ESP)、Internet密钥交换协议(IKE)
``认证头(AH):提供数据完整性和数据源认证，但不提供数据保密服务，实现算法有MD5、SHA.
封装安全负荷(ESP):提供数据加密功能，加密算法有DES、3DES、AES等。
Internet密钥交换协议(IKE):用于生成和分发在ESP和AH中使用的密钥。

IPSec两种封装模式
关键词：原始模式，传输模式，隧道模式

传输模式效率高，隧道模式更安全
GRE虚拟专网

例题











应用层安全协议
SSL/TLS

HTTPS与S-HTTP





SET和PGP




S/MIME
安全多用途互联网邮件扩展协议(Security/Multipurpose Internet Mail Extensions,S/MIME)提供电子邮件安全服务
S/MIME采用MD5生成数字指纹，利用RSA进行数字签名，并采用3DES加密数字签名
不要混淆MIME和S/MIME，MIME不具备安全功能。
kerberos和PKI








防火墙与入侵检测
防火墙
防火墙可以实现内部网络信任网络与外部不可信任网络(Internet)之间或是内部网络不同区域隔离与访问控制。
教材防火墙功能:访问控制、NAT、路由、VLAN、链路聚合、网络监控等，不包含应用层功能。
防火墙模式:路由模式、透明模式和混合模式。

透明/网桥模式：防火墙相当于二层交换机，无需配置IP地址。
路由模式：防火墙具有三层功能，需要配置IP地址，可以做NAT。
混合模式：根据需求，可以同时以透明模式和路由模式工作。


防火墙区域划分
根据网络的安全信任程度和需要保护的对象，人为划分若干安全区域，包括
本地区域(Local):防火墙本身。
信任区域(Trust):内部安全网络，如内部文件服务器、数据库服务器。
非信任区域(Untrust):外部网络，比如互联网。
军事缓冲区域(DMZ):内部网络和外部网络之间的网络，常放置公共服务设备，向外提供信息服务。

例题












入侵检测（IDS）


华为交换机端口镜像配置
将交换机网口GigabitEthernet1/0/2的流量镜像到部署Snort的网口GigabitEthernet1/0/1上。
进入系统模式
system-view 
定义索引号为1的观察端口g1/0/1
observe-port 1 interface gigabitethernet 1/0/1`
进入流量采集接口
interface gigabitethernet 1/0/2 
将g1/0/2入方向的流量镜像到g1/0/1
port-mirroring to observe-port 1 inbound
入侵检测分类
按信息来源分:HIDS、NIDS、DIDS(主机/网络/分布式)。
按响应方式分:实时检测和非实时检测。
按数据分析技术和处理方式分:异常检测、误用检测和混合检测。


异常检测:建立并不断更新和维护系统正常行为的轮廓，定义报警值，超过阈值则报警能够检测从未出现的攻击，但误报率高。


误用检测:对已知的入侵行为特征进行提取，形成入侵式库，匹配则进行报警。已知入侵检测准确率高，对于未知入侵检测准确率低，高度依赖特征库
检测技术:专家系统和模式匹配。


入侵防御系统（IPS）

入侵防御系统IPS vS 入侵检测系统IDS  （案例分析一行2分）
部署位置不同:IPS一般串行部署，IDS一般旁路部署
入侵响应能力不同:IPS能检测入侵，并能阻断，IDS只能检测记录日志，发出警报




网络安全防护系统
Web应用防火墙
Web应用防火墙(Web Application Firewal,WAF)是一种用于HTTP应用的防火墙，工作在应用层可以更深入地检测Web流量，通过匹配Web攻击特征库，发现攻击并阻断。
Web攻击:SQL注入、XSS、反序列化、远程命令执行、文件上传、Webshell。
WAF功能：
(1)Web攻击防护，通过特征匹配阻断SQL注入、跨站脚本攻击、Web扫描等攻击行为。
(2)Web登录攻击防护，包括暴力破解防护、撞库防护、弱口令防护等。
(3)漏洞利用防护，包括反序列化漏洞利用、远程命令执行利用等其他软件漏洞利用攻击防护
(4)Web恶意行为防护，包括恶意注册防护、高频交易防护、薅羊毛行为防护、短信验证码滥刷防护等.
(5)恶意流量防护，包括CC攻击防护、人机识别、TCP Flood攻击防护等
漏洞扫描

统一威胁管理UTM和下一代防火墙NGFW

数据库审计

态势感知

分钟级联动防护-全网协同

华为沙箱-全面领先的未知文件检测机制


运维安全管理与审计系统(堡垒机)

蜜罐

例题






网络操作系统与应用服务器
Windows Server 2016

国产操作系统




UOS Linux网络配置
关键词：ifcfg-ens

关键词：nmcli



关键词：ifconfig

关键词：route

关键词：ip


关键词：netstat





UOS Linux文件和目录管理
文件管理

文件类型

例题：

文件权限


例题：


13个Linux基础命令







UOS Linux用户和组管理



例题：


UOS Linux防火墙配置
防火墙类型

firewalld

iptables的四表五链


语法格式



nftables






例题：







Web应用服务配置
web服务软件






例题：




DHCP
工作原理

DHCP租期更新

DHCP报文格式

Option 43 应用举例

华为DHCP option43配置

DHCP常规配置，为AP分配IP地址。

[DHCP-HW] dhcp enable
[DHCP-HW] ip pool huawei
[DHCP-HW-ip-pool-huawei] network 192.168.100.0 255.255.255.0
[DHCP-HW-ip-pool-huawei] gateway-list 192.168.100.1

配置Option 43，使AP能够获得AC的IP地址，假设AC的IP地址是10.10.10.1。

[DHCP-HW-ip-pool-huawei] option 43 sub-option 3 ascii 10.10.10.1
dhcp server option 43 sub-option 1 hex c0a86401
dhcp server option 43 sub-option 2 ip-address 192.168.100.1
dhcp server option 43 sub-option 3 ascii 192.168.100.1
DHCP分配固定ip地址

DHCP中继（DHCP Relay）

DHCP Relay 工作原理

配置举例

[R1]interface GigabitEthernet0/0/0[R1-GigabitEthernet0/0/0]ip address DHCP-alloc[R1-GigabitEthernet0/0/0]quit[R2]DHCP server group HW[R2-DHCP-server-group-HW]DHCP-server 10.1.1.2[R2-DHCP-server-group-HW]quit[R2]interface GigabitEthernet 0/0/1[R2-GigabitEthernet0/0/1]ip address 10.1.1.124[R2-GigabitEthernet0/0/1]quit[R2]interface GigabitEthernet 0/0/0[R2-GigabitEthernet0/0/0]ip address 192.168.1.124[R2-GigabitEthernet0/0/0]DHCP select relay[R2-GigabitEthernet0/0/0]DHCP relay server-select HW[R2-GigabitEthernet0/0/0]quit
DHCP Snooping防止私接DHCP服务器

DHCP Snooping配置
DHCP Snooping配置
1.开启DHCP功能，所有接口默认为 untrusted
[Huawei] dhcp enable
2.开启DHCP snooping功能
[Huawei] dhcp snooping enable
[Huawei]int GigabitEthernet0/0/1
3.接口下开启DHCP Snooping功能
[Huawei-GigabitEthernet0/0/1]dhcp snooping enable
4.把接口g0/0/1设置为信任接口
[Huawei-GigabitEthernet0/0/1]dhcp snooping trusted
例题：








DNS
DNS作用:把域名转换成IP地址。
DNS/DHCP服务器必须为静态IP地址，而Web/FTP均可为动态IP
Linux系统中提供DNS服务的组件为 bind，主配置文件为named.conf
诊断域名系统基础结构的信息和查看DNS服务器的IP地址命令是:nslookup.
DNS记录类型

DNS查询过程

DNS查询方式
递归查询:域名服务器帮助用户进行名字解析，并返回最后的结果。【老好人】
迭代查询:域名服务器进行迭代访问，反复多次，直到最后找到结果。【踢皮球】

例题





组网技术
交换机基础
交换机分类

其他分类方式


华为横向虎拟化 CSS+iStack
框式交换机堆叠多用于网络核心层或汇聚层，而盒式交换机堆叠一般用于汇聚层或接入层。

多台设备堆叠和集群


堆叠的优势

堆叠的劣势

(1)堆叠技术优点:
①逻辑上变为一台设备，简化网络管理;
②提升系统可靠性，避免单点故障;
③配合链路聚合等技术，防止接口被生成树阻塞，提升链路利用率。
堆叠技术缺点:
①堆叠是私有协议，不支持跨厂商设备堆叠;
②系统升级会造成业务中断;
③多台设备堆叠，只有一个主控处于工作状态，存在资源浪费。
堆叠的三种角色

交换机性能参数


例题





路由器基础
路由器接口
广域网WAN端口和局域网LAN端口。
RJ45端口:常规以太网电口。
以太网光口:GBIC/SFP/SFP+/QSFP+/SFP28.
AUI端口:用于令牌环或总线型以太网接口。
Serial串口:用于连接DDN、帧中继、X.25、PSTN等网络。
ISDN BRI/PRI端口:ISDN线路互联。BRI:144K,PRI:2.048M
SDH POS接口:155M/622M/2.5G/10G。
设备管理方式

例题：



网络管理
网络管理基础
网络管理体系结构
网络管理五大功能域:故障管理、配置管理、计费管理、性能管理和安全管理。
助记:“安配能计障’
故障管理:尽快发现故障，找出故障原因，以便采取补救措施。
网络监控系统体系结构
代理与监视器两种通信方式:轮询和事件报告

轮询时间与管理设备数量


例题：



网络管理协议五大标准
ISO制定:CMIS/CMIP公共管理信息服务规范。
基于TCP/IP:简单网络管理协议SNMPv1、SNMPv2、SNMPv3。
基于局域网:远程监控网络RMON-1和RMON-2。
IEEE制定:基于物理层和数据链路层CMOL。
ITU-T:电信网络管理标准TMN。
例题：

RMON


SNMP和RMON区别

例题：

SNMP
SNMP-2个服务2个端口5个报文（225）


SNMP协议的操作-5个报文


例题：






SNMP版本对比




例题：




管理数据库MIB-2

例题

网络管理命令
网络诊断命令 ipconfig


故障诊断 ping traceroute

ARP命令

Netstat


route和nslookup

例题：



网络故障排除工具
网络故障排查工具


display显示认证失败原因




双绞线测试工具
多功能网络寻线仪/测线仪


光纤测试工具-红光笔

光纤熔接机


光纤测试工具-光功率计


光模块功率
华三40G光模块输出光功率/接收光功率


光纤测试工具-光时域反射仪OTDR


光功率计和光时域反射仪OTDR原理对比

例题：






网络故障诊断与排查



网络规划和设计
综合布线



注意线缆长度的限制






例题：
















网络分析与设计
网络规划设计模型



网络设计的约束因素


网络流量分析


例题：







技术评价

例题：

网络结构与功能
局域网结构类型

单核心局域网特点

双核心局域网特点

环形局域网结构

环形局域网结构特点

层次化网络设计/三层组网架构

层次局域网结构特点

三层组网架构

例题：

层次化网络模型优点

层次化网络模型原则

例题：





网络冗余设计


例题：


广域网接入技术

接入和终结设备

PON接入

其他接入技术
关键词：MSTP

例题：






存储与RAID技术专题
存储基础
存储系统层次结构
存储体系结构如下图，分为寄存器、高速缓冲存储器、主存储器和外存储器

存储介质

HDD机械硬盘与SSD固态硬盘

例题：








传统RAID技术
RAID定义
RAID(Redundant Array of Independent Disks)即独立磁盘冗余阵列，RAID技术将多个单独的物理硬盘以不同的方式组合成一个逻辑硬盘，从而提高了硬盘的读写性能和数据安全性。

RAID数据组织及存取方式

RAID热备与重构

RAID基本概念-逻辑卷

常用RAID技术-RAID O和RAID 1

常用RAID技术-RAID 3和RAID 5

常用RAID技术-RAID 6

RAID组合-RAID 10和RAID 50

常用RAID技术对比

例题：



RAID2.0技术
RAID故障自检自愈，保证系统可靠性

RAID2.0技术优势


网络存储体系DAS/NAS/SAN

DAS起源

FC SAN起源（由DAS到FC SAN）

IP SAN起源(由FC SAN到IP SAN)

NAS起源(网络数据共享与交换需求)

几种存储对比


例题：


分布式存储/超融合

分布式存储组成与特点

数据备份策略

例题：

数据备份网络架构

数据备份网络架构:Server-Free

例题：








二层协议专题
HDLC
HDLC高级数据链路控制


HDLC帧结构
HDLC帧格式及控制字段


例题：








PPP/L2TP/PPTP
7.6.1 虚拟专用网基础
以太网帧格式
4.5以太网帧
VLAN扩展技术
端口隔离技术

技术背景
vlan聚合

Super-VLAN/Aggregate-vlan



QinQ
由于VLAN的VID范围是[0,4095](212=4096),而QinQ技术要打上两层VLAN所以他的范围是4096*4096，224位可以表示出1600万个网络

基本QinQ
基于端口，比如说经过G0/0/1的端口的所有流量我都给你打上100的标签
缺点：无法细化流量，太简单粗暴

灵活QinQ
对经过这个端口的流量，假设你是10标签的流量就给打上100的标签，假设你是20标签的流量，就给你打上两百的标签
也可以基于很多方式进行分类看下图红色注释

QinQ在园区网络中的应用

QinQ配置举例-基本QinQ

QinQ配置举例-灵活QinQ

VxLan
传统网络面临的问题

VxLAN报文格式

例题：
VNI用来表示不同用户的字段。224位可以表示出1600万个网络















三层封装协议专题
ipv4和ipv6
6.2 IPV4
6.3 IPV6



6.2.2 ip分片与计算


6.2.2 ip分片与计算

ARP
6.4 ARP与RARP协议
ARP泛洪攻击




ARP欺骗






传输层封装协议TCP和UDP
5.7 TCP和UDP协议






路由协议基础RIP OSPF BGP ISIS
5.8 路由协议



路由综合配置专题
园区路由规划






动态路由配置








路由引入和路由策略





应用层协议DHCP SNMP DNS
5.7.4 重点协议端口号总结
dhcp
7.8 DHCP







snmp
9.2 SNMP
DNS
7.4 DNS
ip子网划分专题
[5.11 子网划分专题VLSM  CIDR](#子网划分专题VLSM  CIDR)
计算专题突破
补充知识的进制转换
补充知识的内存数量计算
3.3 二进制指数退避算法
2.1.4 奈奎斯特定理
2.1.5 香农定理
2.6 PCM
2.4 数据编码
2.2 信道延迟
2.10 CRC循环冗余校验码
计算机组成原理
进制转换
十进制：

十进制是Decimal，简写为D
以 0b 或 0B 作为前缀
都是以0-9这九个数字组成。
如:12,34,345等数字

二进制：

二进制是Binary，简写为B
由0和1两个数字组成。
10001等
二进制权表展开如下




27
26
25
24
23
22
21
20




128
64
32
16
8
4
2
1



八进制：

八进制是Octal，简写为O
由0-7数字组成，为了区分与其他进制的数字区别，开头都是以0开始。
如八进制0127

十六进制：

十六进制为Hexadecimal，简写为H
表示方式为0x或0X开头
计数到F后，再增加1个，就进位。
由0-9和A-F组成，英文字母A，B，C，D，E，F分别表示数字10～15。
如十六进制0xA87




1
2
3
4
5
6
7
8
9
A
B
C
D
E
F




1
2
3
4
5
6
7
8
9
10
11
12
13
14
15



整数转换
十进制转二进制

十进制转二进制的原理：十进制数除以2，余数为权位上的数，得到商继续除以2，直到商为0终止，然后反向取余数。

例如：(87)10转（1010111）2




十进制转八、十六进制的原理：跟十转二原理一样，十进制数除以8或16，余数为权位上的数，得到商继续除以8或16，直到商为0终止，然后反向取余数。

例如：(763)10转（1373）8或(2FB)16



二进制转换十进制
方法：把二进制数按权展开、相加即得十进制数。
如（1001）2转（9）10
1001从右往左表示如下



1
0
0
1




23
22
21
20


8
4
2
1



也就是  1*23+0*22+0*21+1*20=9
如(17)8转(15)10  展开2*81+7*80=15
如(1AB)16转(427)10  A对应10，B对应11  展开式 1*162+A*161+B*160=16*16+10*16+11*1=427
二进制转八进制、十六进制
如(10101110)2转(256)8

如(1110101110)2转(256)16

八进制和十六进制要转二进制倒推即可
小数转换
十进制转二进制
方法：十进制小数转换成二进制小数采用“乘2取整，顺序输出”
（0.125）10转(0.001）2
0.125*2=0.25  取0   向
0.25*2=0.5    取0   下
0.5*2=1       取1   取
也就是（0.001）2
二进制转十进制
(0.001）2转（0.125）10
0*($\frac{1}{2}$​)      即乘以2的负一次方
0*($\frac{1}{4}$)      即乘以2的负二次方
1*($\frac{1}{8}$)      即乘以2的负三次方
($\frac{1}{2}$)+($\frac{1}{4}$)+($\frac{1}{8}$)=0.125
二进制转换成十六进制
(0.101）2转（0.125）16
0.101  补4合1  小数部分从左往右补4位，缺的补0
0.1010   然后从右往左补（十六进制1248秘诀）
8421   相加得10且对应的是A
最后结果是(0.A)16
进制转换应用

例题：

带宽与存储计算
位(bit):网络数据传输的最小单位。
字节(byte):由8个bit组成，存储空间的最小单位。1byte=1B=8bit=8b
时间领域：1s = 103ms = 106us = 109ns
存储领域：1K=1024,即1KB=1024B,1MB=1024KB=1024 ✕ 1024B=210 ✕ 210B=220B
网络领域：1k=1000,即1kb/s=1000b/s,1Mb/s=1000kb/s=106b/s
​         1Gb = 103Mb = 106kb = 109b
其他单位:1G=1024M，1T=1024G，1P=1024T(存储单位是B字节)
例题：




计算机硬件架构
计算机硬件系统是冯诺依曼设计的体系结构，由运算器、控制器、存储器、输入/输出设备(I/O)五大部件组成,运算器和控制器组成中央处理器(CPU)






指令集RISC和CISC




存储分类与应用
存储器分类
计算机存储器分为:寄存器、Cache(高速缓冲存储器)、主存储器(内存)、辅助存储器( 硬盘)。
从下往上，速度越来越快，容量越来越小，成本越来越高,











存储芯片数量计算
内存芯片数量计算




输入输出I/O






原码，反码和补码
计算机只识别和处理二进制，而对于数据又分为有符号数据和无符号数据
有符号数据是指有正负之分的数据(如-10或+8)，无符号数据是没有正负之分的，如ASCI字符。
而原码，反码，补码是计算机存储有符号数据的编码方式。
在计算机中用一个数的最高位表示符号位，正数为0，负数为1。




正数
负数




原码
最高位添加符号位0
最高位添加符号位1


反码
最高位添加符号位0
保持其原码符号位不变，其余数值按位取反


补码
最高位添加符号位0
先保持器原码符号位不变，数值按位取反，然后再加1



一般八位二进制数




正数
数值
负数
数值




原码
00001001
9
10001001
-9


反码
00001001
9
11110110
-9


补码
00001001
9
11110111
-9



若某证书的16位补码为FFFFH（H表示十六进制），则该数的十进制的值位（）
F    F     F     F
展开补码如下
1111  1111  1111  1111
符号位不变，其余按位取反
1000  0000  0000  0000
最后一位加一
1000  0000  0000  0001
结论：对于负数来说：补码求原码就是补码按位取反然后最后一位加1
逻辑运算
逻辑运算只有两个布尔值：

0 ，表示假值（False）。
1 ，表示真值（True）。




逻辑运算
运算规则
常见运算符




与
有 0 为 0，全 1 为 1（有一个假全假）
“×”、“·”、“∧”


或
有 1 为 1，全 0 为 0（有一个真全真）
“＋”、“∨”


非
1 为 0，0 为 1（非黑即白，相反）
“¬”、“!”、“—”


同或
相同为 1，不同为 0
“⊙”


异或
不同为 1，相同为 0
“⊕”




例题：X,Y都是逻辑变量，与逻辑表达式X+!XY等价的是（D）
A.X+!Y   B.!X+!Y  C.!XX+Y  D.X+Y
解：可以假设x=0,y=0,以及x=1,y=1等如下




x=0,y=0
x=0,y=1
x=1,y=0
x=1,y=1




X+!XY
0+1*0=0
1
1
1


A.X+!Y
1
0
1
1


B.!X+!Y
1
1
1
0


C.!XX+Y
1
1
0
1


D.X+Y
0
1
1
1



操作系统
操作系统基础
操作系统5大功能进程管理、存储管理、文件管理、设备管理、作业管理。
PC和服务器端操作系统分类:Windows、Unix、Linux(开源)
国产操作系统:UOS、麒麟、中科方德、深度、红旗。【都是基于Linux开发)】
移动操作系统:IOS(苹果)、安卓(Android)和鸿蒙(HarmonyOs)





进程和线程

区别总结

图类对比





资源互斥
进程死锁


位示图计算



管理方法
说明
特点




位图法
用一个向量描述整个磁盘，每一位表示一个物理块的状态
易于寻找空闲块，适合各种文件分配法，本身小，可放于主存


链接法
使用链表将空闲块组织起来
适合各种文件分配法


索引法
将空闲块作为一个文件，并采用索引技术
适合各种文件分配法



位示图




0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15




0
1
1
0
0
0
1
1
1
0
1
0
1
1
1
1
0


1
0
0
0
1
1
1
1
1
1
0
0
0
0
1
1
1


2
1
1
1
1
0
0
0
1
1
1
1
1
0
0
0
0


3


















…


















15



















我们的磁盘空间都划分了很多个块，每个块都有一个标识，要么为0要么为1，若为1则表示已经被使用，反之未使用



内存页面转换




文件目录
Windows C:\windows\system32\test.txt 以某个盘符为根。
Linux: /etc/test.txt 只有一个根目录。
绝对路径:完整访问路径。
相对路径:从当今路径出发的访问路径，调用速度快。


软件工程
软件工程基础
软件危机:随着软件复杂度提升，开发成本、软件质量、生产效率等问题。
软件工程:为了消除软件危机，以工程学思路进行软件开发，即软件工程。
软件工程三个要素：
方法:完成软件工程项目的技术手段。
工具:支持软件的开发、管理、文档生成。
过程:支持软件开发的各个环节控制、管理。

信息系统生命周期模型


软件生命周期



软件开发模型
典型的软件生命周期模型
·1.瀑布模型
·2.螺旋模型
·3.迭代模型
·4.V模型
·5.原型化模型
·6.敏捷方法
瀑布模型

瀑布模型的优缺点


螺旋模型


迭代模型

增量模型

V模型

原型化模型

敏捷开发模型

不同软件生命周期模型优缺点对比




软件开发方法
信息系统开发方法

面向对象的开发方法


原型化方法


面向服务的方法

开发方法对比（掌握）





软件开发语言





软件测试













标准化知识产权
标准化知识
我国标准的级别






知识产权
知识产权保护相关法律



知识产权保护期限



知识产权归属

侵权判断





 特别声明
千屹博客旗下的所有文章，是通过本人课堂学习和课外自学所精心整理的知识巨著难免会有出错的地方如果细心的你发现了小失误，可以在下方评论区告诉我，或者私信我！非常感谢大家的热烈支持！

]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
</search>
